{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "No1PqmvcBc2P",
    "outputId": "52f6f292-fdb0-426a-aba7-aba54ad07163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5yjM_aPCJ7x"
   },
   "outputs": [],
   "source": [
    "# !mkdir ./drive/My\\ Drive/Git/convolution_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WoAPPgPTBmAe"
   },
   "outputs": [],
   "source": [
    "# !git clone --branch convolution_transformation https://korney3:iwanttobeahero1@github.com/korney3/Tox21-MultiTox.git ./drive/My\\ Drive/Git/convolution_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLONEvZ8CPh5"
   },
   "outputs": [],
   "source": [
    "# !cd ./drive/My\\ Drive/Git/; git fetch convolution_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "dqekWD7hAPvO",
    "outputId": "7ab711d2-8488-4ba8-e339-bce574d7dfc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[convolution_transformation 5d2ee03] MultiTox changes\n",
      " 36 files changed, 51 insertions(+), 27 deletions(-)\n",
      " create mode 100644 MultiTox/__pycache__/Model_train_test_regression.cpython-36.pyc\n",
      " create mode 100644 MultiTox/__pycache__/dataloaders_sigma.cpython-36.pyc\n",
      " rewrite MultiTox/__pycache__/load_data_multitox.cpython-36.pyc (81%)\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/3_parameters.json\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1575987293.0fda15d4a3f7\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1575987431.abe2a34cffa3\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1575988011.abe2a34cffa3\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1575988036.abe2a34cffa3\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576021539.2c0a13241436\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576057859.4e6bfb14c5d8\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576060863.4e6bfb14c5d8\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576062976.f2a839967a75\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576063074.f2a839967a75\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576085798.b61376d0ec69\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576086509.b61376d0ec69\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576090221.b61376d0ec69\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576091364.b61376d0ec69\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576092433.8c8b09dbf267\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576116683.8c8b09dbf267\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576149915.c61153a64857\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_4/4_parameters.json\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576063136.34ab70d2b7fc\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576064281.34ab70d2b7fc\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576064685.34ab70d2b7fc\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576082565.1a361aef7254\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576083998.34ab70d2b7fc\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576085813.45e191a0b0bb\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576086478.45e191a0b0bb\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576116687.45e191a0b0bb\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576149926.6ea7762b45bb\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_5/5_parameters.json\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_5/events.out.tfevents.1576082750.1a361aef7254\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_5/events.out.tfevents.1576083472.1a361aef7254\n",
      " create mode 100644 MultiTox/logs_sigma_right/exp_6/events.out.tfevents.1576187293.a37471223511\n",
      "Counting objects: 20, done.\n",
      "Delta compression using up to 2 threads.\n",
      "Compressing objects: 100% (20/20), done.\n",
      "Writing objects: 100% (20/20), 95.00 KiB | 3.80 MiB/s, done.\n",
      "Total 20 (delta 6), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (6/6), completed with 5 local objects.\u001b[K\n",
      "To https://github.com/korney3/Tox21-MultiTox.git\n",
      "   d4a2fe4..5d2ee03  convolution_transformation -> convolution_transformation\n"
     ]
    }
   ],
   "source": [
    "# !cd /content/drive/My\\ Drive/Git/convolution_transformation/; git config --global user.name \"korney3\";git config --global user.email \"koren.iz3x@yandex.ru\"; git add ./MultiTox; git commit -m 'MultiTox changes'; git push origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "kwSGfSgZEtcn",
    "outputId": "4168a0b1-7427-48c2-bc9a-a02fe4c4d819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 786kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /trinity/shared/opt/python-3.7.1/lib/python3.7/site-packages (from tensorboardX) (1.11.0)\n",
      "Collecting protobuf>=3.8.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/14/f5c294f1e36a031f165128c25feba93b3116f15a74398d0b2747ed75744f/protobuf-3.11.2-cp37-cp37m-manylinux1_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 83.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /trinity/shared/opt/python-3.7.1/lib/python3.7/site-packages (from tensorboardX) (1.15.4)\n",
      "Requirement already satisfied: setuptools in /trinity/shared/opt/python-3.7.1/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorboardX) (39.0.1)\n",
      "Installing collected packages: protobuf, tensorboardX\n",
      "Successfully installed protobuf-3.11.2 tensorboardX-1.9\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorboardX --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-MFAwhXpEnGL"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(1, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/96/8034e350d4550748277e514d0d6d91bdd36be19e6c5f40b8af0d74cb0c84/scikit_learn-0.22-cp37-cp37m-manylinux1_x86_64.whl (7.0MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0MB 882kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /trinity/shared/opt/python-3.7.1/lib/python3.7/site-packages (from scikit-learn) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /trinity/shared/opt/python-3.7.1/lib/python3.7/site-packages (from scikit-learn) (1.1.0)\n",
      "Collecting joblib>=0.11\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 79.3MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib, scikit-learn\n",
      "Successfully installed joblib-0.14.1 scikit-learn-0.22\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSBi4CNFDv7f"
   },
   "outputs": [],
   "source": [
    "path=\"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GefQbgEJEun3"
   },
   "outputs": [],
   "source": [
    "import load_data_multitox as ld\n",
    "import dataloaders_sigma as dl\n",
    "from Model_train_test_regression import Net, EarlyStopping, train, test\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils import data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "import sys \n",
    "import os\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8Y7_h3fHVKj"
   },
   "outputs": [],
   "source": [
    "# number of conformers created for every molecule\n",
    "NUM_CONFS = 100\n",
    "\n",
    "# amount of chemical elements taking into account\n",
    "AMOUNT_OF_ELEM = 9\n",
    "\n",
    "# amount of target values\n",
    "TARGET_NUM = 29\n",
    "\n",
    "#dataset folder\n",
    "# DATASET_PATH=\"~/Tox21-MultiTox/MultiTox\"\n",
    "DATASET_PATH=os.path.join(path)\n",
    "\n",
    "#logs path\n",
    "\n",
    "LOG_PATH=os.path.join(path,\"logs_sigma_right\")\n",
    "\n",
    "\n",
    "#models path\n",
    "MODEL_PATH=os.path.join(path,\"models_sigma_right\")\n",
    "\n",
    "EXPERIMENT_NUM=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9aTDKD9IYEe"
   },
   "outputs": [],
   "source": [
    "dir_path = os.path.join(LOG_PATH,'exp_'+str(EXPERIMENT_NUM))\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "LOG_PATH = dir_path\n",
    "dir_path = os.path.join(MODEL_PATH,'exp_'+str(EXPERIMENT_NUM))\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "MODEL_PATH = dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args={}\n",
    "# args['EPOCHS_NUM']=100\n",
    "# args['PATIENCE']=10\n",
    "# args['SIGMA']=1.3\n",
    "# args['BATCH_SIZE']=20\n",
    "# args['TRANSF']='g'\n",
    "# args['NUM_EXP']=EXPERIMENT_NUM\n",
    "# args['VOXEL_DIM']=50\n",
    "# args['LEARN_RATE']=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(LOG_PATH,str(EXPERIMENT_NUM)+'_parameters.json'),'w') as f:\n",
    "#     json.dump(args,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyNgROulIfVv"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(LOG_PATH,'exp_'+str(EXPERIMENT_NUM),str(EXPERIMENT_NUM)+'_parameters.json'),'r') as f:\n",
    "    args = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EPOCHS_NUM': 100,\n",
       " 'PATIENCE': 10,\n",
       " 'SIGMA': 1.3,\n",
       " 'BATCH_SIZE': 20,\n",
       " 'TRANSF': 'g',\n",
       " 'NUM_EXP': 6,\n",
       " 'VOXEL_DIM': 50,\n",
       " 'LEARN_RATE': 0.0001}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "_9IEkMR9JaR8",
    "outputId": "4ce16f53-0242-4f81-dcb1-d08e8273ac74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Tesla M60\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "Start loading dataset...\n"
     ]
    }
   ],
   "source": [
    "writer=SummaryWriter(LOG_PATH)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "print('Start loading dataset...')\n",
    "# get dataset without duplicates from csv\n",
    "data = pd.read_csv(os.path.join(path,'database','MultiTox.csv'))\n",
    "props = list(data)[1:]\n",
    "scaler = MinMaxScaler() #StandardScaler()\n",
    "data[props]=scaler.fit_transform(data[props])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XcvKWBkzJW-r"
   },
   "outputs": [],
   "source": [
    "elements={'N':0,'C':1,'Cl':2,'I':3,'Br':4,'F':5,'O':6,'P':7,'S':8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOj8yothLTyu"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATASET_PATH,'many_elems.json'), 'r') as fp:\n",
    "    conf_calc = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6DV7DsSkK_Rr",
    "outputId": "22006532-0b59-441e-8949-759b8dc316ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset size =  13091\n"
     ]
    }
   ],
   "source": [
    "keys=list(conf_calc.keys())\n",
    "print ('Initial dataset size = ', len(keys))\n",
    "\n",
    "new_conf_calc={}\n",
    "for smiles in conf_calc.keys():\n",
    "    for conf_num in conf_calc[smiles]:\n",
    "        if smiles in new_conf_calc.keys():\n",
    "            new_conf_calc[smiles][int(conf_num)]=conf_calc[smiles][conf_num]\n",
    "        else:\n",
    "            new_conf_calc[smiles]={}\n",
    "            new_conf_calc[smiles][int(conf_num)]=conf_calc[smiles][conf_num]\n",
    "\n",
    "conf_calc=new_conf_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SD70TRGgLdNR"
   },
   "outputs": [],
   "source": [
    "elems = []\n",
    "for key in keys:\n",
    "    conformers=list(conf_calc[key].keys())\n",
    "    for conformer in conformers:\n",
    "        try:\n",
    "            energy = conf_calc[key][conformer]['energy']\n",
    "            elems = list(set(elems+list(conf_calc[key][conformer]['coordinates'].keys())))\n",
    "        except:\n",
    "            del conf_calc[key][conformer]\n",
    "    if set(conf_calc[key].keys())!=set(range(100)):\n",
    "          del conf_calc[key]\n",
    "    elif conf_calc[key]=={}:\n",
    "        del conf_calc[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bKTYBgA8Lgqo",
    "outputId": "fa32c278-d59e-4342-bb8e-1bbf2add8126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processed dataset size =  13084\n"
     ]
    }
   ],
   "source": [
    "print ('Post-processed dataset size = ', len(list(conf_calc.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjuF5DP2Lv_h"
   },
   "outputs": [],
   "source": [
    "indexing, label_dict = ld.indexing_label_dict(data, conf_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEF2jRMYLhKf"
   },
   "outputs": [],
   "source": [
    "train_indexes, test_indexes, _, _ = train_test_split(np.arange(0, len(conf_calc.keys())),\n",
    "                                                         np.arange(0, len(conf_calc.keys())), test_size=0.2,\n",
    "                                                         random_state=115)\n",
    "train_indexes,val_indexes, _, _ = train_test_split(train_indexes,\n",
    "                                                   train_indexes, test_size=0.5,\n",
    "                                                   random_state=115)\n",
    "train_set = dl.Cube_dataset(conf_calc, label_dict, elements, indexing, train_indexes, dim = args['VOXEL_DIM'])\n",
    "train_generator = td.DataLoader(train_set, batch_size=args['BATCH_SIZE'], shuffle=True)\n",
    "\n",
    "test_set = dl.Cube_dataset(conf_calc, label_dict, elements, indexing, test_indexes, dim = args['VOXEL_DIM'])\n",
    "test_generator = td.DataLoader(test_set, batch_size=args['BATCH_SIZE'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "MEgUWzWULnsy",
    "outputId": "0ed344ef-f8c0-4624-f68c-954291abf904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma <class 'torch.Tensor'> torch.Size([9])\n",
      "conv1.weight <class 'torch.Tensor'> torch.Size([32, 9, 3, 3, 3])\n",
      "conv1.bias <class 'torch.Tensor'> torch.Size([32])\n",
      "conv2.weight <class 'torch.Tensor'> torch.Size([64, 32, 3, 3, 3])\n",
      "conv2.bias <class 'torch.Tensor'> torch.Size([64])\n",
      "conv3.weight <class 'torch.Tensor'> torch.Size([128, 64, 3, 3, 3])\n",
      "conv3.bias <class 'torch.Tensor'> torch.Size([128])\n",
      "conv4.weight <class 'torch.Tensor'> torch.Size([256, 128, 3, 3, 3])\n",
      "conv4.bias <class 'torch.Tensor'> torch.Size([256])\n",
      "fc1.weight <class 'torch.Tensor'> torch.Size([128, 256])\n",
      "fc1.bias <class 'torch.Tensor'> torch.Size([128])\n",
      "fc2.weight <class 'torch.Tensor'> torch.Size([29, 128])\n",
      "fc2.bias <class 'torch.Tensor'> torch.Size([29])\n"
     ]
    }
   ],
   "source": [
    "model = Net(dim=args['VOXEL_DIM'], num_elems=AMOUNT_OF_ELEM, num_targets=TARGET_NUM, elements=elements, transformation=args['TRANSF'],device=device,sigma_0 = args['SIGMA'],sigma_trainable = True)\n",
    "model=model.to(device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, type(param.data), param.size())\n",
    "# set optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['LEARN_RATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5KaX3eU5LpuB"
   },
   "outputs": [],
   "source": [
    "f_train_loss=open(os.path.join(LOG_PATH,str(args['NUM_EXP'])+'_log_train_loss.txt'),'w')\n",
    "f_train_loss_ch=open(os.path.join(LOG_PATH,str(args['NUM_EXP'])+'_log_train_loss_channels.txt'),'w')\n",
    "f_test_loss=open(os.path.join(LOG_PATH,str(args['NUM_EXP'])+'_log_test_loss.txt'),'w')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=args['PATIENCE'], verbose=True,model_path=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9QGnFPJ9NSsQ",
    "outputId": "636654a2-c5cb-43c8-f19d-cb7fd74fc3c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/5233 (0%)]\tLoss: 0.146966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/a.alenicheva/thesis/MultiTox/Model_train_test_regression.py:350: RuntimeWarning: invalid value encountered in true_divide\n",
      "  losses/=num_losses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [20/5233 (0%)]\tLoss: 0.160280\n",
      "Train Epoch: 1 [40/5233 (1%)]\tLoss: 0.117230\n",
      "Train Epoch: 1 [60/5233 (1%)]\tLoss: 0.086641\n",
      "Train Epoch: 1 [80/5233 (2%)]\tLoss: 0.122674\n",
      "Train Epoch: 1 [100/5233 (2%)]\tLoss: 0.084315\n",
      "Train Epoch: 1 [120/5233 (2%)]\tLoss: 0.106373\n",
      "Train Epoch: 1 [140/5233 (3%)]\tLoss: 0.076305\n",
      "Train Epoch: 1 [160/5233 (3%)]\tLoss: 0.074599\n",
      "Train Epoch: 1 [180/5233 (3%)]\tLoss: 0.054803\n",
      "Train Epoch: 1 [200/5233 (4%)]\tLoss: 0.062628\n",
      "Train Epoch: 1 [220/5233 (4%)]\tLoss: 0.019787\n",
      "Train Epoch: 1 [240/5233 (5%)]\tLoss: 0.023306\n",
      "Train Epoch: 1 [260/5233 (5%)]\tLoss: 0.042678\n",
      "Train Epoch: 1 [280/5233 (5%)]\tLoss: 0.041739\n",
      "Train Epoch: 1 [300/5233 (6%)]\tLoss: 0.105684\n",
      "Train Epoch: 1 [320/5233 (6%)]\tLoss: 0.038466\n",
      "Train Epoch: 1 [340/5233 (6%)]\tLoss: 0.035533\n",
      "Train Epoch: 1 [360/5233 (7%)]\tLoss: 0.038820\n",
      "Train Epoch: 1 [380/5233 (7%)]\tLoss: 0.051620\n",
      "Train Epoch: 1 [400/5233 (8%)]\tLoss: 0.042458\n",
      "Train Epoch: 1 [420/5233 (8%)]\tLoss: 0.062655\n",
      "Train Epoch: 1 [440/5233 (8%)]\tLoss: 0.025820\n",
      "Train Epoch: 1 [460/5233 (9%)]\tLoss: 0.047848\n",
      "Train Epoch: 1 [480/5233 (9%)]\tLoss: 0.033718\n",
      "Train Epoch: 1 [500/5233 (10%)]\tLoss: 0.035939\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "# train procedure\n",
    "for epoch in range(1, args['EPOCHS_NUM'] + 1):\n",
    "    try:\n",
    "        train(model, optimizer, train_generator, epoch,device,writer=writer,f_loss=f_train_loss,f_loss_ch=f_train_loss_ch, elements=elements,batch_size = args['BATCH_SIZE'])\n",
    "        test_loss = test(model, test_generator,epoch, device,writer=writer,f_loss=f_test_loss, elements=elements,batch_size = args['BATCH_SIZE'])\n",
    "        early_stopping(test_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(epoch,\"Early stopping\")\n",
    "            break\n",
    "        if epoch%1==0:\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_PATH, args['NUM_EXP']+'_model_'+str(epoch)))\n",
    "    except KeyError:\n",
    "        print(epoch,'Key Error problem')\n",
    "    \n",
    "model.load_state_dict(torch.load(os.path.join(MODEL_PATH,'checkpoint.pt')))\n",
    "torch.save(model.state_dict(), os.path.join(MODEL_PATH, args['NUM_EXP']+'_model'+str(epoch)+'_fin'))\n",
    "f_train_loss.close()\n",
    "f_test_loss.close()\n",
    "writer.close()\n",
    "print('Training has finished in ',round((time.time()-start_time)/60,3),' min.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train_new_MultiTox_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
