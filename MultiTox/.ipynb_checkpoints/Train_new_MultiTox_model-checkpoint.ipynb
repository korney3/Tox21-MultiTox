{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "No1PqmvcBc2P",
    "outputId": "52f6f292-fdb0-426a-aba7-aba54ad07163"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5yjM_aPCJ7x"
   },
   "outputs": [],
   "source": [
    "# !mkdir ./drive/My\\ Drive/Git/convolution_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WoAPPgPTBmAe"
   },
   "outputs": [],
   "source": [
    "# !git clone --branch convolution_transformation https://korney3:iwanttobeahero1@github.com/korney3/Tox21-MultiTox.git ./drive/My\\ Drive/Git/convolution_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLONEvZ8CPh5"
   },
   "outputs": [],
   "source": [
    "# !cd ./drive/My\\ Drive/Git/; git fetch convolution_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSBi4CNFDv7f"
   },
   "outputs": [],
   "source": [
    "path=\"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "dqekWD7hAPvO",
    "outputId": "7ab711d2-8488-4ba8-e339-bce574d7dfc8"
   },
   "outputs": [],
   "source": [
    "# !cd /content/drive/My\\ Drive/Git/convolution_transformation/; git config --global user.name \"korney3\";git config --global user.email \"koren.iz3x@yandex.ru\"; git add ./MultiTox; git commit -m 'MultiTox changes'; git push origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYpkKQkbBV3M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-MFAwhXpEnGL"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "kwSGfSgZEtcn",
    "outputId": "4168a0b1-7427-48c2-bc9a-a02fe4c4d819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
      "\r",
      "\u001b[K     |█▊                              | 10kB 31.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 20kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 30kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 40kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 51kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 61kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 71kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 81kB 4.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 92kB 4.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 102kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 112kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 122kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 133kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 143kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 153kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 163kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 174kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 184kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 194kB 3.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (42.0.2)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GefQbgEJEun3"
   },
   "outputs": [],
   "source": [
    "import load_data_multitox as ld\n",
    "import dataloaders_sigma as dl\n",
    "from Model_train_test_regression import Net, EarlyStopping, train, test\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils import data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "# from torchsummary import summary\n",
    "\n",
    "import sys \n",
    "import os\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8Y7_h3fHVKj"
   },
   "outputs": [],
   "source": [
    "# number of conformers created for every molecule\n",
    "NUM_CONFS = 100\n",
    "\n",
    "# amount of chemical elements taking into account\n",
    "AMOUNT_OF_ELEM = 9\n",
    "\n",
    "# amount of target values\n",
    "TARGET_NUM = 29\n",
    "\n",
    "#dataset folder\n",
    "# DATASET_PATH=\"~/Tox21-MultiTox/MultiTox\"\n",
    "DATASET_PATH=os.path.join(path)\n",
    "\n",
    "#logs path\n",
    "\n",
    "LOG_PATH=os.path.join(path,\"logs_sigma_right\")\n",
    "\n",
    "\n",
    "#models path\n",
    "MODEL_PATH=os.path.join(path,\"models_sigma_right\")\n",
    "\n",
    "EXPERIMENT_NUM=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9aTDKD9IYEe"
   },
   "outputs": [],
   "source": [
    "dir_path = os.path.join(LOG_PATH,'exp_'+str(EXPERIMENT_NUM))\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "LOG_PATH = dir_path\n",
    "dir_path = os.path.join(MODEL_PATH,'exp_'+str(EXPERIMENT_NUM))\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "MODEL_PATH = dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyNgROulIfVv"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(path,\"logs_sigma_right\",'exp_'+str(3),str(3)+'_parameters.json'),'r') as f:\n",
    "  args = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EPOCHS_NUM': 100,\n",
       " 'PATIENCE': 25,\n",
       " 'SIGMA': 1.3,\n",
       " 'BATCH_SIZE': 32,\n",
       " 'TRANSF': 'g',\n",
       " 'NUM_EXP': '3',\n",
       " 'VOXEL_DIM': 50,\n",
       " 'LEARN_RATE': 0.0001}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "_9IEkMR9JaR8",
    "outputId": "4ce16f53-0242-4f81-dcb1-d08e8273ac74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Tesla M60\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "Start loading dataset...\n"
     ]
    }
   ],
   "source": [
    "writer=SummaryWriter(LOG_PATH)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "print('Start loading dataset...')\n",
    "# get dataset without duplicates from csv\n",
    "data = pd.read_csv(os.path.join(DATASET_PATH,'database','MultiTox.csv'))\n",
    "props = list(data)[1:]\n",
    "scaler = MinMaxScaler() #StandardScaler()\n",
    "data[props]=scaler.fit_transform(data[props])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XcvKWBkzJW-r"
   },
   "outputs": [],
   "source": [
    "elements={'N':0,'C':1,'Cl':2,'I':3,'Br':4,'F':5,'O':6,'P':7,'S':8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOj8yothLTyu"
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f2b452e359ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'many_elems.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mconf_calc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/trinity/shared/opt/python-3.7.1/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/trinity/shared/opt/python-3.7.1/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/trinity/shared/opt/python-3.7.1/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/trinity/shared/opt/python-3.7.1/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(DATASET_PATH,'many_elems.json'), 'r') as fp:\n",
    "    conf_calc = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6DV7DsSkK_Rr",
    "outputId": "22006532-0b59-441e-8949-759b8dc316ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset size =  13091\n"
     ]
    }
   ],
   "source": [
    "keys=list(conf_calc.keys())\n",
    "print ('Initial dataset size = ', len(keys))\n",
    "\n",
    "new_conf_calc={}\n",
    "for smiles in conf_calc.keys():\n",
    "  for conf_num in conf_calc[smiles]:\n",
    "    if smiles in new_conf_calc.keys():\n",
    "      new_conf_calc[smiles][int(conf_num)]=conf_calc[smiles][conf_num]\n",
    "    else:\n",
    "      new_conf_calc[smiles]={}\n",
    "      new_conf_calc[smiles][int(conf_num)]=conf_calc[smiles][conf_num]\n",
    "\n",
    "conf_calc=new_conf_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SD70TRGgLdNR"
   },
   "outputs": [],
   "source": [
    "elems = []\n",
    "for key in keys:\n",
    "    conformers=list(conf_calc[key].keys())\n",
    "    for conformer in conformers:\n",
    "        try:\n",
    "            energy = conf_calc[key][conformer]['energy']\n",
    "            elems = list(set(elems+list(conf_calc[key][conformer]['coordinates'].keys())))\n",
    "        except:\n",
    "            del conf_calc[key][conformer]\n",
    "    if set(conf_calc[key].keys())!=set(range(100)):\n",
    "          del conf_calc[key]\n",
    "    elif conf_calc[key]=={}:\n",
    "        del conf_calc[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bKTYBgA8Lgqo",
    "outputId": "fa32c278-d59e-4342-bb8e-1bbf2add8126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processed dataset size =  13084\n"
     ]
    }
   ],
   "source": [
    "print ('Post-processed dataset size = ', len(list(conf_calc.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjuF5DP2Lv_h"
   },
   "outputs": [],
   "source": [
    "indexing, label_dict = ld.indexing_label_dict(data, conf_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEF2jRMYLhKf"
   },
   "outputs": [],
   "source": [
    "train_indexes, test_indexes, _, _ = train_test_split(np.arange(0, len(conf_calc.keys())),\n",
    "                                                         np.arange(0, len(conf_calc.keys())), test_size=0.2,\n",
    "                                                         random_state=115)\n",
    "train_indexes,val_indexes, _, _ = train_test_split(train_indexes,\n",
    "                                                   train_indexes, test_size=0.5,\n",
    "                                                   random_state=115)\n",
    "train_set = dl.Cube_dataset(conf_calc, label_dict, elements, indexing, train_indexes, dim = args['VOXEL_DIM'])\n",
    "train_generator = td.DataLoader(train_set, batch_size=args['BATCH_SIZE'], shuffle=True)\n",
    "\n",
    "test_set = dl.Cube_dataset(conf_calc, label_dict, elements, indexing, test_indexes, dim = args['VOXEL_DIM'])\n",
    "test_generator = td.DataLoader(test_set, batch_size=args['BATCH_SIZE'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "MEgUWzWULnsy",
    "outputId": "0ed344ef-f8c0-4624-f68c-954291abf904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma <class 'torch.Tensor'> torch.Size([9])\n",
      "conv1.weight <class 'torch.Tensor'> torch.Size([32, 9, 3, 3, 3])\n",
      "conv1.bias <class 'torch.Tensor'> torch.Size([32])\n",
      "conv2.weight <class 'torch.Tensor'> torch.Size([64, 32, 3, 3, 3])\n",
      "conv2.bias <class 'torch.Tensor'> torch.Size([64])\n",
      "conv3.weight <class 'torch.Tensor'> torch.Size([128, 64, 3, 3, 3])\n",
      "conv3.bias <class 'torch.Tensor'> torch.Size([128])\n",
      "conv4.weight <class 'torch.Tensor'> torch.Size([256, 128, 3, 3, 3])\n",
      "conv4.bias <class 'torch.Tensor'> torch.Size([256])\n",
      "fc1.weight <class 'torch.Tensor'> torch.Size([128, 256])\n",
      "fc1.bias <class 'torch.Tensor'> torch.Size([128])\n",
      "fc2.weight <class 'torch.Tensor'> torch.Size([29, 128])\n",
      "fc2.bias <class 'torch.Tensor'> torch.Size([29])\n"
     ]
    }
   ],
   "source": [
    "model = Net(dim=args['VOXEL_DIM'], num_elems=AMOUNT_OF_ELEM, num_targets=TARGET_NUM, elements=elements, transformation=args['TRANSF'],device=device,sigma_0 = args['SIGMA'],sigma_trainable = True)\n",
    "model=model.to(device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, type(param.data), param.size())\n",
    "# set optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['LEARN_RATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5KaX3eU5LpuB"
   },
   "outputs": [],
   "source": [
    "f_train_loss=open(os.path.join(LOG_PATH,args['NUM_EXP']+'_log_train_loss.txt'),'w')\n",
    "f_train_loss_ch=open(os.path.join(LOG_PATH,args['NUM_EXP']+'_log_train_loss_channels.txt'),'w')\n",
    "f_test_loss=open(os.path.join(LOG_PATH,args['NUM_EXP']+'_log_test_loss.txt'),'w')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=args['PATIENCE'], verbose=True,model_path=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTziWTncQIve"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9QGnFPJ9NSsQ",
    "outputId": "636654a2-c5cb-43c8-f19d-cb7fd74fc3c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/5233 (0%)]\tLoss: 0.123635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Git/convolution_transformation/MultiTox/Model_train_test_regression.py:350: RuntimeWarning: invalid value encountered in true_divide\n",
      "  losses/=num_losses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [20/5233 (0%)]\tLoss: 0.106812\n",
      "Train Epoch: 1 [40/5233 (1%)]\tLoss: 0.109314\n",
      "Train Epoch: 1 [60/5233 (1%)]\tLoss: 0.098603\n",
      "Train Epoch: 1 [80/5233 (2%)]\tLoss: 0.091190\n",
      "Train Epoch: 1 [100/5233 (2%)]\tLoss: 0.102715\n",
      "Train Epoch: 1 [120/5233 (2%)]\tLoss: 0.059024\n",
      "Train Epoch: 1 [140/5233 (3%)]\tLoss: 0.094373\n",
      "Train Epoch: 1 [160/5233 (3%)]\tLoss: 0.107456\n",
      "Train Epoch: 1 [180/5233 (3%)]\tLoss: 0.043547\n",
      "Train Epoch: 1 [200/5233 (4%)]\tLoss: 0.072736\n",
      "Train Epoch: 1 [220/5233 (4%)]\tLoss: 0.051376\n",
      "Train Epoch: 1 [240/5233 (5%)]\tLoss: 0.051248\n",
      "Train Epoch: 1 [260/5233 (5%)]\tLoss: 0.078302\n",
      "Train Epoch: 1 [280/5233 (5%)]\tLoss: 0.067695\n",
      "Train Epoch: 1 [300/5233 (6%)]\tLoss: 0.066674\n",
      "Train Epoch: 1 [320/5233 (6%)]\tLoss: 0.043413\n",
      "Train Epoch: 1 [340/5233 (6%)]\tLoss: 0.027421\n",
      "Train Epoch: 1 [360/5233 (7%)]\tLoss: 0.075306\n",
      "Train Epoch: 1 [380/5233 (7%)]\tLoss: 0.051482\n",
      "Train Epoch: 1 [400/5233 (8%)]\tLoss: 0.030326\n",
      "Train Epoch: 1 [420/5233 (8%)]\tLoss: 0.098317\n",
      "Train Epoch: 1 [440/5233 (8%)]\tLoss: 0.033606\n",
      "Train Epoch: 1 [460/5233 (9%)]\tLoss: 0.033617\n",
      "Train Epoch: 1 [480/5233 (9%)]\tLoss: 0.041877\n",
      "Train Epoch: 1 [500/5233 (10%)]\tLoss: 0.009310\n",
      "Train Epoch: 1 [520/5233 (10%)]\tLoss: 0.028456\n",
      "Train Epoch: 1 [540/5233 (10%)]\tLoss: 0.013173\n",
      "Train Epoch: 1 [560/5233 (11%)]\tLoss: 0.045709\n",
      "Train Epoch: 1 [580/5233 (11%)]\tLoss: 0.016033\n",
      "Train Epoch: 1 [600/5233 (11%)]\tLoss: 0.017701\n",
      "Train Epoch: 1 [620/5233 (12%)]\tLoss: 0.023970\n",
      "Train Epoch: 1 [640/5233 (12%)]\tLoss: 0.033678\n",
      "Train Epoch: 1 [660/5233 (13%)]\tLoss: 0.019071\n",
      "Train Epoch: 1 [680/5233 (13%)]\tLoss: 0.038012\n",
      "Train Epoch: 1 [700/5233 (13%)]\tLoss: 0.049149\n",
      "Train Epoch: 1 [720/5233 (14%)]\tLoss: 0.009232\n",
      "Train Epoch: 1 [740/5233 (14%)]\tLoss: 0.014845\n",
      "Train Epoch: 1 [760/5233 (15%)]\tLoss: 0.022229\n",
      "Train Epoch: 1 [780/5233 (15%)]\tLoss: 0.020554\n",
      "Train Epoch: 1 [800/5233 (15%)]\tLoss: 0.035334\n",
      "Train Epoch: 1 [820/5233 (16%)]\tLoss: 0.014028\n",
      "Train Epoch: 1 [840/5233 (16%)]\tLoss: 0.076712\n",
      "Train Epoch: 1 [860/5233 (16%)]\tLoss: 0.017658\n",
      "Train Epoch: 1 [880/5233 (17%)]\tLoss: 0.021845\n",
      "Train Epoch: 1 [900/5233 (17%)]\tLoss: 0.031760\n",
      "Train Epoch: 1 [920/5233 (18%)]\tLoss: 0.089457\n",
      "Train Epoch: 1 [940/5233 (18%)]\tLoss: 0.023457\n",
      "Train Epoch: 1 [960/5233 (18%)]\tLoss: 0.017464\n",
      "Train Epoch: 1 [980/5233 (19%)]\tLoss: 0.030788\n",
      "Train Epoch: 1 [1000/5233 (19%)]\tLoss: 0.026615\n",
      "Train Epoch: 1 [1020/5233 (19%)]\tLoss: 0.019710\n",
      "Train Epoch: 1 [1040/5233 (20%)]\tLoss: 0.039768\n",
      "Train Epoch: 1 [1060/5233 (20%)]\tLoss: 0.009166\n",
      "Train Epoch: 1 [1080/5233 (21%)]\tLoss: 0.014809\n",
      "Train Epoch: 1 [1100/5233 (21%)]\tLoss: 0.015910\n",
      "Train Epoch: 1 [1120/5233 (21%)]\tLoss: 0.007170\n",
      "Train Epoch: 1 [1140/5233 (22%)]\tLoss: 0.068186\n",
      "Train Epoch: 1 [1160/5233 (22%)]\tLoss: 0.018237\n",
      "Train Epoch: 1 [1180/5233 (23%)]\tLoss: 0.003163\n",
      "Train Epoch: 1 [1200/5233 (23%)]\tLoss: 0.013925\n",
      "Train Epoch: 1 [1220/5233 (23%)]\tLoss: 0.007808\n",
      "Train Epoch: 1 [1240/5233 (24%)]\tLoss: 0.013644\n",
      "Train Epoch: 1 [1260/5233 (24%)]\tLoss: 0.036595\n",
      "Train Epoch: 1 [1280/5233 (24%)]\tLoss: 0.038096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time=time.time()\n",
    "# train procedure\n",
    "for epoch in range(1, args['EPOCHS_NUM'] + 1):\n",
    "    try:\n",
    "        train(model, optimizer, train_generator, epoch,device,writer=writer,f_loss=f_train_loss,f_loss_ch=f_train_loss_ch, elements=elements,batch_size = args['BATCH_SIZE'])\n",
    "        test_loss = test(model, test_generator,epoch, device,writer=writer,f_loss=f_test_loss, elements=elements,batch_size = args['BATCH_SIZE'])\n",
    "        early_stopping(test_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(epoch,\"Early stopping\")\n",
    "            break\n",
    "        if epoch%1==0:\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_PATH, args['NUM_EXP']+'_model_'+str(epoch)))\n",
    "    except KeyError:\n",
    "        print(epoch,'Key Error problem')\n",
    "    \n",
    "model.load_state_dict(torch.load(os.path.join(MODEL_PATH,'checkpoint.pt')))\n",
    "torch.save(model.state_dict(), os.path.join(MODEL_PATH, args['NUM_EXP']+'_model'+str(epoch)+'_fin'))\n",
    "f_train_loss.close()\n",
    "f_test_loss.close()\n",
    "writer.close()\n",
    "print('Training has finished in ',round((time.time()-start_time)/60,3),' min.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pbZUYbUYOgfm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# writer=SummaryWriter(LOG_PATH)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "print('Start loading dataset...')\n",
    "# get dataset without duplicates from csv\n",
    "# data = pd.read_csv(os.path.join(DATASET_PATH,'database','MultiTox.csv'))\n",
    "# props = list(data)[1:]\n",
    "# scaler = MinMaxScaler() #StandardScaler()\n",
    "# data[props]=scaler.fit_transform(data[props])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6vjAivSSB_gC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train_new_MultiTox_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
