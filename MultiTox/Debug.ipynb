{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data_multitox as ld\n",
    "import dataloaders_sigma as dl\n",
    "from Model_train_test_regression import Net, EarlyStopping, train, test\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils import data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "import sys \n",
    "import os\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler#StandardScaler\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "# number of conformers created for every molecule\n",
    "NUM_CONFS = 100\n",
    "\n",
    "# amount of chemical elements taking into account\n",
    "AMOUNT_OF_ELEM = 9\n",
    "\n",
    "# amount of target values\n",
    "TARGET_NUM = 29\n",
    "\n",
    "#dataset folder\n",
    "# DATASET_PATH=\"~/Tox21-MultiTox/MultiTox\"\n",
    "DATASET_PATH=\"./\"\n",
    "\n",
    "#logs path\n",
    "LOG_PATH=os.path.join(DATASET_PATH,\"logs_sigma_right\")\n",
    "\n",
    "\n",
    "#models path\n",
    "MODEL_PATH=os.path.join(DATASET_PATH,\"models_sigma_right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NUM=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.join(LOG_PATH,'exp_'+str(EXPERIMENT_NUM))\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "LOG_PATH = dir_path\n",
    "dir_path = os.path.join(MODEL_PATH,'exp_'+str(EXPERIMENT_NUM))\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "MODEL_PATH = dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path,\"logs_sigma_right\",'exp_'+str(24),str(24)+'_parameters.json'),'r') as f:\n",
    "  args = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args['NUM_EXP']=str(EXPERIMENT_NUM)\n",
    "# args['BATCH_SIZE']=64\n",
    "\n",
    "# args['TRANSF']='w'\n",
    "# args['SIGMA_TRAIN']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EPOCHS_NUM': 100,\n",
       " 'PATIENCE': 25,\n",
       " 'SIGMA': 1.4,\n",
       " 'BATCH_SIZE': 128,\n",
       " 'TRANSF': 'g',\n",
       " 'NUM_EXP': '24',\n",
       " 'VOXEL_DIM': 50,\n",
       " 'LEARN_RATE': 1e-05,\n",
       " 'SIGMA_TRAIN': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf_calc = ld.reading_sql_database(database_dir='./database/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "GeForce GTX 1080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 2.6 GB\n",
      "Cached:    4.8 GB\n",
      "Start loading dataset...\n",
      "Initial dataset size =  13091\n",
      "Post-processed dataset size =  13084\n",
      "Dataset has been loaded,  163  s\n",
      "Neural network initialization...\n"
     ]
    }
   ],
   "source": [
    "f_log=open(os.path.join(LOG_PATH,args['NUM_EXP']+'_logs.txt'),'w')\n",
    "f_log.close()\n",
    "start_time=time.time()\n",
    "writer=SummaryWriter(LOG_PATH)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "with open(os.path.join(LOG_PATH,args['NUM_EXP']+'_logs.txt'),'a') as f_log:\n",
    "    f_log.write('Using device:'+str(device)+'\\n')\n",
    "print()\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "\n",
    "    with open(os.path.join(LOG_PATH,args['NUM_EXP']+'_logs.txt'),'a') as f_log:\n",
    "        f_log.write(torch.cuda.get_device_name(0)+'\\n'+'Memory Usage:'+'\\n'+'Allocated:'+str(round(torch.cuda.memory_allocated(0)/1024**3,1))+ 'GB'+'\\n'+'Cached:   '+str(round(torch.cuda.memory_cached(0)/1024**3,1))+'GB'+'\\n')\n",
    "print('Start loading dataset...')\n",
    "with open(os.path.join(LOG_PATH,args['NUM_EXP']+'_logs.txt'),'a') as f_log:\n",
    "    f_log.write('Start loading dataset...'+'\\n')\n",
    "# get dataset without duplicates from csv\n",
    "data = pd.read_csv(os.path.join(DATASET_PATH,'database', 'MultiTox.csv'))\n",
    "props = list(data)[1:]\n",
    "scaler = MinMaxScaler()\n",
    "data[props]=scaler.fit_transform(data[props])\n",
    "\n",
    "# create elements dictionary\n",
    "#     elements = ld.create_element_dict(data, amount=AMOUNT_OF_ELEM+1)\n",
    "elements={'N':0,'C':1,'Cl':2,'I':3,'Br':4,'F':5,'O':6,'P':7,'S':8}\n",
    "\n",
    "# read databases to dictionary\n",
    "#     conf_calc = ld.reading_sql_database(database_dir='./dat/')\n",
    "with open(os.path.join(DATASET_PATH,'many_elems.json'), 'r') as fp:\n",
    "    conf_calc = json.load(fp)\n",
    "\n",
    "keys=list(conf_calc.keys())\n",
    "print ('Initial dataset size = ', len(keys))\n",
    "with open(os.path.join(LOG_PATH,args['NUM_EXP']+'_logs.txt'),'a') as f_log:\n",
    "    f_log.write('Initial dataset size = '+str(len(keys))+'\\n')\n",
    "new_conf_calc={}\n",
    "for smiles in conf_calc.keys():\n",
    "    for conf_num in conf_calc[smiles]:\n",
    "        if smiles in new_conf_calc.keys():\n",
    "            new_conf_calc[smiles][int(conf_num)]=conf_calc[smiles][conf_num]\n",
    "        else:\n",
    "            new_conf_calc[smiles]={}\n",
    "            new_conf_calc[smiles][int(conf_num)]=conf_calc[smiles][conf_num]\n",
    "\n",
    "conf_calc=new_conf_calc\n",
    "\n",
    "elems = []\n",
    "for key in keys:\n",
    "    conformers=list(conf_calc[key].keys())\n",
    "    for conformer in conformers:\n",
    "        try:\n",
    "            energy = conf_calc[key][conformer]['energy']\n",
    "            elems = list(set(elems+list(conf_calc[key][conformer]['coordinates'].keys())))\n",
    "        except:\n",
    "            del conf_calc[key][conformer]\n",
    "    if set(conf_calc[key].keys())!=set(range(100)):\n",
    "          del conf_calc[key]\n",
    "    elif conf_calc[key]=={}:\n",
    "        del conf_calc[key]\n",
    "\n",
    "print ('Post-processed dataset size = ', len(list(conf_calc.keys())))\n",
    "with open(os.path.join(LOG_PATH,args['NUM_EXP']+'_logs.txt'),'a') as f_log:\n",
    "    f_log.write('Post-processed dataset size = '+str(len(list(conf_calc.keys())))+'\\n')\n",
    "# create indexing and label_dict for iteration\n",
    "indexing, label_dict = ld.indexing_label_dict(data, conf_calc)\n",
    "print('Dataset has been loaded, ', int(time.time()-start_time),' s')\n",
    "with open(os.path.join(LOG_PATH,args['NUM_EXP']+'_logs.txt'),'a') as f_log:\n",
    "    f_log.write('Dataset has been loaded, '+str(int(time.time()-start_time))+' s'+'\\n')\n",
    "\n",
    "start_time=time.time()\n",
    "# create train and validation sets' indexes\n",
    "print('Neural network initialization...')\n",
    "with open(os.path.join(LOG_PATH,args['NUM_EXP']+'_logs.txt'),'a') as f_log:\n",
    "    f_log.write('Neural network initialization...'+'\\n')\n",
    "train_indexes, test_indexes, _, _ = train_test_split(np.arange(0, len(conf_calc.keys())),\n",
    "                                                     np.arange(0, len(conf_calc.keys())), test_size=0.2,\n",
    "                                                     random_state=115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SIGMA_TRAIN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d3480362f7c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BATCH_SIZE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VOXEL_DIM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_elems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAMOUNT_OF_ELEM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_targets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARGET_NUM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TRANSF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SIGMA'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SIGMA_TRAIN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SIGMA_TRAIN'"
     ]
    }
   ],
   "source": [
    "train_set = dl.Cube_dataset(conf_calc, label_dict, elements, indexing, train_indexes, dim = args['VOXEL_DIM'])\n",
    "train_generator = td.DataLoader(train_set, batch_size=args['BATCH_SIZE'], shuffle=True)\n",
    "\n",
    "test_set = dl.Cube_dataset(conf_calc, label_dict, elements, indexing, test_indexes, dim = args['VOXEL_DIM'])\n",
    "test_generator = td.DataLoader(test_set, batch_size=args['BATCH_SIZE'], shuffle=True)\n",
    "\n",
    "model = Net(dim=args['VOXEL_DIM'], num_elems=AMOUNT_OF_ELEM, num_targets=TARGET_NUM, elements=elements, transformation=args['TRANSF'],device=device,sigma_0 = args['SIGMA'],sigma_trainable = args['SIGMA_TRAIN'])\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    print ('Run in parallel!')\n",
    "    with open(os.path.join(LOG_PATH,args['NUM_EXP']+'_logs.txt'),'a') as f_log:\n",
    "        f_log.write('Run in parallel!'+'\\n')\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "\n",
    "model=model.to(device)\n",
    "\n",
    "for (batch, target) in train_generator:\n",
    "    batch = batch.to(device)\n",
    "    target = target.to(device)\n",
    "    with open(os.path.join(LOG_PATH,args['NUM_EXP']+'_logs.txt'),'a') as f_log:\n",
    "        f_log.write('Batch to device!'+'\\n')\n",
    "    print('Batch to device!')\n",
    "    output = model(batch)\n",
    "    with open(os.path.join(LOG_PATH,args['NUM_EXP']+'_logs.txt'),'a') as f_log:\n",
    "        f_log.write('Batch output!'+'\\n')\n",
    "    print('Batch output!')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
