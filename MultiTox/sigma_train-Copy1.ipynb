{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = False\n",
    "for i in range(int(len(data)/10)):\n",
    "    data_selected = data.iloc[:i*10]\n",
    "    for column in columns:\n",
    "        if (~data_selected[column].isna()).sum()<THRESH_NAN:\n",
    "            nan = True\n",
    "            break\n",
    "    if nan: \n",
    "        nan = False\n",
    "        continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min not Nan values\t 3\n",
      "dataset length\t\t 1480\n",
      "o_mus_ipr_LD \t\t 576\n",
      "o_rat_orl_TDLo \t\t 19\n",
      "o_mus_ipr_LDLo \t\t 37\n",
      "o_mus_orl_TDLo \t\t 19\n",
      "o_rat_ipr_TDLo \t\t 22\n",
      "o_mus_ivn_LD \t\t 265\n",
      "o_rat_ipr_LD \t\t 74\n",
      "o_mus_orl_LD \t\t 536\n",
      "o_mus_unr_LD \t\t 27\n",
      "o_rat_unr_LD \t\t 5\n",
      "o_mus_scu_LDLo \t\t 4\n",
      "o_rat_scu_LD \t\t 26\n",
      "o_mus_scu_LD \t\t 81\n",
      "o_rat_ipr_LDLo \t\t 17\n",
      "o_mus_ipr_TDLo \t\t 22\n",
      "o_rbt_skn_LD \t\t 40\n",
      "o_rat_orl_LD \t\t 168\n",
      "o_rat_ivn_TDLo \t\t 6\n",
      "o_rat_orl_LDLo \t\t 21\n",
      "o_rbt_orl_LD \t\t 11\n",
      "o_rbt_ivn_LD \t\t 3\n",
      "o_rat_ivn_LD \t\t 23\n",
      "o_mus_orl_LDLo \t\t 22\n",
      "o_rat_skn_LD \t\t 15\n",
      "o_mam_unr_LD \t\t 9\n",
      "o_gpg_orl_LD \t\t 14\n",
      "o_wmn_orl_TDLo \t\t 9\n",
      "o_man_orl_TDLo \t\t 3\n",
      "o_rat_scu_TDLo \t\t 5\n"
     ]
    }
   ],
   "source": [
    "print('min not Nan values\\t', THRESH_NAN)\n",
    "print('dataset length\\t\\t', len(data_selected))\n",
    "for column in columns:\n",
    "    print(column,'\\t\\t', (~data_selected[column].isna()).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils import data as td\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable#, Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './database'\n",
    "filename = 'MultiTox.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data from overall dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformer_choice(props):\n",
    "    probabilities=[props[key]['energy'] for key in props.keys()]\n",
    "    conformer=np.random.choice(range(len(props)),1,probabilities)\n",
    "    return np.asscalar(conformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(DATA_DIR,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data)\n",
    "columns.remove('SMILES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdMolTransforms as rdmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of conformers created for every molecule\n",
    "NUM_CONFS=100\n",
    "\n",
    "#amount of chemical elements taking into account\n",
    "AMOUNT_OF_ELEM=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_element_dict(data,amount=9,treshold=10, add_H=False):\n",
    "    elements={}\n",
    "    norm=0\n",
    "    for smile in data['SMILES']:\n",
    "        molecule=Chem.MolFromSmiles(smile)\n",
    "        molecule=Chem.AddHs(molecule)\n",
    "\n",
    "        for i in range(molecule.GetNumAtoms()):\n",
    "            atom = molecule.GetAtomWithIdx(i)\n",
    "            element=atom.GetSymbol()\n",
    "            norm+=1\n",
    "            if element in elements.keys():\n",
    "                elements[element]+=1\n",
    "            else:\n",
    "                elements[element]=1\n",
    "    for key in elements.keys():\n",
    "        elements[key]/=norm\n",
    "    from collections import OrderedDict\n",
    "    dd = OrderedDict(sorted(elements.items(), key=lambda x: x[1]))\n",
    "    elements=list(dd.keys())[-amount:]  \n",
    "    elements=dict((elem,i) for i, elem in enumerate(elements))  \n",
    "    if not add_H:\n",
    "        del elements['H']\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements=create_element_dict(data,amount=AMOUNT_OF_ELEM,add_H=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cl': 0, 'S': 1, 'N': 2, 'O': 3, 'C': 4, 'H': 5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data_multitox as ld\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH='./'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_calc = ld.reading_sql_database(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data[columns]=scaler.fit_transform(data[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing, label_dict = ld.indexing_label_dict(data, conf_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gauss_dataset(td.Dataset):\n",
    "    def __init__(self,conf_calc,label_dict,elements,indexing, indexes,dx=0.5,dim=70):\n",
    "        self.Xs=conf_calc\n",
    "        self.Ys=label_dict\n",
    "        self.elements=elements\n",
    "        self.indexing = indexing\n",
    "        self.dx = dx\n",
    "#         self.sigma=sigma\n",
    "#         self.dim=dim\n",
    "#         self.dx=dx\n",
    "#         self.kern_dim=kern_dim\n",
    "        self.indexes=indexes\n",
    "        self.dim = dim\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.indexes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        from math import floor\n",
    "        'Generates one sample of data'\n",
    "        dimelem = len(self.elements)\n",
    "        \n",
    "        cube=torch.zeros((dimelem,self.dim,self.dim,self.dim))\n",
    "        \n",
    "        i=self.indexes[index]\n",
    "        smiles=self.indexing[i]\n",
    "        \n",
    "        y= self.Ys[smiles]\n",
    "\n",
    "        description=self.Xs[smiles][conformer_choice(self.Xs[smiles])]['coordinates']\n",
    "#         X = gaussian_blur(description,self.elements,sigma=self.sigma,dimx=self.dim,dx=self.dx,kern_dim=self.kern_dim)\n",
    "\n",
    "        for atom in description.keys():\n",
    "        \n",
    "            num_atom=elements[atom]\n",
    "\n",
    "            for x0,y0,z0 in description[atom]:\n",
    "                cube[num_atom, max(0,min(self.dim-1,floor(self.dim/2+x0/self.dx))), max(0,min(self.dim-1,floor(self.dim/2+y0/self.dx))), max(0,min(self.dim-1,floor(self.dim/2+z0/self.dx)))]=1\n",
    "        X= cube\n",
    "        return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOXEL_DIM = 70\n",
    "TARGET_NUM = 29\n",
    "args={'BATCH_SIZE':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dim=70, dx=0.5,kern_dim=50,num_elems=6, num_targets=12, batch_size=args['BATCH_SIZE'], elements=elements):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.sigma = Parameter(6*torch.ones(num_elems).float().to(device),requires_grad=True)\n",
    "#         self.sigma.\n",
    "        \n",
    "        # initialize dimensions\n",
    "        self.dim = dim\n",
    "        self.num_elems = num_elems\n",
    "        self.num_targets = num_targets\n",
    "        self.batch_size = batch_size\n",
    "        self.elements=elements\n",
    "        self.dx=dx\n",
    "        self.kern_dim=kern_dim\n",
    "\n",
    "        # create layers\n",
    "        self.conv1 = nn.Conv3d(num_elems, 32, kernel_size=(3, 3, 3))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        self.conv3 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        self.conv4 = nn.Conv3d(128, 256, kernel_size=(3, 3, 3))\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, TARGET_NUM)\n",
    "\n",
    "        # initialize dense layer's weights\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        self.fc1.bias.data.fill_(0.01)\n",
    "\n",
    "        self.convolution = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.pool1,\n",
    "            nn.ReLU(),\n",
    "            self.conv2,\n",
    "            self.pool2,\n",
    "            nn.ReLU(),\n",
    "            self.conv3,\n",
    "            self.pool3,\n",
    "            nn.ReLU(),\n",
    "            self.conv4,\n",
    "            self.pool4,\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def weights_init(m):\n",
    "            if type(m) == nn.Conv3d:\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        # initialize convolutional layers' weights\n",
    "        self.convolution.apply(weights_init)\n",
    "        \n",
    "    def gaussian_blur (self, batch):#(molecule,elements,sigma=2,dimx=70,dx=0.5,kern_dim=50):\n",
    "        from math import floor\n",
    "\n",
    "        dimx=self.dim\n",
    "        dx=self.dx\n",
    "        kern_dim=self.kern_dim\n",
    "        \n",
    "        \n",
    "        batch=batch.to(device)\n",
    "        dimelem=len(elements)\n",
    "#         cube=torch.zeros((dimelem,dimx,dimx,dimx))\n",
    "        cube=torch.zeros(batch.shape)\n",
    "        cube=cube.to(device)\n",
    "\n",
    "        #build the kernel\n",
    "#         x = torch.arange(-kern_dim/4,kern_dim/4,dx)\n",
    "        \n",
    "#         y = torch.arange(-kern_dim/4,kern_dim/4,dx)\n",
    "#         z = torch.arange(-kern_dim/4,kern_dim/4,dx)\n",
    "        x = torch.arange(-dimx/2,dimx/2)\n",
    "        \n",
    "        y = torch.arange(-dimx/2,dimx/2)\n",
    "        z = torch.arange(-dimx/2,dimx/2)\n",
    "        xx, yy, zz = torch.meshgrid((x,y,z))\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        z=z.to(device)\n",
    "        xx=xx.to(device)\n",
    "        yy=yy.to(device)\n",
    "        zz=zz.to(device)\n",
    "#         print(xx)\n",
    "#         kernel = torch.exp(-(xx**2 + yy**2 + zz**2)/(2*self.sigma**2))\n",
    "\n",
    "#         for atom in molecule.keys():\n",
    "\n",
    "#             num_atom=elements[atom]\n",
    "        for idx,molecule in enumerate(batch):\n",
    "            for num_atom in range(len(elements)):\n",
    "                \n",
    "#     #             print(xx.shape)\n",
    "#     #             print(self.sigma.shape)\n",
    "# #                 print(xx,yy,zz,self.sigma[num_atom])\n",
    "#                 kernel = torch.exp(-(xx**2 + yy**2 + zz**2)/(2*self.sigma[num_atom]**2))\n",
    "# #                 print(kernel)\n",
    "                \n",
    "                for x0,y0,z0 in molecule[num_atom].nonzero():\n",
    "                    cube = cube + torch.exp(-((xx-x0)**2 + (yy-y0)**2 + (zz-z0)**2)/(2*self.sigma[num_atom]**2))\n",
    "#                     cube = cube + torch.exp(-(xx**2 + yy**2 + zz**2)/(2*self.sigma[num_atom]**2))*torch.cos(2*np.pi/self.sigma[num_atom]*torch.sqrt(xx**2+yy**2+zz**2))\n",
    "                    \n",
    "#                     for i in range(cube.shape[2]):\n",
    "#                         for j in range(cube.shape[3]):\n",
    "#                             for k in range(cube.shape[2]):\n",
    "#                                 cube[idx,num_atom,i,j,k]+=torch.exp(-((i-x0)**2 + (j-y0)**2 + (k-z0)**2)/(2*self.sigma[num_atom]**2))\n",
    "# #                     print(x[0],x0,dimx)\n",
    "#                     x0=x0.float()\n",
    "#                     y0=y0.float()\n",
    "#                     z0=z0.float()\n",
    "#                     x_range=[max(floor(x[0]/dx+x0),0),min(floor(x[-1]/dx+x0+1),cube.shape[2])]\n",
    "#                     y_range=[max(floor(y[0]/dx+y0),0),min(floor(y[-1]/dx+y0+1),cube.shape[3])]\n",
    "#                     z_range=[max(floor(z[0]/dx+z0),0),min(floor(z[-1]/dx+z0+1),cube.shape[4])]\n",
    "#                     coord_ranges=[x_range,y_range,z_range]\n",
    "# #                     print(coord_ranges)\n",
    "#                     for i in range(3):\n",
    "#                         if coord_ranges[i][1]-coord_ranges[i][0]>50:\n",
    "#                             coord_ranges[i][1]=coord_ranges[i][0]+50\n",
    "#                     cube_part=cube[idx,num_atom,coord_ranges[0][0]:coord_ranges[0][1],\n",
    "#                                    coord_ranges[1][0]:coord_ranges[1][1],\n",
    "#                                    coord_ranges[2][0]:coord_ranges[2][1]]\n",
    "# #                     print(cube.shape)\n",
    "#                     kern_ranges=[[],[],[]]\n",
    "#                     for i in range(3):\n",
    "#                         if coord_ranges[i][0]==0:\n",
    "#                             kern_ranges[i].append(kern_dim-cube_part.shape[i])\n",
    "#                         else:\n",
    "#                             kern_ranges[i].append(0)\n",
    "#                         if coord_ranges[i][1]==cube.shape[i+1]:\n",
    "#                             kern_ranges[i].append(cube_part.shape[i])\n",
    "#                         else:\n",
    "#                             kern_ranges[i].append(kern_dim)\n",
    "                    \n",
    "#                     new_kern = kernel[kern_ranges[0][0]:kern_ranges[0][1],\n",
    "#                                                kern_ranges[1][0]:kern_ranges[1][1],\n",
    "#                                                kern_ranges[2][0]:kern_ranges[2][1]]*1.0\n",
    "#                     mask = torch.zeros_like(kernel)\n",
    "#                     for i_k,i_c in zip(range(kern_ranges[0][0],kern_ranges[0][1]),range(coord_ranges[0][0],coord_ranges[0][1])):\n",
    "#                         for j_k,j_c in zip(range(kern_ranges[1][0],kern_ranges[1][1]),range(coord_ranges[1][0],coord_ranges[1][1])):\n",
    "#                             for k_k,k_c in zip(range(kern_ranges[2][0],kern_ranges[2][1]),range(coord_ranges[2][0],coord_ranges[2][1])):\n",
    "# #                                 mask[i,j,k]=1\n",
    "#                                 cube[idx,num_atom,i_c,j_c,k_c]+=kernel[i_k,j_k,k_k]\n",
    "                                \n",
    "# #                     new_kern=kernel[mask.byte()]\n",
    "#                     f = (torch.exp(-((xx-x0)**2 + (yy-y0)**2 + (zz-z0)**2)/(2*self.sigma[num_atom]**2))).sum()\n",
    "# # #                     f.backward()\n",
    "#                     f = cube.sum()\n",
    "#                     f.backward()\n",
    "#                     print(self.sigma.grad)\n",
    "\n",
    "#                     cube_part=cube_part+kernel[kern_ranges[0][0]:kern_ranges[0][1],\n",
    "#                                                kern_ranges[1][0]:kern_ranges[1][1],\n",
    "#                                                kern_ranges[2][0]:kern_ranges[2][1]]\n",
    "                    \n",
    "\n",
    "#                     cube[idx,num_atom,coord_ranges[0][0]:coord_ranges[0][1],\n",
    "#                          coord_ranges[1][0]:coord_ranges[1][1],\n",
    "#                          coord_ranges[2][0]:coord_ranges[2][1]]=cube_part\n",
    "        \n",
    "        return cube\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_gauss = self.gaussian_blur(x)\n",
    "#         f = x_gauss.sum()\n",
    "#         f.backward()\n",
    "#         print(self.sigma.grad)\n",
    "#         x = x.to(device)\n",
    "#         x_gauss=x_gauss.to(device)\n",
    "        \n",
    "        x_conv = self.convolution(x_gauss)\n",
    "#         conv = nn.Conv3d(6, 32, kernel_size=(3, 3, 3)).to(device)\n",
    "#         f = x_conv.sum()\n",
    "#         f.backward()\n",
    "#         print(self.sigma.grad)\n",
    "        x_vect = x_conv.view(x.shape[0], -1)\n",
    "        y1 = F.relu(self.fc1(x_vect))\n",
    "        y2=self.fc2(y1)\n",
    "        #         multi-label (not multi-class!) classification => sigmoid non-linearity\n",
    "        return y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv3d(6, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (pool1): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (pool2): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (pool3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (pool4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=29, bias=True)\n",
      "  (convolution): Sequential(\n",
      "    (0): Conv3d(6, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "    (6): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (7): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): ReLU()\n",
      "    (9): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (10): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(dim=VOXEL_DIM, num_elems=AMOUNT_OF_ELEM, num_targets=TARGET_NUM, batch_size=args['BATCH_SIZE'])\n",
    "print(model)\n",
    "model=model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_indexes, test_indexes, _, _ = train_test_split(np.arange(0, len(conf_calc.keys())),\n",
    "                                                         np.arange(0, len(conf_calc.keys())), test_size=0.2,\n",
    "                                                         random_state=115)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Gauss_dataset(conf_calc, label_dict, elements, indexing,train_indexes)\n",
    "train_generator = td.DataLoader(train_set, batch_size=args['BATCH_SIZE'], shuffle=True)\n",
    "test_set = Gauss_dataset(conf_calc, label_dict, elements, indexing, test_indexes)\n",
    "test_generator = td.DataLoader(test_set, batch_size=args['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_generator,epoch,device,writer=None,f_loss=None):\n",
    "#    print(f_auc is not None)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        losses=np.zeros(TARGET_NUM)\n",
    "        num_losses=np.zeros(TARGET_NUM)\n",
    "        for batch_idx, (data, target) in enumerate(test_generator):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)   \n",
    "            i=0\n",
    "            for one_target,one_output in zip(target.cpu().t(),output.cpu().t()):\n",
    "                with torch.no_grad():\n",
    "                    mask = (one_target == one_target)\n",
    "                    output_masked = torch.masked_select(one_output, mask).type_as(one_output)\n",
    "                    target_masked = torch.masked_select(one_target, mask).type_as(one_output)\n",
    "                    criterion=nn.MSELoss()\n",
    "                    loss = criterion(output_masked.cpu(),target_masked.cpu())\n",
    "                    if loss == loss:\n",
    "                        losses[i]+=loss\n",
    "                        num_losses[i]+=1\n",
    "#                        if f_loss is not None:\n",
    "#                            f_loss.write(str(epoch)+'\\t'+str(batch_idx)+'\\t'+str(i)+'\\t'+str(loss.cpu().numpy().item())+'\\n')\n",
    "                    i+=1\n",
    "            mask = (target == target)\n",
    "            output_masked = torch.masked_select(output, mask).type_as(output)\n",
    "            target_masked = torch.masked_select(target, mask).type_as(output)\n",
    "#            penalty_masked = torch.masked_select(PENALTY.to(device), mask).type_as(output)\n",
    "#            class_weights=(1-penalty_masked)*(target_masked).to(device)+penalty_masked\n",
    "\n",
    "            criterion=nn.MSELoss()\n",
    "            loss = criterion(output_masked, target_masked)\n",
    "\n",
    "            test_loss += loss\n",
    "            \n",
    "            \n",
    "#            if f_loss is not None:\n",
    "#                f_loss.write(str(epoch)+'\\t'+str(batch_idx)+'\\t'+str(loss.cpu().numpy().item())+'\\n')\n",
    "#            total += output_masked.shape[0]\n",
    "        test_loss /= len(test_generator.dataset)\n",
    "        test_loss *= args['BATCH_SIZE']\n",
    "\n",
    "        print('\\nTest set: Average loss: {:.4f}\\n'\n",
    "              .format(test_loss))\n",
    "    if writer is not None:\n",
    "        writer.add_scalar('Test/Loss/'+str(epoch), test_loss, epoch)\n",
    "    losses/=num_losses    \n",
    "    for i,loss in enumerate(losses):\n",
    "        if f_loss is not None and loss == loss:\n",
    "            f_loss.write(str(epoch)+'\\t'+str(batch_idx)+'\\t'+str(i)+'\\t'+str(loss)+'\\n')\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_generator, epoch, device, writer = None,f_loss=None,f_loss_ch = None):\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "    losses=np.zeros(TARGET_NUM)\n",
    "    num_losses=np.zeros(TARGET_NUM)\n",
    "    for batch_idx, (data, target) in enumerate(train_generator):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        i=0\n",
    "        for one_target,one_output in zip(target.cpu().t(),output.cpu().t()):\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                mask = (one_target == one_target)\n",
    "                output_masked = torch.masked_select(one_output, mask).type_as(one_output)\n",
    "                target_masked = torch.masked_select(one_target, mask).type_as(one_output)\n",
    "                criterion=nn.MSELoss()\n",
    "                loss = criterion(output_masked.cpu(),target_masked.cpu())\n",
    "                if loss == loss:\n",
    "                    losses[i]+=loss\n",
    "                    num_losses[i]+=1\n",
    "#                        if f_loss is not None:\n",
    "#                            f_loss.write(str(epoch)+'\\t'+str(batch_idx)+'\\t'+str(i)+'\\t'+str(loss.cpu().numpy().item())+'\\n')\n",
    "            i+=1\n",
    "        # calculate output vector\n",
    "        \n",
    "        # create mask to get rid of Nan's in target\n",
    "        mask = (target == target)\n",
    "        output_masked = torch.masked_select(output, mask).type_as(output)\n",
    "        target_masked = torch.masked_select(target, mask).type_as(output)\n",
    "#        penalty_masked = torch.masked_select(PENALTY.to(device), mask).type_as(output)\n",
    "#        pred = output_masked.ge(0.5).type_as(output)\n",
    "#        try:\n",
    "#            auc=roc_auc_score(target_masked.cpu().detach(),pred.cpu().detach())\n",
    "#            if f_auc is not None:\n",
    "#                f_auc.write(str(epoch)+'\\t'+str(batch_idx)+'\\t'+str(auc)+'\\n')\n",
    "#        except ValueError:\n",
    "#            pass\n",
    "        # multi-label (not multi-class!) classification=>binary cross entropy loss\n",
    "#        class_weights=(1-penalty_masked)*(target_masked).to(device)+penalty_masked\n",
    "        criterion=nn.MSELoss()\n",
    "        loss = criterion(output_masked, target_masked)\n",
    "        \n",
    "        if f_loss is not None:\n",
    "            f_loss.write(str(epoch)+'\\t'+str(batch_idx)+'\\t'+str(loss.cpu().detach().numpy().item())+'\\n')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+=loss.cpu().detach().numpy().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_generator.dataset),\n",
    "                       100. * batch_idx / len(train_generator), loss.item()))\n",
    "    train_loss /= len(train_generator.dataset)\n",
    "    train_loss *= args['BATCH_SIZE']\n",
    "    print('train loss average = ', train_loss)\n",
    "    if writer is not None:\n",
    "        writer.add_scalar('Train/Loss/'+str(epoch), train_loss, epoch)\n",
    "    losses/=num_losses    \n",
    "    for i,loss in enumerate(losses):\n",
    "        if f_loss_ch is not None and loss==loss:\n",
    "            f_loss_ch.write(str(epoch)+'\\t'+str(batch_idx)+'\\t'+str(i)+'\\t'+str(loss)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6658 (0%)]\tLoss: 0.204927\n",
      "Train Epoch: 1 [100/6658 (2%)]\tLoss: 0.641661\n",
      "Train Epoch: 1 [200/6658 (3%)]\tLoss: 0.640593\n",
      "Train Epoch: 1 [300/6658 (5%)]\tLoss: 0.016408\n",
      "Train Epoch: 1 [400/6658 (6%)]\tLoss: 4.382380\n",
      "Train Epoch: 1 [500/6658 (8%)]\tLoss: 0.000536\n",
      "Train Epoch: 1 [600/6658 (9%)]\tLoss: 0.885476\n",
      "Train Epoch: 1 [700/6658 (11%)]\tLoss: 0.009306\n",
      "Train Epoch: 1 [800/6658 (12%)]\tLoss: 1.685156\n",
      "Train Epoch: 1 [900/6658 (14%)]\tLoss: 8.491997\n",
      "Train Epoch: 1 [1000/6658 (15%)]\tLoss: 0.048426\n",
      "Train Epoch: 1 [1100/6658 (17%)]\tLoss: 0.215476\n",
      "Train Epoch: 1 [1200/6658 (18%)]\tLoss: 3.179224\n",
      "Train Epoch: 1 [1300/6658 (20%)]\tLoss: 38.691326\n",
      "Train Epoch: 1 [1400/6658 (21%)]\tLoss: 0.289713\n",
      "Train Epoch: 1 [1500/6658 (23%)]\tLoss: 0.386250\n",
      "Train Epoch: 1 [1600/6658 (24%)]\tLoss: 0.523959\n",
      "Train Epoch: 1 [1700/6658 (26%)]\tLoss: 0.037003\n",
      "Train Epoch: 1 [1800/6658 (27%)]\tLoss: 1.289353\n",
      "Train Epoch: 1 [1900/6658 (29%)]\tLoss: 1.542191\n",
      "Train Epoch: 1 [2000/6658 (30%)]\tLoss: 0.021326\n",
      "Train Epoch: 1 [2100/6658 (32%)]\tLoss: 0.888444\n",
      "Train Epoch: 1 [2200/6658 (33%)]\tLoss: 0.469788\n",
      "Train Epoch: 1 [2300/6658 (35%)]\tLoss: 0.264315\n",
      "Train Epoch: 1 [2400/6658 (36%)]\tLoss: 0.099712\n",
      "Train Epoch: 1 [2500/6658 (38%)]\tLoss: 3.695742\n",
      "Train Epoch: 1 [2600/6658 (39%)]\tLoss: 0.546462\n",
      "Train Epoch: 1 [2700/6658 (41%)]\tLoss: 0.208471\n",
      "Train Epoch: 1 [2800/6658 (42%)]\tLoss: 0.006822\n",
      "Train Epoch: 1 [2900/6658 (44%)]\tLoss: 0.215809\n",
      "Train Epoch: 1 [3000/6658 (45%)]\tLoss: 0.351031\n",
      "Train Epoch: 1 [3100/6658 (47%)]\tLoss: 0.605539\n",
      "Train Epoch: 1 [3200/6658 (48%)]\tLoss: 0.026489\n",
      "Train Epoch: 1 [3300/6658 (50%)]\tLoss: 0.060342\n",
      "Train Epoch: 1 [3400/6658 (51%)]\tLoss: 0.311824\n",
      "Train Epoch: 1 [3500/6658 (53%)]\tLoss: 0.342330\n",
      "Train Epoch: 1 [3600/6658 (54%)]\tLoss: 1.105616\n",
      "Train Epoch: 1 [3700/6658 (56%)]\tLoss: 0.109949\n",
      "Train Epoch: 1 [3800/6658 (57%)]\tLoss: 1.573613\n",
      "Train Epoch: 1 [3900/6658 (59%)]\tLoss: 0.270327\n",
      "Train Epoch: 1 [4000/6658 (60%)]\tLoss: 0.643651\n",
      "Train Epoch: 1 [4100/6658 (62%)]\tLoss: 0.340236\n",
      "Train Epoch: 1 [4200/6658 (63%)]\tLoss: 0.375739\n",
      "Train Epoch: 1 [4300/6658 (65%)]\tLoss: 0.000110\n",
      "Train Epoch: 1 [4400/6658 (66%)]\tLoss: 0.022076\n",
      "Train Epoch: 1 [4500/6658 (68%)]\tLoss: 0.076736\n",
      "Train Epoch: 1 [4600/6658 (69%)]\tLoss: 0.938469\n",
      "Train Epoch: 1 [4700/6658 (71%)]\tLoss: 0.165114\n",
      "Train Epoch: 1 [4800/6658 (72%)]\tLoss: 0.030169\n",
      "Train Epoch: 1 [4900/6658 (74%)]\tLoss: 0.031943\n",
      "Train Epoch: 1 [5000/6658 (75%)]\tLoss: 0.875925\n",
      "Train Epoch: 1 [5100/6658 (77%)]\tLoss: 0.234692\n",
      "Train Epoch: 1 [5200/6658 (78%)]\tLoss: 0.017211\n",
      "Train Epoch: 1 [5300/6658 (80%)]\tLoss: 0.191330\n",
      "Train Epoch: 1 [5400/6658 (81%)]\tLoss: 0.033931\n",
      "Train Epoch: 1 [5500/6658 (83%)]\tLoss: 8.755436\n",
      "Train Epoch: 1 [5600/6658 (84%)]\tLoss: 0.003529\n",
      "Train Epoch: 1 [5700/6658 (86%)]\tLoss: 0.217659\n",
      "Train Epoch: 1 [5800/6658 (87%)]\tLoss: 0.716194\n",
      "Train Epoch: 1 [5900/6658 (89%)]\tLoss: 0.947101\n",
      "Train Epoch: 1 [6000/6658 (90%)]\tLoss: 0.596858\n",
      "Train Epoch: 1 [6100/6658 (92%)]\tLoss: 0.976648\n",
      "Train Epoch: 1 [6200/6658 (93%)]\tLoss: 0.013958\n",
      "Train Epoch: 1 [6300/6658 (95%)]\tLoss: 0.213207\n",
      "Train Epoch: 1 [6400/6658 (96%)]\tLoss: 0.241966\n",
      "Train Epoch: 1 [6500/6658 (98%)]\tLoss: 0.048314\n",
      "Train Epoch: 1 [6600/6658 (99%)]\tLoss: 4.340607\n",
      "train loss average =  0.8242493553083686\n",
      "\n",
      "Test set: Average loss: 0.7837\n",
      "\n",
      "Validation loss decreased (inf --> 0.783702).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([5.9996, 5.9994, 5.9985, 6.0003, 6.0058, 6.0054], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 2 [0/6658 (0%)]\tLoss: 6.251888\n",
      "Train Epoch: 2 [100/6658 (2%)]\tLoss: 0.354245\n",
      "Train Epoch: 2 [200/6658 (3%)]\tLoss: 8.130463\n",
      "Train Epoch: 2 [300/6658 (5%)]\tLoss: 0.392878\n",
      "Train Epoch: 2 [400/6658 (6%)]\tLoss: 0.262490\n",
      "Train Epoch: 2 [500/6658 (8%)]\tLoss: 3.116607\n",
      "Train Epoch: 2 [600/6658 (9%)]\tLoss: 0.252060\n",
      "Train Epoch: 2 [700/6658 (11%)]\tLoss: 0.799701\n",
      "Train Epoch: 2 [800/6658 (12%)]\tLoss: 1.051490\n",
      "Train Epoch: 2 [900/6658 (14%)]\tLoss: 0.179287\n",
      "Train Epoch: 2 [1000/6658 (15%)]\tLoss: 0.130052\n",
      "Train Epoch: 2 [1100/6658 (17%)]\tLoss: 0.002546\n",
      "Train Epoch: 2 [1200/6658 (18%)]\tLoss: 0.250111\n",
      "Train Epoch: 2 [1300/6658 (20%)]\tLoss: 0.031888\n",
      "Train Epoch: 2 [1400/6658 (21%)]\tLoss: 0.454070\n",
      "Train Epoch: 2 [1500/6658 (23%)]\tLoss: 0.063858\n",
      "Train Epoch: 2 [1600/6658 (24%)]\tLoss: 1.176345\n",
      "Train Epoch: 2 [1700/6658 (26%)]\tLoss: 0.823234\n",
      "Train Epoch: 2 [1800/6658 (27%)]\tLoss: 3.747146\n",
      "Train Epoch: 2 [1900/6658 (29%)]\tLoss: 0.257664\n",
      "Train Epoch: 2 [2000/6658 (30%)]\tLoss: 40.610828\n",
      "Train Epoch: 2 [2100/6658 (32%)]\tLoss: 0.072377\n",
      "Train Epoch: 2 [2200/6658 (33%)]\tLoss: 0.471290\n",
      "Train Epoch: 2 [2300/6658 (35%)]\tLoss: 1.994151\n",
      "Train Epoch: 2 [2400/6658 (36%)]\tLoss: 0.749109\n",
      "Train Epoch: 2 [2500/6658 (38%)]\tLoss: 0.173362\n",
      "Train Epoch: 2 [2600/6658 (39%)]\tLoss: 0.478569\n",
      "Train Epoch: 2 [2700/6658 (41%)]\tLoss: 0.129536\n",
      "Train Epoch: 2 [2800/6658 (42%)]\tLoss: 0.398047\n",
      "Train Epoch: 2 [2900/6658 (44%)]\tLoss: 0.561350\n",
      "Train Epoch: 2 [3000/6658 (45%)]\tLoss: 0.022127\n",
      "Train Epoch: 2 [3100/6658 (47%)]\tLoss: 0.059648\n",
      "Train Epoch: 2 [3200/6658 (48%)]\tLoss: 0.000330\n",
      "Train Epoch: 2 [3300/6658 (50%)]\tLoss: 0.310388\n",
      "Train Epoch: 2 [3400/6658 (51%)]\tLoss: 0.001697\n",
      "Train Epoch: 2 [3500/6658 (53%)]\tLoss: 0.022770\n",
      "Train Epoch: 2 [3600/6658 (54%)]\tLoss: 4.460075\n",
      "Train Epoch: 2 [3700/6658 (56%)]\tLoss: 1.753156\n",
      "Train Epoch: 2 [3800/6658 (57%)]\tLoss: 0.008470\n",
      "Train Epoch: 2 [3900/6658 (59%)]\tLoss: 0.037917\n",
      "Train Epoch: 2 [4000/6658 (60%)]\tLoss: 0.000721\n",
      "Train Epoch: 2 [4100/6658 (62%)]\tLoss: 1.328421\n",
      "Train Epoch: 2 [4200/6658 (63%)]\tLoss: 0.843361\n",
      "Train Epoch: 2 [4300/6658 (65%)]\tLoss: 0.970145\n",
      "Train Epoch: 2 [4400/6658 (66%)]\tLoss: 0.059483\n",
      "Train Epoch: 2 [4500/6658 (68%)]\tLoss: 0.451870\n",
      "Train Epoch: 2 [4600/6658 (69%)]\tLoss: 0.011291\n",
      "Train Epoch: 2 [4700/6658 (71%)]\tLoss: 0.034500\n",
      "Train Epoch: 2 [4800/6658 (72%)]\tLoss: 0.284789\n",
      "Train Epoch: 2 [4900/6658 (74%)]\tLoss: 0.063576\n",
      "Train Epoch: 2 [5000/6658 (75%)]\tLoss: 0.312819\n",
      "Train Epoch: 2 [5100/6658 (77%)]\tLoss: 0.777975\n",
      "Train Epoch: 2 [5200/6658 (78%)]\tLoss: 0.467787\n",
      "Train Epoch: 2 [5300/6658 (80%)]\tLoss: 0.271389\n",
      "Train Epoch: 2 [5400/6658 (81%)]\tLoss: 0.676449\n",
      "Train Epoch: 2 [5500/6658 (83%)]\tLoss: 5.056205\n",
      "Train Epoch: 2 [5600/6658 (84%)]\tLoss: 0.000011\n",
      "Train Epoch: 2 [5700/6658 (86%)]\tLoss: 0.127656\n",
      "Train Epoch: 2 [5800/6658 (87%)]\tLoss: 0.147183\n",
      "Train Epoch: 2 [5900/6658 (89%)]\tLoss: 6.451726\n",
      "Train Epoch: 2 [6000/6658 (90%)]\tLoss: 1.467362\n",
      "Train Epoch: 2 [6100/6658 (92%)]\tLoss: 0.174068\n",
      "Train Epoch: 2 [6200/6658 (93%)]\tLoss: 0.006083\n",
      "Train Epoch: 2 [6300/6658 (95%)]\tLoss: 0.055568\n",
      "Train Epoch: 2 [6400/6658 (96%)]\tLoss: 0.776166\n",
      "Train Epoch: 2 [6500/6658 (98%)]\tLoss: 0.013017\n",
      "Train Epoch: 2 [6600/6658 (99%)]\tLoss: 0.000629\n",
      "train loss average =  0.7980906078852917\n",
      "\n",
      "Test set: Average loss: 0.7532\n",
      "\n",
      "Validation loss decreased (0.783702 --> 0.753181).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.0004, 5.9989, 5.9959, 5.9992, 6.0075, 6.0075], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 3 [0/6658 (0%)]\tLoss: 0.925778\n",
      "Train Epoch: 3 [100/6658 (2%)]\tLoss: 0.816119\n",
      "Train Epoch: 3 [200/6658 (3%)]\tLoss: 0.264999\n",
      "Train Epoch: 3 [300/6658 (5%)]\tLoss: 1.944708\n",
      "Train Epoch: 3 [400/6658 (6%)]\tLoss: 0.270355\n",
      "Train Epoch: 3 [500/6658 (8%)]\tLoss: 0.169120\n",
      "Train Epoch: 3 [600/6658 (9%)]\tLoss: 2.137298\n",
      "Train Epoch: 3 [700/6658 (11%)]\tLoss: 0.043730\n",
      "Train Epoch: 3 [800/6658 (12%)]\tLoss: 2.170333\n",
      "Train Epoch: 3 [900/6658 (14%)]\tLoss: 0.195479\n",
      "Train Epoch: 3 [1000/6658 (15%)]\tLoss: 0.013475\n",
      "Train Epoch: 3 [1100/6658 (17%)]\tLoss: 0.298406\n",
      "Train Epoch: 3 [1200/6658 (18%)]\tLoss: 0.000607\n",
      "Train Epoch: 3 [1300/6658 (20%)]\tLoss: 0.245820\n",
      "Train Epoch: 3 [1400/6658 (21%)]\tLoss: 1.778502\n",
      "Train Epoch: 3 [1500/6658 (23%)]\tLoss: 0.065599\n",
      "Train Epoch: 3 [1600/6658 (24%)]\tLoss: 0.222495\n",
      "Train Epoch: 3 [1700/6658 (26%)]\tLoss: 2.150947\n",
      "Train Epoch: 3 [1800/6658 (27%)]\tLoss: 0.008178\n",
      "Train Epoch: 3 [1900/6658 (29%)]\tLoss: 0.041639\n",
      "Train Epoch: 3 [2000/6658 (30%)]\tLoss: 0.126084\n",
      "Train Epoch: 3 [2100/6658 (32%)]\tLoss: 0.481776\n",
      "Train Epoch: 3 [2200/6658 (33%)]\tLoss: 0.058516\n",
      "Train Epoch: 3 [2300/6658 (35%)]\tLoss: 0.898027\n",
      "Train Epoch: 3 [2400/6658 (36%)]\tLoss: 0.132836\n",
      "Train Epoch: 3 [2500/6658 (38%)]\tLoss: 0.029074\n",
      "Train Epoch: 3 [2600/6658 (39%)]\tLoss: 0.357690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [2700/6658 (41%)]\tLoss: 0.340682\n",
      "Train Epoch: 3 [2800/6658 (42%)]\tLoss: 0.636856\n",
      "Train Epoch: 3 [2900/6658 (44%)]\tLoss: 1.638823\n",
      "Train Epoch: 3 [3000/6658 (45%)]\tLoss: 0.012027\n",
      "Train Epoch: 3 [3100/6658 (47%)]\tLoss: 3.443660\n",
      "Train Epoch: 3 [3200/6658 (48%)]\tLoss: 0.197425\n",
      "Train Epoch: 3 [3300/6658 (50%)]\tLoss: 0.395010\n",
      "Train Epoch: 3 [3400/6658 (51%)]\tLoss: 0.101371\n",
      "Train Epoch: 3 [3500/6658 (53%)]\tLoss: 0.684132\n",
      "Train Epoch: 3 [3600/6658 (54%)]\tLoss: 0.230485\n",
      "Train Epoch: 3 [3700/6658 (56%)]\tLoss: 0.324638\n",
      "Train Epoch: 3 [3800/6658 (57%)]\tLoss: 1.929216\n",
      "Train Epoch: 3 [3900/6658 (59%)]\tLoss: 2.069705\n",
      "Train Epoch: 3 [4000/6658 (60%)]\tLoss: 0.303666\n",
      "Train Epoch: 3 [4100/6658 (62%)]\tLoss: 0.569543\n",
      "Train Epoch: 3 [4200/6658 (63%)]\tLoss: 0.083735\n",
      "Train Epoch: 3 [4300/6658 (65%)]\tLoss: 0.183292\n",
      "Train Epoch: 3 [4400/6658 (66%)]\tLoss: 0.280745\n",
      "Train Epoch: 3 [4500/6658 (68%)]\tLoss: 0.045669\n",
      "Train Epoch: 3 [4600/6658 (69%)]\tLoss: 0.476032\n",
      "Train Epoch: 3 [4700/6658 (71%)]\tLoss: 0.030351\n",
      "Train Epoch: 3 [4800/6658 (72%)]\tLoss: 0.571857\n",
      "Train Epoch: 3 [4900/6658 (74%)]\tLoss: 0.023076\n",
      "Train Epoch: 3 [5000/6658 (75%)]\tLoss: 0.186179\n",
      "Train Epoch: 3 [5100/6658 (77%)]\tLoss: 0.180746\n",
      "Train Epoch: 3 [5200/6658 (78%)]\tLoss: 0.083157\n",
      "Train Epoch: 3 [5300/6658 (80%)]\tLoss: 0.004406\n",
      "Train Epoch: 3 [5400/6658 (81%)]\tLoss: 10.956003\n",
      "Train Epoch: 3 [5500/6658 (83%)]\tLoss: 0.521613\n",
      "Train Epoch: 3 [5600/6658 (84%)]\tLoss: 0.331338\n",
      "Train Epoch: 3 [5700/6658 (86%)]\tLoss: 0.829380\n",
      "Train Epoch: 3 [5800/6658 (87%)]\tLoss: 0.210567\n",
      "Train Epoch: 3 [5900/6658 (89%)]\tLoss: 0.052966\n",
      "Train Epoch: 3 [6000/6658 (90%)]\tLoss: 0.034660\n",
      "Train Epoch: 3 [6100/6658 (92%)]\tLoss: 0.003737\n",
      "Train Epoch: 3 [6200/6658 (93%)]\tLoss: 0.101363\n",
      "Train Epoch: 3 [6300/6658 (95%)]\tLoss: 0.554310\n",
      "Train Epoch: 3 [6400/6658 (96%)]\tLoss: 0.403466\n",
      "Train Epoch: 3 [6500/6658 (98%)]\tLoss: 0.110879\n",
      "Train Epoch: 3 [6600/6658 (99%)]\tLoss: 13.286930\n",
      "train loss average =  0.7793898381686835\n",
      "\n",
      "Test set: Average loss: 0.7776\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0020, 5.9992, 5.9940, 5.9970, 6.0099, 6.0094], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 4 [0/6658 (0%)]\tLoss: 0.107419\n",
      "Train Epoch: 4 [100/6658 (2%)]\tLoss: 0.017592\n",
      "Train Epoch: 4 [200/6658 (3%)]\tLoss: 0.122976\n",
      "Train Epoch: 4 [300/6658 (5%)]\tLoss: 0.289771\n",
      "Train Epoch: 4 [400/6658 (6%)]\tLoss: 0.253116\n",
      "Train Epoch: 4 [500/6658 (8%)]\tLoss: 0.332146\n",
      "Train Epoch: 4 [600/6658 (9%)]\tLoss: 0.159182\n",
      "Train Epoch: 4 [700/6658 (11%)]\tLoss: 0.121853\n",
      "Train Epoch: 4 [800/6658 (12%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [900/6658 (14%)]\tLoss: 1.106182\n",
      "Train Epoch: 4 [1000/6658 (15%)]\tLoss: 0.610549\n",
      "Train Epoch: 4 [1100/6658 (17%)]\tLoss: 0.001625\n",
      "Train Epoch: 4 [1200/6658 (18%)]\tLoss: 7.134008\n",
      "Train Epoch: 4 [1300/6658 (20%)]\tLoss: 0.109309\n",
      "Train Epoch: 4 [1400/6658 (21%)]\tLoss: 0.207326\n",
      "Train Epoch: 4 [1500/6658 (23%)]\tLoss: 0.054936\n",
      "Train Epoch: 4 [1600/6658 (24%)]\tLoss: 0.484937\n",
      "Train Epoch: 4 [1700/6658 (26%)]\tLoss: 0.240219\n",
      "Train Epoch: 4 [1800/6658 (27%)]\tLoss: 0.125544\n",
      "Train Epoch: 4 [1900/6658 (29%)]\tLoss: 1.472822\n",
      "Train Epoch: 4 [2000/6658 (30%)]\tLoss: 0.017870\n",
      "Train Epoch: 4 [2100/6658 (32%)]\tLoss: 0.559432\n",
      "Train Epoch: 4 [2200/6658 (33%)]\tLoss: 0.131056\n",
      "Train Epoch: 4 [2300/6658 (35%)]\tLoss: 0.208979\n",
      "Train Epoch: 4 [2400/6658 (36%)]\tLoss: 0.021878\n",
      "Train Epoch: 4 [2500/6658 (38%)]\tLoss: 0.285219\n",
      "Train Epoch: 4 [2600/6658 (39%)]\tLoss: 0.012216\n",
      "Train Epoch: 4 [2700/6658 (41%)]\tLoss: 0.683461\n",
      "Train Epoch: 4 [2800/6658 (42%)]\tLoss: 0.113502\n",
      "Train Epoch: 4 [2900/6658 (44%)]\tLoss: 0.068493\n",
      "Train Epoch: 4 [3000/6658 (45%)]\tLoss: 0.843924\n",
      "Train Epoch: 4 [3100/6658 (47%)]\tLoss: 0.001960\n",
      "Train Epoch: 4 [3200/6658 (48%)]\tLoss: 0.027297\n",
      "Train Epoch: 4 [3300/6658 (50%)]\tLoss: 0.861549\n",
      "Train Epoch: 4 [3400/6658 (51%)]\tLoss: 0.224711\n",
      "Train Epoch: 4 [3500/6658 (53%)]\tLoss: 1.841510\n",
      "Train Epoch: 4 [3600/6658 (54%)]\tLoss: 0.304837\n",
      "Train Epoch: 4 [3700/6658 (56%)]\tLoss: 0.239371\n",
      "Train Epoch: 4 [3800/6658 (57%)]\tLoss: 0.019395\n",
      "Train Epoch: 4 [3900/6658 (59%)]\tLoss: 0.001726\n",
      "Train Epoch: 4 [4000/6658 (60%)]\tLoss: 0.020025\n",
      "Train Epoch: 4 [4100/6658 (62%)]\tLoss: 0.618967\n",
      "Train Epoch: 4 [4200/6658 (63%)]\tLoss: 0.000044\n",
      "Train Epoch: 4 [4300/6658 (65%)]\tLoss: 0.062944\n",
      "Train Epoch: 4 [4400/6658 (66%)]\tLoss: 0.132881\n",
      "Train Epoch: 4 [4500/6658 (68%)]\tLoss: 0.108854\n",
      "Train Epoch: 4 [4600/6658 (69%)]\tLoss: 2.999795\n",
      "Train Epoch: 4 [4700/6658 (71%)]\tLoss: 0.379240\n",
      "Train Epoch: 4 [4800/6658 (72%)]\tLoss: 0.055225\n",
      "Train Epoch: 4 [4900/6658 (74%)]\tLoss: 0.182905\n",
      "Train Epoch: 4 [5000/6658 (75%)]\tLoss: 0.091562\n",
      "Train Epoch: 4 [5100/6658 (77%)]\tLoss: 0.477268\n",
      "Train Epoch: 4 [5200/6658 (78%)]\tLoss: 2.097645\n",
      "Train Epoch: 4 [5300/6658 (80%)]\tLoss: 0.068862\n",
      "Train Epoch: 4 [5400/6658 (81%)]\tLoss: 0.398526\n",
      "Train Epoch: 4 [5500/6658 (83%)]\tLoss: 0.017425\n",
      "Train Epoch: 4 [5600/6658 (84%)]\tLoss: 0.016649\n",
      "Train Epoch: 4 [5700/6658 (86%)]\tLoss: 0.020903\n",
      "Train Epoch: 4 [5800/6658 (87%)]\tLoss: 0.321076\n",
      "Train Epoch: 4 [5900/6658 (89%)]\tLoss: 0.039896\n",
      "Train Epoch: 4 [6000/6658 (90%)]\tLoss: 0.679186\n",
      "Train Epoch: 4 [6100/6658 (92%)]\tLoss: 0.000311\n",
      "Train Epoch: 4 [6200/6658 (93%)]\tLoss: 0.000208\n",
      "Train Epoch: 4 [6300/6658 (95%)]\tLoss: 0.107210\n",
      "Train Epoch: 4 [6400/6658 (96%)]\tLoss: 0.317502\n",
      "Train Epoch: 4 [6500/6658 (98%)]\tLoss: 0.310427\n",
      "Train Epoch: 4 [6600/6658 (99%)]\tLoss: 0.013765\n",
      "train loss average =  0.7721711959665839\n",
      "\n",
      "Test set: Average loss: 0.7412\n",
      "\n",
      "Validation loss decreased (0.753181 --> 0.741185).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.0036, 6.0001, 5.9927, 5.9940, 6.0112, 6.0103], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 5 [0/6658 (0%)]\tLoss: 0.104587\n",
      "Train Epoch: 5 [100/6658 (2%)]\tLoss: 1.104497\n",
      "Train Epoch: 5 [200/6658 (3%)]\tLoss: 0.487434\n",
      "Train Epoch: 5 [300/6658 (5%)]\tLoss: 0.009195\n",
      "Train Epoch: 5 [400/6658 (6%)]\tLoss: 0.446253\n",
      "Train Epoch: 5 [500/6658 (8%)]\tLoss: 0.118386\n",
      "Train Epoch: 5 [600/6658 (9%)]\tLoss: 0.000685\n",
      "Train Epoch: 5 [700/6658 (11%)]\tLoss: 0.344479\n",
      "Train Epoch: 5 [800/6658 (12%)]\tLoss: 0.062376\n",
      "Train Epoch: 5 [900/6658 (14%)]\tLoss: 0.000353\n",
      "Train Epoch: 5 [1000/6658 (15%)]\tLoss: 1.435868\n",
      "Train Epoch: 5 [1100/6658 (17%)]\tLoss: 0.446098\n",
      "Train Epoch: 5 [1200/6658 (18%)]\tLoss: 0.516055\n",
      "Train Epoch: 5 [1300/6658 (20%)]\tLoss: 0.001489\n",
      "Train Epoch: 5 [1400/6658 (21%)]\tLoss: 2.403264\n",
      "Train Epoch: 5 [1500/6658 (23%)]\tLoss: 0.230904\n",
      "Train Epoch: 5 [1600/6658 (24%)]\tLoss: 0.308681\n",
      "Train Epoch: 5 [1700/6658 (26%)]\tLoss: 0.008926\n",
      "Train Epoch: 5 [1800/6658 (27%)]\tLoss: 0.267894\n",
      "Train Epoch: 5 [1900/6658 (29%)]\tLoss: 0.545527\n",
      "Train Epoch: 5 [2000/6658 (30%)]\tLoss: 0.001066\n",
      "Train Epoch: 5 [2100/6658 (32%)]\tLoss: 0.138541\n",
      "Train Epoch: 5 [2200/6658 (33%)]\tLoss: 0.501993\n",
      "Train Epoch: 5 [2300/6658 (35%)]\tLoss: 0.802211\n",
      "Train Epoch: 5 [2400/6658 (36%)]\tLoss: 0.101297\n",
      "Train Epoch: 5 [2500/6658 (38%)]\tLoss: 1.026073\n",
      "Train Epoch: 5 [2600/6658 (39%)]\tLoss: 2.400729\n",
      "Train Epoch: 5 [2700/6658 (41%)]\tLoss: 0.036660\n",
      "Train Epoch: 5 [2800/6658 (42%)]\tLoss: 0.948152\n",
      "Train Epoch: 5 [2900/6658 (44%)]\tLoss: 0.222925\n",
      "Train Epoch: 5 [3000/6658 (45%)]\tLoss: 0.823511\n",
      "Train Epoch: 5 [3100/6658 (47%)]\tLoss: 0.043713\n",
      "Train Epoch: 5 [3200/6658 (48%)]\tLoss: 0.230675\n",
      "Train Epoch: 5 [3300/6658 (50%)]\tLoss: 0.166164\n",
      "Train Epoch: 5 [3400/6658 (51%)]\tLoss: 0.144720\n",
      "Train Epoch: 5 [3500/6658 (53%)]\tLoss: 0.100912\n",
      "Train Epoch: 5 [3600/6658 (54%)]\tLoss: 0.092315\n",
      "Train Epoch: 5 [3700/6658 (56%)]\tLoss: 0.043955\n",
      "Train Epoch: 5 [3800/6658 (57%)]\tLoss: 0.831659\n",
      "Train Epoch: 5 [3900/6658 (59%)]\tLoss: 39.845699\n",
      "Train Epoch: 5 [4000/6658 (60%)]\tLoss: 0.249836\n",
      "Train Epoch: 5 [4100/6658 (62%)]\tLoss: 1.372003\n",
      "Train Epoch: 5 [4200/6658 (63%)]\tLoss: 0.740999\n",
      "Train Epoch: 5 [4300/6658 (65%)]\tLoss: 0.208180\n",
      "Train Epoch: 5 [4400/6658 (66%)]\tLoss: 0.000050\n",
      "Train Epoch: 5 [4500/6658 (68%)]\tLoss: 1.162159\n",
      "Train Epoch: 5 [4600/6658 (69%)]\tLoss: 0.419826\n",
      "Train Epoch: 5 [4700/6658 (71%)]\tLoss: 1.369105\n",
      "Train Epoch: 5 [4800/6658 (72%)]\tLoss: 2.574641\n",
      "Train Epoch: 5 [4900/6658 (74%)]\tLoss: 4.672603\n",
      "Train Epoch: 5 [5000/6658 (75%)]\tLoss: 0.837958\n",
      "Train Epoch: 5 [5100/6658 (77%)]\tLoss: 0.089172\n",
      "Train Epoch: 5 [5200/6658 (78%)]\tLoss: 0.002468\n",
      "Train Epoch: 5 [5300/6658 (80%)]\tLoss: 0.122471\n",
      "Train Epoch: 5 [5400/6658 (81%)]\tLoss: 0.359151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [5500/6658 (83%)]\tLoss: 0.050608\n",
      "Train Epoch: 5 [5600/6658 (84%)]\tLoss: 0.056397\n",
      "Train Epoch: 5 [5700/6658 (86%)]\tLoss: 0.589307\n",
      "Train Epoch: 5 [5800/6658 (87%)]\tLoss: 0.137523\n",
      "Train Epoch: 5 [5900/6658 (89%)]\tLoss: 5.882550\n",
      "Train Epoch: 5 [6000/6658 (90%)]\tLoss: 3.825533\n",
      "Train Epoch: 5 [6100/6658 (92%)]\tLoss: 0.416294\n",
      "Train Epoch: 5 [6200/6658 (93%)]\tLoss: 0.135205\n",
      "Train Epoch: 5 [6300/6658 (95%)]\tLoss: 2.962800\n",
      "Train Epoch: 5 [6400/6658 (96%)]\tLoss: 1.375264\n",
      "Train Epoch: 5 [6500/6658 (98%)]\tLoss: 0.504901\n",
      "Train Epoch: 5 [6600/6658 (99%)]\tLoss: 0.363050\n",
      "train loss average =  0.7690870785780153\n",
      "\n",
      "Test set: Average loss: 0.7344\n",
      "\n",
      "Validation loss decreased (0.741185 --> 0.734377).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.0055, 6.0004, 5.9910, 5.9915, 6.0126, 6.0111], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 6 [0/6658 (0%)]\tLoss: 3.174445\n",
      "Train Epoch: 6 [100/6658 (2%)]\tLoss: 0.003561\n",
      "Train Epoch: 6 [200/6658 (3%)]\tLoss: 3.454376\n",
      "Train Epoch: 6 [300/6658 (5%)]\tLoss: 2.271690\n",
      "Train Epoch: 6 [400/6658 (6%)]\tLoss: 0.289332\n",
      "Train Epoch: 6 [500/6658 (8%)]\tLoss: 0.294384\n",
      "Train Epoch: 6 [600/6658 (9%)]\tLoss: 0.006002\n",
      "Train Epoch: 6 [700/6658 (11%)]\tLoss: 0.054279\n",
      "Train Epoch: 6 [800/6658 (12%)]\tLoss: 2.552518\n",
      "Train Epoch: 6 [900/6658 (14%)]\tLoss: 0.324016\n",
      "Train Epoch: 6 [1000/6658 (15%)]\tLoss: 0.072908\n",
      "Train Epoch: 6 [1100/6658 (17%)]\tLoss: 0.099472\n",
      "Train Epoch: 6 [1200/6658 (18%)]\tLoss: 1.424013\n",
      "Train Epoch: 6 [1300/6658 (20%)]\tLoss: 1.441412\n",
      "Train Epoch: 6 [1400/6658 (21%)]\tLoss: 0.038911\n",
      "Train Epoch: 6 [1500/6658 (23%)]\tLoss: 0.347735\n",
      "Train Epoch: 6 [1600/6658 (24%)]\tLoss: 1.015711\n",
      "Train Epoch: 6 [1700/6658 (26%)]\tLoss: 0.446033\n",
      "Train Epoch: 6 [1800/6658 (27%)]\tLoss: 0.560246\n",
      "Train Epoch: 6 [1900/6658 (29%)]\tLoss: 2.553324\n",
      "Train Epoch: 6 [2000/6658 (30%)]\tLoss: 0.472602\n",
      "Train Epoch: 6 [2100/6658 (32%)]\tLoss: 0.496766\n",
      "Train Epoch: 6 [2200/6658 (33%)]\tLoss: 0.366556\n",
      "Train Epoch: 6 [2300/6658 (35%)]\tLoss: 0.071877\n",
      "Train Epoch: 6 [2400/6658 (36%)]\tLoss: 0.756732\n",
      "Train Epoch: 6 [2500/6658 (38%)]\tLoss: 0.146274\n",
      "Train Epoch: 6 [2600/6658 (39%)]\tLoss: 1.549838\n",
      "Train Epoch: 6 [2700/6658 (41%)]\tLoss: 0.005871\n",
      "Train Epoch: 6 [2800/6658 (42%)]\tLoss: 0.525548\n",
      "Train Epoch: 6 [2900/6658 (44%)]\tLoss: 0.000303\n",
      "Train Epoch: 6 [3000/6658 (45%)]\tLoss: 0.283296\n",
      "Train Epoch: 6 [3100/6658 (47%)]\tLoss: 1.121820\n",
      "Train Epoch: 6 [3200/6658 (48%)]\tLoss: 0.563723\n",
      "Train Epoch: 6 [3300/6658 (50%)]\tLoss: 0.264627\n",
      "Train Epoch: 6 [3400/6658 (51%)]\tLoss: 1.458923\n",
      "Train Epoch: 6 [3500/6658 (53%)]\tLoss: 0.158618\n",
      "Train Epoch: 6 [3600/6658 (54%)]\tLoss: 0.024349\n",
      "Train Epoch: 6 [3700/6658 (56%)]\tLoss: 0.312281\n",
      "Train Epoch: 6 [3800/6658 (57%)]\tLoss: 0.760701\n",
      "Train Epoch: 6 [3900/6658 (59%)]\tLoss: 1.922455\n",
      "Train Epoch: 6 [4000/6658 (60%)]\tLoss: 1.774380\n",
      "Train Epoch: 6 [4100/6658 (62%)]\tLoss: 7.001317\n",
      "Train Epoch: 6 [4200/6658 (63%)]\tLoss: 1.423712\n",
      "Train Epoch: 6 [4300/6658 (65%)]\tLoss: 0.133092\n",
      "Train Epoch: 6 [4400/6658 (66%)]\tLoss: 0.090341\n",
      "Train Epoch: 6 [4500/6658 (68%)]\tLoss: 1.198127\n",
      "Train Epoch: 6 [4600/6658 (69%)]\tLoss: 0.206547\n",
      "Train Epoch: 6 [4700/6658 (71%)]\tLoss: 0.000116\n",
      "Train Epoch: 6 [4800/6658 (72%)]\tLoss: 0.001943\n",
      "Train Epoch: 6 [4900/6658 (74%)]\tLoss: 0.082113\n",
      "Train Epoch: 6 [5000/6658 (75%)]\tLoss: 0.023844\n",
      "Train Epoch: 6 [5100/6658 (77%)]\tLoss: 0.092876\n",
      "Train Epoch: 6 [5200/6658 (78%)]\tLoss: 0.853520\n",
      "Train Epoch: 6 [5300/6658 (80%)]\tLoss: 0.065841\n",
      "Train Epoch: 6 [5400/6658 (81%)]\tLoss: 0.481491\n",
      "Train Epoch: 6 [5500/6658 (83%)]\tLoss: 0.475518\n",
      "Train Epoch: 6 [5600/6658 (84%)]\tLoss: 0.478445\n",
      "Train Epoch: 6 [5700/6658 (86%)]\tLoss: 0.478384\n",
      "Train Epoch: 6 [5800/6658 (87%)]\tLoss: 0.517223\n",
      "Train Epoch: 6 [5900/6658 (89%)]\tLoss: 0.017361\n",
      "Train Epoch: 6 [6000/6658 (90%)]\tLoss: 0.153714\n",
      "Train Epoch: 6 [6100/6658 (92%)]\tLoss: 1.024663\n",
      "Train Epoch: 6 [6200/6658 (93%)]\tLoss: 0.657482\n",
      "Train Epoch: 6 [6300/6658 (95%)]\tLoss: 0.164539\n",
      "Train Epoch: 6 [6400/6658 (96%)]\tLoss: 0.093294\n",
      "Train Epoch: 6 [6500/6658 (98%)]\tLoss: 0.312644\n",
      "Train Epoch: 6 [6600/6658 (99%)]\tLoss: 0.392434\n",
      "train loss average =  0.7663664109134021\n",
      "\n",
      "Test set: Average loss: 0.7517\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0087, 6.0008, 5.9890, 5.9891, 6.0141, 6.0119], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 7 [0/6658 (0%)]\tLoss: 1.133766\n",
      "Train Epoch: 7 [100/6658 (2%)]\tLoss: 0.690107\n",
      "Train Epoch: 7 [200/6658 (3%)]\tLoss: 0.749329\n",
      "Train Epoch: 7 [300/6658 (5%)]\tLoss: 1.057153\n",
      "Train Epoch: 7 [400/6658 (6%)]\tLoss: 1.703980\n",
      "Train Epoch: 7 [500/6658 (8%)]\tLoss: 0.001284\n",
      "Train Epoch: 7 [600/6658 (9%)]\tLoss: 0.148848\n",
      "Train Epoch: 7 [700/6658 (11%)]\tLoss: 0.948968\n",
      "Train Epoch: 7 [800/6658 (12%)]\tLoss: 0.041356\n",
      "Train Epoch: 7 [900/6658 (14%)]\tLoss: 0.457751\n",
      "Train Epoch: 7 [1000/6658 (15%)]\tLoss: 0.015868\n",
      "Train Epoch: 7 [1100/6658 (17%)]\tLoss: 0.000486\n",
      "Train Epoch: 7 [1200/6658 (18%)]\tLoss: 0.062280\n",
      "Train Epoch: 7 [1300/6658 (20%)]\tLoss: 0.188699\n",
      "Train Epoch: 7 [1400/6658 (21%)]\tLoss: 0.027566\n",
      "Train Epoch: 7 [1500/6658 (23%)]\tLoss: 0.352328\n",
      "Train Epoch: 7 [1600/6658 (24%)]\tLoss: 0.961768\n",
      "Train Epoch: 7 [1700/6658 (26%)]\tLoss: 1.013265\n",
      "Train Epoch: 7 [1800/6658 (27%)]\tLoss: 0.024300\n",
      "Train Epoch: 7 [1900/6658 (29%)]\tLoss: 10.931019\n",
      "Train Epoch: 7 [2000/6658 (30%)]\tLoss: 0.972003\n",
      "Train Epoch: 7 [2100/6658 (32%)]\tLoss: 3.026674\n",
      "Train Epoch: 7 [2200/6658 (33%)]\tLoss: 3.693602\n",
      "Train Epoch: 7 [2300/6658 (35%)]\tLoss: 0.707185\n",
      "Train Epoch: 7 [2400/6658 (36%)]\tLoss: 0.330467\n",
      "Train Epoch: 7 [2500/6658 (38%)]\tLoss: 0.889707\n",
      "Train Epoch: 7 [2600/6658 (39%)]\tLoss: 3.716278\n",
      "Train Epoch: 7 [2700/6658 (41%)]\tLoss: 0.442021\n",
      "Train Epoch: 7 [2800/6658 (42%)]\tLoss: 0.088385\n",
      "Train Epoch: 7 [2900/6658 (44%)]\tLoss: 0.017996\n",
      "Train Epoch: 7 [3000/6658 (45%)]\tLoss: 0.224654\n",
      "Train Epoch: 7 [3100/6658 (47%)]\tLoss: 1.491395\n",
      "Train Epoch: 7 [3200/6658 (48%)]\tLoss: 0.129675\n",
      "Train Epoch: 7 [3300/6658 (50%)]\tLoss: 2.692453\n",
      "Train Epoch: 7 [3400/6658 (51%)]\tLoss: 0.985540\n",
      "Train Epoch: 7 [3500/6658 (53%)]\tLoss: 0.543339\n",
      "Train Epoch: 7 [3600/6658 (54%)]\tLoss: 0.607485\n",
      "Train Epoch: 7 [3700/6658 (56%)]\tLoss: 0.262545\n",
      "Train Epoch: 7 [3800/6658 (57%)]\tLoss: 2.220245\n",
      "Train Epoch: 7 [3900/6658 (59%)]\tLoss: 0.047466\n",
      "Train Epoch: 7 [4000/6658 (60%)]\tLoss: 0.002550\n",
      "Train Epoch: 7 [4100/6658 (62%)]\tLoss: 0.000007\n",
      "Train Epoch: 7 [4200/6658 (63%)]\tLoss: 0.010870\n",
      "Train Epoch: 7 [4300/6658 (65%)]\tLoss: 0.214956\n",
      "Train Epoch: 7 [4400/6658 (66%)]\tLoss: 1.149242\n",
      "Train Epoch: 7 [4500/6658 (68%)]\tLoss: 2.921734\n",
      "Train Epoch: 7 [4600/6658 (69%)]\tLoss: 0.136423\n",
      "Train Epoch: 7 [4700/6658 (71%)]\tLoss: 0.089254\n",
      "Train Epoch: 7 [4800/6658 (72%)]\tLoss: 0.730391\n",
      "Train Epoch: 7 [4900/6658 (74%)]\tLoss: 0.014784\n",
      "Train Epoch: 7 [5000/6658 (75%)]\tLoss: 0.081545\n",
      "Train Epoch: 7 [5100/6658 (77%)]\tLoss: 0.103515\n",
      "Train Epoch: 7 [5200/6658 (78%)]\tLoss: 2.585651\n",
      "Train Epoch: 7 [5300/6658 (80%)]\tLoss: 0.706829\n",
      "Train Epoch: 7 [5400/6658 (81%)]\tLoss: 0.044108\n",
      "Train Epoch: 7 [5500/6658 (83%)]\tLoss: 0.729100\n",
      "Train Epoch: 7 [5600/6658 (84%)]\tLoss: 0.341700\n",
      "Train Epoch: 7 [5700/6658 (86%)]\tLoss: 0.028841\n",
      "Train Epoch: 7 [5800/6658 (87%)]\tLoss: 0.439608\n",
      "Train Epoch: 7 [5900/6658 (89%)]\tLoss: 0.147885\n",
      "Train Epoch: 7 [6000/6658 (90%)]\tLoss: 0.195463\n",
      "Train Epoch: 7 [6100/6658 (92%)]\tLoss: 1.005045\n",
      "Train Epoch: 7 [6200/6658 (93%)]\tLoss: 0.071939\n",
      "Train Epoch: 7 [6300/6658 (95%)]\tLoss: 0.030273\n",
      "Train Epoch: 7 [6400/6658 (96%)]\tLoss: 0.131121\n",
      "Train Epoch: 7 [6500/6658 (98%)]\tLoss: 0.044253\n",
      "Train Epoch: 7 [6600/6658 (99%)]\tLoss: 0.040476\n",
      "train loss average =  0.758838794084777\n",
      "\n",
      "Test set: Average loss: 0.7491\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0108, 6.0015, 5.9882, 5.9864, 6.0153, 6.0126], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 8 [0/6658 (0%)]\tLoss: 0.339559\n",
      "Train Epoch: 8 [100/6658 (2%)]\tLoss: 0.024899\n",
      "Train Epoch: 8 [200/6658 (3%)]\tLoss: 0.104396\n",
      "Train Epoch: 8 [300/6658 (5%)]\tLoss: 2.017734\n",
      "Train Epoch: 8 [400/6658 (6%)]\tLoss: 0.703261\n",
      "Train Epoch: 8 [500/6658 (8%)]\tLoss: 0.024912\n",
      "Train Epoch: 8 [600/6658 (9%)]\tLoss: 1.253200\n",
      "Train Epoch: 8 [700/6658 (11%)]\tLoss: 0.000023\n",
      "Train Epoch: 8 [800/6658 (12%)]\tLoss: 0.192994\n",
      "Train Epoch: 8 [900/6658 (14%)]\tLoss: 0.117484\n",
      "Train Epoch: 8 [1000/6658 (15%)]\tLoss: 0.289656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [1100/6658 (17%)]\tLoss: 0.768740\n",
      "Train Epoch: 8 [1200/6658 (18%)]\tLoss: 0.441208\n",
      "Train Epoch: 8 [1300/6658 (20%)]\tLoss: 0.106583\n",
      "Train Epoch: 8 [1400/6658 (21%)]\tLoss: 0.517191\n",
      "Train Epoch: 8 [1500/6658 (23%)]\tLoss: 0.624832\n",
      "Train Epoch: 8 [1600/6658 (24%)]\tLoss: 0.035671\n",
      "Train Epoch: 8 [1700/6658 (26%)]\tLoss: 2.923753\n",
      "Train Epoch: 8 [1800/6658 (27%)]\tLoss: 0.015587\n",
      "Train Epoch: 8 [1900/6658 (29%)]\tLoss: 0.623728\n",
      "Train Epoch: 8 [2000/6658 (30%)]\tLoss: 4.971010\n",
      "Train Epoch: 8 [2100/6658 (32%)]\tLoss: 0.629784\n",
      "Train Epoch: 8 [2200/6658 (33%)]\tLoss: 1.716340\n",
      "Train Epoch: 8 [2300/6658 (35%)]\tLoss: 0.406492\n",
      "Train Epoch: 8 [2400/6658 (36%)]\tLoss: 1.253078\n",
      "Train Epoch: 8 [2500/6658 (38%)]\tLoss: 0.876151\n",
      "Train Epoch: 8 [2600/6658 (39%)]\tLoss: 0.000907\n",
      "Train Epoch: 8 [2700/6658 (41%)]\tLoss: 0.361095\n",
      "Train Epoch: 8 [2800/6658 (42%)]\tLoss: 6.930704\n",
      "Train Epoch: 8 [2900/6658 (44%)]\tLoss: 0.128574\n",
      "Train Epoch: 8 [3000/6658 (45%)]\tLoss: 0.285632\n",
      "Train Epoch: 8 [3100/6658 (47%)]\tLoss: 0.096148\n",
      "Train Epoch: 8 [3200/6658 (48%)]\tLoss: 3.471541\n",
      "Train Epoch: 8 [3300/6658 (50%)]\tLoss: 0.164736\n",
      "Train Epoch: 8 [3400/6658 (51%)]\tLoss: 0.015817\n",
      "Train Epoch: 8 [3500/6658 (53%)]\tLoss: 0.034438\n",
      "Train Epoch: 8 [3600/6658 (54%)]\tLoss: 0.477025\n",
      "Train Epoch: 8 [3700/6658 (56%)]\tLoss: 3.532518\n",
      "Train Epoch: 8 [3800/6658 (57%)]\tLoss: 0.581373\n",
      "Train Epoch: 8 [3900/6658 (59%)]\tLoss: 1.300711\n",
      "Train Epoch: 8 [4000/6658 (60%)]\tLoss: 0.098339\n",
      "Train Epoch: 8 [4100/6658 (62%)]\tLoss: 0.500730\n",
      "Train Epoch: 8 [4200/6658 (63%)]\tLoss: 0.019853\n",
      "Train Epoch: 8 [4300/6658 (65%)]\tLoss: 1.268937\n",
      "Train Epoch: 8 [4400/6658 (66%)]\tLoss: 0.101329\n",
      "Train Epoch: 8 [4500/6658 (68%)]\tLoss: 0.258655\n",
      "Train Epoch: 8 [4600/6658 (69%)]\tLoss: 0.009353\n",
      "Train Epoch: 8 [4700/6658 (71%)]\tLoss: 0.015764\n",
      "Train Epoch: 8 [4800/6658 (72%)]\tLoss: 0.243058\n",
      "Train Epoch: 8 [4900/6658 (74%)]\tLoss: 0.020811\n",
      "Train Epoch: 8 [5000/6658 (75%)]\tLoss: 0.513426\n",
      "Train Epoch: 8 [5100/6658 (77%)]\tLoss: 0.488364\n",
      "Train Epoch: 8 [5200/6658 (78%)]\tLoss: 0.169388\n",
      "Train Epoch: 8 [5300/6658 (80%)]\tLoss: 0.554036\n",
      "Train Epoch: 8 [5400/6658 (81%)]\tLoss: 0.005394\n",
      "Train Epoch: 8 [5500/6658 (83%)]\tLoss: 0.222718\n",
      "Train Epoch: 8 [5600/6658 (84%)]\tLoss: 0.008655\n",
      "Train Epoch: 8 [5700/6658 (86%)]\tLoss: 0.000152\n",
      "Train Epoch: 8 [5800/6658 (87%)]\tLoss: 0.655919\n",
      "Train Epoch: 8 [5900/6658 (89%)]\tLoss: 0.019208\n",
      "Train Epoch: 8 [6000/6658 (90%)]\tLoss: 0.672918\n",
      "Train Epoch: 8 [6100/6658 (92%)]\tLoss: 0.027267\n",
      "Train Epoch: 8 [6200/6658 (93%)]\tLoss: 0.658191\n",
      "Train Epoch: 8 [6300/6658 (95%)]\tLoss: 0.046732\n",
      "Train Epoch: 8 [6400/6658 (96%)]\tLoss: 0.299739\n",
      "Train Epoch: 8 [6500/6658 (98%)]\tLoss: 0.095666\n",
      "Train Epoch: 8 [6600/6658 (99%)]\tLoss: 1.326616\n",
      "train loss average =  0.7556002302571865\n",
      "\n",
      "Test set: Average loss: 0.7345\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0126, 6.0029, 5.9878, 5.9843, 6.0168, 6.0135], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 9 [0/6658 (0%)]\tLoss: 0.000388\n",
      "Train Epoch: 9 [100/6658 (2%)]\tLoss: 0.004747\n",
      "Train Epoch: 9 [200/6658 (3%)]\tLoss: 0.728513\n",
      "Train Epoch: 9 [300/6658 (5%)]\tLoss: 0.825919\n",
      "Train Epoch: 9 [400/6658 (6%)]\tLoss: 1.447533\n",
      "Train Epoch: 9 [500/6658 (8%)]\tLoss: 0.273527\n",
      "Train Epoch: 9 [600/6658 (9%)]\tLoss: 0.931226\n",
      "Train Epoch: 9 [700/6658 (11%)]\tLoss: 1.034333\n",
      "Train Epoch: 9 [800/6658 (12%)]\tLoss: 8.372964\n",
      "Train Epoch: 9 [900/6658 (14%)]\tLoss: 0.097965\n",
      "Train Epoch: 9 [1000/6658 (15%)]\tLoss: 0.120479\n",
      "Train Epoch: 9 [1100/6658 (17%)]\tLoss: 0.609712\n",
      "Train Epoch: 9 [1200/6658 (18%)]\tLoss: 0.526373\n",
      "Train Epoch: 9 [1300/6658 (20%)]\tLoss: 0.008735\n",
      "Train Epoch: 9 [1400/6658 (21%)]\tLoss: 1.264141\n",
      "Train Epoch: 9 [1500/6658 (23%)]\tLoss: 1.143594\n",
      "Train Epoch: 9 [1600/6658 (24%)]\tLoss: 2.012422\n",
      "Train Epoch: 9 [1700/6658 (26%)]\tLoss: 0.121242\n",
      "Train Epoch: 9 [1800/6658 (27%)]\tLoss: 0.133114\n",
      "Train Epoch: 9 [1900/6658 (29%)]\tLoss: 0.017783\n",
      "Train Epoch: 9 [2000/6658 (30%)]\tLoss: 0.234573\n",
      "Train Epoch: 9 [2100/6658 (32%)]\tLoss: 0.000006\n",
      "Train Epoch: 9 [2200/6658 (33%)]\tLoss: 0.250124\n",
      "Train Epoch: 9 [2300/6658 (35%)]\tLoss: 0.410714\n",
      "Train Epoch: 9 [2400/6658 (36%)]\tLoss: 0.067043\n",
      "Train Epoch: 9 [2500/6658 (38%)]\tLoss: 0.687833\n",
      "Train Epoch: 9 [2600/6658 (39%)]\tLoss: 0.227560\n",
      "Train Epoch: 9 [2700/6658 (41%)]\tLoss: 0.847642\n",
      "Train Epoch: 9 [2800/6658 (42%)]\tLoss: 0.248907\n",
      "Train Epoch: 9 [2900/6658 (44%)]\tLoss: 0.028972\n",
      "Train Epoch: 9 [3000/6658 (45%)]\tLoss: 0.182460\n",
      "Train Epoch: 9 [3100/6658 (47%)]\tLoss: 0.274454\n",
      "Train Epoch: 9 [3200/6658 (48%)]\tLoss: 0.049067\n",
      "Train Epoch: 9 [3300/6658 (50%)]\tLoss: 0.744053\n",
      "Train Epoch: 9 [3400/6658 (51%)]\tLoss: 1.431071\n",
      "Train Epoch: 9 [3500/6658 (53%)]\tLoss: 2.823620\n",
      "Train Epoch: 9 [3600/6658 (54%)]\tLoss: 0.855383\n",
      "Train Epoch: 9 [3700/6658 (56%)]\tLoss: 0.151632\n",
      "Train Epoch: 9 [3800/6658 (57%)]\tLoss: 0.001526\n",
      "Train Epoch: 9 [3900/6658 (59%)]\tLoss: 0.166907\n",
      "Train Epoch: 9 [4000/6658 (60%)]\tLoss: 0.438312\n",
      "Train Epoch: 9 [4100/6658 (62%)]\tLoss: 0.068791\n",
      "Train Epoch: 9 [4200/6658 (63%)]\tLoss: 0.000375\n",
      "Train Epoch: 9 [4300/6658 (65%)]\tLoss: 0.056198\n",
      "Train Epoch: 9 [4400/6658 (66%)]\tLoss: 0.111777\n",
      "Train Epoch: 9 [4500/6658 (68%)]\tLoss: 0.015721\n",
      "Train Epoch: 9 [4600/6658 (69%)]\tLoss: 0.106909\n",
      "Train Epoch: 9 [4700/6658 (71%)]\tLoss: 0.001288\n",
      "Train Epoch: 9 [4800/6658 (72%)]\tLoss: 0.002278\n",
      "Train Epoch: 9 [4900/6658 (74%)]\tLoss: 0.083598\n",
      "Train Epoch: 9 [5000/6658 (75%)]\tLoss: 0.114203\n",
      "Train Epoch: 9 [5100/6658 (77%)]\tLoss: 0.918346\n",
      "Train Epoch: 9 [5200/6658 (78%)]\tLoss: 0.466740\n",
      "Train Epoch: 9 [5300/6658 (80%)]\tLoss: 0.072535\n",
      "Train Epoch: 9 [5400/6658 (81%)]\tLoss: 0.543186\n",
      "Train Epoch: 9 [5500/6658 (83%)]\tLoss: 0.090150\n",
      "Train Epoch: 9 [5600/6658 (84%)]\tLoss: 0.368762\n",
      "Train Epoch: 9 [5700/6658 (86%)]\tLoss: 0.083870\n",
      "Train Epoch: 9 [5800/6658 (87%)]\tLoss: 0.008706\n",
      "Train Epoch: 9 [5900/6658 (89%)]\tLoss: 0.164434\n",
      "Train Epoch: 9 [6000/6658 (90%)]\tLoss: 2.796394\n",
      "Train Epoch: 9 [6100/6658 (92%)]\tLoss: 5.347868\n",
      "Train Epoch: 9 [6200/6658 (93%)]\tLoss: 2.222471\n",
      "Train Epoch: 9 [6300/6658 (95%)]\tLoss: 4.162019\n",
      "Train Epoch: 9 [6400/6658 (96%)]\tLoss: 0.021202\n",
      "Train Epoch: 9 [6500/6658 (98%)]\tLoss: 3.134854\n",
      "Train Epoch: 9 [6600/6658 (99%)]\tLoss: 0.129197\n",
      "train loss average =  0.754171433755797\n",
      "\n",
      "Test set: Average loss: 0.7242\n",
      "\n",
      "Validation loss decreased (0.734377 --> 0.724164).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.0150, 6.0035, 5.9867, 5.9824, 6.0175, 6.0140], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 10 [0/6658 (0%)]\tLoss: 0.276760\n",
      "Train Epoch: 10 [100/6658 (2%)]\tLoss: 0.595951\n",
      "Train Epoch: 10 [200/6658 (3%)]\tLoss: 0.033400\n",
      "Train Epoch: 10 [300/6658 (5%)]\tLoss: 0.060911\n",
      "Train Epoch: 10 [400/6658 (6%)]\tLoss: 1.788592\n",
      "Train Epoch: 10 [500/6658 (8%)]\tLoss: 0.327422\n",
      "Train Epoch: 10 [600/6658 (9%)]\tLoss: 2.140634\n",
      "Train Epoch: 10 [700/6658 (11%)]\tLoss: 0.273696\n",
      "Train Epoch: 10 [800/6658 (12%)]\tLoss: 0.066435\n",
      "Train Epoch: 10 [900/6658 (14%)]\tLoss: 0.310645\n",
      "Train Epoch: 10 [1000/6658 (15%)]\tLoss: 0.004477\n",
      "Train Epoch: 10 [1100/6658 (17%)]\tLoss: 0.328813\n",
      "Train Epoch: 10 [1200/6658 (18%)]\tLoss: 0.093245\n",
      "Train Epoch: 10 [1300/6658 (20%)]\tLoss: 0.006367\n",
      "Train Epoch: 10 [1400/6658 (21%)]\tLoss: 1.252754\n",
      "Train Epoch: 10 [1500/6658 (23%)]\tLoss: 0.584575\n",
      "Train Epoch: 10 [1600/6658 (24%)]\tLoss: 0.442751\n",
      "Train Epoch: 10 [1700/6658 (26%)]\tLoss: 0.269075\n",
      "Train Epoch: 10 [1800/6658 (27%)]\tLoss: 0.281933\n",
      "Train Epoch: 10 [1900/6658 (29%)]\tLoss: 0.002417\n",
      "Train Epoch: 10 [2000/6658 (30%)]\tLoss: 0.434428\n",
      "Train Epoch: 10 [2100/6658 (32%)]\tLoss: 1.820687\n",
      "Train Epoch: 10 [2200/6658 (33%)]\tLoss: 1.233888\n",
      "Train Epoch: 10 [2300/6658 (35%)]\tLoss: 0.387985\n",
      "Train Epoch: 10 [2400/6658 (36%)]\tLoss: 1.648231\n",
      "Train Epoch: 10 [2500/6658 (38%)]\tLoss: 0.005325\n",
      "Train Epoch: 10 [2600/6658 (39%)]\tLoss: 1.584767\n",
      "Train Epoch: 10 [2700/6658 (41%)]\tLoss: 4.393173\n",
      "Train Epoch: 10 [2800/6658 (42%)]\tLoss: 0.392025\n",
      "Train Epoch: 10 [2900/6658 (44%)]\tLoss: 0.003073\n",
      "Train Epoch: 10 [3000/6658 (45%)]\tLoss: 1.264210\n",
      "Train Epoch: 10 [3100/6658 (47%)]\tLoss: 0.192975\n",
      "Train Epoch: 10 [3200/6658 (48%)]\tLoss: 1.090812\n",
      "Train Epoch: 10 [3300/6658 (50%)]\tLoss: 0.199106\n",
      "Train Epoch: 10 [3400/6658 (51%)]\tLoss: 0.240610\n",
      "Train Epoch: 10 [3500/6658 (53%)]\tLoss: 0.103408\n",
      "Train Epoch: 10 [3600/6658 (54%)]\tLoss: 0.348737\n",
      "Train Epoch: 10 [3700/6658 (56%)]\tLoss: 0.610121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [3800/6658 (57%)]\tLoss: 0.278035\n",
      "Train Epoch: 10 [3900/6658 (59%)]\tLoss: 0.140748\n",
      "Train Epoch: 10 [4000/6658 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [4100/6658 (62%)]\tLoss: 0.034320\n",
      "Train Epoch: 10 [4200/6658 (63%)]\tLoss: 0.101736\n",
      "Train Epoch: 10 [4300/6658 (65%)]\tLoss: 0.079601\n",
      "Train Epoch: 10 [4400/6658 (66%)]\tLoss: 1.839964\n",
      "Train Epoch: 10 [4500/6658 (68%)]\tLoss: 0.115404\n",
      "Train Epoch: 10 [4600/6658 (69%)]\tLoss: 0.077935\n",
      "Train Epoch: 10 [4700/6658 (71%)]\tLoss: 0.283223\n",
      "Train Epoch: 10 [4800/6658 (72%)]\tLoss: 0.325487\n",
      "Train Epoch: 10 [4900/6658 (74%)]\tLoss: 0.184435\n",
      "Train Epoch: 10 [5000/6658 (75%)]\tLoss: 1.678139\n",
      "Train Epoch: 10 [5100/6658 (77%)]\tLoss: 0.861652\n",
      "Train Epoch: 10 [5200/6658 (78%)]\tLoss: 0.748976\n",
      "Train Epoch: 10 [5300/6658 (80%)]\tLoss: 0.169916\n",
      "Train Epoch: 10 [5400/6658 (81%)]\tLoss: 0.001432\n",
      "Train Epoch: 10 [5500/6658 (83%)]\tLoss: 0.111823\n",
      "Train Epoch: 10 [5600/6658 (84%)]\tLoss: 0.000878\n",
      "Train Epoch: 10 [5700/6658 (86%)]\tLoss: 1.755130\n",
      "Train Epoch: 10 [5800/6658 (87%)]\tLoss: 0.057433\n",
      "Train Epoch: 10 [5900/6658 (89%)]\tLoss: 0.000591\n",
      "Train Epoch: 10 [6000/6658 (90%)]\tLoss: 0.429562\n",
      "Train Epoch: 10 [6100/6658 (92%)]\tLoss: 0.471937\n",
      "Train Epoch: 10 [6200/6658 (93%)]\tLoss: 0.929657\n",
      "Train Epoch: 10 [6300/6658 (95%)]\tLoss: 0.109210\n",
      "Train Epoch: 10 [6400/6658 (96%)]\tLoss: 0.201342\n",
      "Train Epoch: 10 [6500/6658 (98%)]\tLoss: 0.329708\n",
      "Train Epoch: 10 [6600/6658 (99%)]\tLoss: 0.000204\n",
      "train loss average =  0.7548352175990132\n",
      "\n",
      "Test set: Average loss: 0.7258\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0177, 6.0041, 5.9864, 5.9801, 6.0190, 6.0147], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 11 [0/6658 (0%)]\tLoss: 3.875818\n",
      "Train Epoch: 11 [100/6658 (2%)]\tLoss: 0.141559\n",
      "Train Epoch: 11 [200/6658 (3%)]\tLoss: 0.045121\n",
      "Train Epoch: 11 [300/6658 (5%)]\tLoss: 0.000164\n",
      "Train Epoch: 11 [400/6658 (6%)]\tLoss: 3.686269\n",
      "Train Epoch: 11 [500/6658 (8%)]\tLoss: 0.224717\n",
      "Train Epoch: 11 [600/6658 (9%)]\tLoss: 0.324139\n",
      "Train Epoch: 11 [700/6658 (11%)]\tLoss: 0.686751\n",
      "Train Epoch: 11 [800/6658 (12%)]\tLoss: 0.087889\n",
      "Train Epoch: 11 [900/6658 (14%)]\tLoss: 0.140170\n",
      "Train Epoch: 11 [1000/6658 (15%)]\tLoss: 2.356262\n",
      "Train Epoch: 11 [1100/6658 (17%)]\tLoss: 0.101634\n",
      "Train Epoch: 11 [1200/6658 (18%)]\tLoss: 0.413821\n",
      "Train Epoch: 11 [1300/6658 (20%)]\tLoss: 0.381759\n",
      "Train Epoch: 11 [1400/6658 (21%)]\tLoss: 0.001023\n",
      "Train Epoch: 11 [1500/6658 (23%)]\tLoss: 0.174879\n",
      "Train Epoch: 11 [1600/6658 (24%)]\tLoss: 0.374217\n",
      "Train Epoch: 11 [1700/6658 (26%)]\tLoss: 0.566502\n",
      "Train Epoch: 11 [1800/6658 (27%)]\tLoss: 0.077437\n",
      "Train Epoch: 11 [1900/6658 (29%)]\tLoss: 0.029260\n",
      "Train Epoch: 11 [2000/6658 (30%)]\tLoss: 0.688727\n",
      "Train Epoch: 11 [2100/6658 (32%)]\tLoss: 0.644824\n",
      "Train Epoch: 11 [2200/6658 (33%)]\tLoss: 1.167821\n",
      "Train Epoch: 11 [2300/6658 (35%)]\tLoss: 0.042487\n",
      "Train Epoch: 11 [2400/6658 (36%)]\tLoss: 0.311577\n",
      "Train Epoch: 11 [2500/6658 (38%)]\tLoss: 0.020242\n",
      "Train Epoch: 11 [2600/6658 (39%)]\tLoss: 8.733683\n",
      "Train Epoch: 11 [2700/6658 (41%)]\tLoss: 0.018664\n",
      "Train Epoch: 11 [2800/6658 (42%)]\tLoss: 0.119181\n",
      "Train Epoch: 11 [2900/6658 (44%)]\tLoss: 0.057882\n",
      "Train Epoch: 11 [3000/6658 (45%)]\tLoss: 0.006490\n",
      "Train Epoch: 11 [3100/6658 (47%)]\tLoss: 0.127409\n",
      "Train Epoch: 11 [3200/6658 (48%)]\tLoss: 1.981878\n",
      "Train Epoch: 11 [3300/6658 (50%)]\tLoss: 0.024801\n",
      "Train Epoch: 11 [3400/6658 (51%)]\tLoss: 0.060834\n",
      "Train Epoch: 11 [3500/6658 (53%)]\tLoss: 0.010798\n",
      "Train Epoch: 11 [3600/6658 (54%)]\tLoss: 1.067698\n",
      "Train Epoch: 11 [3700/6658 (56%)]\tLoss: 0.704700\n",
      "Train Epoch: 11 [3800/6658 (57%)]\tLoss: 5.178533\n",
      "Train Epoch: 11 [3900/6658 (59%)]\tLoss: 0.032368\n",
      "Train Epoch: 11 [4000/6658 (60%)]\tLoss: 0.348853\n",
      "Train Epoch: 11 [4100/6658 (62%)]\tLoss: 0.036960\n",
      "Train Epoch: 11 [4200/6658 (63%)]\tLoss: 0.149676\n",
      "Train Epoch: 11 [4300/6658 (65%)]\tLoss: 0.005051\n",
      "Train Epoch: 11 [4400/6658 (66%)]\tLoss: 0.881966\n",
      "Train Epoch: 11 [4500/6658 (68%)]\tLoss: 0.071860\n",
      "Train Epoch: 11 [4600/6658 (69%)]\tLoss: 0.561262\n",
      "Train Epoch: 11 [4700/6658 (71%)]\tLoss: 0.000835\n",
      "Train Epoch: 11 [4800/6658 (72%)]\tLoss: 0.002297\n",
      "Train Epoch: 11 [4900/6658 (74%)]\tLoss: 0.523325\n",
      "Train Epoch: 11 [5000/6658 (75%)]\tLoss: 0.057333\n",
      "Train Epoch: 11 [5100/6658 (77%)]\tLoss: 0.456430\n",
      "Train Epoch: 11 [5200/6658 (78%)]\tLoss: 0.980970\n",
      "Train Epoch: 11 [5300/6658 (80%)]\tLoss: 0.184364\n",
      "Train Epoch: 11 [5400/6658 (81%)]\tLoss: 0.353654\n",
      "Train Epoch: 11 [5500/6658 (83%)]\tLoss: 0.167634\n",
      "Train Epoch: 11 [5600/6658 (84%)]\tLoss: 1.371729\n",
      "Train Epoch: 11 [5700/6658 (86%)]\tLoss: 0.043198\n",
      "Train Epoch: 11 [5800/6658 (87%)]\tLoss: 0.165449\n",
      "Train Epoch: 11 [5900/6658 (89%)]\tLoss: 0.476979\n",
      "Train Epoch: 11 [6000/6658 (90%)]\tLoss: 0.009223\n",
      "Train Epoch: 11 [6100/6658 (92%)]\tLoss: 0.425072\n",
      "Train Epoch: 11 [6200/6658 (93%)]\tLoss: 0.178363\n",
      "Train Epoch: 11 [6300/6658 (95%)]\tLoss: 0.172749\n",
      "Train Epoch: 11 [6400/6658 (96%)]\tLoss: 0.006676\n",
      "Train Epoch: 11 [6500/6658 (98%)]\tLoss: 0.815595\n",
      "Train Epoch: 11 [6600/6658 (99%)]\tLoss: 0.366215\n",
      "train loss average =  0.7533979883659956\n",
      "\n",
      "Test set: Average loss: 0.7240\n",
      "\n",
      "Validation loss decreased (0.724164 --> 0.723979).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.0194, 6.0050, 5.9855, 5.9776, 6.0202, 6.0152], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 12 [0/6658 (0%)]\tLoss: 0.038452\n",
      "Train Epoch: 12 [100/6658 (2%)]\tLoss: 0.000015\n",
      "Train Epoch: 12 [200/6658 (3%)]\tLoss: 0.000430\n",
      "Train Epoch: 12 [300/6658 (5%)]\tLoss: 0.832539\n",
      "Train Epoch: 12 [400/6658 (6%)]\tLoss: 0.121103\n",
      "Train Epoch: 12 [500/6658 (8%)]\tLoss: 0.092119\n",
      "Train Epoch: 12 [600/6658 (9%)]\tLoss: 0.062150\n",
      "Train Epoch: 12 [700/6658 (11%)]\tLoss: 0.172135\n",
      "Train Epoch: 12 [800/6658 (12%)]\tLoss: 1.510115\n",
      "Train Epoch: 12 [900/6658 (14%)]\tLoss: 2.827505\n",
      "Train Epoch: 12 [1000/6658 (15%)]\tLoss: 0.287300\n",
      "Train Epoch: 12 [1100/6658 (17%)]\tLoss: 0.014089\n",
      "Train Epoch: 12 [1200/6658 (18%)]\tLoss: 1.268294\n",
      "Train Epoch: 12 [1300/6658 (20%)]\tLoss: 0.008538\n",
      "Train Epoch: 12 [1400/6658 (21%)]\tLoss: 0.495134\n",
      "Train Epoch: 12 [1500/6658 (23%)]\tLoss: 0.068193\n",
      "Train Epoch: 12 [1600/6658 (24%)]\tLoss: 0.483363\n",
      "Train Epoch: 12 [1700/6658 (26%)]\tLoss: 0.235767\n",
      "Train Epoch: 12 [1800/6658 (27%)]\tLoss: 0.160301\n",
      "Train Epoch: 12 [1900/6658 (29%)]\tLoss: 0.024074\n",
      "Train Epoch: 12 [2000/6658 (30%)]\tLoss: 7.129168\n",
      "Train Epoch: 12 [2100/6658 (32%)]\tLoss: 0.063402\n",
      "Train Epoch: 12 [2200/6658 (33%)]\tLoss: 0.821360\n",
      "Train Epoch: 12 [2300/6658 (35%)]\tLoss: 1.149337\n",
      "Train Epoch: 12 [2400/6658 (36%)]\tLoss: 3.808822\n",
      "Train Epoch: 12 [2500/6658 (38%)]\tLoss: 1.485119\n",
      "Train Epoch: 12 [2600/6658 (39%)]\tLoss: 0.008545\n",
      "Train Epoch: 12 [2700/6658 (41%)]\tLoss: 0.305446\n",
      "Train Epoch: 12 [2800/6658 (42%)]\tLoss: 0.184112\n",
      "Train Epoch: 12 [2900/6658 (44%)]\tLoss: 0.764660\n",
      "Train Epoch: 12 [3000/6658 (45%)]\tLoss: 0.050521\n",
      "Train Epoch: 12 [3100/6658 (47%)]\tLoss: 0.447806\n",
      "Train Epoch: 12 [3200/6658 (48%)]\tLoss: 2.002095\n",
      "Train Epoch: 12 [3300/6658 (50%)]\tLoss: 1.378404\n",
      "Train Epoch: 12 [3400/6658 (51%)]\tLoss: 0.020202\n",
      "Train Epoch: 12 [3500/6658 (53%)]\tLoss: 0.517873\n",
      "Train Epoch: 12 [3600/6658 (54%)]\tLoss: 0.612337\n",
      "Train Epoch: 12 [3700/6658 (56%)]\tLoss: 0.008152\n",
      "Train Epoch: 12 [3800/6658 (57%)]\tLoss: 0.624177\n",
      "Train Epoch: 12 [3900/6658 (59%)]\tLoss: 0.058292\n",
      "Train Epoch: 12 [4000/6658 (60%)]\tLoss: 0.368690\n",
      "Train Epoch: 12 [4100/6658 (62%)]\tLoss: 0.266232\n",
      "Train Epoch: 12 [4200/6658 (63%)]\tLoss: 1.100765\n",
      "Train Epoch: 12 [4300/6658 (65%)]\tLoss: 1.218611\n",
      "Train Epoch: 12 [4400/6658 (66%)]\tLoss: 0.036859\n",
      "Train Epoch: 12 [4500/6658 (68%)]\tLoss: 0.010787\n",
      "Train Epoch: 12 [4600/6658 (69%)]\tLoss: 0.145178\n",
      "Train Epoch: 12 [4700/6658 (71%)]\tLoss: 0.174692\n",
      "Train Epoch: 12 [4800/6658 (72%)]\tLoss: 0.032351\n",
      "Train Epoch: 12 [4900/6658 (74%)]\tLoss: 5.765754\n",
      "Train Epoch: 12 [5000/6658 (75%)]\tLoss: 3.183648\n",
      "Train Epoch: 12 [5100/6658 (77%)]\tLoss: 0.088886\n",
      "Train Epoch: 12 [5200/6658 (78%)]\tLoss: 1.120820\n",
      "Train Epoch: 12 [5300/6658 (80%)]\tLoss: 0.766232\n",
      "Train Epoch: 12 [5400/6658 (81%)]\tLoss: 0.487451\n",
      "Train Epoch: 12 [5500/6658 (83%)]\tLoss: 0.485601\n",
      "Train Epoch: 12 [5600/6658 (84%)]\tLoss: 0.822108\n",
      "Train Epoch: 12 [5700/6658 (86%)]\tLoss: 1.087662\n",
      "Train Epoch: 12 [5800/6658 (87%)]\tLoss: 0.004032\n",
      "Train Epoch: 12 [5900/6658 (89%)]\tLoss: 0.686790\n",
      "Train Epoch: 12 [6000/6658 (90%)]\tLoss: 0.038685\n",
      "Train Epoch: 12 [6100/6658 (92%)]\tLoss: 2.856908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [6200/6658 (93%)]\tLoss: 0.998221\n",
      "Train Epoch: 12 [6300/6658 (95%)]\tLoss: 0.001914\n",
      "Train Epoch: 12 [6400/6658 (96%)]\tLoss: 1.333871\n",
      "Train Epoch: 12 [6500/6658 (98%)]\tLoss: 0.351436\n",
      "Train Epoch: 12 [6600/6658 (99%)]\tLoss: 0.201800\n",
      "train loss average =  0.7532454205643252\n",
      "\n",
      "Test set: Average loss: 0.7263\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0216, 6.0059, 5.9840, 5.9757, 6.0212, 6.0161], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 13 [0/6658 (0%)]\tLoss: 0.223785\n",
      "Train Epoch: 13 [100/6658 (2%)]\tLoss: 0.142767\n",
      "Train Epoch: 13 [200/6658 (3%)]\tLoss: 1.598811\n",
      "Train Epoch: 13 [300/6658 (5%)]\tLoss: 0.003750\n",
      "Train Epoch: 13 [400/6658 (6%)]\tLoss: 2.251429\n",
      "Train Epoch: 13 [500/6658 (8%)]\tLoss: 0.557668\n",
      "Train Epoch: 13 [600/6658 (9%)]\tLoss: 0.000001\n",
      "Train Epoch: 13 [700/6658 (11%)]\tLoss: 0.577411\n",
      "Train Epoch: 13 [800/6658 (12%)]\tLoss: 0.014960\n",
      "Train Epoch: 13 [900/6658 (14%)]\tLoss: 0.011061\n",
      "Train Epoch: 13 [1000/6658 (15%)]\tLoss: 1.325270\n",
      "Train Epoch: 13 [1100/6658 (17%)]\tLoss: 0.140172\n",
      "Train Epoch: 13 [1200/6658 (18%)]\tLoss: 0.362756\n",
      "Train Epoch: 13 [1300/6658 (20%)]\tLoss: 0.082799\n",
      "Train Epoch: 13 [1400/6658 (21%)]\tLoss: 7.871065\n",
      "Train Epoch: 13 [1500/6658 (23%)]\tLoss: 3.443797\n",
      "Train Epoch: 13 [1600/6658 (24%)]\tLoss: 0.216382\n",
      "Train Epoch: 13 [1700/6658 (26%)]\tLoss: 0.385137\n",
      "Train Epoch: 13 [1800/6658 (27%)]\tLoss: 0.261149\n",
      "Train Epoch: 13 [1900/6658 (29%)]\tLoss: 0.001527\n",
      "Train Epoch: 13 [2000/6658 (30%)]\tLoss: 0.507864\n",
      "Train Epoch: 13 [2100/6658 (32%)]\tLoss: 0.180713\n",
      "Train Epoch: 13 [2200/6658 (33%)]\tLoss: 0.062933\n",
      "Train Epoch: 13 [2300/6658 (35%)]\tLoss: 0.236326\n",
      "Train Epoch: 13 [2400/6658 (36%)]\tLoss: 0.000082\n",
      "Train Epoch: 13 [2500/6658 (38%)]\tLoss: 0.282495\n",
      "Train Epoch: 13 [2600/6658 (39%)]\tLoss: 1.360799\n",
      "Train Epoch: 13 [2700/6658 (41%)]\tLoss: 0.150514\n",
      "Train Epoch: 13 [2800/6658 (42%)]\tLoss: 0.024191\n",
      "Train Epoch: 13 [2900/6658 (44%)]\tLoss: 0.332542\n",
      "Train Epoch: 13 [3000/6658 (45%)]\tLoss: 0.183560\n",
      "Train Epoch: 13 [3100/6658 (47%)]\tLoss: 0.073901\n",
      "Train Epoch: 13 [3200/6658 (48%)]\tLoss: 0.005172\n",
      "Train Epoch: 13 [3300/6658 (50%)]\tLoss: 0.162767\n",
      "Train Epoch: 13 [3400/6658 (51%)]\tLoss: 0.817271\n",
      "Train Epoch: 13 [3500/6658 (53%)]\tLoss: 0.054249\n",
      "Train Epoch: 13 [3600/6658 (54%)]\tLoss: 0.120193\n",
      "Train Epoch: 13 [3700/6658 (56%)]\tLoss: 0.436352\n",
      "Train Epoch: 13 [3800/6658 (57%)]\tLoss: 0.009159\n",
      "Train Epoch: 13 [3900/6658 (59%)]\tLoss: 2.049372\n",
      "Train Epoch: 13 [4000/6658 (60%)]\tLoss: 11.660919\n",
      "Train Epoch: 13 [4100/6658 (62%)]\tLoss: 0.093962\n",
      "Train Epoch: 13 [4200/6658 (63%)]\tLoss: 0.193225\n",
      "Train Epoch: 13 [4300/6658 (65%)]\tLoss: 0.438377\n",
      "Train Epoch: 13 [4400/6658 (66%)]\tLoss: 0.136371\n",
      "Train Epoch: 13 [4500/6658 (68%)]\tLoss: 0.454452\n",
      "Train Epoch: 13 [4600/6658 (69%)]\tLoss: 0.020631\n",
      "Train Epoch: 13 [4700/6658 (71%)]\tLoss: 0.680561\n",
      "Train Epoch: 13 [4800/6658 (72%)]\tLoss: 0.171048\n",
      "Train Epoch: 13 [4900/6658 (74%)]\tLoss: 0.182886\n",
      "Train Epoch: 13 [5000/6658 (75%)]\tLoss: 0.082073\n",
      "Train Epoch: 13 [5100/6658 (77%)]\tLoss: 2.804575\n",
      "Train Epoch: 13 [5200/6658 (78%)]\tLoss: 0.046933\n",
      "Train Epoch: 13 [5300/6658 (80%)]\tLoss: 0.215520\n",
      "Train Epoch: 13 [5400/6658 (81%)]\tLoss: 0.126680\n",
      "Train Epoch: 13 [5500/6658 (83%)]\tLoss: 0.458638\n",
      "Train Epoch: 13 [5600/6658 (84%)]\tLoss: 0.301574\n",
      "Train Epoch: 13 [5700/6658 (86%)]\tLoss: 1.150695\n",
      "Train Epoch: 13 [5800/6658 (87%)]\tLoss: 1.308453\n",
      "Train Epoch: 13 [5900/6658 (89%)]\tLoss: 0.004161\n",
      "Train Epoch: 13 [6000/6658 (90%)]\tLoss: 0.122593\n",
      "Train Epoch: 13 [6100/6658 (92%)]\tLoss: 1.090897\n",
      "Train Epoch: 13 [6200/6658 (93%)]\tLoss: 1.360991\n",
      "Train Epoch: 13 [6300/6658 (95%)]\tLoss: 0.159488\n",
      "Train Epoch: 13 [6400/6658 (96%)]\tLoss: 4.997061\n",
      "Train Epoch: 13 [6500/6658 (98%)]\tLoss: 1.117821\n",
      "Train Epoch: 13 [6600/6658 (99%)]\tLoss: 0.369065\n",
      "train loss average =  0.7525387663179571\n",
      "\n",
      "Test set: Average loss: 0.7237\n",
      "\n",
      "Validation loss decreased (0.723979 --> 0.723722).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.0240, 6.0062, 5.9836, 5.9741, 6.0229, 6.0167], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 14 [0/6658 (0%)]\tLoss: 0.132366\n",
      "Train Epoch: 14 [100/6658 (2%)]\tLoss: 0.479607\n",
      "Train Epoch: 14 [200/6658 (3%)]\tLoss: 0.526782\n",
      "Train Epoch: 14 [300/6658 (5%)]\tLoss: 0.374183\n",
      "Train Epoch: 14 [400/6658 (6%)]\tLoss: 0.085258\n",
      "Train Epoch: 14 [500/6658 (8%)]\tLoss: 0.210161\n",
      "Train Epoch: 14 [600/6658 (9%)]\tLoss: 0.968483\n",
      "Train Epoch: 14 [700/6658 (11%)]\tLoss: 0.002968\n",
      "Train Epoch: 14 [800/6658 (12%)]\tLoss: 0.424501\n",
      "Train Epoch: 14 [900/6658 (14%)]\tLoss: 0.000886\n",
      "Train Epoch: 14 [1000/6658 (15%)]\tLoss: 0.450043\n",
      "Train Epoch: 14 [1100/6658 (17%)]\tLoss: 0.001604\n",
      "Train Epoch: 14 [1200/6658 (18%)]\tLoss: 0.233845\n",
      "Train Epoch: 14 [1300/6658 (20%)]\tLoss: 0.021789\n",
      "Train Epoch: 14 [1400/6658 (21%)]\tLoss: 0.236690\n",
      "Train Epoch: 14 [1500/6658 (23%)]\tLoss: 0.704917\n",
      "Train Epoch: 14 [1600/6658 (24%)]\tLoss: 0.098497\n",
      "Train Epoch: 14 [1700/6658 (26%)]\tLoss: 1.111828\n",
      "Train Epoch: 14 [1800/6658 (27%)]\tLoss: 0.068887\n",
      "Train Epoch: 14 [1900/6658 (29%)]\tLoss: 0.717165\n",
      "Train Epoch: 14 [2000/6658 (30%)]\tLoss: 0.002048\n",
      "Train Epoch: 14 [2100/6658 (32%)]\tLoss: 0.138005\n",
      "Train Epoch: 14 [2200/6658 (33%)]\tLoss: 0.682596\n",
      "Train Epoch: 14 [2300/6658 (35%)]\tLoss: 0.008561\n",
      "Train Epoch: 14 [2400/6658 (36%)]\tLoss: 0.600799\n",
      "Train Epoch: 14 [2500/6658 (38%)]\tLoss: 0.017284\n",
      "Train Epoch: 14 [2600/6658 (39%)]\tLoss: 0.081786\n",
      "Train Epoch: 14 [2700/6658 (41%)]\tLoss: 0.003027\n",
      "Train Epoch: 14 [2800/6658 (42%)]\tLoss: 0.459193\n",
      "Train Epoch: 14 [2900/6658 (44%)]\tLoss: 0.145880\n",
      "Train Epoch: 14 [3000/6658 (45%)]\tLoss: 0.204605\n",
      "Train Epoch: 14 [3100/6658 (47%)]\tLoss: 1.840177\n",
      "Train Epoch: 14 [3200/6658 (48%)]\tLoss: 0.407417\n",
      "Train Epoch: 14 [3300/6658 (50%)]\tLoss: 1.022952\n",
      "Train Epoch: 14 [3400/6658 (51%)]\tLoss: 0.015826\n",
      "Train Epoch: 14 [3500/6658 (53%)]\tLoss: 0.620302\n",
      "Train Epoch: 14 [3600/6658 (54%)]\tLoss: 1.946736\n",
      "Train Epoch: 14 [3700/6658 (56%)]\tLoss: 0.356635\n",
      "Train Epoch: 14 [3800/6658 (57%)]\tLoss: 0.057623\n",
      "Train Epoch: 14 [3900/6658 (59%)]\tLoss: 0.173821\n",
      "Train Epoch: 14 [4000/6658 (60%)]\tLoss: 0.078360\n",
      "Train Epoch: 14 [4100/6658 (62%)]\tLoss: 0.300098\n",
      "Train Epoch: 14 [4200/6658 (63%)]\tLoss: 0.429224\n",
      "Train Epoch: 14 [4300/6658 (65%)]\tLoss: 0.220794\n",
      "Train Epoch: 14 [4400/6658 (66%)]\tLoss: 2.572071\n",
      "Train Epoch: 14 [4500/6658 (68%)]\tLoss: 0.601561\n",
      "Train Epoch: 14 [4600/6658 (69%)]\tLoss: 1.558903\n",
      "Train Epoch: 14 [4700/6658 (71%)]\tLoss: 0.598670\n",
      "Train Epoch: 14 [4800/6658 (72%)]\tLoss: 0.046672\n",
      "Train Epoch: 14 [4900/6658 (74%)]\tLoss: 0.029032\n",
      "Train Epoch: 14 [5000/6658 (75%)]\tLoss: 1.895133\n",
      "Train Epoch: 14 [5100/6658 (77%)]\tLoss: 13.318410\n",
      "Train Epoch: 14 [5200/6658 (78%)]\tLoss: 0.609284\n",
      "Train Epoch: 14 [5300/6658 (80%)]\tLoss: 0.002675\n",
      "Train Epoch: 14 [5400/6658 (81%)]\tLoss: 1.355423\n",
      "Train Epoch: 14 [5500/6658 (83%)]\tLoss: 0.773356\n",
      "Train Epoch: 14 [5600/6658 (84%)]\tLoss: 0.113173\n",
      "Train Epoch: 14 [5700/6658 (86%)]\tLoss: 1.300887\n",
      "Train Epoch: 14 [5800/6658 (87%)]\tLoss: 0.031274\n",
      "Train Epoch: 14 [5900/6658 (89%)]\tLoss: 0.336479\n",
      "Train Epoch: 14 [6000/6658 (90%)]\tLoss: 0.177756\n",
      "Train Epoch: 14 [6100/6658 (92%)]\tLoss: 0.009990\n",
      "Train Epoch: 14 [6200/6658 (93%)]\tLoss: 0.379139\n",
      "Train Epoch: 14 [6300/6658 (95%)]\tLoss: 0.610238\n",
      "Train Epoch: 14 [6400/6658 (96%)]\tLoss: 0.661505\n",
      "Train Epoch: 14 [6500/6658 (98%)]\tLoss: 0.474342\n",
      "Train Epoch: 14 [6600/6658 (99%)]\tLoss: 1.626691\n",
      "train loss average =  0.7523297941396888\n",
      "\n",
      "Test set: Average loss: 0.7212\n",
      "\n",
      "Validation loss decreased (0.723722 --> 0.721228).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.0266, 6.0068, 5.9826, 5.9720, 6.0245, 6.0174], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 15 [0/6658 (0%)]\tLoss: 38.999249\n",
      "Train Epoch: 15 [100/6658 (2%)]\tLoss: 0.020989\n",
      "Train Epoch: 15 [200/6658 (3%)]\tLoss: 0.477660\n",
      "Train Epoch: 15 [300/6658 (5%)]\tLoss: 0.056700\n",
      "Train Epoch: 15 [400/6658 (6%)]\tLoss: 0.987267\n",
      "Train Epoch: 15 [500/6658 (8%)]\tLoss: 0.285166\n",
      "Train Epoch: 15 [600/6658 (9%)]\tLoss: 0.388997\n",
      "Train Epoch: 15 [700/6658 (11%)]\tLoss: 0.201265\n",
      "Train Epoch: 15 [800/6658 (12%)]\tLoss: 0.012333\n",
      "Train Epoch: 15 [900/6658 (14%)]\tLoss: 2.696606\n",
      "Train Epoch: 15 [1000/6658 (15%)]\tLoss: 0.067361\n",
      "Train Epoch: 15 [1100/6658 (17%)]\tLoss: 0.178772\n",
      "Train Epoch: 15 [1200/6658 (18%)]\tLoss: 0.145409\n",
      "Train Epoch: 15 [1300/6658 (20%)]\tLoss: 0.058943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [1400/6658 (21%)]\tLoss: 0.731405\n",
      "Train Epoch: 15 [1500/6658 (23%)]\tLoss: 1.108001\n",
      "Train Epoch: 15 [1600/6658 (24%)]\tLoss: 0.178873\n",
      "Train Epoch: 15 [1700/6658 (26%)]\tLoss: 5.015226\n",
      "Train Epoch: 15 [1800/6658 (27%)]\tLoss: 0.191538\n",
      "Train Epoch: 15 [1900/6658 (29%)]\tLoss: 0.220161\n",
      "Train Epoch: 15 [2000/6658 (30%)]\tLoss: 0.502070\n",
      "Train Epoch: 15 [2100/6658 (32%)]\tLoss: 0.020762\n",
      "Train Epoch: 15 [2200/6658 (33%)]\tLoss: 0.749003\n",
      "Train Epoch: 15 [2300/6658 (35%)]\tLoss: 0.438695\n",
      "Train Epoch: 15 [2400/6658 (36%)]\tLoss: 2.708879\n",
      "Train Epoch: 15 [2500/6658 (38%)]\tLoss: 1.109595\n",
      "Train Epoch: 15 [2600/6658 (39%)]\tLoss: 0.010652\n",
      "Train Epoch: 15 [2700/6658 (41%)]\tLoss: 0.762545\n",
      "Train Epoch: 15 [2800/6658 (42%)]\tLoss: 0.083514\n",
      "Train Epoch: 15 [2900/6658 (44%)]\tLoss: 3.985953\n",
      "Train Epoch: 15 [3000/6658 (45%)]\tLoss: 0.208585\n",
      "Train Epoch: 15 [3100/6658 (47%)]\tLoss: 0.494747\n",
      "Train Epoch: 15 [3200/6658 (48%)]\tLoss: 2.347873\n",
      "Train Epoch: 15 [3300/6658 (50%)]\tLoss: 0.006251\n",
      "Train Epoch: 15 [3400/6658 (51%)]\tLoss: 0.247432\n",
      "Train Epoch: 15 [3500/6658 (53%)]\tLoss: 0.461897\n",
      "Train Epoch: 15 [3600/6658 (54%)]\tLoss: 0.168829\n",
      "Train Epoch: 15 [3700/6658 (56%)]\tLoss: 9.034696\n",
      "Train Epoch: 15 [3800/6658 (57%)]\tLoss: 0.026638\n",
      "Train Epoch: 15 [3900/6658 (59%)]\tLoss: 1.670945\n",
      "Train Epoch: 15 [4000/6658 (60%)]\tLoss: 0.133165\n",
      "Train Epoch: 15 [4100/6658 (62%)]\tLoss: 0.204633\n",
      "Train Epoch: 15 [4200/6658 (63%)]\tLoss: 0.162437\n",
      "Train Epoch: 15 [4300/6658 (65%)]\tLoss: 2.390457\n",
      "Train Epoch: 15 [4400/6658 (66%)]\tLoss: 1.177400\n",
      "Train Epoch: 15 [4500/6658 (68%)]\tLoss: 0.573218\n",
      "Train Epoch: 15 [4600/6658 (69%)]\tLoss: 0.013736\n",
      "Train Epoch: 15 [4700/6658 (71%)]\tLoss: 0.191408\n",
      "Train Epoch: 15 [4800/6658 (72%)]\tLoss: 0.851526\n",
      "Train Epoch: 15 [4900/6658 (74%)]\tLoss: 0.415872\n",
      "Train Epoch: 15 [5000/6658 (75%)]\tLoss: 0.370972\n",
      "Train Epoch: 15 [5100/6658 (77%)]\tLoss: 0.316698\n",
      "Train Epoch: 15 [5200/6658 (78%)]\tLoss: 0.010236\n",
      "Train Epoch: 15 [5300/6658 (80%)]\tLoss: 0.986639\n",
      "Train Epoch: 15 [5400/6658 (81%)]\tLoss: 0.710647\n",
      "Train Epoch: 15 [5500/6658 (83%)]\tLoss: 0.391868\n",
      "Train Epoch: 15 [5600/6658 (84%)]\tLoss: 0.008420\n",
      "Train Epoch: 15 [5700/6658 (86%)]\tLoss: 0.010927\n",
      "Train Epoch: 15 [5800/6658 (87%)]\tLoss: 0.186546\n",
      "Train Epoch: 15 [5900/6658 (89%)]\tLoss: 0.056789\n",
      "Train Epoch: 15 [6000/6658 (90%)]\tLoss: 1.209769\n",
      "Train Epoch: 15 [6100/6658 (92%)]\tLoss: 0.294166\n",
      "Train Epoch: 15 [6200/6658 (93%)]\tLoss: 0.002467\n",
      "Train Epoch: 15 [6300/6658 (95%)]\tLoss: 0.546284\n",
      "Train Epoch: 15 [6400/6658 (96%)]\tLoss: 0.115628\n",
      "Train Epoch: 15 [6500/6658 (98%)]\tLoss: 0.263061\n",
      "Train Epoch: 15 [6600/6658 (99%)]\tLoss: 0.026887\n",
      "train loss average =  0.7552483485364391\n",
      "\n",
      "Test set: Average loss: 0.7192\n",
      "\n",
      "Validation loss decreased (0.721228 --> 0.719227).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.0288, 6.0065, 5.9811, 5.9702, 6.0259, 6.0179], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 16 [0/6658 (0%)]\tLoss: 0.001253\n",
      "Train Epoch: 16 [100/6658 (2%)]\tLoss: 0.307454\n",
      "Train Epoch: 16 [200/6658 (3%)]\tLoss: 0.041759\n",
      "Train Epoch: 16 [300/6658 (5%)]\tLoss: 0.625861\n",
      "Train Epoch: 16 [400/6658 (6%)]\tLoss: 0.159580\n",
      "Train Epoch: 16 [500/6658 (8%)]\tLoss: 0.592509\n",
      "Train Epoch: 16 [600/6658 (9%)]\tLoss: 0.024574\n",
      "Train Epoch: 16 [700/6658 (11%)]\tLoss: 0.489231\n",
      "Train Epoch: 16 [800/6658 (12%)]\tLoss: 9.011176\n",
      "Train Epoch: 16 [900/6658 (14%)]\tLoss: 1.398380\n",
      "Train Epoch: 16 [1000/6658 (15%)]\tLoss: 0.221295\n",
      "Train Epoch: 16 [1100/6658 (17%)]\tLoss: 0.057196\n",
      "Train Epoch: 16 [1200/6658 (18%)]\tLoss: 0.005657\n",
      "Train Epoch: 16 [1300/6658 (20%)]\tLoss: 0.345826\n",
      "Train Epoch: 16 [1400/6658 (21%)]\tLoss: 0.276961\n",
      "Train Epoch: 16 [1500/6658 (23%)]\tLoss: 0.358059\n",
      "Train Epoch: 16 [1600/6658 (24%)]\tLoss: 3.078691\n",
      "Train Epoch: 16 [1700/6658 (26%)]\tLoss: 0.758729\n",
      "Train Epoch: 16 [1800/6658 (27%)]\tLoss: 0.006831\n",
      "Train Epoch: 16 [1900/6658 (29%)]\tLoss: 0.819212\n",
      "Train Epoch: 16 [2000/6658 (30%)]\tLoss: 2.615363\n",
      "Train Epoch: 16 [2100/6658 (32%)]\tLoss: 0.582397\n",
      "Train Epoch: 16 [2200/6658 (33%)]\tLoss: 0.964538\n",
      "Train Epoch: 16 [2300/6658 (35%)]\tLoss: 0.083348\n",
      "Train Epoch: 16 [2400/6658 (36%)]\tLoss: 0.600904\n",
      "Train Epoch: 16 [2500/6658 (38%)]\tLoss: 0.043316\n",
      "Train Epoch: 16 [2600/6658 (39%)]\tLoss: 0.011717\n",
      "Train Epoch: 16 [2700/6658 (41%)]\tLoss: 1.251441\n",
      "Train Epoch: 16 [2800/6658 (42%)]\tLoss: 0.686249\n",
      "Train Epoch: 16 [2900/6658 (44%)]\tLoss: 0.034736\n",
      "Train Epoch: 16 [3000/6658 (45%)]\tLoss: 0.130712\n",
      "Train Epoch: 16 [3100/6658 (47%)]\tLoss: 0.000095\n",
      "Train Epoch: 16 [3200/6658 (48%)]\tLoss: 1.165700\n",
      "Train Epoch: 16 [3300/6658 (50%)]\tLoss: 0.721271\n",
      "Train Epoch: 16 [3400/6658 (51%)]\tLoss: 0.059371\n",
      "Train Epoch: 16 [3500/6658 (53%)]\tLoss: 0.594895\n",
      "Train Epoch: 16 [3600/6658 (54%)]\tLoss: 1.280800\n",
      "Train Epoch: 16 [3700/6658 (56%)]\tLoss: 1.652311\n",
      "Train Epoch: 16 [3800/6658 (57%)]\tLoss: 5.471394\n",
      "Train Epoch: 16 [3900/6658 (59%)]\tLoss: 0.494297\n",
      "Train Epoch: 16 [4000/6658 (60%)]\tLoss: 0.624949\n",
      "Train Epoch: 16 [4100/6658 (62%)]\tLoss: 0.425657\n",
      "Train Epoch: 16 [4200/6658 (63%)]\tLoss: 0.001511\n",
      "Train Epoch: 16 [4300/6658 (65%)]\tLoss: 0.013792\n",
      "Train Epoch: 16 [4400/6658 (66%)]\tLoss: 0.439900\n",
      "Train Epoch: 16 [4500/6658 (68%)]\tLoss: 0.728986\n",
      "Train Epoch: 16 [4600/6658 (69%)]\tLoss: 0.696548\n",
      "Train Epoch: 16 [4700/6658 (71%)]\tLoss: 0.829519\n",
      "Train Epoch: 16 [4800/6658 (72%)]\tLoss: 0.182320\n",
      "Train Epoch: 16 [4900/6658 (74%)]\tLoss: 1.789071\n",
      "Train Epoch: 16 [5000/6658 (75%)]\tLoss: 0.164892\n",
      "Train Epoch: 16 [5100/6658 (77%)]\tLoss: 0.000814\n",
      "Train Epoch: 16 [5200/6658 (78%)]\tLoss: 1.124425\n",
      "Train Epoch: 16 [5300/6658 (80%)]\tLoss: 0.080329\n",
      "Train Epoch: 16 [5400/6658 (81%)]\tLoss: 0.429480\n",
      "Train Epoch: 16 [5500/6658 (83%)]\tLoss: 1.648546\n",
      "Train Epoch: 16 [5600/6658 (84%)]\tLoss: 4.527257\n",
      "Train Epoch: 16 [5700/6658 (86%)]\tLoss: 0.599061\n",
      "Train Epoch: 16 [5800/6658 (87%)]\tLoss: 0.456532\n",
      "Train Epoch: 16 [5900/6658 (89%)]\tLoss: 1.794643\n",
      "Train Epoch: 16 [6000/6658 (90%)]\tLoss: 0.006326\n",
      "Train Epoch: 16 [6100/6658 (92%)]\tLoss: 2.739440\n",
      "Train Epoch: 16 [6200/6658 (93%)]\tLoss: 0.125326\n",
      "Train Epoch: 16 [6300/6658 (95%)]\tLoss: 0.002642\n",
      "Train Epoch: 16 [6400/6658 (96%)]\tLoss: 0.054495\n",
      "Train Epoch: 16 [6500/6658 (98%)]\tLoss: 0.113882\n",
      "Train Epoch: 16 [6600/6658 (99%)]\tLoss: 0.029892\n",
      "train loss average =  0.7385882603838517\n",
      "\n",
      "Test set: Average loss: 0.7351\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0310, 6.0079, 5.9798, 5.9685, 6.0267, 6.0183], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 17 [0/6658 (0%)]\tLoss: 0.074640\n",
      "Train Epoch: 17 [100/6658 (2%)]\tLoss: 1.445887\n",
      "Train Epoch: 17 [200/6658 (3%)]\tLoss: 1.231954\n",
      "Train Epoch: 17 [300/6658 (5%)]\tLoss: 1.840216\n",
      "Train Epoch: 17 [400/6658 (6%)]\tLoss: 1.141418\n",
      "Train Epoch: 17 [500/6658 (8%)]\tLoss: 2.552787\n",
      "Train Epoch: 17 [600/6658 (9%)]\tLoss: 0.806649\n",
      "Train Epoch: 17 [700/6658 (11%)]\tLoss: 0.347889\n",
      "Train Epoch: 17 [800/6658 (12%)]\tLoss: 0.008675\n",
      "Train Epoch: 17 [900/6658 (14%)]\tLoss: 0.000687\n",
      "Train Epoch: 17 [1000/6658 (15%)]\tLoss: 0.156263\n",
      "Train Epoch: 17 [1100/6658 (17%)]\tLoss: 1.509564\n",
      "Train Epoch: 17 [1200/6658 (18%)]\tLoss: 0.078835\n",
      "Train Epoch: 17 [1300/6658 (20%)]\tLoss: 0.226451\n",
      "Train Epoch: 17 [1400/6658 (21%)]\tLoss: 0.041680\n",
      "Train Epoch: 17 [1500/6658 (23%)]\tLoss: 0.263740\n",
      "Train Epoch: 17 [1600/6658 (24%)]\tLoss: 1.042196\n",
      "Train Epoch: 17 [1700/6658 (26%)]\tLoss: 0.936393\n",
      "Train Epoch: 17 [1800/6658 (27%)]\tLoss: 0.070992\n",
      "Train Epoch: 17 [1900/6658 (29%)]\tLoss: 0.025087\n",
      "Train Epoch: 17 [2000/6658 (30%)]\tLoss: 0.441564\n",
      "Train Epoch: 17 [2100/6658 (32%)]\tLoss: 0.078910\n",
      "Train Epoch: 17 [2200/6658 (33%)]\tLoss: 0.050185\n",
      "Train Epoch: 17 [2300/6658 (35%)]\tLoss: 0.452112\n",
      "Train Epoch: 17 [2400/6658 (36%)]\tLoss: 0.371592\n",
      "Train Epoch: 17 [2500/6658 (38%)]\tLoss: 0.726424\n",
      "Train Epoch: 17 [2600/6658 (39%)]\tLoss: 0.690079\n",
      "Train Epoch: 17 [2700/6658 (41%)]\tLoss: 1.022695\n",
      "Train Epoch: 17 [2800/6658 (42%)]\tLoss: 0.930247\n",
      "Train Epoch: 17 [2900/6658 (44%)]\tLoss: 3.287677\n",
      "Train Epoch: 17 [3000/6658 (45%)]\tLoss: 2.704824\n",
      "Train Epoch: 17 [3100/6658 (47%)]\tLoss: 0.949090\n",
      "Train Epoch: 17 [3200/6658 (48%)]\tLoss: 0.116730\n",
      "Train Epoch: 17 [3300/6658 (50%)]\tLoss: 0.432320\n",
      "Train Epoch: 17 [3400/6658 (51%)]\tLoss: 0.064036\n",
      "Train Epoch: 17 [3500/6658 (53%)]\tLoss: 0.015777\n",
      "Train Epoch: 17 [3600/6658 (54%)]\tLoss: 0.015650\n",
      "Train Epoch: 17 [3700/6658 (56%)]\tLoss: 0.085384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [3800/6658 (57%)]\tLoss: 0.359293\n",
      "Train Epoch: 17 [3900/6658 (59%)]\tLoss: 0.000000\n",
      "Train Epoch: 17 [4000/6658 (60%)]\tLoss: 0.108254\n",
      "Train Epoch: 17 [4100/6658 (62%)]\tLoss: 0.479235\n",
      "Train Epoch: 17 [4200/6658 (63%)]\tLoss: 5.612644\n",
      "Train Epoch: 17 [4300/6658 (65%)]\tLoss: 0.199019\n",
      "Train Epoch: 17 [4400/6658 (66%)]\tLoss: 0.192682\n",
      "Train Epoch: 17 [4500/6658 (68%)]\tLoss: 0.551235\n",
      "Train Epoch: 17 [4600/6658 (69%)]\tLoss: 0.018697\n",
      "Train Epoch: 17 [4700/6658 (71%)]\tLoss: 1.156775\n",
      "Train Epoch: 17 [4800/6658 (72%)]\tLoss: 0.118376\n",
      "Train Epoch: 17 [4900/6658 (74%)]\tLoss: 0.147381\n",
      "Train Epoch: 17 [5000/6658 (75%)]\tLoss: 0.982398\n",
      "Train Epoch: 17 [5100/6658 (77%)]\tLoss: 2.285244\n",
      "Train Epoch: 17 [5200/6658 (78%)]\tLoss: 1.292034\n",
      "Train Epoch: 17 [5300/6658 (80%)]\tLoss: 1.935180\n",
      "Train Epoch: 17 [5400/6658 (81%)]\tLoss: 0.194873\n",
      "Train Epoch: 17 [5500/6658 (83%)]\tLoss: 0.247473\n",
      "Train Epoch: 17 [5600/6658 (84%)]\tLoss: 0.311153\n",
      "Train Epoch: 17 [5700/6658 (86%)]\tLoss: 0.581044\n",
      "Train Epoch: 17 [5800/6658 (87%)]\tLoss: 1.499133\n",
      "Train Epoch: 17 [5900/6658 (89%)]\tLoss: 0.005220\n",
      "Train Epoch: 17 [6000/6658 (90%)]\tLoss: 4.339463\n",
      "Train Epoch: 17 [6100/6658 (92%)]\tLoss: 0.051704\n",
      "Train Epoch: 17 [6200/6658 (93%)]\tLoss: 1.045229\n",
      "Train Epoch: 17 [6300/6658 (95%)]\tLoss: 0.000852\n",
      "Train Epoch: 17 [6400/6658 (96%)]\tLoss: 6.426331\n",
      "Train Epoch: 17 [6500/6658 (98%)]\tLoss: 0.215173\n",
      "Train Epoch: 17 [6600/6658 (99%)]\tLoss: 0.747855\n",
      "train loss average =  0.7454592426135246\n",
      "\n",
      "Test set: Average loss: 0.7381\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0331, 6.0085, 5.9785, 5.9678, 6.0278, 6.0185], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 18 [0/6658 (0%)]\tLoss: 0.759646\n",
      "Train Epoch: 18 [100/6658 (2%)]\tLoss: 1.441751\n",
      "Train Epoch: 18 [200/6658 (3%)]\tLoss: 0.156286\n",
      "Train Epoch: 18 [300/6658 (5%)]\tLoss: 0.333959\n",
      "Train Epoch: 18 [400/6658 (6%)]\tLoss: 0.488882\n",
      "Train Epoch: 18 [500/6658 (8%)]\tLoss: 0.088079\n",
      "Train Epoch: 18 [600/6658 (9%)]\tLoss: 0.171749\n",
      "Train Epoch: 18 [700/6658 (11%)]\tLoss: 0.828521\n",
      "Train Epoch: 18 [800/6658 (12%)]\tLoss: 0.176941\n",
      "Train Epoch: 18 [900/6658 (14%)]\tLoss: 0.126799\n",
      "Train Epoch: 18 [1000/6658 (15%)]\tLoss: 9.058781\n",
      "Train Epoch: 18 [1100/6658 (17%)]\tLoss: 0.395184\n",
      "Train Epoch: 18 [1200/6658 (18%)]\tLoss: 0.090305\n",
      "Train Epoch: 18 [1300/6658 (20%)]\tLoss: 0.019751\n",
      "Train Epoch: 18 [1400/6658 (21%)]\tLoss: 0.467698\n",
      "Train Epoch: 18 [1500/6658 (23%)]\tLoss: 0.082869\n",
      "Train Epoch: 18 [1600/6658 (24%)]\tLoss: 0.493241\n",
      "Train Epoch: 18 [1700/6658 (26%)]\tLoss: 0.260152\n",
      "Train Epoch: 18 [1800/6658 (27%)]\tLoss: 0.507392\n",
      "Train Epoch: 18 [1900/6658 (29%)]\tLoss: 0.161577\n",
      "Train Epoch: 18 [2000/6658 (30%)]\tLoss: 0.091589\n",
      "Train Epoch: 18 [2100/6658 (32%)]\tLoss: 0.000557\n",
      "Train Epoch: 18 [2200/6658 (33%)]\tLoss: 1.652712\n",
      "Train Epoch: 18 [2300/6658 (35%)]\tLoss: 0.314099\n",
      "Train Epoch: 18 [2400/6658 (36%)]\tLoss: 0.088449\n",
      "Train Epoch: 18 [2500/6658 (38%)]\tLoss: 1.494503\n",
      "Train Epoch: 18 [2600/6658 (39%)]\tLoss: 0.363681\n",
      "Train Epoch: 18 [2700/6658 (41%)]\tLoss: 0.360984\n",
      "Train Epoch: 18 [2800/6658 (42%)]\tLoss: 2.269494\n",
      "Train Epoch: 18 [2900/6658 (44%)]\tLoss: 0.433932\n",
      "Train Epoch: 18 [3000/6658 (45%)]\tLoss: 0.321519\n",
      "Train Epoch: 18 [3100/6658 (47%)]\tLoss: 0.901689\n",
      "Train Epoch: 18 [3200/6658 (48%)]\tLoss: 0.441437\n",
      "Train Epoch: 18 [3300/6658 (50%)]\tLoss: 0.588835\n",
      "Train Epoch: 18 [3400/6658 (51%)]\tLoss: 0.237436\n",
      "Train Epoch: 18 [3500/6658 (53%)]\tLoss: 1.747981\n",
      "Train Epoch: 18 [3600/6658 (54%)]\tLoss: 0.486015\n",
      "Train Epoch: 18 [3700/6658 (56%)]\tLoss: 0.018721\n",
      "Train Epoch: 18 [3800/6658 (57%)]\tLoss: 1.789532\n",
      "Train Epoch: 18 [3900/6658 (59%)]\tLoss: 0.051378\n",
      "Train Epoch: 18 [4000/6658 (60%)]\tLoss: 0.007495\n",
      "Train Epoch: 18 [4100/6658 (62%)]\tLoss: 0.000165\n",
      "Train Epoch: 18 [4200/6658 (63%)]\tLoss: 0.528912\n",
      "Train Epoch: 18 [4300/6658 (65%)]\tLoss: 0.141451\n",
      "Train Epoch: 18 [4400/6658 (66%)]\tLoss: 0.111003\n",
      "Train Epoch: 18 [4500/6658 (68%)]\tLoss: 0.283522\n",
      "Train Epoch: 18 [4600/6658 (69%)]\tLoss: 0.126732\n",
      "Train Epoch: 18 [4700/6658 (71%)]\tLoss: 0.480258\n",
      "Train Epoch: 18 [4800/6658 (72%)]\tLoss: 0.008543\n",
      "Train Epoch: 18 [4900/6658 (74%)]\tLoss: 0.657018\n",
      "Train Epoch: 18 [5000/6658 (75%)]\tLoss: 0.243304\n",
      "Train Epoch: 18 [5100/6658 (77%)]\tLoss: 0.637465\n",
      "Train Epoch: 18 [5200/6658 (78%)]\tLoss: 0.025204\n",
      "Train Epoch: 18 [5300/6658 (80%)]\tLoss: 1.266028\n",
      "Train Epoch: 18 [5400/6658 (81%)]\tLoss: 0.651536\n",
      "Train Epoch: 18 [5500/6658 (83%)]\tLoss: 0.117242\n",
      "Train Epoch: 18 [5600/6658 (84%)]\tLoss: 0.329897\n",
      "Train Epoch: 18 [5700/6658 (86%)]\tLoss: 0.015913\n",
      "Train Epoch: 18 [5800/6658 (87%)]\tLoss: 0.575758\n",
      "Train Epoch: 18 [5900/6658 (89%)]\tLoss: 1.174851\n",
      "Train Epoch: 18 [6000/6658 (90%)]\tLoss: 1.421336\n",
      "Train Epoch: 18 [6100/6658 (92%)]\tLoss: 1.318688\n",
      "Train Epoch: 18 [6200/6658 (93%)]\tLoss: 0.142937\n",
      "Train Epoch: 18 [6300/6658 (95%)]\tLoss: 1.177820\n",
      "Train Epoch: 18 [6400/6658 (96%)]\tLoss: 0.500114\n",
      "Train Epoch: 18 [6500/6658 (98%)]\tLoss: 0.021107\n",
      "Train Epoch: 18 [6600/6658 (99%)]\tLoss: 0.085782\n",
      "train loss average =  0.7483866234044615\n",
      "\n",
      "Test set: Average loss: 0.7104\n",
      "\n",
      "Validation loss decreased (0.719227 --> 0.710387).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.0355, 6.0098, 5.9775, 5.9659, 6.0291, 6.0191], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 19 [0/6658 (0%)]\tLoss: 0.270423\n",
      "Train Epoch: 19 [100/6658 (2%)]\tLoss: 0.005889\n",
      "Train Epoch: 19 [200/6658 (3%)]\tLoss: 0.112362\n",
      "Train Epoch: 19 [300/6658 (5%)]\tLoss: 0.768456\n",
      "Train Epoch: 19 [400/6658 (6%)]\tLoss: 0.476245\n",
      "Train Epoch: 19 [500/6658 (8%)]\tLoss: 0.000192\n",
      "Train Epoch: 19 [600/6658 (9%)]\tLoss: 0.058331\n",
      "Train Epoch: 19 [700/6658 (11%)]\tLoss: 0.000000\n",
      "Train Epoch: 19 [800/6658 (12%)]\tLoss: 0.024022\n",
      "Train Epoch: 19 [900/6658 (14%)]\tLoss: 1.119753\n",
      "Train Epoch: 19 [1000/6658 (15%)]\tLoss: 8.920387\n",
      "Train Epoch: 19 [1100/6658 (17%)]\tLoss: 0.015036\n",
      "Train Epoch: 19 [1200/6658 (18%)]\tLoss: 0.085874\n",
      "Train Epoch: 19 [1300/6658 (20%)]\tLoss: 0.427492\n",
      "Train Epoch: 19 [1400/6658 (21%)]\tLoss: 0.146396\n",
      "Train Epoch: 19 [1500/6658 (23%)]\tLoss: 0.135592\n",
      "Train Epoch: 19 [1600/6658 (24%)]\tLoss: 0.050032\n",
      "Train Epoch: 19 [1700/6658 (26%)]\tLoss: 0.106224\n",
      "Train Epoch: 19 [1800/6658 (27%)]\tLoss: 0.020606\n",
      "Train Epoch: 19 [1900/6658 (29%)]\tLoss: 0.386859\n",
      "Train Epoch: 19 [2000/6658 (30%)]\tLoss: 0.048125\n",
      "Train Epoch: 19 [2100/6658 (32%)]\tLoss: 0.062016\n",
      "Train Epoch: 19 [2200/6658 (33%)]\tLoss: 0.016136\n",
      "Train Epoch: 19 [2300/6658 (35%)]\tLoss: 0.174742\n",
      "Train Epoch: 19 [2400/6658 (36%)]\tLoss: 0.977690\n",
      "Train Epoch: 19 [2500/6658 (38%)]\tLoss: 0.734302\n",
      "Train Epoch: 19 [2600/6658 (39%)]\tLoss: 0.289854\n",
      "Train Epoch: 19 [2700/6658 (41%)]\tLoss: 0.784622\n",
      "Train Epoch: 19 [2800/6658 (42%)]\tLoss: 0.212214\n",
      "Train Epoch: 19 [2900/6658 (44%)]\tLoss: 0.781884\n",
      "Train Epoch: 19 [3000/6658 (45%)]\tLoss: 0.202517\n",
      "Train Epoch: 19 [3100/6658 (47%)]\tLoss: 11.086811\n",
      "Train Epoch: 19 [3200/6658 (48%)]\tLoss: 0.100017\n",
      "Train Epoch: 19 [3300/6658 (50%)]\tLoss: 0.201497\n",
      "Train Epoch: 19 [3400/6658 (51%)]\tLoss: 0.001200\n",
      "Train Epoch: 19 [3500/6658 (53%)]\tLoss: 0.106579\n",
      "Train Epoch: 19 [3600/6658 (54%)]\tLoss: 0.083464\n",
      "Train Epoch: 19 [3700/6658 (56%)]\tLoss: 0.054572\n",
      "Train Epoch: 19 [3800/6658 (57%)]\tLoss: 0.760824\n",
      "Train Epoch: 19 [3900/6658 (59%)]\tLoss: 0.000667\n",
      "Train Epoch: 19 [4000/6658 (60%)]\tLoss: 0.099660\n",
      "Train Epoch: 19 [4100/6658 (62%)]\tLoss: 1.041153\n",
      "Train Epoch: 19 [4200/6658 (63%)]\tLoss: 0.362395\n",
      "Train Epoch: 19 [4300/6658 (65%)]\tLoss: 1.267196\n",
      "Train Epoch: 19 [4400/6658 (66%)]\tLoss: 2.642271\n",
      "Train Epoch: 19 [4500/6658 (68%)]\tLoss: 0.047360\n",
      "Train Epoch: 19 [4600/6658 (69%)]\tLoss: 0.032603\n",
      "Train Epoch: 19 [4700/6658 (71%)]\tLoss: 0.044375\n",
      "Train Epoch: 19 [4800/6658 (72%)]\tLoss: 0.201103\n",
      "Train Epoch: 19 [4900/6658 (74%)]\tLoss: 0.003150\n",
      "Train Epoch: 19 [5000/6658 (75%)]\tLoss: 0.000055\n",
      "Train Epoch: 19 [5100/6658 (77%)]\tLoss: 3.802017\n",
      "Train Epoch: 19 [5200/6658 (78%)]\tLoss: 0.335145\n",
      "Train Epoch: 19 [5300/6658 (80%)]\tLoss: 0.131527\n",
      "Train Epoch: 19 [5400/6658 (81%)]\tLoss: 0.650785\n",
      "Train Epoch: 19 [5500/6658 (83%)]\tLoss: 0.033366\n",
      "Train Epoch: 19 [5600/6658 (84%)]\tLoss: 0.569517\n",
      "Train Epoch: 19 [5700/6658 (86%)]\tLoss: 0.125189\n",
      "Train Epoch: 19 [5800/6658 (87%)]\tLoss: 0.515088\n",
      "Train Epoch: 19 [5900/6658 (89%)]\tLoss: 0.361815\n",
      "Train Epoch: 19 [6000/6658 (90%)]\tLoss: 5.351275\n",
      "Train Epoch: 19 [6100/6658 (92%)]\tLoss: 0.088380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19 [6200/6658 (93%)]\tLoss: 0.023169\n",
      "Train Epoch: 19 [6300/6658 (95%)]\tLoss: 0.349033\n",
      "Train Epoch: 19 [6400/6658 (96%)]\tLoss: 1.540862\n",
      "Train Epoch: 19 [6500/6658 (98%)]\tLoss: 0.249811\n",
      "Train Epoch: 19 [6600/6658 (99%)]\tLoss: 0.101056\n",
      "train loss average =  0.7463612246978579\n",
      "\n",
      "Test set: Average loss: 0.7164\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0378, 6.0100, 5.9762, 5.9636, 6.0302, 6.0192], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 20 [0/6658 (0%)]\tLoss: 1.565541\n",
      "Train Epoch: 20 [100/6658 (2%)]\tLoss: 0.013955\n",
      "Train Epoch: 20 [200/6658 (3%)]\tLoss: 0.776672\n",
      "Train Epoch: 20 [300/6658 (5%)]\tLoss: 0.004461\n",
      "Train Epoch: 20 [400/6658 (6%)]\tLoss: 0.428649\n",
      "Train Epoch: 20 [500/6658 (8%)]\tLoss: 1.603338\n",
      "Train Epoch: 20 [600/6658 (9%)]\tLoss: 0.086542\n",
      "Train Epoch: 20 [700/6658 (11%)]\tLoss: 0.168167\n",
      "Train Epoch: 20 [800/6658 (12%)]\tLoss: 0.046821\n",
      "Train Epoch: 20 [900/6658 (14%)]\tLoss: 0.989983\n",
      "Train Epoch: 20 [1000/6658 (15%)]\tLoss: 0.214819\n",
      "Train Epoch: 20 [1100/6658 (17%)]\tLoss: 0.575721\n",
      "Train Epoch: 20 [1200/6658 (18%)]\tLoss: 0.229563\n",
      "Train Epoch: 20 [1300/6658 (20%)]\tLoss: 0.228587\n",
      "Train Epoch: 20 [1400/6658 (21%)]\tLoss: 0.001267\n",
      "Train Epoch: 20 [1500/6658 (23%)]\tLoss: 0.004987\n",
      "Train Epoch: 20 [1600/6658 (24%)]\tLoss: 0.151725\n",
      "Train Epoch: 20 [1700/6658 (26%)]\tLoss: 1.132903\n",
      "Train Epoch: 20 [1800/6658 (27%)]\tLoss: 6.727733\n",
      "Train Epoch: 20 [1900/6658 (29%)]\tLoss: 0.582588\n",
      "Train Epoch: 20 [2000/6658 (30%)]\tLoss: 0.009172\n",
      "Train Epoch: 20 [2100/6658 (32%)]\tLoss: 0.278334\n",
      "Train Epoch: 20 [2200/6658 (33%)]\tLoss: 0.002270\n",
      "Train Epoch: 20 [2300/6658 (35%)]\tLoss: 1.772247\n",
      "Train Epoch: 20 [2400/6658 (36%)]\tLoss: 0.168934\n",
      "Train Epoch: 20 [2500/6658 (38%)]\tLoss: 0.238531\n",
      "Train Epoch: 20 [2600/6658 (39%)]\tLoss: 0.247305\n",
      "Train Epoch: 20 [2700/6658 (41%)]\tLoss: 0.932517\n",
      "Train Epoch: 20 [2800/6658 (42%)]\tLoss: 0.051510\n",
      "Train Epoch: 20 [2900/6658 (44%)]\tLoss: 0.526475\n",
      "Train Epoch: 20 [3000/6658 (45%)]\tLoss: 0.011842\n",
      "Train Epoch: 20 [3100/6658 (47%)]\tLoss: 0.506208\n",
      "Train Epoch: 20 [3200/6658 (48%)]\tLoss: 0.586003\n",
      "Train Epoch: 20 [3300/6658 (50%)]\tLoss: 11.111967\n",
      "Train Epoch: 20 [3400/6658 (51%)]\tLoss: 0.523742\n",
      "Train Epoch: 20 [3500/6658 (53%)]\tLoss: 0.492462\n",
      "Train Epoch: 20 [3600/6658 (54%)]\tLoss: 0.089178\n",
      "Train Epoch: 20 [3700/6658 (56%)]\tLoss: 0.155901\n",
      "Train Epoch: 20 [3800/6658 (57%)]\tLoss: 0.033908\n",
      "Train Epoch: 20 [3900/6658 (59%)]\tLoss: 0.000199\n",
      "Train Epoch: 20 [4000/6658 (60%)]\tLoss: 0.007524\n",
      "Train Epoch: 20 [4100/6658 (62%)]\tLoss: 0.676221\n",
      "Train Epoch: 20 [4200/6658 (63%)]\tLoss: 1.503550\n",
      "Train Epoch: 20 [4300/6658 (65%)]\tLoss: 0.023632\n",
      "Train Epoch: 20 [4400/6658 (66%)]\tLoss: 0.003241\n",
      "Train Epoch: 20 [4500/6658 (68%)]\tLoss: 0.579029\n",
      "Train Epoch: 20 [4600/6658 (69%)]\tLoss: 0.017009\n",
      "Train Epoch: 20 [4700/6658 (71%)]\tLoss: 0.362327\n",
      "Train Epoch: 20 [4800/6658 (72%)]\tLoss: 0.146015\n",
      "Train Epoch: 20 [4900/6658 (74%)]\tLoss: 0.017796\n",
      "Train Epoch: 20 [5000/6658 (75%)]\tLoss: 0.005351\n",
      "Train Epoch: 20 [5100/6658 (77%)]\tLoss: 0.288391\n",
      "Train Epoch: 20 [5200/6658 (78%)]\tLoss: 0.047563\n",
      "Train Epoch: 20 [5300/6658 (80%)]\tLoss: 0.051545\n",
      "Train Epoch: 20 [5400/6658 (81%)]\tLoss: 0.156077\n",
      "Train Epoch: 20 [5500/6658 (83%)]\tLoss: 0.060936\n",
      "Train Epoch: 20 [5600/6658 (84%)]\tLoss: 0.227885\n",
      "Train Epoch: 20 [5700/6658 (86%)]\tLoss: 0.059274\n",
      "Train Epoch: 20 [5800/6658 (87%)]\tLoss: 0.121104\n",
      "Train Epoch: 20 [5900/6658 (89%)]\tLoss: 0.413948\n",
      "Train Epoch: 20 [6000/6658 (90%)]\tLoss: 0.039136\n",
      "Train Epoch: 20 [6100/6658 (92%)]\tLoss: 2.335591\n",
      "Train Epoch: 20 [6200/6658 (93%)]\tLoss: 1.551926\n",
      "Train Epoch: 20 [6300/6658 (95%)]\tLoss: 0.077885\n",
      "Train Epoch: 20 [6400/6658 (96%)]\tLoss: 0.670535\n",
      "Train Epoch: 20 [6500/6658 (98%)]\tLoss: 0.014299\n",
      "Train Epoch: 20 [6600/6658 (99%)]\tLoss: 0.006966\n",
      "train loss average =  0.7458502728234705\n",
      "\n",
      "Test set: Average loss: 0.7275\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0407, 6.0111, 5.9751, 5.9629, 6.0313, 6.0200], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 21 [0/6658 (0%)]\tLoss: 0.052077\n",
      "Train Epoch: 21 [100/6658 (2%)]\tLoss: 0.067122\n",
      "Train Epoch: 21 [200/6658 (3%)]\tLoss: 0.332300\n",
      "Train Epoch: 21 [300/6658 (5%)]\tLoss: 0.036288\n",
      "Train Epoch: 21 [400/6658 (6%)]\tLoss: 8.152657\n",
      "Train Epoch: 21 [500/6658 (8%)]\tLoss: 0.356391\n",
      "Train Epoch: 21 [600/6658 (9%)]\tLoss: 0.075535\n",
      "Train Epoch: 21 [700/6658 (11%)]\tLoss: 1.473180\n",
      "Train Epoch: 21 [800/6658 (12%)]\tLoss: 0.096828\n",
      "Train Epoch: 21 [900/6658 (14%)]\tLoss: 1.150451\n",
      "Train Epoch: 21 [1000/6658 (15%)]\tLoss: 0.006560\n",
      "Train Epoch: 21 [1100/6658 (17%)]\tLoss: 0.132944\n",
      "Train Epoch: 21 [1200/6658 (18%)]\tLoss: 0.990963\n",
      "Train Epoch: 21 [1300/6658 (20%)]\tLoss: 0.488462\n",
      "Train Epoch: 21 [1400/6658 (21%)]\tLoss: 0.026524\n",
      "Train Epoch: 21 [1500/6658 (23%)]\tLoss: 0.466839\n",
      "Train Epoch: 21 [1600/6658 (24%)]\tLoss: 0.348573\n",
      "Train Epoch: 21 [1700/6658 (26%)]\tLoss: 0.629878\n",
      "Train Epoch: 21 [1800/6658 (27%)]\tLoss: 0.014850\n",
      "Train Epoch: 21 [1900/6658 (29%)]\tLoss: 8.338086\n",
      "Train Epoch: 21 [2000/6658 (30%)]\tLoss: 0.127069\n",
      "Train Epoch: 21 [2100/6658 (32%)]\tLoss: 0.904744\n",
      "Train Epoch: 21 [2200/6658 (33%)]\tLoss: 0.182571\n",
      "Train Epoch: 21 [2300/6658 (35%)]\tLoss: 0.064263\n",
      "Train Epoch: 21 [2400/6658 (36%)]\tLoss: 0.125958\n",
      "Train Epoch: 21 [2500/6658 (38%)]\tLoss: 0.197606\n",
      "Train Epoch: 21 [2600/6658 (39%)]\tLoss: 6.154903\n",
      "Train Epoch: 21 [2700/6658 (41%)]\tLoss: 0.832648\n",
      "Train Epoch: 21 [2800/6658 (42%)]\tLoss: 0.234051\n",
      "Train Epoch: 21 [2900/6658 (44%)]\tLoss: 0.461210\n",
      "Train Epoch: 21 [3000/6658 (45%)]\tLoss: 0.123229\n",
      "Train Epoch: 21 [3100/6658 (47%)]\tLoss: 0.058315\n",
      "Train Epoch: 21 [3200/6658 (48%)]\tLoss: 0.570419\n",
      "Train Epoch: 21 [3300/6658 (50%)]\tLoss: 0.268530\n",
      "Train Epoch: 21 [3400/6658 (51%)]\tLoss: 1.215584\n",
      "Train Epoch: 21 [3500/6658 (53%)]\tLoss: 0.362852\n",
      "Train Epoch: 21 [3600/6658 (54%)]\tLoss: 0.328365\n",
      "Train Epoch: 21 [3700/6658 (56%)]\tLoss: 1.260963\n",
      "Train Epoch: 21 [3800/6658 (57%)]\tLoss: 1.640764\n",
      "Train Epoch: 21 [3900/6658 (59%)]\tLoss: 11.572228\n",
      "Train Epoch: 21 [4000/6658 (60%)]\tLoss: 0.121132\n",
      "Train Epoch: 21 [4100/6658 (62%)]\tLoss: 3.140934\n",
      "Train Epoch: 21 [4200/6658 (63%)]\tLoss: 1.127880\n",
      "Train Epoch: 21 [4300/6658 (65%)]\tLoss: 0.119505\n",
      "Train Epoch: 21 [4400/6658 (66%)]\tLoss: 0.026596\n",
      "Train Epoch: 21 [4500/6658 (68%)]\tLoss: 0.316463\n",
      "Train Epoch: 21 [4600/6658 (69%)]\tLoss: 1.335808\n",
      "Train Epoch: 21 [4700/6658 (71%)]\tLoss: 0.009533\n",
      "Train Epoch: 21 [4800/6658 (72%)]\tLoss: 0.203003\n",
      "Train Epoch: 21 [4900/6658 (74%)]\tLoss: 0.138923\n",
      "Train Epoch: 21 [5000/6658 (75%)]\tLoss: 1.708825\n",
      "Train Epoch: 21 [5100/6658 (77%)]\tLoss: 0.166396\n",
      "Train Epoch: 21 [5200/6658 (78%)]\tLoss: 1.457933\n",
      "Train Epoch: 21 [5300/6658 (80%)]\tLoss: 0.000650\n",
      "Train Epoch: 21 [5400/6658 (81%)]\tLoss: 0.308298\n",
      "Train Epoch: 21 [5500/6658 (83%)]\tLoss: 0.012932\n",
      "Train Epoch: 21 [5600/6658 (84%)]\tLoss: 0.193707\n",
      "Train Epoch: 21 [5700/6658 (86%)]\tLoss: 0.094407\n",
      "Train Epoch: 21 [5800/6658 (87%)]\tLoss: 0.055782\n",
      "Train Epoch: 21 [5900/6658 (89%)]\tLoss: 1.151127\n",
      "Train Epoch: 21 [6000/6658 (90%)]\tLoss: 0.096408\n",
      "Train Epoch: 21 [6100/6658 (92%)]\tLoss: 0.350476\n",
      "Train Epoch: 21 [6200/6658 (93%)]\tLoss: 0.281801\n",
      "Train Epoch: 21 [6300/6658 (95%)]\tLoss: 0.114266\n",
      "Train Epoch: 21 [6400/6658 (96%)]\tLoss: 0.140440\n",
      "Train Epoch: 21 [6500/6658 (98%)]\tLoss: 0.000041\n",
      "Train Epoch: 21 [6600/6658 (99%)]\tLoss: 0.091253\n",
      "train loss average =  0.7439796605809912\n",
      "\n",
      "Test set: Average loss: 0.7234\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0432, 6.0115, 5.9735, 5.9612, 6.0321, 6.0207], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 22 [0/6658 (0%)]\tLoss: 0.006516\n",
      "Train Epoch: 22 [100/6658 (2%)]\tLoss: 0.012523\n",
      "Train Epoch: 22 [200/6658 (3%)]\tLoss: 1.579853\n",
      "Train Epoch: 22 [300/6658 (5%)]\tLoss: 0.106729\n",
      "Train Epoch: 22 [400/6658 (6%)]\tLoss: 0.291425\n",
      "Train Epoch: 22 [500/6658 (8%)]\tLoss: 12.219210\n",
      "Train Epoch: 22 [600/6658 (9%)]\tLoss: 1.487460\n",
      "Train Epoch: 22 [700/6658 (11%)]\tLoss: 0.447265\n",
      "Train Epoch: 22 [800/6658 (12%)]\tLoss: 0.284623\n",
      "Train Epoch: 22 [900/6658 (14%)]\tLoss: 1.395602\n",
      "Train Epoch: 22 [1000/6658 (15%)]\tLoss: 0.147612\n",
      "Train Epoch: 22 [1100/6658 (17%)]\tLoss: 0.038767\n",
      "Train Epoch: 22 [1200/6658 (18%)]\tLoss: 1.166070\n",
      "Train Epoch: 22 [1300/6658 (20%)]\tLoss: 0.930817\n",
      "Train Epoch: 22 [1400/6658 (21%)]\tLoss: 1.756161\n",
      "Train Epoch: 22 [1500/6658 (23%)]\tLoss: 0.050770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22 [1600/6658 (24%)]\tLoss: 0.492549\n",
      "Train Epoch: 22 [1700/6658 (26%)]\tLoss: 0.017199\n",
      "Train Epoch: 22 [1800/6658 (27%)]\tLoss: 0.172829\n",
      "Train Epoch: 22 [1900/6658 (29%)]\tLoss: 0.210765\n",
      "Train Epoch: 22 [2000/6658 (30%)]\tLoss: 0.042206\n",
      "Train Epoch: 22 [2100/6658 (32%)]\tLoss: 0.304113\n",
      "Train Epoch: 22 [2200/6658 (33%)]\tLoss: 0.148868\n",
      "Train Epoch: 22 [2300/6658 (35%)]\tLoss: 0.433373\n",
      "Train Epoch: 22 [2400/6658 (36%)]\tLoss: 1.063595\n",
      "Train Epoch: 22 [2500/6658 (38%)]\tLoss: 0.145533\n",
      "Train Epoch: 22 [2600/6658 (39%)]\tLoss: 0.061689\n",
      "Train Epoch: 22 [2700/6658 (41%)]\tLoss: 0.952070\n",
      "Train Epoch: 22 [2800/6658 (42%)]\tLoss: 2.049512\n",
      "Train Epoch: 22 [2900/6658 (44%)]\tLoss: 0.450909\n",
      "Train Epoch: 22 [3000/6658 (45%)]\tLoss: 1.414335\n",
      "Train Epoch: 22 [3100/6658 (47%)]\tLoss: 0.034331\n",
      "Train Epoch: 22 [3200/6658 (48%)]\tLoss: 0.473362\n",
      "Train Epoch: 22 [3300/6658 (50%)]\tLoss: 0.508216\n",
      "Train Epoch: 22 [3400/6658 (51%)]\tLoss: 2.666251\n",
      "Train Epoch: 22 [3500/6658 (53%)]\tLoss: 0.208372\n",
      "Train Epoch: 22 [3600/6658 (54%)]\tLoss: 0.105735\n",
      "Train Epoch: 22 [3700/6658 (56%)]\tLoss: 0.718165\n",
      "Train Epoch: 22 [3800/6658 (57%)]\tLoss: 0.387031\n",
      "Train Epoch: 22 [3900/6658 (59%)]\tLoss: 2.107274\n",
      "Train Epoch: 22 [4000/6658 (60%)]\tLoss: 0.160669\n",
      "Train Epoch: 22 [4100/6658 (62%)]\tLoss: 0.027444\n",
      "Train Epoch: 22 [4200/6658 (63%)]\tLoss: 0.022149\n",
      "Train Epoch: 22 [4300/6658 (65%)]\tLoss: 0.086429\n",
      "Train Epoch: 22 [4400/6658 (66%)]\tLoss: 0.730624\n",
      "Train Epoch: 22 [4500/6658 (68%)]\tLoss: 0.584505\n",
      "Train Epoch: 22 [4600/6658 (69%)]\tLoss: 1.132490\n",
      "Train Epoch: 22 [4700/6658 (71%)]\tLoss: 1.616897\n",
      "Train Epoch: 22 [4800/6658 (72%)]\tLoss: 0.083191\n",
      "Train Epoch: 22 [4900/6658 (74%)]\tLoss: 8.465261\n",
      "Train Epoch: 22 [5000/6658 (75%)]\tLoss: 0.593031\n",
      "Train Epoch: 22 [5100/6658 (77%)]\tLoss: 0.001408\n",
      "Train Epoch: 22 [5200/6658 (78%)]\tLoss: 0.201030\n",
      "Train Epoch: 22 [5300/6658 (80%)]\tLoss: 0.435336\n",
      "Train Epoch: 22 [5400/6658 (81%)]\tLoss: 0.000218\n",
      "Train Epoch: 22 [5500/6658 (83%)]\tLoss: 0.007918\n",
      "Train Epoch: 22 [5600/6658 (84%)]\tLoss: 0.405502\n",
      "Train Epoch: 22 [5700/6658 (86%)]\tLoss: 0.093442\n",
      "Train Epoch: 22 [5800/6658 (87%)]\tLoss: 0.157118\n",
      "Train Epoch: 22 [5900/6658 (89%)]\tLoss: 4.716664\n",
      "Train Epoch: 22 [6000/6658 (90%)]\tLoss: 1.088189\n",
      "Train Epoch: 22 [6100/6658 (92%)]\tLoss: 0.134611\n",
      "Train Epoch: 22 [6200/6658 (93%)]\tLoss: 0.025066\n",
      "Train Epoch: 22 [6300/6658 (95%)]\tLoss: 0.131375\n",
      "Train Epoch: 22 [6400/6658 (96%)]\tLoss: 1.935225\n",
      "Train Epoch: 22 [6500/6658 (98%)]\tLoss: 0.068527\n",
      "Train Epoch: 22 [6600/6658 (99%)]\tLoss: 0.691411\n",
      "train loss average =  0.7472421225920726\n",
      "\n",
      "Test set: Average loss: 0.7245\n",
      "\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0467, 6.0119, 5.9723, 5.9592, 6.0331, 6.0216], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 23 [0/6658 (0%)]\tLoss: 0.688041\n",
      "Train Epoch: 23 [100/6658 (2%)]\tLoss: 0.219602\n",
      "Train Epoch: 23 [200/6658 (3%)]\tLoss: 0.105894\n",
      "Train Epoch: 23 [300/6658 (5%)]\tLoss: 2.432086\n",
      "Train Epoch: 23 [400/6658 (6%)]\tLoss: 0.009228\n",
      "Train Epoch: 23 [500/6658 (8%)]\tLoss: 2.807698\n",
      "Train Epoch: 23 [600/6658 (9%)]\tLoss: 0.000374\n",
      "Train Epoch: 23 [700/6658 (11%)]\tLoss: 0.116951\n",
      "Train Epoch: 23 [800/6658 (12%)]\tLoss: 0.021327\n",
      "Train Epoch: 23 [900/6658 (14%)]\tLoss: 0.189805\n",
      "Train Epoch: 23 [1000/6658 (15%)]\tLoss: 0.257216\n",
      "Train Epoch: 23 [1100/6658 (17%)]\tLoss: 3.030772\n",
      "Train Epoch: 23 [1200/6658 (18%)]\tLoss: 0.251883\n",
      "Train Epoch: 23 [1300/6658 (20%)]\tLoss: 0.018532\n",
      "Train Epoch: 23 [1400/6658 (21%)]\tLoss: 0.083579\n",
      "Train Epoch: 23 [1500/6658 (23%)]\tLoss: 0.173493\n",
      "Train Epoch: 23 [1600/6658 (24%)]\tLoss: 0.528374\n",
      "Train Epoch: 23 [1700/6658 (26%)]\tLoss: 0.052142\n",
      "Train Epoch: 23 [1800/6658 (27%)]\tLoss: 0.484141\n",
      "Train Epoch: 23 [1900/6658 (29%)]\tLoss: 0.019282\n",
      "Train Epoch: 23 [2000/6658 (30%)]\tLoss: 0.418930\n",
      "Train Epoch: 23 [2100/6658 (32%)]\tLoss: 0.763138\n",
      "Train Epoch: 23 [2200/6658 (33%)]\tLoss: 0.051954\n",
      "Train Epoch: 23 [2300/6658 (35%)]\tLoss: 0.535439\n",
      "Train Epoch: 23 [2400/6658 (36%)]\tLoss: 1.609391\n",
      "Train Epoch: 23 [2500/6658 (38%)]\tLoss: 0.005811\n",
      "Train Epoch: 23 [2600/6658 (39%)]\tLoss: 0.182813\n",
      "Train Epoch: 23 [2700/6658 (41%)]\tLoss: 0.283896\n",
      "Train Epoch: 23 [2800/6658 (42%)]\tLoss: 0.010064\n",
      "Train Epoch: 23 [2900/6658 (44%)]\tLoss: 0.141227\n",
      "Train Epoch: 23 [3000/6658 (45%)]\tLoss: 0.013428\n",
      "Train Epoch: 23 [3100/6658 (47%)]\tLoss: 0.876695\n",
      "Train Epoch: 23 [3200/6658 (48%)]\tLoss: 0.539951\n",
      "Train Epoch: 23 [3300/6658 (50%)]\tLoss: 0.159791\n",
      "Train Epoch: 23 [3400/6658 (51%)]\tLoss: 2.119358\n",
      "Train Epoch: 23 [3500/6658 (53%)]\tLoss: 0.000122\n",
      "Train Epoch: 23 [3600/6658 (54%)]\tLoss: 0.379295\n",
      "Train Epoch: 23 [3700/6658 (56%)]\tLoss: 0.013642\n",
      "Train Epoch: 23 [3800/6658 (57%)]\tLoss: 0.642251\n",
      "Train Epoch: 23 [3900/6658 (59%)]\tLoss: 0.279852\n",
      "Train Epoch: 23 [4000/6658 (60%)]\tLoss: 0.079641\n",
      "Train Epoch: 23 [4100/6658 (62%)]\tLoss: 0.770125\n",
      "Train Epoch: 23 [4200/6658 (63%)]\tLoss: 0.005327\n",
      "Train Epoch: 23 [4300/6658 (65%)]\tLoss: 0.031602\n",
      "Train Epoch: 23 [4400/6658 (66%)]\tLoss: 0.309467\n",
      "Train Epoch: 23 [4500/6658 (68%)]\tLoss: 0.271475\n",
      "Train Epoch: 23 [4600/6658 (69%)]\tLoss: 0.092688\n",
      "Train Epoch: 23 [4700/6658 (71%)]\tLoss: 0.513010\n",
      "Train Epoch: 23 [4800/6658 (72%)]\tLoss: 7.441697\n",
      "Train Epoch: 23 [4900/6658 (74%)]\tLoss: 0.548844\n",
      "Train Epoch: 23 [5000/6658 (75%)]\tLoss: 0.256524\n",
      "Train Epoch: 23 [5100/6658 (77%)]\tLoss: 0.505413\n",
      "Train Epoch: 23 [5200/6658 (78%)]\tLoss: 0.037453\n",
      "Train Epoch: 23 [5300/6658 (80%)]\tLoss: 0.240924\n",
      "Train Epoch: 23 [5400/6658 (81%)]\tLoss: 3.560954\n",
      "Train Epoch: 23 [5500/6658 (83%)]\tLoss: 0.069692\n",
      "Train Epoch: 23 [5600/6658 (84%)]\tLoss: 0.072620\n",
      "Train Epoch: 23 [5700/6658 (86%)]\tLoss: 1.407066\n",
      "Train Epoch: 23 [5800/6658 (87%)]\tLoss: 1.124475\n",
      "Train Epoch: 23 [5900/6658 (89%)]\tLoss: 0.012920\n",
      "Train Epoch: 23 [6000/6658 (90%)]\tLoss: 0.730982\n",
      "Train Epoch: 23 [6100/6658 (92%)]\tLoss: 0.017197\n",
      "Train Epoch: 23 [6200/6658 (93%)]\tLoss: 0.489175\n",
      "Train Epoch: 23 [6300/6658 (95%)]\tLoss: 0.266175\n",
      "Train Epoch: 23 [6400/6658 (96%)]\tLoss: 0.278719\n",
      "Train Epoch: 23 [6500/6658 (98%)]\tLoss: 0.428322\n",
      "Train Epoch: 23 [6600/6658 (99%)]\tLoss: 0.163824\n",
      "train loss average =  0.7464565131678661\n",
      "\n",
      "Test set: Average loss: 0.7144\n",
      "\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0490, 6.0124, 5.9716, 5.9577, 6.0346, 6.0217], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 24 [0/6658 (0%)]\tLoss: 0.035570\n",
      "Train Epoch: 24 [100/6658 (2%)]\tLoss: 0.000642\n",
      "Train Epoch: 24 [200/6658 (3%)]\tLoss: 0.640699\n",
      "Train Epoch: 24 [300/6658 (5%)]\tLoss: 0.210401\n",
      "Train Epoch: 24 [400/6658 (6%)]\tLoss: 0.328081\n",
      "Train Epoch: 24 [500/6658 (8%)]\tLoss: 1.349989\n",
      "Train Epoch: 24 [600/6658 (9%)]\tLoss: 0.013340\n",
      "Train Epoch: 24 [700/6658 (11%)]\tLoss: 0.083673\n",
      "Train Epoch: 24 [800/6658 (12%)]\tLoss: 0.070879\n",
      "Train Epoch: 24 [900/6658 (14%)]\tLoss: 0.141812\n",
      "Train Epoch: 24 [1000/6658 (15%)]\tLoss: 0.037510\n",
      "Train Epoch: 24 [1100/6658 (17%)]\tLoss: 0.039305\n",
      "Train Epoch: 24 [1200/6658 (18%)]\tLoss: 0.200067\n",
      "Train Epoch: 24 [1300/6658 (20%)]\tLoss: 0.728470\n",
      "Train Epoch: 24 [1400/6658 (21%)]\tLoss: 0.049837\n",
      "Train Epoch: 24 [1500/6658 (23%)]\tLoss: 0.572482\n",
      "Train Epoch: 24 [1600/6658 (24%)]\tLoss: 0.162019\n",
      "Train Epoch: 24 [1700/6658 (26%)]\tLoss: 0.872963\n",
      "Train Epoch: 24 [1800/6658 (27%)]\tLoss: 0.038876\n",
      "Train Epoch: 24 [1900/6658 (29%)]\tLoss: 0.094343\n",
      "Train Epoch: 24 [2000/6658 (30%)]\tLoss: 9.149431\n",
      "Train Epoch: 24 [2100/6658 (32%)]\tLoss: 0.019965\n",
      "Train Epoch: 24 [2200/6658 (33%)]\tLoss: 0.306107\n",
      "Train Epoch: 24 [2300/6658 (35%)]\tLoss: 0.770765\n",
      "Train Epoch: 24 [2400/6658 (36%)]\tLoss: 1.122774\n",
      "Train Epoch: 24 [2500/6658 (38%)]\tLoss: 0.379774\n",
      "Train Epoch: 24 [2600/6658 (39%)]\tLoss: 1.479604\n",
      "Train Epoch: 24 [2700/6658 (41%)]\tLoss: 0.170879\n",
      "Train Epoch: 24 [2800/6658 (42%)]\tLoss: 0.049739\n",
      "Train Epoch: 24 [2900/6658 (44%)]\tLoss: 0.553174\n",
      "Train Epoch: 24 [3000/6658 (45%)]\tLoss: 0.023090\n",
      "Train Epoch: 24 [3100/6658 (47%)]\tLoss: 0.417449\n",
      "Train Epoch: 24 [3200/6658 (48%)]\tLoss: 0.220050\n",
      "Train Epoch: 24 [3300/6658 (50%)]\tLoss: 0.238736\n",
      "Train Epoch: 24 [3400/6658 (51%)]\tLoss: 0.008228\n",
      "Train Epoch: 24 [3500/6658 (53%)]\tLoss: 0.215085\n",
      "Train Epoch: 24 [3600/6658 (54%)]\tLoss: 0.344604\n",
      "Train Epoch: 24 [3700/6658 (56%)]\tLoss: 0.387167\n",
      "Train Epoch: 24 [3800/6658 (57%)]\tLoss: 0.284657\n",
      "Train Epoch: 24 [3900/6658 (59%)]\tLoss: 1.232065\n",
      "Train Epoch: 24 [4000/6658 (60%)]\tLoss: 0.018705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [4100/6658 (62%)]\tLoss: 0.003697\n",
      "Train Epoch: 24 [4200/6658 (63%)]\tLoss: 0.006600\n",
      "Train Epoch: 24 [4300/6658 (65%)]\tLoss: 0.028240\n",
      "Train Epoch: 24 [4400/6658 (66%)]\tLoss: 0.238136\n",
      "Train Epoch: 24 [4500/6658 (68%)]\tLoss: 0.309334\n",
      "Train Epoch: 24 [4600/6658 (69%)]\tLoss: 0.130173\n",
      "Train Epoch: 24 [4700/6658 (71%)]\tLoss: 0.398725\n",
      "Train Epoch: 24 [4800/6658 (72%)]\tLoss: 0.379284\n",
      "Train Epoch: 24 [4900/6658 (74%)]\tLoss: 0.250107\n",
      "Train Epoch: 24 [5000/6658 (75%)]\tLoss: 0.588840\n",
      "Train Epoch: 24 [5100/6658 (77%)]\tLoss: 0.159931\n",
      "Train Epoch: 24 [5200/6658 (78%)]\tLoss: 2.128728\n",
      "Train Epoch: 24 [5300/6658 (80%)]\tLoss: 1.951343\n",
      "Train Epoch: 24 [5400/6658 (81%)]\tLoss: 0.033538\n",
      "Train Epoch: 24 [5500/6658 (83%)]\tLoss: 0.000238\n",
      "Train Epoch: 24 [5600/6658 (84%)]\tLoss: 11.017915\n",
      "Train Epoch: 24 [5700/6658 (86%)]\tLoss: 1.402519\n",
      "Train Epoch: 24 [5800/6658 (87%)]\tLoss: 0.017523\n",
      "Train Epoch: 24 [5900/6658 (89%)]\tLoss: 0.137686\n",
      "Train Epoch: 24 [6000/6658 (90%)]\tLoss: 2.558346\n",
      "Train Epoch: 24 [6100/6658 (92%)]\tLoss: 0.281688\n",
      "Train Epoch: 24 [6200/6658 (93%)]\tLoss: 0.805493\n",
      "Train Epoch: 24 [6300/6658 (95%)]\tLoss: 0.506558\n",
      "Train Epoch: 24 [6400/6658 (96%)]\tLoss: 0.964935\n",
      "Train Epoch: 24 [6500/6658 (98%)]\tLoss: 0.018682\n",
      "Train Epoch: 24 [6600/6658 (99%)]\tLoss: 0.373309\n",
      "train loss average =  0.7467239676973971\n",
      "\n",
      "Test set: Average loss: 0.7214\n",
      "\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0511, 6.0128, 5.9704, 5.9553, 6.0360, 6.0223], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 25 [0/6658 (0%)]\tLoss: 0.389049\n",
      "Train Epoch: 25 [100/6658 (2%)]\tLoss: 0.694006\n",
      "Train Epoch: 25 [200/6658 (3%)]\tLoss: 0.025598\n",
      "Train Epoch: 25 [300/6658 (5%)]\tLoss: 0.014853\n",
      "Train Epoch: 25 [400/6658 (6%)]\tLoss: 6.336689\n",
      "Train Epoch: 25 [500/6658 (8%)]\tLoss: 0.317174\n",
      "Train Epoch: 25 [600/6658 (9%)]\tLoss: 0.246545\n",
      "Train Epoch: 25 [700/6658 (11%)]\tLoss: 0.005427\n",
      "Train Epoch: 25 [800/6658 (12%)]\tLoss: 0.001361\n",
      "Train Epoch: 25 [900/6658 (14%)]\tLoss: 0.000862\n",
      "Train Epoch: 25 [1000/6658 (15%)]\tLoss: 0.143574\n",
      "Train Epoch: 25 [1100/6658 (17%)]\tLoss: 0.307706\n",
      "Train Epoch: 25 [1200/6658 (18%)]\tLoss: 0.068061\n",
      "Train Epoch: 25 [1300/6658 (20%)]\tLoss: 0.167689\n",
      "Train Epoch: 25 [1400/6658 (21%)]\tLoss: 0.702373\n",
      "Train Epoch: 25 [1500/6658 (23%)]\tLoss: 0.095246\n",
      "Train Epoch: 25 [1600/6658 (24%)]\tLoss: 0.153117\n",
      "Train Epoch: 25 [1700/6658 (26%)]\tLoss: 0.561929\n",
      "Train Epoch: 25 [1800/6658 (27%)]\tLoss: 0.181228\n",
      "Train Epoch: 25 [1900/6658 (29%)]\tLoss: 0.937609\n",
      "Train Epoch: 25 [2000/6658 (30%)]\tLoss: 0.744728\n",
      "Train Epoch: 25 [2100/6658 (32%)]\tLoss: 0.952381\n",
      "Train Epoch: 25 [2200/6658 (33%)]\tLoss: 0.066690\n",
      "Train Epoch: 25 [2300/6658 (35%)]\tLoss: 0.198760\n",
      "Train Epoch: 25 [2400/6658 (36%)]\tLoss: 0.236357\n",
      "Train Epoch: 25 [2500/6658 (38%)]\tLoss: 0.007776\n",
      "Train Epoch: 25 [2600/6658 (39%)]\tLoss: 0.059162\n",
      "Train Epoch: 25 [2700/6658 (41%)]\tLoss: 0.624567\n",
      "Train Epoch: 25 [2800/6658 (42%)]\tLoss: 0.041639\n",
      "Train Epoch: 25 [2900/6658 (44%)]\tLoss: 1.503736\n",
      "Train Epoch: 25 [3000/6658 (45%)]\tLoss: 0.249970\n",
      "Train Epoch: 25 [3100/6658 (47%)]\tLoss: 3.132623\n",
      "Train Epoch: 25 [3200/6658 (48%)]\tLoss: 11.688160\n",
      "Train Epoch: 25 [3300/6658 (50%)]\tLoss: 0.124676\n",
      "Train Epoch: 25 [3400/6658 (51%)]\tLoss: 0.405013\n",
      "Train Epoch: 25 [3500/6658 (53%)]\tLoss: 0.034088\n",
      "Train Epoch: 25 [3600/6658 (54%)]\tLoss: 0.196290\n",
      "Train Epoch: 25 [3700/6658 (56%)]\tLoss: 0.307629\n",
      "Train Epoch: 25 [3800/6658 (57%)]\tLoss: 0.010340\n",
      "Train Epoch: 25 [3900/6658 (59%)]\tLoss: 0.058819\n",
      "Train Epoch: 25 [4000/6658 (60%)]\tLoss: 0.347421\n",
      "Train Epoch: 25 [4100/6658 (62%)]\tLoss: 0.000285\n",
      "Train Epoch: 25 [4200/6658 (63%)]\tLoss: 0.498595\n",
      "Train Epoch: 25 [4300/6658 (65%)]\tLoss: 0.954956\n",
      "Train Epoch: 25 [4400/6658 (66%)]\tLoss: 0.353565\n",
      "Train Epoch: 25 [4500/6658 (68%)]\tLoss: 0.234033\n",
      "Train Epoch: 25 [4600/6658 (69%)]\tLoss: 0.016711\n",
      "Train Epoch: 25 [4700/6658 (71%)]\tLoss: 0.131490\n",
      "Train Epoch: 25 [4800/6658 (72%)]\tLoss: 0.085762\n",
      "Train Epoch: 25 [4900/6658 (74%)]\tLoss: 0.301106\n",
      "Train Epoch: 25 [5000/6658 (75%)]\tLoss: 0.026295\n",
      "Train Epoch: 25 [5100/6658 (77%)]\tLoss: 0.392388\n",
      "Train Epoch: 25 [5200/6658 (78%)]\tLoss: 0.317943\n",
      "Train Epoch: 25 [5300/6658 (80%)]\tLoss: 0.576970\n",
      "Train Epoch: 25 [5400/6658 (81%)]\tLoss: 0.948512\n",
      "Train Epoch: 25 [5500/6658 (83%)]\tLoss: 0.083801\n",
      "Train Epoch: 25 [5600/6658 (84%)]\tLoss: 0.131095\n",
      "Train Epoch: 25 [5700/6658 (86%)]\tLoss: 5.793492\n",
      "Train Epoch: 25 [5800/6658 (87%)]\tLoss: 0.367697\n",
      "Train Epoch: 25 [5900/6658 (89%)]\tLoss: 0.523617\n",
      "Train Epoch: 25 [6000/6658 (90%)]\tLoss: 0.293698\n",
      "Train Epoch: 25 [6100/6658 (92%)]\tLoss: 0.388400\n",
      "Train Epoch: 25 [6200/6658 (93%)]\tLoss: 2.684409\n",
      "Train Epoch: 25 [6300/6658 (95%)]\tLoss: 0.086151\n",
      "Train Epoch: 25 [6400/6658 (96%)]\tLoss: 3.469339\n",
      "Train Epoch: 25 [6500/6658 (98%)]\tLoss: 0.344366\n",
      "Train Epoch: 25 [6600/6658 (99%)]\tLoss: 0.736774\n",
      "train loss average =  0.7385989460548724\n",
      "\n",
      "Test set: Average loss: 0.7128\n",
      "\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0539, 6.0136, 5.9689, 5.9535, 6.0374, 6.0234], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 26 [0/6658 (0%)]\tLoss: 0.583203\n",
      "Train Epoch: 26 [100/6658 (2%)]\tLoss: 0.414420\n",
      "Train Epoch: 26 [200/6658 (3%)]\tLoss: 0.441641\n",
      "Train Epoch: 26 [300/6658 (5%)]\tLoss: 0.118260\n",
      "Train Epoch: 26 [400/6658 (6%)]\tLoss: 0.069375\n",
      "Train Epoch: 26 [500/6658 (8%)]\tLoss: 0.007252\n",
      "Train Epoch: 26 [600/6658 (9%)]\tLoss: 0.472280\n",
      "Train Epoch: 26 [700/6658 (11%)]\tLoss: 0.631326\n",
      "Train Epoch: 26 [800/6658 (12%)]\tLoss: 0.532118\n",
      "Train Epoch: 26 [900/6658 (14%)]\tLoss: 0.042645\n",
      "Train Epoch: 26 [1000/6658 (15%)]\tLoss: 0.122523\n",
      "Train Epoch: 26 [1100/6658 (17%)]\tLoss: 0.326259\n",
      "Train Epoch: 26 [1200/6658 (18%)]\tLoss: 0.017997\n",
      "Train Epoch: 26 [1300/6658 (20%)]\tLoss: 0.000052\n",
      "Train Epoch: 26 [1400/6658 (21%)]\tLoss: 1.437147\n",
      "Train Epoch: 26 [1500/6658 (23%)]\tLoss: 2.978510\n",
      "Train Epoch: 26 [1600/6658 (24%)]\tLoss: 0.522018\n",
      "Train Epoch: 26 [1700/6658 (26%)]\tLoss: 0.825896\n",
      "Train Epoch: 26 [1800/6658 (27%)]\tLoss: 0.359793\n",
      "Train Epoch: 26 [1900/6658 (29%)]\tLoss: 0.508721\n",
      "Train Epoch: 26 [2000/6658 (30%)]\tLoss: 3.040956\n",
      "Train Epoch: 26 [2100/6658 (32%)]\tLoss: 0.585949\n",
      "Train Epoch: 26 [2200/6658 (33%)]\tLoss: 0.447122\n",
      "Train Epoch: 26 [2300/6658 (35%)]\tLoss: 0.005808\n",
      "Train Epoch: 26 [2400/6658 (36%)]\tLoss: 9.035570\n",
      "Train Epoch: 26 [2500/6658 (38%)]\tLoss: 0.259308\n",
      "Train Epoch: 26 [2600/6658 (39%)]\tLoss: 0.178867\n",
      "Train Epoch: 26 [2700/6658 (41%)]\tLoss: 0.060334\n",
      "Train Epoch: 26 [2800/6658 (42%)]\tLoss: 0.057861\n",
      "Train Epoch: 26 [2900/6658 (44%)]\tLoss: 1.208391\n",
      "Train Epoch: 26 [3000/6658 (45%)]\tLoss: 0.226229\n",
      "Train Epoch: 26 [3100/6658 (47%)]\tLoss: 0.163042\n",
      "Train Epoch: 26 [3200/6658 (48%)]\tLoss: 5.983916\n",
      "Train Epoch: 26 [3300/6658 (50%)]\tLoss: 0.054625\n",
      "Train Epoch: 26 [3400/6658 (51%)]\tLoss: 1.067575\n",
      "Train Epoch: 26 [3500/6658 (53%)]\tLoss: 0.063766\n",
      "Train Epoch: 26 [3600/6658 (54%)]\tLoss: 0.000156\n",
      "Train Epoch: 26 [3700/6658 (56%)]\tLoss: 0.552943\n",
      "Train Epoch: 26 [3800/6658 (57%)]\tLoss: 0.724050\n",
      "Train Epoch: 26 [3900/6658 (59%)]\tLoss: 1.135106\n",
      "Train Epoch: 26 [4000/6658 (60%)]\tLoss: 0.808047\n",
      "Train Epoch: 26 [4100/6658 (62%)]\tLoss: 0.008718\n",
      "Train Epoch: 26 [4200/6658 (63%)]\tLoss: 0.001847\n",
      "Train Epoch: 26 [4300/6658 (65%)]\tLoss: 0.383823\n",
      "Train Epoch: 26 [4400/6658 (66%)]\tLoss: 1.352363\n",
      "Train Epoch: 26 [4500/6658 (68%)]\tLoss: 4.452861\n",
      "Train Epoch: 26 [4600/6658 (69%)]\tLoss: 0.110038\n",
      "Train Epoch: 26 [4700/6658 (71%)]\tLoss: 0.000175\n",
      "Train Epoch: 26 [4800/6658 (72%)]\tLoss: 0.021858\n",
      "Train Epoch: 26 [4900/6658 (74%)]\tLoss: 0.114345\n",
      "Train Epoch: 26 [5000/6658 (75%)]\tLoss: 0.145795\n",
      "Train Epoch: 26 [5100/6658 (77%)]\tLoss: 0.545986\n",
      "Train Epoch: 26 [5200/6658 (78%)]\tLoss: 0.347444\n",
      "Train Epoch: 26 [5300/6658 (80%)]\tLoss: 2.159913\n",
      "Train Epoch: 26 [5400/6658 (81%)]\tLoss: 2.455999\n",
      "Train Epoch: 26 [5500/6658 (83%)]\tLoss: 0.294310\n",
      "Train Epoch: 26 [5600/6658 (84%)]\tLoss: 0.793433\n",
      "Train Epoch: 26 [5700/6658 (86%)]\tLoss: 1.101535\n",
      "Train Epoch: 26 [5800/6658 (87%)]\tLoss: 1.160796\n",
      "Train Epoch: 26 [5900/6658 (89%)]\tLoss: 0.002469\n",
      "Train Epoch: 26 [6000/6658 (90%)]\tLoss: 0.045158\n",
      "Train Epoch: 26 [6100/6658 (92%)]\tLoss: 0.778198\n",
      "Train Epoch: 26 [6200/6658 (93%)]\tLoss: 0.520587\n",
      "Train Epoch: 26 [6300/6658 (95%)]\tLoss: 0.307917\n",
      "Train Epoch: 26 [6400/6658 (96%)]\tLoss: 1.427495\n",
      "Train Epoch: 26 [6500/6658 (98%)]\tLoss: 0.188661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26 [6600/6658 (99%)]\tLoss: 0.252287\n",
      "train loss average =  0.7417878392637188\n",
      "\n",
      "Test set: Average loss: 0.7237\n",
      "\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0560, 6.0144, 5.9683, 5.9519, 6.0386, 6.0233], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 27 [0/6658 (0%)]\tLoss: 2.032781\n",
      "Train Epoch: 27 [100/6658 (2%)]\tLoss: 0.127428\n",
      "Train Epoch: 27 [200/6658 (3%)]\tLoss: 0.929205\n",
      "Train Epoch: 27 [300/6658 (5%)]\tLoss: 8.270803\n",
      "Train Epoch: 27 [400/6658 (6%)]\tLoss: 0.845954\n",
      "Train Epoch: 27 [500/6658 (8%)]\tLoss: 0.101241\n",
      "Train Epoch: 27 [600/6658 (9%)]\tLoss: 0.757393\n",
      "Train Epoch: 27 [700/6658 (11%)]\tLoss: 0.349748\n",
      "Train Epoch: 27 [800/6658 (12%)]\tLoss: 0.166572\n",
      "Train Epoch: 27 [900/6658 (14%)]\tLoss: 1.835791\n",
      "Train Epoch: 27 [1000/6658 (15%)]\tLoss: 1.416655\n",
      "Train Epoch: 27 [1100/6658 (17%)]\tLoss: 1.714077\n",
      "Train Epoch: 27 [1200/6658 (18%)]\tLoss: 0.011093\n",
      "Train Epoch: 27 [1300/6658 (20%)]\tLoss: 0.029923\n",
      "Train Epoch: 27 [1400/6658 (21%)]\tLoss: 0.010896\n",
      "Train Epoch: 27 [1500/6658 (23%)]\tLoss: 0.428664\n",
      "Train Epoch: 27 [1600/6658 (24%)]\tLoss: 0.273969\n",
      "Train Epoch: 27 [1700/6658 (26%)]\tLoss: 0.220917\n",
      "Train Epoch: 27 [1800/6658 (27%)]\tLoss: 0.062498\n",
      "Train Epoch: 27 [1900/6658 (29%)]\tLoss: 0.112051\n",
      "Train Epoch: 27 [2000/6658 (30%)]\tLoss: 0.026548\n",
      "Train Epoch: 27 [2100/6658 (32%)]\tLoss: 0.511999\n",
      "Train Epoch: 27 [2200/6658 (33%)]\tLoss: 0.007760\n",
      "Train Epoch: 27 [2300/6658 (35%)]\tLoss: 0.128944\n",
      "Train Epoch: 27 [2400/6658 (36%)]\tLoss: 0.276819\n",
      "Train Epoch: 27 [2500/6658 (38%)]\tLoss: 0.045175\n",
      "Train Epoch: 27 [2600/6658 (39%)]\tLoss: 0.095502\n",
      "Train Epoch: 27 [2700/6658 (41%)]\tLoss: 0.030462\n",
      "Train Epoch: 27 [2800/6658 (42%)]\tLoss: 0.854631\n",
      "Train Epoch: 27 [2900/6658 (44%)]\tLoss: 2.038753\n",
      "Train Epoch: 27 [3000/6658 (45%)]\tLoss: 0.010835\n",
      "Train Epoch: 27 [3100/6658 (47%)]\tLoss: 0.030702\n",
      "Train Epoch: 27 [3200/6658 (48%)]\tLoss: 1.222938\n",
      "Train Epoch: 27 [3300/6658 (50%)]\tLoss: 2.910285\n",
      "Train Epoch: 27 [3400/6658 (51%)]\tLoss: 0.327129\n",
      "Train Epoch: 27 [3500/6658 (53%)]\tLoss: 2.668571\n",
      "Train Epoch: 27 [3600/6658 (54%)]\tLoss: 0.042242\n",
      "Train Epoch: 27 [3700/6658 (56%)]\tLoss: 0.287765\n",
      "Train Epoch: 27 [3800/6658 (57%)]\tLoss: 0.350811\n",
      "Train Epoch: 27 [3900/6658 (59%)]\tLoss: 0.940095\n",
      "Train Epoch: 27 [4000/6658 (60%)]\tLoss: 0.296131\n",
      "Train Epoch: 27 [4100/6658 (62%)]\tLoss: 1.413635\n",
      "Train Epoch: 27 [4200/6658 (63%)]\tLoss: 0.032107\n",
      "Train Epoch: 27 [4300/6658 (65%)]\tLoss: 0.014650\n",
      "Train Epoch: 27 [4400/6658 (66%)]\tLoss: 0.742054\n",
      "Train Epoch: 27 [4500/6658 (68%)]\tLoss: 0.329638\n",
      "Train Epoch: 27 [4600/6658 (69%)]\tLoss: 0.333361\n",
      "Train Epoch: 27 [4700/6658 (71%)]\tLoss: 0.187008\n",
      "Train Epoch: 27 [4800/6658 (72%)]\tLoss: 0.151013\n",
      "Train Epoch: 27 [4900/6658 (74%)]\tLoss: 0.408882\n",
      "Train Epoch: 27 [5000/6658 (75%)]\tLoss: 0.398888\n",
      "Train Epoch: 27 [5100/6658 (77%)]\tLoss: 1.254004\n",
      "Train Epoch: 27 [5200/6658 (78%)]\tLoss: 0.907517\n",
      "Train Epoch: 27 [5300/6658 (80%)]\tLoss: 0.120801\n",
      "Train Epoch: 27 [5400/6658 (81%)]\tLoss: 0.510148\n",
      "Train Epoch: 27 [5500/6658 (83%)]\tLoss: 0.010999\n",
      "Train Epoch: 27 [5600/6658 (84%)]\tLoss: 0.155626\n",
      "Train Epoch: 27 [5700/6658 (86%)]\tLoss: 0.033148\n",
      "Train Epoch: 27 [5800/6658 (87%)]\tLoss: 0.043867\n",
      "Train Epoch: 27 [5900/6658 (89%)]\tLoss: 1.617794\n",
      "Train Epoch: 27 [6000/6658 (90%)]\tLoss: 0.410071\n",
      "Train Epoch: 27 [6100/6658 (92%)]\tLoss: 0.224849\n",
      "Train Epoch: 27 [6200/6658 (93%)]\tLoss: 2.018889\n",
      "Train Epoch: 27 [6300/6658 (95%)]\tLoss: 0.681751\n",
      "Train Epoch: 27 [6400/6658 (96%)]\tLoss: 0.009408\n",
      "Train Epoch: 27 [6500/6658 (98%)]\tLoss: 0.067321\n",
      "Train Epoch: 27 [6600/6658 (99%)]\tLoss: 1.910842\n",
      "train loss average =  0.7411772685540345\n",
      "\n",
      "Test set: Average loss: 0.7113\n",
      "\n",
      "EarlyStopping counter: 9 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0591, 6.0154, 5.9670, 5.9505, 6.0398, 6.0236], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 28 [0/6658 (0%)]\tLoss: 0.044013\n",
      "Train Epoch: 28 [100/6658 (2%)]\tLoss: 1.516554\n",
      "Train Epoch: 28 [200/6658 (3%)]\tLoss: 0.233344\n",
      "Train Epoch: 28 [300/6658 (5%)]\tLoss: 1.369218\n",
      "Train Epoch: 28 [400/6658 (6%)]\tLoss: 0.578272\n",
      "Train Epoch: 28 [500/6658 (8%)]\tLoss: 0.001777\n",
      "Train Epoch: 28 [600/6658 (9%)]\tLoss: 0.515019\n",
      "Train Epoch: 28 [700/6658 (11%)]\tLoss: 0.937423\n",
      "Train Epoch: 28 [800/6658 (12%)]\tLoss: 2.301228\n",
      "Train Epoch: 28 [900/6658 (14%)]\tLoss: 0.686877\n",
      "Train Epoch: 28 [1000/6658 (15%)]\tLoss: 0.235646\n",
      "Train Epoch: 28 [1100/6658 (17%)]\tLoss: 0.000529\n",
      "Train Epoch: 28 [1200/6658 (18%)]\tLoss: 1.187435\n",
      "Train Epoch: 28 [1300/6658 (20%)]\tLoss: 2.391624\n",
      "Train Epoch: 28 [1400/6658 (21%)]\tLoss: 0.176586\n",
      "Train Epoch: 28 [1500/6658 (23%)]\tLoss: 0.057170\n",
      "Train Epoch: 28 [1600/6658 (24%)]\tLoss: 0.844868\n",
      "Train Epoch: 28 [1700/6658 (26%)]\tLoss: 0.109970\n",
      "Train Epoch: 28 [1800/6658 (27%)]\tLoss: 0.179385\n",
      "Train Epoch: 28 [1900/6658 (29%)]\tLoss: 0.134393\n",
      "Train Epoch: 28 [2000/6658 (30%)]\tLoss: 0.278007\n",
      "Train Epoch: 28 [2100/6658 (32%)]\tLoss: 1.568045\n",
      "Train Epoch: 28 [2200/6658 (33%)]\tLoss: 5.204408\n",
      "Train Epoch: 28 [2300/6658 (35%)]\tLoss: 0.701947\n",
      "Train Epoch: 28 [2400/6658 (36%)]\tLoss: 0.286729\n",
      "Train Epoch: 28 [2500/6658 (38%)]\tLoss: 0.161318\n",
      "Train Epoch: 28 [2600/6658 (39%)]\tLoss: 0.633289\n",
      "Train Epoch: 28 [2700/6658 (41%)]\tLoss: 2.108414\n",
      "Train Epoch: 28 [2800/6658 (42%)]\tLoss: 0.011070\n",
      "Train Epoch: 28 [2900/6658 (44%)]\tLoss: 0.000840\n",
      "Train Epoch: 28 [3000/6658 (45%)]\tLoss: 0.637584\n",
      "Train Epoch: 28 [3100/6658 (47%)]\tLoss: 1.930143\n",
      "Train Epoch: 28 [3200/6658 (48%)]\tLoss: 2.161727\n",
      "Train Epoch: 28 [3300/6658 (50%)]\tLoss: 0.375078\n",
      "Train Epoch: 28 [3400/6658 (51%)]\tLoss: 1.078423\n",
      "Train Epoch: 28 [3500/6658 (53%)]\tLoss: 3.934638\n",
      "Train Epoch: 28 [3600/6658 (54%)]\tLoss: 0.868082\n",
      "Train Epoch: 28 [3700/6658 (56%)]\tLoss: 0.561237\n",
      "Train Epoch: 28 [3800/6658 (57%)]\tLoss: 0.146082\n",
      "Train Epoch: 28 [3900/6658 (59%)]\tLoss: 0.058953\n",
      "Train Epoch: 28 [4000/6658 (60%)]\tLoss: 0.116004\n",
      "Train Epoch: 28 [4100/6658 (62%)]\tLoss: 0.150887\n",
      "Train Epoch: 28 [4200/6658 (63%)]\tLoss: 0.313618\n",
      "Train Epoch: 28 [4300/6658 (65%)]\tLoss: 2.777448\n",
      "Train Epoch: 28 [4400/6658 (66%)]\tLoss: 0.072468\n",
      "Train Epoch: 28 [4500/6658 (68%)]\tLoss: 0.962466\n",
      "Train Epoch: 28 [4600/6658 (69%)]\tLoss: 0.189475\n",
      "Train Epoch: 28 [4700/6658 (71%)]\tLoss: 0.019499\n",
      "Train Epoch: 28 [4800/6658 (72%)]\tLoss: 1.785077\n",
      "Train Epoch: 28 [4900/6658 (74%)]\tLoss: 1.026071\n",
      "Train Epoch: 28 [5000/6658 (75%)]\tLoss: 0.240690\n",
      "Train Epoch: 28 [5100/6658 (77%)]\tLoss: 0.282312\n",
      "Train Epoch: 28 [5200/6658 (78%)]\tLoss: 0.403370\n",
      "Train Epoch: 28 [5300/6658 (80%)]\tLoss: 4.456763\n",
      "Train Epoch: 28 [5400/6658 (81%)]\tLoss: 0.451732\n",
      "Train Epoch: 28 [5500/6658 (83%)]\tLoss: 0.021738\n",
      "Train Epoch: 28 [5600/6658 (84%)]\tLoss: 0.080044\n",
      "Train Epoch: 28 [5700/6658 (86%)]\tLoss: 0.392769\n",
      "Train Epoch: 28 [5800/6658 (87%)]\tLoss: 0.767201\n",
      "Train Epoch: 28 [5900/6658 (89%)]\tLoss: 0.129457\n",
      "Train Epoch: 28 [6000/6658 (90%)]\tLoss: 0.007096\n",
      "Train Epoch: 28 [6100/6658 (92%)]\tLoss: 0.459786\n",
      "Train Epoch: 28 [6200/6658 (93%)]\tLoss: 0.620320\n",
      "Train Epoch: 28 [6300/6658 (95%)]\tLoss: 0.040605\n",
      "Train Epoch: 28 [6400/6658 (96%)]\tLoss: 0.575163\n",
      "Train Epoch: 28 [6500/6658 (98%)]\tLoss: 0.091849\n",
      "Train Epoch: 28 [6600/6658 (99%)]\tLoss: 0.868480\n",
      "train loss average =  0.7424000420994161\n",
      "\n",
      "Test set: Average loss: 0.7237\n",
      "\n",
      "EarlyStopping counter: 10 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0616, 6.0165, 5.9653, 5.9491, 6.0411, 6.0244], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 29 [0/6658 (0%)]\tLoss: 0.041688\n",
      "Train Epoch: 29 [100/6658 (2%)]\tLoss: 0.173227\n",
      "Train Epoch: 29 [200/6658 (3%)]\tLoss: 0.312061\n",
      "Train Epoch: 29 [300/6658 (5%)]\tLoss: 0.035466\n",
      "Train Epoch: 29 [400/6658 (6%)]\tLoss: 0.150573\n",
      "Train Epoch: 29 [500/6658 (8%)]\tLoss: 0.000628\n",
      "Train Epoch: 29 [600/6658 (9%)]\tLoss: 0.116962\n",
      "Train Epoch: 29 [700/6658 (11%)]\tLoss: 0.032768\n",
      "Train Epoch: 29 [800/6658 (12%)]\tLoss: 0.382748\n",
      "Train Epoch: 29 [900/6658 (14%)]\tLoss: 0.000109\n",
      "Train Epoch: 29 [1000/6658 (15%)]\tLoss: 0.236544\n",
      "Train Epoch: 29 [1100/6658 (17%)]\tLoss: 0.747952\n",
      "Train Epoch: 29 [1200/6658 (18%)]\tLoss: 0.382360\n",
      "Train Epoch: 29 [1300/6658 (20%)]\tLoss: 1.722692\n",
      "Train Epoch: 29 [1400/6658 (21%)]\tLoss: 1.002684\n",
      "Train Epoch: 29 [1500/6658 (23%)]\tLoss: 0.254196\n",
      "Train Epoch: 29 [1600/6658 (24%)]\tLoss: 2.325643\n",
      "Train Epoch: 29 [1700/6658 (26%)]\tLoss: 0.772557\n",
      "Train Epoch: 29 [1800/6658 (27%)]\tLoss: 0.304739\n",
      "Train Epoch: 29 [1900/6658 (29%)]\tLoss: 0.018306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 29 [2000/6658 (30%)]\tLoss: 0.745987\n",
      "Train Epoch: 29 [2100/6658 (32%)]\tLoss: 0.671240\n",
      "Train Epoch: 29 [2200/6658 (33%)]\tLoss: 0.497305\n",
      "Train Epoch: 29 [2300/6658 (35%)]\tLoss: 0.160409\n",
      "Train Epoch: 29 [2400/6658 (36%)]\tLoss: 0.001525\n",
      "Train Epoch: 29 [2500/6658 (38%)]\tLoss: 0.932848\n",
      "Train Epoch: 29 [2600/6658 (39%)]\tLoss: 3.050464\n",
      "Train Epoch: 29 [2700/6658 (41%)]\tLoss: 0.000151\n",
      "Train Epoch: 29 [2800/6658 (42%)]\tLoss: 0.020591\n",
      "Train Epoch: 29 [2900/6658 (44%)]\tLoss: 0.143717\n",
      "Train Epoch: 29 [3000/6658 (45%)]\tLoss: 0.386783\n",
      "Train Epoch: 29 [3100/6658 (47%)]\tLoss: 0.609273\n",
      "Train Epoch: 29 [3200/6658 (48%)]\tLoss: 0.518867\n",
      "Train Epoch: 29 [3300/6658 (50%)]\tLoss: 0.000189\n",
      "Train Epoch: 29 [3400/6658 (51%)]\tLoss: 1.744581\n",
      "Train Epoch: 29 [3500/6658 (53%)]\tLoss: 0.000078\n",
      "Train Epoch: 29 [3600/6658 (54%)]\tLoss: 1.187367\n",
      "Train Epoch: 29 [3700/6658 (56%)]\tLoss: 8.539219\n",
      "Train Epoch: 29 [3800/6658 (57%)]\tLoss: 0.367954\n",
      "Train Epoch: 29 [3900/6658 (59%)]\tLoss: 0.594325\n",
      "Train Epoch: 29 [4000/6658 (60%)]\tLoss: 0.251371\n",
      "Train Epoch: 29 [4100/6658 (62%)]\tLoss: 0.030631\n",
      "Train Epoch: 29 [4200/6658 (63%)]\tLoss: 2.260481\n",
      "Train Epoch: 29 [4300/6658 (65%)]\tLoss: 0.019554\n",
      "Train Epoch: 29 [4400/6658 (66%)]\tLoss: 0.710734\n",
      "Train Epoch: 29 [4500/6658 (68%)]\tLoss: 0.007004\n",
      "Train Epoch: 29 [4600/6658 (69%)]\tLoss: 0.065091\n",
      "Train Epoch: 29 [4700/6658 (71%)]\tLoss: 0.651262\n",
      "Train Epoch: 29 [4800/6658 (72%)]\tLoss: 0.267705\n",
      "Train Epoch: 29 [4900/6658 (74%)]\tLoss: 0.173021\n",
      "Train Epoch: 29 [5000/6658 (75%)]\tLoss: 0.045847\n",
      "Train Epoch: 29 [5100/6658 (77%)]\tLoss: 0.274885\n",
      "Train Epoch: 29 [5200/6658 (78%)]\tLoss: 0.017462\n",
      "Train Epoch: 29 [5300/6658 (80%)]\tLoss: 0.021268\n",
      "Train Epoch: 29 [5400/6658 (81%)]\tLoss: 0.000110\n",
      "Train Epoch: 29 [5500/6658 (83%)]\tLoss: 1.128849\n",
      "Train Epoch: 29 [5600/6658 (84%)]\tLoss: 0.620413\n",
      "Train Epoch: 29 [5700/6658 (86%)]\tLoss: 2.159615\n",
      "Train Epoch: 29 [5800/6658 (87%)]\tLoss: 0.033535\n",
      "Train Epoch: 29 [5900/6658 (89%)]\tLoss: 0.424293\n",
      "Train Epoch: 29 [6000/6658 (90%)]\tLoss: 0.099070\n",
      "Train Epoch: 29 [6100/6658 (92%)]\tLoss: 0.212592\n",
      "Train Epoch: 29 [6200/6658 (93%)]\tLoss: 0.040538\n",
      "Train Epoch: 29 [6300/6658 (95%)]\tLoss: 0.244060\n",
      "Train Epoch: 29 [6400/6658 (96%)]\tLoss: 0.391224\n",
      "Train Epoch: 29 [6500/6658 (98%)]\tLoss: 0.000158\n",
      "Train Epoch: 29 [6600/6658 (99%)]\tLoss: 0.207707\n",
      "train loss average =  0.7392040122476312\n",
      "\n",
      "Test set: Average loss: 0.7250\n",
      "\n",
      "EarlyStopping counter: 11 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0648, 6.0169, 5.9637, 5.9480, 6.0423, 6.0246], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 30 [0/6658 (0%)]\tLoss: 0.061264\n",
      "Train Epoch: 30 [100/6658 (2%)]\tLoss: 0.004711\n",
      "Train Epoch: 30 [200/6658 (3%)]\tLoss: 0.072409\n",
      "Train Epoch: 30 [300/6658 (5%)]\tLoss: 0.019950\n",
      "Train Epoch: 30 [400/6658 (6%)]\tLoss: 0.474288\n",
      "Train Epoch: 30 [500/6658 (8%)]\tLoss: 0.483622\n",
      "Train Epoch: 30 [600/6658 (9%)]\tLoss: 0.299359\n",
      "Train Epoch: 30 [700/6658 (11%)]\tLoss: 0.090871\n",
      "Train Epoch: 30 [800/6658 (12%)]\tLoss: 0.528809\n",
      "Train Epoch: 30 [900/6658 (14%)]\tLoss: 0.331834\n",
      "Train Epoch: 30 [1000/6658 (15%)]\tLoss: 0.041627\n",
      "Train Epoch: 30 [1100/6658 (17%)]\tLoss: 1.314773\n",
      "Train Epoch: 30 [1200/6658 (18%)]\tLoss: 0.893538\n",
      "Train Epoch: 30 [1300/6658 (20%)]\tLoss: 0.486041\n",
      "Train Epoch: 30 [1400/6658 (21%)]\tLoss: 1.050703\n",
      "Train Epoch: 30 [1500/6658 (23%)]\tLoss: 0.032230\n",
      "Train Epoch: 30 [1600/6658 (24%)]\tLoss: 0.735194\n",
      "Train Epoch: 30 [1700/6658 (26%)]\tLoss: 2.916537\n",
      "Train Epoch: 30 [1800/6658 (27%)]\tLoss: 0.007578\n",
      "Train Epoch: 30 [1900/6658 (29%)]\tLoss: 4.711710\n",
      "Train Epoch: 30 [2000/6658 (30%)]\tLoss: 0.392203\n",
      "Train Epoch: 30 [2100/6658 (32%)]\tLoss: 0.830997\n",
      "Train Epoch: 30 [2200/6658 (33%)]\tLoss: 0.729898\n",
      "Train Epoch: 30 [2300/6658 (35%)]\tLoss: 0.366050\n",
      "Train Epoch: 30 [2400/6658 (36%)]\tLoss: 0.037410\n",
      "Train Epoch: 30 [2500/6658 (38%)]\tLoss: 0.196767\n",
      "Train Epoch: 30 [2600/6658 (39%)]\tLoss: 0.003105\n",
      "Train Epoch: 30 [2700/6658 (41%)]\tLoss: 0.015711\n",
      "Train Epoch: 30 [2800/6658 (42%)]\tLoss: 0.027420\n",
      "Train Epoch: 30 [2900/6658 (44%)]\tLoss: 0.176390\n",
      "Train Epoch: 30 [3000/6658 (45%)]\tLoss: 1.114170\n",
      "Train Epoch: 30 [3100/6658 (47%)]\tLoss: 0.097995\n",
      "Train Epoch: 30 [3200/6658 (48%)]\tLoss: 0.288037\n",
      "Train Epoch: 30 [3300/6658 (50%)]\tLoss: 0.212419\n",
      "Train Epoch: 30 [3400/6658 (51%)]\tLoss: 0.236722\n",
      "Train Epoch: 30 [3500/6658 (53%)]\tLoss: 0.429559\n",
      "Train Epoch: 30 [3600/6658 (54%)]\tLoss: 0.055908\n",
      "Train Epoch: 30 [3700/6658 (56%)]\tLoss: 0.193312\n",
      "Train Epoch: 30 [3800/6658 (57%)]\tLoss: 0.771813\n",
      "Train Epoch: 30 [3900/6658 (59%)]\tLoss: 0.128288\n",
      "Train Epoch: 30 [4000/6658 (60%)]\tLoss: 0.058410\n",
      "Train Epoch: 30 [4100/6658 (62%)]\tLoss: 0.208841\n",
      "Train Epoch: 30 [4200/6658 (63%)]\tLoss: 1.074575\n",
      "Train Epoch: 30 [4300/6658 (65%)]\tLoss: 0.218705\n",
      "Train Epoch: 30 [4400/6658 (66%)]\tLoss: 0.067460\n",
      "Train Epoch: 30 [4500/6658 (68%)]\tLoss: 0.047324\n",
      "Train Epoch: 30 [4600/6658 (69%)]\tLoss: 0.956484\n",
      "Train Epoch: 30 [4700/6658 (71%)]\tLoss: 0.184958\n",
      "Train Epoch: 30 [4800/6658 (72%)]\tLoss: 1.021705\n",
      "Train Epoch: 30 [4900/6658 (74%)]\tLoss: 0.569507\n",
      "Train Epoch: 30 [5000/6658 (75%)]\tLoss: 0.003880\n",
      "Train Epoch: 30 [5100/6658 (77%)]\tLoss: 1.457558\n",
      "Train Epoch: 30 [5200/6658 (78%)]\tLoss: 0.857208\n",
      "Train Epoch: 30 [5300/6658 (80%)]\tLoss: 0.086429\n",
      "Train Epoch: 30 [5400/6658 (81%)]\tLoss: 0.255086\n",
      "Train Epoch: 30 [5500/6658 (83%)]\tLoss: 0.268763\n",
      "Train Epoch: 30 [5600/6658 (84%)]\tLoss: 0.538465\n",
      "Train Epoch: 30 [5700/6658 (86%)]\tLoss: 0.279060\n",
      "Train Epoch: 30 [5800/6658 (87%)]\tLoss: 0.605601\n",
      "Train Epoch: 30 [5900/6658 (89%)]\tLoss: 0.632395\n",
      "Train Epoch: 30 [6000/6658 (90%)]\tLoss: 0.238521\n",
      "Train Epoch: 30 [6100/6658 (92%)]\tLoss: 0.200473\n",
      "Train Epoch: 30 [6200/6658 (93%)]\tLoss: 0.071925\n",
      "Train Epoch: 30 [6300/6658 (95%)]\tLoss: 0.075359\n",
      "Train Epoch: 30 [6400/6658 (96%)]\tLoss: 0.000001\n",
      "Train Epoch: 30 [6500/6658 (98%)]\tLoss: 2.037722\n",
      "Train Epoch: 30 [6600/6658 (99%)]\tLoss: 0.000071\n",
      "train loss average =  0.7484680798809084\n",
      "\n",
      "Test set: Average loss: 0.7208\n",
      "\n",
      "EarlyStopping counter: 12 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0672, 6.0178, 5.9620, 5.9468, 6.0438, 6.0250], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 31 [0/6658 (0%)]\tLoss: 6.519294\n",
      "Train Epoch: 31 [100/6658 (2%)]\tLoss: 1.253175\n",
      "Train Epoch: 31 [200/6658 (3%)]\tLoss: 2.169476\n",
      "Train Epoch: 31 [300/6658 (5%)]\tLoss: 0.076095\n",
      "Train Epoch: 31 [400/6658 (6%)]\tLoss: 0.231476\n",
      "Train Epoch: 31 [500/6658 (8%)]\tLoss: 0.161036\n",
      "Train Epoch: 31 [600/6658 (9%)]\tLoss: 0.233917\n",
      "Train Epoch: 31 [700/6658 (11%)]\tLoss: 0.938113\n",
      "Train Epoch: 31 [800/6658 (12%)]\tLoss: 0.042136\n",
      "Train Epoch: 31 [900/6658 (14%)]\tLoss: 0.425304\n",
      "Train Epoch: 31 [1000/6658 (15%)]\tLoss: 0.295559\n",
      "Train Epoch: 31 [1100/6658 (17%)]\tLoss: 0.299810\n",
      "Train Epoch: 31 [1200/6658 (18%)]\tLoss: 0.028467\n",
      "Train Epoch: 31 [1300/6658 (20%)]\tLoss: 0.204638\n",
      "Train Epoch: 31 [1400/6658 (21%)]\tLoss: 0.346800\n",
      "Train Epoch: 31 [1500/6658 (23%)]\tLoss: 0.043284\n",
      "Train Epoch: 31 [1600/6658 (24%)]\tLoss: 0.016377\n",
      "Train Epoch: 31 [1700/6658 (26%)]\tLoss: 0.228821\n",
      "Train Epoch: 31 [1800/6658 (27%)]\tLoss: 0.003099\n",
      "Train Epoch: 31 [1900/6658 (29%)]\tLoss: 0.087679\n",
      "Train Epoch: 31 [2000/6658 (30%)]\tLoss: 0.046518\n",
      "Train Epoch: 31 [2100/6658 (32%)]\tLoss: 0.464159\n",
      "Train Epoch: 31 [2200/6658 (33%)]\tLoss: 0.003297\n",
      "Train Epoch: 31 [2300/6658 (35%)]\tLoss: 0.015896\n",
      "Train Epoch: 31 [2400/6658 (36%)]\tLoss: 0.034577\n",
      "Train Epoch: 31 [2500/6658 (38%)]\tLoss: 0.132798\n",
      "Train Epoch: 31 [2600/6658 (39%)]\tLoss: 0.460262\n",
      "Train Epoch: 31 [2700/6658 (41%)]\tLoss: 0.157176\n",
      "Train Epoch: 31 [2800/6658 (42%)]\tLoss: 0.854417\n",
      "Train Epoch: 31 [2900/6658 (44%)]\tLoss: 0.154991\n",
      "Train Epoch: 31 [3000/6658 (45%)]\tLoss: 0.314053\n",
      "Train Epoch: 31 [3100/6658 (47%)]\tLoss: 0.759813\n",
      "Train Epoch: 31 [3200/6658 (48%)]\tLoss: 0.254766\n",
      "Train Epoch: 31 [3300/6658 (50%)]\tLoss: 0.051937\n",
      "Train Epoch: 31 [3400/6658 (51%)]\tLoss: 0.065855\n",
      "Train Epoch: 31 [3500/6658 (53%)]\tLoss: 0.966991\n",
      "Train Epoch: 31 [3600/6658 (54%)]\tLoss: 0.000302\n",
      "Train Epoch: 31 [3700/6658 (56%)]\tLoss: 0.002325\n",
      "Train Epoch: 31 [3800/6658 (57%)]\tLoss: 0.319052\n",
      "Train Epoch: 31 [3900/6658 (59%)]\tLoss: 0.074779\n",
      "Train Epoch: 31 [4000/6658 (60%)]\tLoss: 0.368717\n",
      "Train Epoch: 31 [4100/6658 (62%)]\tLoss: 0.869703\n",
      "Train Epoch: 31 [4200/6658 (63%)]\tLoss: 0.018332\n",
      "Train Epoch: 31 [4300/6658 (65%)]\tLoss: 0.401977\n",
      "Train Epoch: 31 [4400/6658 (66%)]\tLoss: 0.058245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 31 [4500/6658 (68%)]\tLoss: 19.261492\n",
      "Train Epoch: 31 [4600/6658 (69%)]\tLoss: 2.147101\n",
      "Train Epoch: 31 [4700/6658 (71%)]\tLoss: 0.031824\n",
      "Train Epoch: 31 [4800/6658 (72%)]\tLoss: 0.365016\n",
      "Train Epoch: 31 [4900/6658 (74%)]\tLoss: 0.025931\n",
      "Train Epoch: 31 [5000/6658 (75%)]\tLoss: 0.004657\n",
      "Train Epoch: 31 [5100/6658 (77%)]\tLoss: 1.698212\n",
      "Train Epoch: 31 [5200/6658 (78%)]\tLoss: 0.362041\n",
      "Train Epoch: 31 [5300/6658 (80%)]\tLoss: 3.442472\n",
      "Train Epoch: 31 [5400/6658 (81%)]\tLoss: 0.065446\n",
      "Train Epoch: 31 [5500/6658 (83%)]\tLoss: 0.001625\n",
      "Train Epoch: 31 [5600/6658 (84%)]\tLoss: 0.013687\n",
      "Train Epoch: 31 [5700/6658 (86%)]\tLoss: 14.678087\n",
      "Train Epoch: 31 [5800/6658 (87%)]\tLoss: 0.318054\n",
      "Train Epoch: 31 [5900/6658 (89%)]\tLoss: 1.387067\n",
      "Train Epoch: 31 [6000/6658 (90%)]\tLoss: 0.200664\n",
      "Train Epoch: 31 [6100/6658 (92%)]\tLoss: 0.411448\n",
      "Train Epoch: 31 [6200/6658 (93%)]\tLoss: 0.886149\n",
      "Train Epoch: 31 [6300/6658 (95%)]\tLoss: 0.139719\n",
      "Train Epoch: 31 [6400/6658 (96%)]\tLoss: 1.461962\n",
      "Train Epoch: 31 [6500/6658 (98%)]\tLoss: 0.307737\n",
      "Train Epoch: 31 [6600/6658 (99%)]\tLoss: 0.074692\n",
      "train loss average =  0.7414105061298631\n",
      "\n",
      "Test set: Average loss: 0.7183\n",
      "\n",
      "EarlyStopping counter: 13 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0697, 6.0183, 5.9599, 5.9456, 6.0449, 6.0252], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 32 [0/6658 (0%)]\tLoss: 0.640917\n",
      "Train Epoch: 32 [100/6658 (2%)]\tLoss: 0.002038\n",
      "Train Epoch: 32 [200/6658 (3%)]\tLoss: 0.762912\n",
      "Train Epoch: 32 [300/6658 (5%)]\tLoss: 0.001309\n",
      "Train Epoch: 32 [400/6658 (6%)]\tLoss: 0.070780\n",
      "Train Epoch: 32 [500/6658 (8%)]\tLoss: 0.427223\n",
      "Train Epoch: 32 [600/6658 (9%)]\tLoss: 1.858757\n",
      "Train Epoch: 32 [700/6658 (11%)]\tLoss: 0.740235\n",
      "Train Epoch: 32 [800/6658 (12%)]\tLoss: 1.202008\n",
      "Train Epoch: 32 [900/6658 (14%)]\tLoss: 0.001900\n",
      "Train Epoch: 32 [1000/6658 (15%)]\tLoss: 0.042447\n",
      "Train Epoch: 32 [1100/6658 (17%)]\tLoss: 2.932075\n",
      "Train Epoch: 32 [1200/6658 (18%)]\tLoss: 1.018090\n",
      "Train Epoch: 32 [1300/6658 (20%)]\tLoss: 0.031746\n",
      "Train Epoch: 32 [1400/6658 (21%)]\tLoss: 0.406604\n",
      "Train Epoch: 32 [1500/6658 (23%)]\tLoss: 0.071192\n",
      "Train Epoch: 32 [1600/6658 (24%)]\tLoss: 0.007831\n",
      "Train Epoch: 32 [1700/6658 (26%)]\tLoss: 0.237339\n",
      "Train Epoch: 32 [1800/6658 (27%)]\tLoss: 0.007524\n",
      "Train Epoch: 32 [1900/6658 (29%)]\tLoss: 0.211274\n",
      "Train Epoch: 32 [2000/6658 (30%)]\tLoss: 0.355981\n",
      "Train Epoch: 32 [2100/6658 (32%)]\tLoss: 0.152305\n",
      "Train Epoch: 32 [2200/6658 (33%)]\tLoss: 0.047692\n",
      "Train Epoch: 32 [2300/6658 (35%)]\tLoss: 0.023217\n",
      "Train Epoch: 32 [2400/6658 (36%)]\tLoss: 3.960795\n",
      "Train Epoch: 32 [2500/6658 (38%)]\tLoss: 0.878755\n",
      "Train Epoch: 32 [2600/6658 (39%)]\tLoss: 0.309240\n",
      "Train Epoch: 32 [2700/6658 (41%)]\tLoss: 0.108176\n",
      "Train Epoch: 32 [2800/6658 (42%)]\tLoss: 0.160098\n",
      "Train Epoch: 32 [2900/6658 (44%)]\tLoss: 0.089673\n",
      "Train Epoch: 32 [3000/6658 (45%)]\tLoss: 0.933800\n",
      "Train Epoch: 32 [3100/6658 (47%)]\tLoss: 0.071713\n",
      "Train Epoch: 32 [3200/6658 (48%)]\tLoss: 0.145755\n",
      "Train Epoch: 32 [3300/6658 (50%)]\tLoss: 0.370162\n",
      "Train Epoch: 32 [3400/6658 (51%)]\tLoss: 0.146101\n",
      "Train Epoch: 32 [3500/6658 (53%)]\tLoss: 0.236984\n",
      "Train Epoch: 32 [3600/6658 (54%)]\tLoss: 0.929705\n",
      "Train Epoch: 32 [3700/6658 (56%)]\tLoss: 0.065347\n",
      "Train Epoch: 32 [3800/6658 (57%)]\tLoss: 0.814393\n",
      "Train Epoch: 32 [3900/6658 (59%)]\tLoss: 0.589990\n",
      "Train Epoch: 32 [4000/6658 (60%)]\tLoss: 0.554786\n",
      "Train Epoch: 32 [4100/6658 (62%)]\tLoss: 0.004090\n",
      "Train Epoch: 32 [4200/6658 (63%)]\tLoss: 0.031057\n",
      "Train Epoch: 32 [4300/6658 (65%)]\tLoss: 0.894510\n",
      "Train Epoch: 32 [4400/6658 (66%)]\tLoss: 0.022029\n",
      "Train Epoch: 32 [4500/6658 (68%)]\tLoss: 0.030280\n",
      "Train Epoch: 32 [4600/6658 (69%)]\tLoss: 0.623162\n",
      "Train Epoch: 32 [4700/6658 (71%)]\tLoss: 0.001888\n",
      "Train Epoch: 32 [4800/6658 (72%)]\tLoss: 0.139723\n",
      "Train Epoch: 32 [4900/6658 (74%)]\tLoss: 0.265933\n",
      "Train Epoch: 32 [5000/6658 (75%)]\tLoss: 0.374295\n",
      "Train Epoch: 32 [5100/6658 (77%)]\tLoss: 0.602565\n",
      "Train Epoch: 32 [5200/6658 (78%)]\tLoss: 0.993732\n",
      "Train Epoch: 32 [5300/6658 (80%)]\tLoss: 0.559873\n",
      "Train Epoch: 32 [5400/6658 (81%)]\tLoss: 0.128248\n",
      "Train Epoch: 32 [5500/6658 (83%)]\tLoss: 3.623426\n",
      "Train Epoch: 32 [5600/6658 (84%)]\tLoss: 0.046170\n",
      "Train Epoch: 32 [5700/6658 (86%)]\tLoss: 0.739350\n",
      "Train Epoch: 32 [5800/6658 (87%)]\tLoss: 0.223753\n",
      "Train Epoch: 32 [5900/6658 (89%)]\tLoss: 0.415651\n",
      "Train Epoch: 32 [6000/6658 (90%)]\tLoss: 1.579456\n",
      "Train Epoch: 32 [6100/6658 (92%)]\tLoss: 0.118143\n",
      "Train Epoch: 32 [6200/6658 (93%)]\tLoss: 0.503792\n",
      "Train Epoch: 32 [6300/6658 (95%)]\tLoss: 0.032446\n",
      "Train Epoch: 32 [6400/6658 (96%)]\tLoss: 0.117067\n",
      "Train Epoch: 32 [6500/6658 (98%)]\tLoss: 0.064353\n",
      "Train Epoch: 32 [6600/6658 (99%)]\tLoss: 0.034545\n",
      "train loss average =  0.7392207515210537\n",
      "\n",
      "Test set: Average loss: 0.7070\n",
      "\n",
      "Validation loss decreased (0.710387 --> 0.706962).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.0721, 6.0191, 5.9588, 5.9444, 6.0465, 6.0262], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 33 [0/6658 (0%)]\tLoss: 0.054168\n",
      "Train Epoch: 33 [100/6658 (2%)]\tLoss: 0.193258\n",
      "Train Epoch: 33 [200/6658 (3%)]\tLoss: 0.162956\n",
      "Train Epoch: 33 [300/6658 (5%)]\tLoss: 0.046236\n",
      "Train Epoch: 33 [400/6658 (6%)]\tLoss: 1.822932\n",
      "Train Epoch: 33 [500/6658 (8%)]\tLoss: 0.000111\n",
      "Train Epoch: 33 [600/6658 (9%)]\tLoss: 2.903091\n",
      "Train Epoch: 33 [700/6658 (11%)]\tLoss: 0.041833\n",
      "Train Epoch: 33 [800/6658 (12%)]\tLoss: 0.389084\n",
      "Train Epoch: 33 [900/6658 (14%)]\tLoss: 0.050928\n",
      "Train Epoch: 33 [1000/6658 (15%)]\tLoss: 0.104841\n",
      "Train Epoch: 33 [1100/6658 (17%)]\tLoss: 0.333833\n",
      "Train Epoch: 33 [1200/6658 (18%)]\tLoss: 0.000650\n",
      "Train Epoch: 33 [1300/6658 (20%)]\tLoss: 0.395678\n",
      "Train Epoch: 33 [1400/6658 (21%)]\tLoss: 0.005915\n",
      "Train Epoch: 33 [1500/6658 (23%)]\tLoss: 0.002564\n",
      "Train Epoch: 33 [1600/6658 (24%)]\tLoss: 0.047522\n",
      "Train Epoch: 33 [1700/6658 (26%)]\tLoss: 0.100053\n",
      "Train Epoch: 33 [1800/6658 (27%)]\tLoss: 1.309072\n",
      "Train Epoch: 33 [1900/6658 (29%)]\tLoss: 1.057528\n",
      "Train Epoch: 33 [2000/6658 (30%)]\tLoss: 0.048174\n",
      "Train Epoch: 33 [2100/6658 (32%)]\tLoss: 0.543278\n",
      "Train Epoch: 33 [2200/6658 (33%)]\tLoss: 0.492696\n",
      "Train Epoch: 33 [2300/6658 (35%)]\tLoss: 0.030835\n",
      "Train Epoch: 33 [2400/6658 (36%)]\tLoss: 1.423028\n",
      "Train Epoch: 33 [2500/6658 (38%)]\tLoss: 0.298820\n",
      "Train Epoch: 33 [2600/6658 (39%)]\tLoss: 0.000036\n",
      "Train Epoch: 33 [2700/6658 (41%)]\tLoss: 0.007593\n",
      "Train Epoch: 33 [2800/6658 (42%)]\tLoss: 0.298180\n",
      "Train Epoch: 33 [2900/6658 (44%)]\tLoss: 1.175854\n",
      "Train Epoch: 33 [3000/6658 (45%)]\tLoss: 0.027649\n",
      "Train Epoch: 33 [3100/6658 (47%)]\tLoss: 1.220956\n",
      "Train Epoch: 33 [3200/6658 (48%)]\tLoss: 0.125945\n",
      "Train Epoch: 33 [3300/6658 (50%)]\tLoss: 0.646983\n",
      "Train Epoch: 33 [3400/6658 (51%)]\tLoss: 0.005260\n",
      "Train Epoch: 33 [3500/6658 (53%)]\tLoss: 1.057889\n",
      "Train Epoch: 33 [3600/6658 (54%)]\tLoss: 1.656809\n",
      "Train Epoch: 33 [3700/6658 (56%)]\tLoss: 0.068372\n",
      "Train Epoch: 33 [3800/6658 (57%)]\tLoss: 0.165998\n",
      "Train Epoch: 33 [3900/6658 (59%)]\tLoss: 0.099374\n",
      "Train Epoch: 33 [4000/6658 (60%)]\tLoss: 10.569687\n",
      "Train Epoch: 33 [4100/6658 (62%)]\tLoss: 0.245736\n",
      "Train Epoch: 33 [4200/6658 (63%)]\tLoss: 0.149011\n",
      "Train Epoch: 33 [4300/6658 (65%)]\tLoss: 0.075637\n",
      "Train Epoch: 33 [4400/6658 (66%)]\tLoss: 0.102750\n",
      "Train Epoch: 33 [4500/6658 (68%)]\tLoss: 0.657929\n",
      "Train Epoch: 33 [4600/6658 (69%)]\tLoss: 1.407934\n",
      "Train Epoch: 33 [4700/6658 (71%)]\tLoss: 0.092454\n",
      "Train Epoch: 33 [4800/6658 (72%)]\tLoss: 2.118652\n",
      "Train Epoch: 33 [4900/6658 (74%)]\tLoss: 0.017324\n",
      "Train Epoch: 33 [5000/6658 (75%)]\tLoss: 0.121187\n",
      "Train Epoch: 33 [5100/6658 (77%)]\tLoss: 0.289883\n",
      "Train Epoch: 33 [5200/6658 (78%)]\tLoss: 0.078676\n",
      "Train Epoch: 33 [5300/6658 (80%)]\tLoss: 0.152458\n",
      "Train Epoch: 33 [5400/6658 (81%)]\tLoss: 0.336849\n",
      "Train Epoch: 33 [5500/6658 (83%)]\tLoss: 0.374585\n",
      "Train Epoch: 33 [5600/6658 (84%)]\tLoss: 0.058670\n",
      "Train Epoch: 33 [5700/6658 (86%)]\tLoss: 0.810917\n",
      "Train Epoch: 33 [5800/6658 (87%)]\tLoss: 0.654060\n",
      "Train Epoch: 33 [5900/6658 (89%)]\tLoss: 0.664358\n",
      "Train Epoch: 33 [6000/6658 (90%)]\tLoss: 1.458508\n",
      "Train Epoch: 33 [6100/6658 (92%)]\tLoss: 1.686079\n",
      "Train Epoch: 33 [6200/6658 (93%)]\tLoss: 0.093018\n",
      "Train Epoch: 33 [6300/6658 (95%)]\tLoss: 0.006782\n",
      "Train Epoch: 33 [6400/6658 (96%)]\tLoss: 0.264231\n",
      "Train Epoch: 33 [6500/6658 (98%)]\tLoss: 0.670283\n",
      "Train Epoch: 33 [6600/6658 (99%)]\tLoss: 0.692825\n",
      "train loss average =  0.7350488219046105\n",
      "\n",
      "Test set: Average loss: 0.7111\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0756, 6.0192, 5.9569, 5.9425, 6.0479, 6.0269], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 34 [0/6658 (0%)]\tLoss: 0.133683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 34 [100/6658 (2%)]\tLoss: 0.789478\n",
      "Train Epoch: 34 [200/6658 (3%)]\tLoss: 0.853170\n",
      "Train Epoch: 34 [300/6658 (5%)]\tLoss: 0.209808\n",
      "Train Epoch: 34 [400/6658 (6%)]\tLoss: 2.048773\n",
      "Train Epoch: 34 [500/6658 (8%)]\tLoss: 0.137669\n",
      "Train Epoch: 34 [600/6658 (9%)]\tLoss: 0.596887\n",
      "Train Epoch: 34 [700/6658 (11%)]\tLoss: 1.127068\n",
      "Train Epoch: 34 [800/6658 (12%)]\tLoss: 0.636578\n",
      "Train Epoch: 34 [900/6658 (14%)]\tLoss: 0.625716\n",
      "Train Epoch: 34 [1000/6658 (15%)]\tLoss: 1.481952\n",
      "Train Epoch: 34 [1100/6658 (17%)]\tLoss: 0.007090\n",
      "Train Epoch: 34 [1200/6658 (18%)]\tLoss: 0.104197\n",
      "Train Epoch: 34 [1300/6658 (20%)]\tLoss: 3.173580\n",
      "Train Epoch: 34 [1400/6658 (21%)]\tLoss: 0.987609\n",
      "Train Epoch: 34 [1500/6658 (23%)]\tLoss: 0.403418\n",
      "Train Epoch: 34 [1600/6658 (24%)]\tLoss: 3.280968\n",
      "Train Epoch: 34 [1700/6658 (26%)]\tLoss: 0.240240\n",
      "Train Epoch: 34 [1800/6658 (27%)]\tLoss: 0.008604\n",
      "Train Epoch: 34 [1900/6658 (29%)]\tLoss: 0.545594\n",
      "Train Epoch: 34 [2000/6658 (30%)]\tLoss: 0.000022\n",
      "Train Epoch: 34 [2100/6658 (32%)]\tLoss: 0.124023\n",
      "Train Epoch: 34 [2200/6658 (33%)]\tLoss: 0.037765\n",
      "Train Epoch: 34 [2300/6658 (35%)]\tLoss: 3.064445\n",
      "Train Epoch: 34 [2400/6658 (36%)]\tLoss: 0.046528\n",
      "Train Epoch: 34 [2500/6658 (38%)]\tLoss: 0.892615\n",
      "Train Epoch: 34 [2600/6658 (39%)]\tLoss: 0.035633\n",
      "Train Epoch: 34 [2700/6658 (41%)]\tLoss: 0.015630\n",
      "Train Epoch: 34 [2800/6658 (42%)]\tLoss: 0.105945\n",
      "Train Epoch: 34 [2900/6658 (44%)]\tLoss: 0.163009\n",
      "Train Epoch: 34 [3000/6658 (45%)]\tLoss: 0.165736\n",
      "Train Epoch: 34 [3100/6658 (47%)]\tLoss: 0.498588\n",
      "Train Epoch: 34 [3200/6658 (48%)]\tLoss: 1.518808\n",
      "Train Epoch: 34 [3300/6658 (50%)]\tLoss: 0.726072\n",
      "Train Epoch: 34 [3400/6658 (51%)]\tLoss: 0.938676\n",
      "Train Epoch: 34 [3500/6658 (53%)]\tLoss: 0.715547\n",
      "Train Epoch: 34 [3600/6658 (54%)]\tLoss: 0.002518\n",
      "Train Epoch: 34 [3700/6658 (56%)]\tLoss: 0.017793\n",
      "Train Epoch: 34 [3800/6658 (57%)]\tLoss: 0.056570\n",
      "Train Epoch: 34 [3900/6658 (59%)]\tLoss: 0.093776\n",
      "Train Epoch: 34 [4000/6658 (60%)]\tLoss: 0.852385\n",
      "Train Epoch: 34 [4100/6658 (62%)]\tLoss: 0.280919\n",
      "Train Epoch: 34 [4200/6658 (63%)]\tLoss: 0.858846\n",
      "Train Epoch: 34 [4300/6658 (65%)]\tLoss: 0.596687\n",
      "Train Epoch: 34 [4400/6658 (66%)]\tLoss: 0.474439\n",
      "Train Epoch: 34 [4500/6658 (68%)]\tLoss: 0.407028\n",
      "Train Epoch: 34 [4600/6658 (69%)]\tLoss: 0.394891\n",
      "Train Epoch: 34 [4700/6658 (71%)]\tLoss: 0.515068\n",
      "Train Epoch: 34 [4800/6658 (72%)]\tLoss: 0.413158\n",
      "Train Epoch: 34 [4900/6658 (74%)]\tLoss: 0.004527\n",
      "Train Epoch: 34 [5000/6658 (75%)]\tLoss: 0.731067\n",
      "Train Epoch: 34 [5100/6658 (77%)]\tLoss: 0.211550\n",
      "Train Epoch: 34 [5200/6658 (78%)]\tLoss: 0.299171\n",
      "Train Epoch: 34 [5300/6658 (80%)]\tLoss: 0.829406\n",
      "Train Epoch: 34 [5400/6658 (81%)]\tLoss: 1.322906\n",
      "Train Epoch: 34 [5500/6658 (83%)]\tLoss: 0.784127\n",
      "Train Epoch: 34 [5600/6658 (84%)]\tLoss: 0.145949\n",
      "Train Epoch: 34 [5700/6658 (86%)]\tLoss: 0.900199\n",
      "Train Epoch: 34 [5800/6658 (87%)]\tLoss: 1.074234\n",
      "Train Epoch: 34 [5900/6658 (89%)]\tLoss: 0.041815\n",
      "Train Epoch: 34 [6000/6658 (90%)]\tLoss: 0.802963\n",
      "Train Epoch: 34 [6100/6658 (92%)]\tLoss: 1.593987\n",
      "Train Epoch: 34 [6200/6658 (93%)]\tLoss: 0.269903\n",
      "Train Epoch: 34 [6300/6658 (95%)]\tLoss: 0.173851\n",
      "Train Epoch: 34 [6400/6658 (96%)]\tLoss: 0.001697\n",
      "Train Epoch: 34 [6500/6658 (98%)]\tLoss: 0.121323\n",
      "Train Epoch: 34 [6600/6658 (99%)]\tLoss: 0.039342\n",
      "train loss average =  0.7379477045561572\n",
      "\n",
      "Test set: Average loss: 0.7250\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0782, 6.0203, 5.9564, 5.9419, 6.0489, 6.0270], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 35 [0/6658 (0%)]\tLoss: 0.014754\n",
      "Train Epoch: 35 [100/6658 (2%)]\tLoss: 0.896322\n",
      "Train Epoch: 35 [200/6658 (3%)]\tLoss: 1.040535\n",
      "Train Epoch: 35 [300/6658 (5%)]\tLoss: 0.791442\n",
      "Train Epoch: 35 [400/6658 (6%)]\tLoss: 17.788273\n",
      "Train Epoch: 35 [500/6658 (8%)]\tLoss: 0.000545\n",
      "Train Epoch: 35 [600/6658 (9%)]\tLoss: 2.295867\n",
      "Train Epoch: 35 [700/6658 (11%)]\tLoss: 0.125791\n",
      "Train Epoch: 35 [800/6658 (12%)]\tLoss: 0.255611\n",
      "Train Epoch: 35 [900/6658 (14%)]\tLoss: 0.063698\n",
      "Train Epoch: 35 [1000/6658 (15%)]\tLoss: 0.324316\n",
      "Train Epoch: 35 [1100/6658 (17%)]\tLoss: 0.029314\n",
      "Train Epoch: 35 [1200/6658 (18%)]\tLoss: 0.537423\n",
      "Train Epoch: 35 [1300/6658 (20%)]\tLoss: 0.214835\n",
      "Train Epoch: 35 [1400/6658 (21%)]\tLoss: 0.088364\n",
      "Train Epoch: 35 [1500/6658 (23%)]\tLoss: 0.252671\n",
      "Train Epoch: 35 [1600/6658 (24%)]\tLoss: 0.129593\n",
      "Train Epoch: 35 [1700/6658 (26%)]\tLoss: 0.094932\n",
      "Train Epoch: 35 [1800/6658 (27%)]\tLoss: 0.141624\n",
      "Train Epoch: 35 [1900/6658 (29%)]\tLoss: 0.041500\n",
      "Train Epoch: 35 [2000/6658 (30%)]\tLoss: 0.039330\n",
      "Train Epoch: 35 [2100/6658 (32%)]\tLoss: 0.549038\n",
      "Train Epoch: 35 [2200/6658 (33%)]\tLoss: 0.000032\n",
      "Train Epoch: 35 [2300/6658 (35%)]\tLoss: 1.018193\n",
      "Train Epoch: 35 [2400/6658 (36%)]\tLoss: 0.301770\n",
      "Train Epoch: 35 [2500/6658 (38%)]\tLoss: 0.010238\n",
      "Train Epoch: 35 [2600/6658 (39%)]\tLoss: 0.114089\n",
      "Train Epoch: 35 [2700/6658 (41%)]\tLoss: 0.120715\n",
      "Train Epoch: 35 [2800/6658 (42%)]\tLoss: 1.156835\n",
      "Train Epoch: 35 [2900/6658 (44%)]\tLoss: 0.269522\n",
      "Train Epoch: 35 [3000/6658 (45%)]\tLoss: 0.567543\n",
      "Train Epoch: 35 [3100/6658 (47%)]\tLoss: 0.300619\n",
      "Train Epoch: 35 [3200/6658 (48%)]\tLoss: 0.359253\n",
      "Train Epoch: 35 [3300/6658 (50%)]\tLoss: 0.033032\n",
      "Train Epoch: 35 [3400/6658 (51%)]\tLoss: 4.923886\n",
      "Train Epoch: 35 [3500/6658 (53%)]\tLoss: 1.444157\n",
      "Train Epoch: 35 [3600/6658 (54%)]\tLoss: 0.394003\n",
      "Train Epoch: 35 [3700/6658 (56%)]\tLoss: 0.097865\n",
      "Train Epoch: 35 [3800/6658 (57%)]\tLoss: 0.230459\n",
      "Train Epoch: 35 [3900/6658 (59%)]\tLoss: 0.161746\n",
      "Train Epoch: 35 [4000/6658 (60%)]\tLoss: 0.167332\n",
      "Train Epoch: 35 [4100/6658 (62%)]\tLoss: 0.174673\n",
      "Train Epoch: 35 [4200/6658 (63%)]\tLoss: 0.098490\n",
      "Train Epoch: 35 [4300/6658 (65%)]\tLoss: 0.388815\n",
      "Train Epoch: 35 [4400/6658 (66%)]\tLoss: 0.089706\n",
      "Train Epoch: 35 [4500/6658 (68%)]\tLoss: 3.601462\n",
      "Train Epoch: 35 [4600/6658 (69%)]\tLoss: 0.000000\n",
      "Train Epoch: 35 [4700/6658 (71%)]\tLoss: 0.000039\n",
      "Train Epoch: 35 [4800/6658 (72%)]\tLoss: 0.078291\n",
      "Train Epoch: 35 [4900/6658 (74%)]\tLoss: 0.020077\n",
      "Train Epoch: 35 [5000/6658 (75%)]\tLoss: 0.156855\n",
      "Train Epoch: 35 [5100/6658 (77%)]\tLoss: 0.254000\n",
      "Train Epoch: 35 [5200/6658 (78%)]\tLoss: 0.193012\n",
      "Train Epoch: 35 [5300/6658 (80%)]\tLoss: 0.238818\n",
      "Train Epoch: 35 [5400/6658 (81%)]\tLoss: 1.471458\n",
      "Train Epoch: 35 [5500/6658 (83%)]\tLoss: 0.194056\n",
      "Train Epoch: 35 [5600/6658 (84%)]\tLoss: 0.001743\n",
      "Train Epoch: 35 [5700/6658 (86%)]\tLoss: 0.690608\n",
      "Train Epoch: 35 [5800/6658 (87%)]\tLoss: 0.021269\n",
      "Train Epoch: 35 [5900/6658 (89%)]\tLoss: 0.810680\n",
      "Train Epoch: 35 [6000/6658 (90%)]\tLoss: 0.210544\n",
      "Train Epoch: 35 [6100/6658 (92%)]\tLoss: 0.866439\n",
      "Train Epoch: 35 [6200/6658 (93%)]\tLoss: 0.101864\n",
      "Train Epoch: 35 [6300/6658 (95%)]\tLoss: 1.114854\n",
      "Train Epoch: 35 [6400/6658 (96%)]\tLoss: 0.346924\n",
      "Train Epoch: 35 [6500/6658 (98%)]\tLoss: 3.008053\n",
      "Train Epoch: 35 [6600/6658 (99%)]\tLoss: 0.490159\n",
      "train loss average =  0.7372806997149846\n",
      "\n",
      "Test set: Average loss: 0.7200\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0807, 6.0212, 5.9548, 5.9409, 6.0498, 6.0273], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 36 [0/6658 (0%)]\tLoss: 0.126600\n",
      "Train Epoch: 36 [100/6658 (2%)]\tLoss: 0.075568\n",
      "Train Epoch: 36 [200/6658 (3%)]\tLoss: 1.541554\n",
      "Train Epoch: 36 [300/6658 (5%)]\tLoss: 0.216667\n",
      "Train Epoch: 36 [400/6658 (6%)]\tLoss: 0.741253\n",
      "Train Epoch: 36 [500/6658 (8%)]\tLoss: 0.051569\n",
      "Train Epoch: 36 [600/6658 (9%)]\tLoss: 6.181745\n",
      "Train Epoch: 36 [700/6658 (11%)]\tLoss: 0.197140\n",
      "Train Epoch: 36 [800/6658 (12%)]\tLoss: 1.210914\n",
      "Train Epoch: 36 [900/6658 (14%)]\tLoss: 1.085228\n",
      "Train Epoch: 36 [1000/6658 (15%)]\tLoss: 0.932105\n",
      "Train Epoch: 36 [1100/6658 (17%)]\tLoss: 0.028098\n",
      "Train Epoch: 36 [1200/6658 (18%)]\tLoss: 0.248444\n",
      "Train Epoch: 36 [1300/6658 (20%)]\tLoss: 0.117941\n",
      "Train Epoch: 36 [1400/6658 (21%)]\tLoss: 0.083194\n",
      "Train Epoch: 36 [1500/6658 (23%)]\tLoss: 0.005112\n",
      "Train Epoch: 36 [1600/6658 (24%)]\tLoss: 0.000227\n",
      "Train Epoch: 36 [1700/6658 (26%)]\tLoss: 0.002309\n",
      "Train Epoch: 36 [1800/6658 (27%)]\tLoss: 0.024589\n",
      "Train Epoch: 36 [1900/6658 (29%)]\tLoss: 0.310082\n",
      "Train Epoch: 36 [2000/6658 (30%)]\tLoss: 0.029243\n",
      "Train Epoch: 36 [2100/6658 (32%)]\tLoss: 0.080872\n",
      "Train Epoch: 36 [2200/6658 (33%)]\tLoss: 0.000272\n",
      "Train Epoch: 36 [2300/6658 (35%)]\tLoss: 0.191840\n",
      "Train Epoch: 36 [2400/6658 (36%)]\tLoss: 0.000775\n",
      "Train Epoch: 36 [2500/6658 (38%)]\tLoss: 1.457865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 36 [2600/6658 (39%)]\tLoss: 0.389922\n",
      "Train Epoch: 36 [2700/6658 (41%)]\tLoss: 0.830315\n",
      "Train Epoch: 36 [2800/6658 (42%)]\tLoss: 0.031697\n",
      "Train Epoch: 36 [2900/6658 (44%)]\tLoss: 0.031870\n",
      "Train Epoch: 36 [3000/6658 (45%)]\tLoss: 0.459288\n",
      "Train Epoch: 36 [3100/6658 (47%)]\tLoss: 0.542643\n",
      "Train Epoch: 36 [3200/6658 (48%)]\tLoss: 0.009556\n",
      "Train Epoch: 36 [3300/6658 (50%)]\tLoss: 2.036627\n",
      "Train Epoch: 36 [3400/6658 (51%)]\tLoss: 0.151905\n",
      "Train Epoch: 36 [3500/6658 (53%)]\tLoss: 0.155683\n",
      "Train Epoch: 36 [3600/6658 (54%)]\tLoss: 0.776595\n",
      "Train Epoch: 36 [3700/6658 (56%)]\tLoss: 0.104190\n",
      "Train Epoch: 36 [3800/6658 (57%)]\tLoss: 1.216470\n",
      "Train Epoch: 36 [3900/6658 (59%)]\tLoss: 0.285762\n",
      "Train Epoch: 36 [4000/6658 (60%)]\tLoss: 0.435194\n",
      "Train Epoch: 36 [4100/6658 (62%)]\tLoss: 0.299953\n",
      "Train Epoch: 36 [4200/6658 (63%)]\tLoss: 0.900454\n",
      "Train Epoch: 36 [4300/6658 (65%)]\tLoss: 0.828929\n",
      "Train Epoch: 36 [4400/6658 (66%)]\tLoss: 1.066220\n",
      "Train Epoch: 36 [4500/6658 (68%)]\tLoss: 2.023540\n",
      "Train Epoch: 36 [4600/6658 (69%)]\tLoss: 0.003476\n",
      "Train Epoch: 36 [4700/6658 (71%)]\tLoss: 0.609388\n",
      "Train Epoch: 36 [4800/6658 (72%)]\tLoss: 0.003881\n",
      "Train Epoch: 36 [4900/6658 (74%)]\tLoss: 4.801805\n",
      "Train Epoch: 36 [5000/6658 (75%)]\tLoss: 0.565389\n",
      "Train Epoch: 36 [5100/6658 (77%)]\tLoss: 0.313437\n",
      "Train Epoch: 36 [5200/6658 (78%)]\tLoss: 1.344700\n",
      "Train Epoch: 36 [5300/6658 (80%)]\tLoss: 0.000002\n",
      "Train Epoch: 36 [5400/6658 (81%)]\tLoss: 1.132844\n",
      "Train Epoch: 36 [5500/6658 (83%)]\tLoss: 0.608436\n",
      "Train Epoch: 36 [5600/6658 (84%)]\tLoss: 0.507157\n",
      "Train Epoch: 36 [5700/6658 (86%)]\tLoss: 0.017702\n",
      "Train Epoch: 36 [5800/6658 (87%)]\tLoss: 0.034919\n",
      "Train Epoch: 36 [5900/6658 (89%)]\tLoss: 0.805751\n",
      "Train Epoch: 36 [6000/6658 (90%)]\tLoss: 0.313417\n",
      "Train Epoch: 36 [6100/6658 (92%)]\tLoss: 0.671538\n",
      "Train Epoch: 36 [6200/6658 (93%)]\tLoss: 0.070551\n",
      "Train Epoch: 36 [6300/6658 (95%)]\tLoss: 0.222891\n",
      "Train Epoch: 36 [6400/6658 (96%)]\tLoss: 0.000119\n",
      "Train Epoch: 36 [6500/6658 (98%)]\tLoss: 1.016403\n",
      "Train Epoch: 36 [6600/6658 (99%)]\tLoss: 0.256767\n",
      "train loss average =  0.7398550375001016\n",
      "\n",
      "Test set: Average loss: 0.7042\n",
      "\n",
      "Validation loss decreased (0.706962 --> 0.704185).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.0832, 6.0218, 5.9533, 5.9393, 6.0511, 6.0282], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 37 [0/6658 (0%)]\tLoss: 0.212937\n",
      "Train Epoch: 37 [100/6658 (2%)]\tLoss: 0.014906\n",
      "Train Epoch: 37 [200/6658 (3%)]\tLoss: 0.427302\n",
      "Train Epoch: 37 [300/6658 (5%)]\tLoss: 1.509418\n",
      "Train Epoch: 37 [400/6658 (6%)]\tLoss: 0.048856\n",
      "Train Epoch: 37 [500/6658 (8%)]\tLoss: 0.277175\n",
      "Train Epoch: 37 [600/6658 (9%)]\tLoss: 0.109459\n",
      "Train Epoch: 37 [700/6658 (11%)]\tLoss: 0.300774\n",
      "Train Epoch: 37 [800/6658 (12%)]\tLoss: 0.002006\n",
      "Train Epoch: 37 [900/6658 (14%)]\tLoss: 0.036273\n",
      "Train Epoch: 37 [1000/6658 (15%)]\tLoss: 1.498399\n",
      "Train Epoch: 37 [1100/6658 (17%)]\tLoss: 0.913377\n",
      "Train Epoch: 37 [1200/6658 (18%)]\tLoss: 0.038696\n",
      "Train Epoch: 37 [1300/6658 (20%)]\tLoss: 0.256977\n",
      "Train Epoch: 37 [1400/6658 (21%)]\tLoss: 2.372605\n",
      "Train Epoch: 37 [1500/6658 (23%)]\tLoss: 0.309074\n",
      "Train Epoch: 37 [1600/6658 (24%)]\tLoss: 1.595617\n",
      "Train Epoch: 37 [1700/6658 (26%)]\tLoss: 0.250380\n",
      "Train Epoch: 37 [1800/6658 (27%)]\tLoss: 0.069991\n",
      "Train Epoch: 37 [1900/6658 (29%)]\tLoss: 0.110365\n",
      "Train Epoch: 37 [2000/6658 (30%)]\tLoss: 3.020558\n",
      "Train Epoch: 37 [2100/6658 (32%)]\tLoss: 0.621293\n",
      "Train Epoch: 37 [2200/6658 (33%)]\tLoss: 0.000392\n",
      "Train Epoch: 37 [2300/6658 (35%)]\tLoss: 1.090157\n",
      "Train Epoch: 37 [2400/6658 (36%)]\tLoss: 0.009242\n",
      "Train Epoch: 37 [2500/6658 (38%)]\tLoss: 0.025620\n",
      "Train Epoch: 37 [2600/6658 (39%)]\tLoss: 0.080089\n",
      "Train Epoch: 37 [2700/6658 (41%)]\tLoss: 0.001350\n",
      "Train Epoch: 37 [2800/6658 (42%)]\tLoss: 1.595932\n",
      "Train Epoch: 37 [2900/6658 (44%)]\tLoss: 0.771194\n",
      "Train Epoch: 37 [3000/6658 (45%)]\tLoss: 0.014438\n",
      "Train Epoch: 37 [3100/6658 (47%)]\tLoss: 1.125802\n",
      "Train Epoch: 37 [3200/6658 (48%)]\tLoss: 1.508292\n",
      "Train Epoch: 37 [3300/6658 (50%)]\tLoss: 2.302011\n",
      "Train Epoch: 37 [3400/6658 (51%)]\tLoss: 0.555157\n",
      "Train Epoch: 37 [3500/6658 (53%)]\tLoss: 0.107390\n",
      "Train Epoch: 37 [3600/6658 (54%)]\tLoss: 1.241143\n",
      "Train Epoch: 37 [3700/6658 (56%)]\tLoss: 0.002062\n",
      "Train Epoch: 37 [3800/6658 (57%)]\tLoss: 1.685884\n",
      "Train Epoch: 37 [3900/6658 (59%)]\tLoss: 0.920868\n",
      "Train Epoch: 37 [4000/6658 (60%)]\tLoss: 0.647579\n",
      "Train Epoch: 37 [4100/6658 (62%)]\tLoss: 0.008692\n",
      "Train Epoch: 37 [4200/6658 (63%)]\tLoss: 2.476552\n",
      "Train Epoch: 37 [4300/6658 (65%)]\tLoss: 0.402875\n",
      "Train Epoch: 37 [4400/6658 (66%)]\tLoss: 0.066802\n",
      "Train Epoch: 37 [4500/6658 (68%)]\tLoss: 2.018309\n",
      "Train Epoch: 37 [4600/6658 (69%)]\tLoss: 1.143339\n",
      "Train Epoch: 37 [4700/6658 (71%)]\tLoss: 2.009077\n",
      "Train Epoch: 37 [4800/6658 (72%)]\tLoss: 0.174872\n",
      "Train Epoch: 37 [4900/6658 (74%)]\tLoss: 1.421808\n",
      "Train Epoch: 37 [5000/6658 (75%)]\tLoss: 0.231460\n",
      "Train Epoch: 37 [5100/6658 (77%)]\tLoss: 0.112384\n",
      "Train Epoch: 37 [5200/6658 (78%)]\tLoss: 0.607014\n",
      "Train Epoch: 37 [5300/6658 (80%)]\tLoss: 0.089096\n",
      "Train Epoch: 37 [5400/6658 (81%)]\tLoss: 0.723040\n",
      "Train Epoch: 37 [5500/6658 (83%)]\tLoss: 0.016381\n",
      "Train Epoch: 37 [5600/6658 (84%)]\tLoss: 0.357441\n",
      "Train Epoch: 37 [5700/6658 (86%)]\tLoss: 3.262984\n",
      "Train Epoch: 37 [5800/6658 (87%)]\tLoss: 0.179594\n",
      "Train Epoch: 37 [5900/6658 (89%)]\tLoss: 0.000040\n",
      "Train Epoch: 37 [6000/6658 (90%)]\tLoss: 0.035372\n",
      "Train Epoch: 37 [6100/6658 (92%)]\tLoss: 0.522020\n",
      "Train Epoch: 37 [6200/6658 (93%)]\tLoss: 0.100298\n",
      "Train Epoch: 37 [6300/6658 (95%)]\tLoss: 0.045827\n",
      "Train Epoch: 37 [6400/6658 (96%)]\tLoss: 1.397015\n",
      "Train Epoch: 37 [6500/6658 (98%)]\tLoss: 0.108477\n",
      "Train Epoch: 37 [6600/6658 (99%)]\tLoss: 1.017855\n",
      "train loss average =  0.7425952811362171\n",
      "\n",
      "Test set: Average loss: 0.7175\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0862, 6.0222, 5.9515, 5.9385, 6.0528, 6.0286], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 38 [0/6658 (0%)]\tLoss: 0.101828\n",
      "Train Epoch: 38 [100/6658 (2%)]\tLoss: 0.000731\n",
      "Train Epoch: 38 [200/6658 (3%)]\tLoss: 3.031857\n",
      "Train Epoch: 38 [300/6658 (5%)]\tLoss: 0.143633\n",
      "Train Epoch: 38 [400/6658 (6%)]\tLoss: 0.993731\n",
      "Train Epoch: 38 [500/6658 (8%)]\tLoss: 0.246338\n",
      "Train Epoch: 38 [600/6658 (9%)]\tLoss: 1.298476\n",
      "Train Epoch: 38 [700/6658 (11%)]\tLoss: 0.820537\n",
      "Train Epoch: 38 [800/6658 (12%)]\tLoss: 0.276798\n",
      "Train Epoch: 38 [900/6658 (14%)]\tLoss: 0.997900\n",
      "Train Epoch: 38 [1000/6658 (15%)]\tLoss: 0.254096\n",
      "Train Epoch: 38 [1100/6658 (17%)]\tLoss: 0.206702\n",
      "Train Epoch: 38 [1200/6658 (18%)]\tLoss: 0.263623\n",
      "Train Epoch: 38 [1300/6658 (20%)]\tLoss: 0.029270\n",
      "Train Epoch: 38 [1400/6658 (21%)]\tLoss: 0.290006\n",
      "Train Epoch: 38 [1500/6658 (23%)]\tLoss: 0.073352\n",
      "Train Epoch: 38 [1600/6658 (24%)]\tLoss: 1.872783\n",
      "Train Epoch: 38 [1700/6658 (26%)]\tLoss: 0.001745\n",
      "Train Epoch: 38 [1800/6658 (27%)]\tLoss: 0.118937\n",
      "Train Epoch: 38 [1900/6658 (29%)]\tLoss: 0.680881\n",
      "Train Epoch: 38 [2000/6658 (30%)]\tLoss: 0.020094\n",
      "Train Epoch: 38 [2100/6658 (32%)]\tLoss: 0.045413\n",
      "Train Epoch: 38 [2200/6658 (33%)]\tLoss: 0.744303\n",
      "Train Epoch: 38 [2300/6658 (35%)]\tLoss: 0.002949\n",
      "Train Epoch: 38 [2400/6658 (36%)]\tLoss: 1.349000\n",
      "Train Epoch: 38 [2500/6658 (38%)]\tLoss: 0.020464\n",
      "Train Epoch: 38 [2600/6658 (39%)]\tLoss: 0.086456\n",
      "Train Epoch: 38 [2700/6658 (41%)]\tLoss: 1.492764\n",
      "Train Epoch: 38 [2800/6658 (42%)]\tLoss: 0.065033\n",
      "Train Epoch: 38 [2900/6658 (44%)]\tLoss: 1.325410\n",
      "Train Epoch: 38 [3000/6658 (45%)]\tLoss: 0.410220\n",
      "Train Epoch: 38 [3100/6658 (47%)]\tLoss: 0.314910\n",
      "Train Epoch: 38 [3200/6658 (48%)]\tLoss: 0.411200\n",
      "Train Epoch: 38 [3300/6658 (50%)]\tLoss: 0.204799\n",
      "Train Epoch: 38 [3400/6658 (51%)]\tLoss: 0.546366\n",
      "Train Epoch: 38 [3500/6658 (53%)]\tLoss: 0.118853\n",
      "Train Epoch: 38 [3600/6658 (54%)]\tLoss: 0.050861\n",
      "Train Epoch: 38 [3700/6658 (56%)]\tLoss: 0.071775\n",
      "Train Epoch: 38 [3800/6658 (57%)]\tLoss: 1.858726\n",
      "Train Epoch: 38 [3900/6658 (59%)]\tLoss: 0.961613\n",
      "Train Epoch: 38 [4000/6658 (60%)]\tLoss: 1.919633\n",
      "Train Epoch: 38 [4100/6658 (62%)]\tLoss: 2.131827\n",
      "Train Epoch: 38 [4200/6658 (63%)]\tLoss: 0.487232\n",
      "Train Epoch: 38 [4300/6658 (65%)]\tLoss: 0.005837\n",
      "Train Epoch: 38 [4400/6658 (66%)]\tLoss: 0.072135\n",
      "Train Epoch: 38 [4500/6658 (68%)]\tLoss: 0.438070\n",
      "Train Epoch: 38 [4600/6658 (69%)]\tLoss: 0.411693\n",
      "Train Epoch: 38 [4700/6658 (71%)]\tLoss: 0.547445\n",
      "Train Epoch: 38 [4800/6658 (72%)]\tLoss: 0.035486\n",
      "Train Epoch: 38 [4900/6658 (74%)]\tLoss: 0.226304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 38 [5000/6658 (75%)]\tLoss: 0.117043\n",
      "Train Epoch: 38 [5100/6658 (77%)]\tLoss: 0.073686\n",
      "Train Epoch: 38 [5200/6658 (78%)]\tLoss: 0.389068\n",
      "Train Epoch: 38 [5300/6658 (80%)]\tLoss: 0.025411\n",
      "Train Epoch: 38 [5400/6658 (81%)]\tLoss: 1.717649\n",
      "Train Epoch: 38 [5500/6658 (83%)]\tLoss: 0.035129\n",
      "Train Epoch: 38 [5600/6658 (84%)]\tLoss: 0.024290\n",
      "Train Epoch: 38 [5700/6658 (86%)]\tLoss: 0.251846\n",
      "Train Epoch: 38 [5800/6658 (87%)]\tLoss: 1.009682\n",
      "Train Epoch: 38 [5900/6658 (89%)]\tLoss: 0.290066\n",
      "Train Epoch: 38 [6000/6658 (90%)]\tLoss: 0.978318\n",
      "Train Epoch: 38 [6100/6658 (92%)]\tLoss: 1.058949\n",
      "Train Epoch: 38 [6200/6658 (93%)]\tLoss: 6.012948\n",
      "Train Epoch: 38 [6300/6658 (95%)]\tLoss: 0.067166\n",
      "Train Epoch: 38 [6400/6658 (96%)]\tLoss: 0.270137\n",
      "Train Epoch: 38 [6500/6658 (98%)]\tLoss: 0.095701\n",
      "Train Epoch: 38 [6600/6658 (99%)]\tLoss: 0.112266\n",
      "train loss average =  0.739877535264321\n",
      "\n",
      "Test set: Average loss: 0.7115\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0888, 6.0226, 5.9511, 5.9383, 6.0538, 6.0288], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 39 [0/6658 (0%)]\tLoss: 0.600558\n",
      "Train Epoch: 39 [100/6658 (2%)]\tLoss: 0.177559\n",
      "Train Epoch: 39 [200/6658 (3%)]\tLoss: 0.915531\n",
      "Train Epoch: 39 [300/6658 (5%)]\tLoss: 0.521020\n",
      "Train Epoch: 39 [400/6658 (6%)]\tLoss: 0.440369\n",
      "Train Epoch: 39 [500/6658 (8%)]\tLoss: 0.081720\n",
      "Train Epoch: 39 [600/6658 (9%)]\tLoss: 0.199375\n",
      "Train Epoch: 39 [700/6658 (11%)]\tLoss: 0.032705\n",
      "Train Epoch: 39 [800/6658 (12%)]\tLoss: 0.277516\n",
      "Train Epoch: 39 [900/6658 (14%)]\tLoss: 0.010555\n",
      "Train Epoch: 39 [1000/6658 (15%)]\tLoss: 0.250510\n",
      "Train Epoch: 39 [1100/6658 (17%)]\tLoss: 0.008400\n",
      "Train Epoch: 39 [1200/6658 (18%)]\tLoss: 0.010490\n",
      "Train Epoch: 39 [1300/6658 (20%)]\tLoss: 0.315959\n",
      "Train Epoch: 39 [1400/6658 (21%)]\tLoss: 0.105799\n",
      "Train Epoch: 39 [1500/6658 (23%)]\tLoss: 0.020384\n",
      "Train Epoch: 39 [1600/6658 (24%)]\tLoss: 0.003157\n",
      "Train Epoch: 39 [1700/6658 (26%)]\tLoss: 0.022034\n",
      "Train Epoch: 39 [1800/6658 (27%)]\tLoss: 0.158379\n",
      "Train Epoch: 39 [1900/6658 (29%)]\tLoss: 0.167695\n",
      "Train Epoch: 39 [2000/6658 (30%)]\tLoss: 0.188840\n",
      "Train Epoch: 39 [2100/6658 (32%)]\tLoss: 0.498466\n",
      "Train Epoch: 39 [2200/6658 (33%)]\tLoss: 0.049135\n",
      "Train Epoch: 39 [2300/6658 (35%)]\tLoss: 1.809860\n",
      "Train Epoch: 39 [2400/6658 (36%)]\tLoss: 3.873928\n",
      "Train Epoch: 39 [2500/6658 (38%)]\tLoss: 1.086270\n",
      "Train Epoch: 39 [2600/6658 (39%)]\tLoss: 0.007251\n",
      "Train Epoch: 39 [2700/6658 (41%)]\tLoss: 0.454289\n",
      "Train Epoch: 39 [2800/6658 (42%)]\tLoss: 0.198563\n",
      "Train Epoch: 39 [2900/6658 (44%)]\tLoss: 0.121319\n",
      "Train Epoch: 39 [3000/6658 (45%)]\tLoss: 0.342636\n",
      "Train Epoch: 39 [3100/6658 (47%)]\tLoss: 0.356759\n",
      "Train Epoch: 39 [3200/6658 (48%)]\tLoss: 0.825959\n",
      "Train Epoch: 39 [3300/6658 (50%)]\tLoss: 0.018033\n",
      "Train Epoch: 39 [3400/6658 (51%)]\tLoss: 0.414291\n",
      "Train Epoch: 39 [3500/6658 (53%)]\tLoss: 0.381016\n",
      "Train Epoch: 39 [3600/6658 (54%)]\tLoss: 0.010562\n",
      "Train Epoch: 39 [3700/6658 (56%)]\tLoss: 0.480189\n",
      "Train Epoch: 39 [3800/6658 (57%)]\tLoss: 0.656662\n",
      "Train Epoch: 39 [3900/6658 (59%)]\tLoss: 0.017727\n",
      "Train Epoch: 39 [4000/6658 (60%)]\tLoss: 0.772227\n",
      "Train Epoch: 39 [4100/6658 (62%)]\tLoss: 0.043456\n",
      "Train Epoch: 39 [4200/6658 (63%)]\tLoss: 0.353384\n",
      "Train Epoch: 39 [4300/6658 (65%)]\tLoss: 1.105978\n",
      "Train Epoch: 39 [4400/6658 (66%)]\tLoss: 0.058834\n",
      "Train Epoch: 39 [4500/6658 (68%)]\tLoss: 0.076878\n",
      "Train Epoch: 39 [4600/6658 (69%)]\tLoss: 0.038383\n",
      "Train Epoch: 39 [4700/6658 (71%)]\tLoss: 0.566732\n",
      "Train Epoch: 39 [4800/6658 (72%)]\tLoss: 0.488572\n",
      "Train Epoch: 39 [4900/6658 (74%)]\tLoss: 1.148473\n",
      "Train Epoch: 39 [5000/6658 (75%)]\tLoss: 0.109944\n",
      "Train Epoch: 39 [5100/6658 (77%)]\tLoss: 1.276069\n",
      "Train Epoch: 39 [5200/6658 (78%)]\tLoss: 0.545653\n",
      "Train Epoch: 39 [5300/6658 (80%)]\tLoss: 0.155943\n",
      "Train Epoch: 39 [5400/6658 (81%)]\tLoss: 0.183437\n",
      "Train Epoch: 39 [5500/6658 (83%)]\tLoss: 8.702471\n",
      "Train Epoch: 39 [5600/6658 (84%)]\tLoss: 0.992545\n",
      "Train Epoch: 39 [5700/6658 (86%)]\tLoss: 0.578155\n",
      "Train Epoch: 39 [5800/6658 (87%)]\tLoss: 0.037238\n",
      "Train Epoch: 39 [5900/6658 (89%)]\tLoss: 0.023496\n",
      "Train Epoch: 39 [6000/6658 (90%)]\tLoss: 0.000010\n",
      "Train Epoch: 39 [6100/6658 (92%)]\tLoss: 1.317150\n",
      "Train Epoch: 39 [6200/6658 (93%)]\tLoss: 0.039784\n",
      "Train Epoch: 39 [6300/6658 (95%)]\tLoss: 0.158096\n",
      "Train Epoch: 39 [6400/6658 (96%)]\tLoss: 0.014006\n",
      "Train Epoch: 39 [6500/6658 (98%)]\tLoss: 0.545453\n",
      "Train Epoch: 39 [6600/6658 (99%)]\tLoss: 0.035191\n",
      "train loss average =  0.7393648033726776\n",
      "\n",
      "Test set: Average loss: 0.7372\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0909, 6.0235, 5.9502, 5.9372, 6.0551, 6.0290], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 40 [0/6658 (0%)]\tLoss: 0.122608\n",
      "Train Epoch: 40 [100/6658 (2%)]\tLoss: 1.194899\n",
      "Train Epoch: 40 [200/6658 (3%)]\tLoss: 0.001178\n",
      "Train Epoch: 40 [300/6658 (5%)]\tLoss: 1.501183\n",
      "Train Epoch: 40 [400/6658 (6%)]\tLoss: 0.044871\n",
      "Train Epoch: 40 [500/6658 (8%)]\tLoss: 0.074623\n",
      "Train Epoch: 40 [600/6658 (9%)]\tLoss: 3.531190\n",
      "Train Epoch: 40 [700/6658 (11%)]\tLoss: 0.069436\n",
      "Train Epoch: 40 [800/6658 (12%)]\tLoss: 0.618032\n",
      "Train Epoch: 40 [900/6658 (14%)]\tLoss: 1.917043\n",
      "Train Epoch: 40 [1000/6658 (15%)]\tLoss: 0.131557\n",
      "Train Epoch: 40 [1100/6658 (17%)]\tLoss: 0.040634\n",
      "Train Epoch: 40 [1200/6658 (18%)]\tLoss: 0.039076\n",
      "Train Epoch: 40 [1300/6658 (20%)]\tLoss: 0.879689\n",
      "Train Epoch: 40 [1400/6658 (21%)]\tLoss: 0.505548\n",
      "Train Epoch: 40 [1500/6658 (23%)]\tLoss: 0.000459\n",
      "Train Epoch: 40 [1600/6658 (24%)]\tLoss: 0.507168\n",
      "Train Epoch: 40 [1700/6658 (26%)]\tLoss: 0.067059\n",
      "Train Epoch: 40 [1800/6658 (27%)]\tLoss: 0.046462\n",
      "Train Epoch: 40 [1900/6658 (29%)]\tLoss: 0.299160\n",
      "Train Epoch: 40 [2000/6658 (30%)]\tLoss: 0.000760\n",
      "Train Epoch: 40 [2100/6658 (32%)]\tLoss: 0.362840\n",
      "Train Epoch: 40 [2200/6658 (33%)]\tLoss: 0.015181\n",
      "Train Epoch: 40 [2300/6658 (35%)]\tLoss: 1.332787\n",
      "Train Epoch: 40 [2400/6658 (36%)]\tLoss: 0.102066\n",
      "Train Epoch: 40 [2500/6658 (38%)]\tLoss: 2.082815\n",
      "Train Epoch: 40 [2600/6658 (39%)]\tLoss: 0.299624\n",
      "Train Epoch: 40 [2700/6658 (41%)]\tLoss: 0.204500\n",
      "Train Epoch: 40 [2800/6658 (42%)]\tLoss: 0.001038\n",
      "Train Epoch: 40 [2900/6658 (44%)]\tLoss: 0.611046\n",
      "Train Epoch: 40 [3000/6658 (45%)]\tLoss: 0.091170\n",
      "Train Epoch: 40 [3100/6658 (47%)]\tLoss: 0.000300\n",
      "Train Epoch: 40 [3200/6658 (48%)]\tLoss: 0.152636\n",
      "Train Epoch: 40 [3300/6658 (50%)]\tLoss: 0.760868\n",
      "Train Epoch: 40 [3400/6658 (51%)]\tLoss: 1.005841\n",
      "Train Epoch: 40 [3500/6658 (53%)]\tLoss: 0.609594\n",
      "Train Epoch: 40 [3600/6658 (54%)]\tLoss: 0.136077\n",
      "Train Epoch: 40 [3700/6658 (56%)]\tLoss: 0.011296\n",
      "Train Epoch: 40 [3800/6658 (57%)]\tLoss: 1.323511\n",
      "Train Epoch: 40 [3900/6658 (59%)]\tLoss: 0.106223\n",
      "Train Epoch: 40 [4000/6658 (60%)]\tLoss: 0.381567\n",
      "Train Epoch: 40 [4100/6658 (62%)]\tLoss: 0.052048\n",
      "Train Epoch: 40 [4200/6658 (63%)]\tLoss: 1.037442\n",
      "Train Epoch: 40 [4300/6658 (65%)]\tLoss: 1.389456\n",
      "Train Epoch: 40 [4400/6658 (66%)]\tLoss: 0.000332\n",
      "Train Epoch: 40 [4500/6658 (68%)]\tLoss: 0.940902\n",
      "Train Epoch: 40 [4600/6658 (69%)]\tLoss: 1.789471\n",
      "Train Epoch: 40 [4700/6658 (71%)]\tLoss: 0.000202\n",
      "Train Epoch: 40 [4800/6658 (72%)]\tLoss: 0.003691\n",
      "Train Epoch: 40 [4900/6658 (74%)]\tLoss: 0.913439\n",
      "Train Epoch: 40 [5000/6658 (75%)]\tLoss: 3.845953\n",
      "Train Epoch: 40 [5100/6658 (77%)]\tLoss: 0.401741\n",
      "Train Epoch: 40 [5200/6658 (78%)]\tLoss: 0.752757\n",
      "Train Epoch: 40 [5300/6658 (80%)]\tLoss: 0.583144\n",
      "Train Epoch: 40 [5400/6658 (81%)]\tLoss: 1.285517\n",
      "Train Epoch: 40 [5500/6658 (83%)]\tLoss: 0.032262\n",
      "Train Epoch: 40 [5600/6658 (84%)]\tLoss: 0.452146\n",
      "Train Epoch: 40 [5700/6658 (86%)]\tLoss: 0.258469\n",
      "Train Epoch: 40 [5800/6658 (87%)]\tLoss: 1.370853\n",
      "Train Epoch: 40 [5900/6658 (89%)]\tLoss: 0.141457\n",
      "Train Epoch: 40 [6000/6658 (90%)]\tLoss: 0.015686\n",
      "Train Epoch: 40 [6100/6658 (92%)]\tLoss: 0.006423\n",
      "Train Epoch: 40 [6200/6658 (93%)]\tLoss: 1.573051\n",
      "Train Epoch: 40 [6300/6658 (95%)]\tLoss: 0.024454\n",
      "Train Epoch: 40 [6400/6658 (96%)]\tLoss: 0.012208\n",
      "Train Epoch: 40 [6500/6658 (98%)]\tLoss: 0.837227\n",
      "Train Epoch: 40 [6600/6658 (99%)]\tLoss: 0.392768\n",
      "train loss average =  0.7406577665173308\n",
      "\n",
      "Test set: Average loss: 0.7095\n",
      "\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0932, 6.0235, 5.9496, 5.9367, 6.0565, 6.0295], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 41 [0/6658 (0%)]\tLoss: 0.425871\n",
      "Train Epoch: 41 [100/6658 (2%)]\tLoss: 0.305820\n",
      "Train Epoch: 41 [200/6658 (3%)]\tLoss: 4.072581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 41 [300/6658 (5%)]\tLoss: 0.041898\n",
      "Train Epoch: 41 [400/6658 (6%)]\tLoss: 0.114517\n",
      "Train Epoch: 41 [500/6658 (8%)]\tLoss: 0.128657\n",
      "Train Epoch: 41 [600/6658 (9%)]\tLoss: 0.571858\n",
      "Train Epoch: 41 [700/6658 (11%)]\tLoss: 0.553491\n",
      "Train Epoch: 41 [800/6658 (12%)]\tLoss: 0.735433\n",
      "Train Epoch: 41 [900/6658 (14%)]\tLoss: 8.438291\n",
      "Train Epoch: 41 [1000/6658 (15%)]\tLoss: 0.114068\n",
      "Train Epoch: 41 [1100/6658 (17%)]\tLoss: 0.323798\n",
      "Train Epoch: 41 [1200/6658 (18%)]\tLoss: 0.099891\n",
      "Train Epoch: 41 [1300/6658 (20%)]\tLoss: 0.205725\n",
      "Train Epoch: 41 [1400/6658 (21%)]\tLoss: 0.698111\n",
      "Train Epoch: 41 [1500/6658 (23%)]\tLoss: 0.210914\n",
      "Train Epoch: 41 [1600/6658 (24%)]\tLoss: 0.789665\n",
      "Train Epoch: 41 [1700/6658 (26%)]\tLoss: 2.603326\n",
      "Train Epoch: 41 [1800/6658 (27%)]\tLoss: 0.061957\n",
      "Train Epoch: 41 [1900/6658 (29%)]\tLoss: 0.590016\n",
      "Train Epoch: 41 [2000/6658 (30%)]\tLoss: 8.634033\n",
      "Train Epoch: 41 [2100/6658 (32%)]\tLoss: 0.017435\n",
      "Train Epoch: 41 [2200/6658 (33%)]\tLoss: 0.485519\n",
      "Train Epoch: 41 [2300/6658 (35%)]\tLoss: 0.334082\n",
      "Train Epoch: 41 [2400/6658 (36%)]\tLoss: 0.295410\n",
      "Train Epoch: 41 [2500/6658 (38%)]\tLoss: 0.009372\n",
      "Train Epoch: 41 [2600/6658 (39%)]\tLoss: 0.346637\n",
      "Train Epoch: 41 [2700/6658 (41%)]\tLoss: 0.095791\n",
      "Train Epoch: 41 [2800/6658 (42%)]\tLoss: 0.008236\n",
      "Train Epoch: 41 [2900/6658 (44%)]\tLoss: 1.634033\n",
      "Train Epoch: 41 [3000/6658 (45%)]\tLoss: 0.066900\n",
      "Train Epoch: 41 [3100/6658 (47%)]\tLoss: 0.218267\n",
      "Train Epoch: 41 [3200/6658 (48%)]\tLoss: 0.091985\n",
      "Train Epoch: 41 [3300/6658 (50%)]\tLoss: 1.178964\n",
      "Train Epoch: 41 [3400/6658 (51%)]\tLoss: 0.079001\n",
      "Train Epoch: 41 [3500/6658 (53%)]\tLoss: 0.738425\n",
      "Train Epoch: 41 [3600/6658 (54%)]\tLoss: 0.853122\n",
      "Train Epoch: 41 [3700/6658 (56%)]\tLoss: 0.226874\n",
      "Train Epoch: 41 [3800/6658 (57%)]\tLoss: 0.120676\n",
      "Train Epoch: 41 [3900/6658 (59%)]\tLoss: 0.461842\n",
      "Train Epoch: 41 [4000/6658 (60%)]\tLoss: 0.616598\n",
      "Train Epoch: 41 [4100/6658 (62%)]\tLoss: 0.060266\n",
      "Train Epoch: 41 [4200/6658 (63%)]\tLoss: 1.887438\n",
      "Train Epoch: 41 [4300/6658 (65%)]\tLoss: 0.073591\n",
      "Train Epoch: 41 [4400/6658 (66%)]\tLoss: 0.174549\n",
      "Train Epoch: 41 [4500/6658 (68%)]\tLoss: 0.021171\n",
      "Train Epoch: 41 [4600/6658 (69%)]\tLoss: 8.017829\n",
      "Train Epoch: 41 [4700/6658 (71%)]\tLoss: 0.642342\n",
      "Train Epoch: 41 [4800/6658 (72%)]\tLoss: 0.001908\n",
      "Train Epoch: 41 [4900/6658 (74%)]\tLoss: 0.353715\n",
      "Train Epoch: 41 [5000/6658 (75%)]\tLoss: 9.107181\n",
      "Train Epoch: 41 [5100/6658 (77%)]\tLoss: 0.398212\n",
      "Train Epoch: 41 [5200/6658 (78%)]\tLoss: 0.050576\n",
      "Train Epoch: 41 [5300/6658 (80%)]\tLoss: 0.006719\n",
      "Train Epoch: 41 [5400/6658 (81%)]\tLoss: 0.681370\n",
      "Train Epoch: 41 [5500/6658 (83%)]\tLoss: 0.020571\n",
      "Train Epoch: 41 [5600/6658 (84%)]\tLoss: 1.445194\n",
      "Train Epoch: 41 [5700/6658 (86%)]\tLoss: 2.822611\n",
      "Train Epoch: 41 [5800/6658 (87%)]\tLoss: 0.028710\n",
      "Train Epoch: 41 [5900/6658 (89%)]\tLoss: 0.592912\n",
      "Train Epoch: 41 [6000/6658 (90%)]\tLoss: 0.006809\n",
      "Train Epoch: 41 [6100/6658 (92%)]\tLoss: 0.020758\n",
      "Train Epoch: 41 [6200/6658 (93%)]\tLoss: 2.338849\n",
      "Train Epoch: 41 [6300/6658 (95%)]\tLoss: 0.527376\n",
      "Train Epoch: 41 [6400/6658 (96%)]\tLoss: 0.000016\n",
      "Train Epoch: 41 [6500/6658 (98%)]\tLoss: 6.904146\n",
      "Train Epoch: 41 [6600/6658 (99%)]\tLoss: 0.107753\n",
      "train loss average =  0.7348728519229257\n",
      "\n",
      "Test set: Average loss: 0.7102\n",
      "\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0952, 6.0232, 5.9485, 5.9354, 6.0576, 6.0298], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 42 [0/6658 (0%)]\tLoss: 0.199848\n",
      "Train Epoch: 42 [100/6658 (2%)]\tLoss: 3.066134\n",
      "Train Epoch: 42 [200/6658 (3%)]\tLoss: 0.000414\n",
      "Train Epoch: 42 [300/6658 (5%)]\tLoss: 1.007836\n",
      "Train Epoch: 42 [400/6658 (6%)]\tLoss: 0.421393\n",
      "Train Epoch: 42 [500/6658 (8%)]\tLoss: 0.245908\n",
      "Train Epoch: 42 [600/6658 (9%)]\tLoss: 0.007420\n",
      "Train Epoch: 42 [700/6658 (11%)]\tLoss: 0.247216\n",
      "Train Epoch: 42 [800/6658 (12%)]\tLoss: 0.113638\n",
      "Train Epoch: 42 [900/6658 (14%)]\tLoss: 0.022188\n",
      "Train Epoch: 42 [1000/6658 (15%)]\tLoss: 4.494683\n",
      "Train Epoch: 42 [1100/6658 (17%)]\tLoss: 0.005707\n",
      "Train Epoch: 42 [1200/6658 (18%)]\tLoss: 0.015745\n",
      "Train Epoch: 42 [1300/6658 (20%)]\tLoss: 1.870421\n",
      "Train Epoch: 42 [1400/6658 (21%)]\tLoss: 0.032923\n",
      "Train Epoch: 42 [1500/6658 (23%)]\tLoss: 0.819513\n",
      "Train Epoch: 42 [1600/6658 (24%)]\tLoss: 0.188371\n",
      "Train Epoch: 42 [1700/6658 (26%)]\tLoss: 0.027821\n",
      "Train Epoch: 42 [1800/6658 (27%)]\tLoss: 0.491632\n",
      "Train Epoch: 42 [1900/6658 (29%)]\tLoss: 5.704921\n",
      "Train Epoch: 42 [2000/6658 (30%)]\tLoss: 0.078783\n",
      "Train Epoch: 42 [2100/6658 (32%)]\tLoss: 1.661243\n",
      "Train Epoch: 42 [2200/6658 (33%)]\tLoss: 0.231702\n",
      "Train Epoch: 42 [2300/6658 (35%)]\tLoss: 0.064971\n",
      "Train Epoch: 42 [2400/6658 (36%)]\tLoss: 0.198459\n",
      "Train Epoch: 42 [2500/6658 (38%)]\tLoss: 0.016015\n",
      "Train Epoch: 42 [2600/6658 (39%)]\tLoss: 0.559034\n",
      "Train Epoch: 42 [2700/6658 (41%)]\tLoss: 0.258717\n",
      "Train Epoch: 42 [2800/6658 (42%)]\tLoss: 0.033810\n",
      "Train Epoch: 42 [2900/6658 (44%)]\tLoss: 5.827259\n",
      "Train Epoch: 42 [3000/6658 (45%)]\tLoss: 1.131825\n",
      "Train Epoch: 42 [3100/6658 (47%)]\tLoss: 0.198768\n",
      "Train Epoch: 42 [3200/6658 (48%)]\tLoss: 0.374473\n",
      "Train Epoch: 42 [3300/6658 (50%)]\tLoss: 1.054094\n",
      "Train Epoch: 42 [3400/6658 (51%)]\tLoss: 0.033021\n",
      "Train Epoch: 42 [3500/6658 (53%)]\tLoss: 1.527089\n",
      "Train Epoch: 42 [3600/6658 (54%)]\tLoss: 0.899053\n",
      "Train Epoch: 42 [3700/6658 (56%)]\tLoss: 0.009672\n",
      "Train Epoch: 42 [3800/6658 (57%)]\tLoss: 0.653209\n",
      "Train Epoch: 42 [3900/6658 (59%)]\tLoss: 0.151249\n",
      "Train Epoch: 42 [4000/6658 (60%)]\tLoss: 0.040839\n",
      "Train Epoch: 42 [4100/6658 (62%)]\tLoss: 0.404876\n",
      "Train Epoch: 42 [4200/6658 (63%)]\tLoss: 0.171416\n",
      "Train Epoch: 42 [4300/6658 (65%)]\tLoss: 0.717851\n",
      "Train Epoch: 42 [4400/6658 (66%)]\tLoss: 0.093392\n",
      "Train Epoch: 42 [4500/6658 (68%)]\tLoss: 0.377806\n",
      "Train Epoch: 42 [4600/6658 (69%)]\tLoss: 2.755059\n",
      "Train Epoch: 42 [4700/6658 (71%)]\tLoss: 0.277671\n",
      "Train Epoch: 42 [4800/6658 (72%)]\tLoss: 0.136079\n",
      "Train Epoch: 42 [4900/6658 (74%)]\tLoss: 0.014012\n",
      "Train Epoch: 42 [5000/6658 (75%)]\tLoss: 9.143078\n",
      "Train Epoch: 42 [5100/6658 (77%)]\tLoss: 0.129347\n",
      "Train Epoch: 42 [5200/6658 (78%)]\tLoss: 0.447702\n",
      "Train Epoch: 42 [5300/6658 (80%)]\tLoss: 0.318043\n",
      "Train Epoch: 42 [5400/6658 (81%)]\tLoss: 0.316059\n",
      "Train Epoch: 42 [5500/6658 (83%)]\tLoss: 2.129699\n",
      "Train Epoch: 42 [5600/6658 (84%)]\tLoss: 0.677995\n",
      "Train Epoch: 42 [5700/6658 (86%)]\tLoss: 1.864973\n",
      "Train Epoch: 42 [5800/6658 (87%)]\tLoss: 0.023230\n",
      "Train Epoch: 42 [5900/6658 (89%)]\tLoss: 0.226310\n",
      "Train Epoch: 42 [6000/6658 (90%)]\tLoss: 0.051143\n",
      "Train Epoch: 42 [6100/6658 (92%)]\tLoss: 0.560808\n",
      "Train Epoch: 42 [6200/6658 (93%)]\tLoss: 0.054346\n",
      "Train Epoch: 42 [6300/6658 (95%)]\tLoss: 0.097433\n",
      "Train Epoch: 42 [6400/6658 (96%)]\tLoss: 0.000843\n",
      "Train Epoch: 42 [6500/6658 (98%)]\tLoss: 0.783337\n",
      "Train Epoch: 42 [6600/6658 (99%)]\tLoss: 0.022818\n",
      "train loss average =  0.7410020739798664\n",
      "\n",
      "Test set: Average loss: 0.7096\n",
      "\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0976, 6.0237, 5.9473, 5.9330, 6.0586, 6.0302], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 43 [0/6658 (0%)]\tLoss: 0.035943\n",
      "Train Epoch: 43 [100/6658 (2%)]\tLoss: 1.607792\n",
      "Train Epoch: 43 [200/6658 (3%)]\tLoss: 0.251454\n",
      "Train Epoch: 43 [300/6658 (5%)]\tLoss: 0.058292\n",
      "Train Epoch: 43 [400/6658 (6%)]\tLoss: 3.750677\n",
      "Train Epoch: 43 [500/6658 (8%)]\tLoss: 0.377679\n",
      "Train Epoch: 43 [600/6658 (9%)]\tLoss: 0.350646\n",
      "Train Epoch: 43 [700/6658 (11%)]\tLoss: 1.284012\n",
      "Train Epoch: 43 [800/6658 (12%)]\tLoss: 0.004116\n",
      "Train Epoch: 43 [900/6658 (14%)]\tLoss: 0.166890\n",
      "Train Epoch: 43 [1000/6658 (15%)]\tLoss: 0.168685\n",
      "Train Epoch: 43 [1100/6658 (17%)]\tLoss: 0.656739\n",
      "Train Epoch: 43 [1200/6658 (18%)]\tLoss: 0.453748\n",
      "Train Epoch: 43 [1300/6658 (20%)]\tLoss: 0.618431\n",
      "Train Epoch: 43 [1400/6658 (21%)]\tLoss: 0.042458\n",
      "Train Epoch: 43 [1500/6658 (23%)]\tLoss: 0.196526\n",
      "Train Epoch: 43 [1600/6658 (24%)]\tLoss: 0.021395\n",
      "Train Epoch: 43 [1700/6658 (26%)]\tLoss: 0.119870\n",
      "Train Epoch: 43 [1800/6658 (27%)]\tLoss: 0.727397\n",
      "Train Epoch: 43 [1900/6658 (29%)]\tLoss: 1.650153\n",
      "Train Epoch: 43 [2000/6658 (30%)]\tLoss: 0.058882\n",
      "Train Epoch: 43 [2100/6658 (32%)]\tLoss: 2.731547\n",
      "Train Epoch: 43 [2200/6658 (33%)]\tLoss: 0.019363\n",
      "Train Epoch: 43 [2300/6658 (35%)]\tLoss: 0.298569\n",
      "Train Epoch: 43 [2400/6658 (36%)]\tLoss: 0.158176\n",
      "Train Epoch: 43 [2500/6658 (38%)]\tLoss: 0.652039\n",
      "Train Epoch: 43 [2600/6658 (39%)]\tLoss: 2.376276\n",
      "Train Epoch: 43 [2700/6658 (41%)]\tLoss: 0.125165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 43 [2800/6658 (42%)]\tLoss: 0.257714\n",
      "Train Epoch: 43 [2900/6658 (44%)]\tLoss: 0.159515\n",
      "Train Epoch: 43 [3000/6658 (45%)]\tLoss: 0.680801\n",
      "Train Epoch: 43 [3100/6658 (47%)]\tLoss: 0.218160\n",
      "Train Epoch: 43 [3200/6658 (48%)]\tLoss: 0.168850\n",
      "Train Epoch: 43 [3300/6658 (50%)]\tLoss: 0.059530\n",
      "Train Epoch: 43 [3400/6658 (51%)]\tLoss: 0.139665\n",
      "Train Epoch: 43 [3500/6658 (53%)]\tLoss: 0.106321\n",
      "Train Epoch: 43 [3600/6658 (54%)]\tLoss: 0.000410\n",
      "Train Epoch: 43 [3700/6658 (56%)]\tLoss: 0.065340\n",
      "Train Epoch: 43 [3800/6658 (57%)]\tLoss: 0.919347\n",
      "Train Epoch: 43 [3900/6658 (59%)]\tLoss: 0.481519\n",
      "Train Epoch: 43 [4000/6658 (60%)]\tLoss: 0.000643\n",
      "Train Epoch: 43 [4100/6658 (62%)]\tLoss: 0.585329\n",
      "Train Epoch: 43 [4200/6658 (63%)]\tLoss: 0.034922\n",
      "Train Epoch: 43 [4300/6658 (65%)]\tLoss: 0.524143\n",
      "Train Epoch: 43 [4400/6658 (66%)]\tLoss: 1.463127\n",
      "Train Epoch: 43 [4500/6658 (68%)]\tLoss: 9.096704\n",
      "Train Epoch: 43 [4600/6658 (69%)]\tLoss: 2.342295\n",
      "Train Epoch: 43 [4700/6658 (71%)]\tLoss: 0.618803\n",
      "Train Epoch: 43 [4800/6658 (72%)]\tLoss: 7.923725\n",
      "Train Epoch: 43 [4900/6658 (74%)]\tLoss: 0.004742\n",
      "Train Epoch: 43 [5000/6658 (75%)]\tLoss: 4.573610\n",
      "Train Epoch: 43 [5100/6658 (77%)]\tLoss: 0.606711\n",
      "Train Epoch: 43 [5200/6658 (78%)]\tLoss: 0.000034\n",
      "Train Epoch: 43 [5300/6658 (80%)]\tLoss: 0.036304\n",
      "Train Epoch: 43 [5400/6658 (81%)]\tLoss: 0.741989\n",
      "Train Epoch: 43 [5500/6658 (83%)]\tLoss: 0.819335\n",
      "Train Epoch: 43 [5600/6658 (84%)]\tLoss: 0.007920\n",
      "Train Epoch: 43 [5700/6658 (86%)]\tLoss: 2.041072\n",
      "Train Epoch: 43 [5800/6658 (87%)]\tLoss: 0.179425\n",
      "Train Epoch: 43 [5900/6658 (89%)]\tLoss: 6.909364\n",
      "Train Epoch: 43 [6000/6658 (90%)]\tLoss: 0.201044\n",
      "Train Epoch: 43 [6100/6658 (92%)]\tLoss: 0.704288\n",
      "Train Epoch: 43 [6200/6658 (93%)]\tLoss: 0.746958\n",
      "Train Epoch: 43 [6300/6658 (95%)]\tLoss: 0.606086\n",
      "Train Epoch: 43 [6400/6658 (96%)]\tLoss: 15.138948\n",
      "Train Epoch: 43 [6500/6658 (98%)]\tLoss: 0.139210\n",
      "Train Epoch: 43 [6600/6658 (99%)]\tLoss: 0.109099\n",
      "train loss average =  0.7355215566556893\n",
      "\n",
      "Test set: Average loss: 0.7067\n",
      "\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.0997, 6.0246, 5.9457, 5.9322, 6.0603, 6.0310], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 44 [0/6658 (0%)]\tLoss: 0.490546\n",
      "Train Epoch: 44 [100/6658 (2%)]\tLoss: 0.805002\n",
      "Train Epoch: 44 [200/6658 (3%)]\tLoss: 0.148826\n",
      "Train Epoch: 44 [300/6658 (5%)]\tLoss: 0.677825\n",
      "Train Epoch: 44 [400/6658 (6%)]\tLoss: 0.107658\n",
      "Train Epoch: 44 [500/6658 (8%)]\tLoss: 2.087467\n",
      "Train Epoch: 44 [600/6658 (9%)]\tLoss: 0.242720\n",
      "Train Epoch: 44 [700/6658 (11%)]\tLoss: 0.184938\n",
      "Train Epoch: 44 [800/6658 (12%)]\tLoss: 0.431341\n",
      "Train Epoch: 44 [900/6658 (14%)]\tLoss: 1.112284\n",
      "Train Epoch: 44 [1000/6658 (15%)]\tLoss: 1.074983\n",
      "Train Epoch: 44 [1100/6658 (17%)]\tLoss: 0.020095\n",
      "Train Epoch: 44 [1200/6658 (18%)]\tLoss: 0.581281\n",
      "Train Epoch: 44 [1300/6658 (20%)]\tLoss: 0.545594\n",
      "Train Epoch: 44 [1400/6658 (21%)]\tLoss: 2.632773\n",
      "Train Epoch: 44 [1500/6658 (23%)]\tLoss: 0.117537\n",
      "Train Epoch: 44 [1600/6658 (24%)]\tLoss: 0.575811\n",
      "Train Epoch: 44 [1700/6658 (26%)]\tLoss: 1.190855\n",
      "Train Epoch: 44 [1800/6658 (27%)]\tLoss: 2.130758\n",
      "Train Epoch: 44 [1900/6658 (29%)]\tLoss: 0.058835\n",
      "Train Epoch: 44 [2000/6658 (30%)]\tLoss: 0.425423\n",
      "Train Epoch: 44 [2100/6658 (32%)]\tLoss: 1.041819\n",
      "Train Epoch: 44 [2200/6658 (33%)]\tLoss: 2.187799\n",
      "Train Epoch: 44 [2300/6658 (35%)]\tLoss: 0.049396\n",
      "Train Epoch: 44 [2400/6658 (36%)]\tLoss: 0.127240\n",
      "Train Epoch: 44 [2500/6658 (38%)]\tLoss: 0.774316\n",
      "Train Epoch: 44 [2600/6658 (39%)]\tLoss: 0.288175\n",
      "Train Epoch: 44 [2700/6658 (41%)]\tLoss: 0.149013\n",
      "Train Epoch: 44 [2800/6658 (42%)]\tLoss: 0.000075\n",
      "Train Epoch: 44 [2900/6658 (44%)]\tLoss: 0.119555\n",
      "Train Epoch: 44 [3000/6658 (45%)]\tLoss: 0.287000\n",
      "Train Epoch: 44 [3100/6658 (47%)]\tLoss: 0.016068\n",
      "Train Epoch: 44 [3200/6658 (48%)]\tLoss: 0.372353\n",
      "Train Epoch: 44 [3300/6658 (50%)]\tLoss: 0.135555\n",
      "Train Epoch: 44 [3400/6658 (51%)]\tLoss: 0.084587\n",
      "Train Epoch: 44 [3500/6658 (53%)]\tLoss: 0.182412\n",
      "Train Epoch: 44 [3600/6658 (54%)]\tLoss: 0.628152\n",
      "Train Epoch: 44 [3700/6658 (56%)]\tLoss: 0.661817\n",
      "Train Epoch: 44 [3800/6658 (57%)]\tLoss: 0.009127\n",
      "Train Epoch: 44 [3900/6658 (59%)]\tLoss: 0.039940\n",
      "Train Epoch: 44 [4000/6658 (60%)]\tLoss: 1.663440\n",
      "Train Epoch: 44 [4100/6658 (62%)]\tLoss: 0.775706\n",
      "Train Epoch: 44 [4200/6658 (63%)]\tLoss: 0.075252\n",
      "Train Epoch: 44 [4300/6658 (65%)]\tLoss: 0.721905\n",
      "Train Epoch: 44 [4400/6658 (66%)]\tLoss: 0.136870\n",
      "Train Epoch: 44 [4500/6658 (68%)]\tLoss: 0.146367\n",
      "Train Epoch: 44 [4600/6658 (69%)]\tLoss: 0.008837\n",
      "Train Epoch: 44 [4700/6658 (71%)]\tLoss: 0.696042\n",
      "Train Epoch: 44 [4800/6658 (72%)]\tLoss: 0.007716\n",
      "Train Epoch: 44 [4900/6658 (74%)]\tLoss: 0.006414\n",
      "Train Epoch: 44 [5000/6658 (75%)]\tLoss: 1.367834\n",
      "Train Epoch: 44 [5100/6658 (77%)]\tLoss: 0.002095\n",
      "Train Epoch: 44 [5200/6658 (78%)]\tLoss: 0.287679\n",
      "Train Epoch: 44 [5300/6658 (80%)]\tLoss: 0.120014\n",
      "Train Epoch: 44 [5400/6658 (81%)]\tLoss: 0.899155\n",
      "Train Epoch: 44 [5500/6658 (83%)]\tLoss: 0.022722\n",
      "Train Epoch: 44 [5600/6658 (84%)]\tLoss: 0.970005\n",
      "Train Epoch: 44 [5700/6658 (86%)]\tLoss: 0.050965\n",
      "Train Epoch: 44 [5800/6658 (87%)]\tLoss: 0.801104\n",
      "Train Epoch: 44 [5900/6658 (89%)]\tLoss: 0.689146\n",
      "Train Epoch: 44 [6000/6658 (90%)]\tLoss: 0.096920\n",
      "Train Epoch: 44 [6100/6658 (92%)]\tLoss: 0.287619\n",
      "Train Epoch: 44 [6200/6658 (93%)]\tLoss: 0.895609\n",
      "Train Epoch: 44 [6300/6658 (95%)]\tLoss: 0.541169\n",
      "Train Epoch: 44 [6400/6658 (96%)]\tLoss: 0.000017\n",
      "Train Epoch: 44 [6500/6658 (98%)]\tLoss: 0.030347\n",
      "Train Epoch: 44 [6600/6658 (99%)]\tLoss: 0.047004\n",
      "train loss average =  0.7353988851921452\n",
      "\n",
      "Test set: Average loss: 0.7312\n",
      "\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1019, 6.0254, 5.9443, 5.9320, 6.0613, 6.0318], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 45 [0/6658 (0%)]\tLoss: 0.310401\n",
      "Train Epoch: 45 [100/6658 (2%)]\tLoss: 0.025935\n",
      "Train Epoch: 45 [200/6658 (3%)]\tLoss: 0.070177\n",
      "Train Epoch: 45 [300/6658 (5%)]\tLoss: 0.200093\n",
      "Train Epoch: 45 [400/6658 (6%)]\tLoss: 0.014823\n",
      "Train Epoch: 45 [500/6658 (8%)]\tLoss: 0.896003\n",
      "Train Epoch: 45 [600/6658 (9%)]\tLoss: 0.661756\n",
      "Train Epoch: 45 [700/6658 (11%)]\tLoss: 0.011887\n",
      "Train Epoch: 45 [800/6658 (12%)]\tLoss: 0.007805\n",
      "Train Epoch: 45 [900/6658 (14%)]\tLoss: 0.647656\n",
      "Train Epoch: 45 [1000/6658 (15%)]\tLoss: 0.383060\n",
      "Train Epoch: 45 [1100/6658 (17%)]\tLoss: 0.322632\n",
      "Train Epoch: 45 [1200/6658 (18%)]\tLoss: 0.119266\n",
      "Train Epoch: 45 [1300/6658 (20%)]\tLoss: 0.020476\n",
      "Train Epoch: 45 [1400/6658 (21%)]\tLoss: 0.456082\n",
      "Train Epoch: 45 [1500/6658 (23%)]\tLoss: 0.951496\n",
      "Train Epoch: 45 [1600/6658 (24%)]\tLoss: 0.741027\n",
      "Train Epoch: 45 [1700/6658 (26%)]\tLoss: 0.303413\n",
      "Train Epoch: 45 [1800/6658 (27%)]\tLoss: 0.114933\n",
      "Train Epoch: 45 [1900/6658 (29%)]\tLoss: 0.005639\n",
      "Train Epoch: 45 [2000/6658 (30%)]\tLoss: 0.075994\n",
      "Train Epoch: 45 [2100/6658 (32%)]\tLoss: 0.374708\n",
      "Train Epoch: 45 [2200/6658 (33%)]\tLoss: 0.092186\n",
      "Train Epoch: 45 [2300/6658 (35%)]\tLoss: 0.273881\n",
      "Train Epoch: 45 [2400/6658 (36%)]\tLoss: 0.060707\n",
      "Train Epoch: 45 [2500/6658 (38%)]\tLoss: 0.807523\n",
      "Train Epoch: 45 [2600/6658 (39%)]\tLoss: 0.009216\n",
      "Train Epoch: 45 [2700/6658 (41%)]\tLoss: 0.213591\n",
      "Train Epoch: 45 [2800/6658 (42%)]\tLoss: 1.228471\n",
      "Train Epoch: 45 [2900/6658 (44%)]\tLoss: 2.435870\n",
      "Train Epoch: 45 [3000/6658 (45%)]\tLoss: 0.125414\n",
      "Train Epoch: 45 [3100/6658 (47%)]\tLoss: 0.035933\n",
      "Train Epoch: 45 [3200/6658 (48%)]\tLoss: 0.043748\n",
      "Train Epoch: 45 [3300/6658 (50%)]\tLoss: 0.164995\n",
      "Train Epoch: 45 [3400/6658 (51%)]\tLoss: 1.851945\n",
      "Train Epoch: 45 [3500/6658 (53%)]\tLoss: 0.636254\n",
      "Train Epoch: 45 [3600/6658 (54%)]\tLoss: 1.022553\n",
      "Train Epoch: 45 [3700/6658 (56%)]\tLoss: 0.015934\n",
      "Train Epoch: 45 [3800/6658 (57%)]\tLoss: 0.275234\n",
      "Train Epoch: 45 [3900/6658 (59%)]\tLoss: 3.149633\n",
      "Train Epoch: 45 [4000/6658 (60%)]\tLoss: 0.080626\n",
      "Train Epoch: 45 [4100/6658 (62%)]\tLoss: 0.413754\n",
      "Train Epoch: 45 [4200/6658 (63%)]\tLoss: 5.190742\n",
      "Train Epoch: 45 [4300/6658 (65%)]\tLoss: 0.152175\n",
      "Train Epoch: 45 [4400/6658 (66%)]\tLoss: 0.079695\n",
      "Train Epoch: 45 [4500/6658 (68%)]\tLoss: 0.613237\n",
      "Train Epoch: 45 [4600/6658 (69%)]\tLoss: 0.015450\n",
      "Train Epoch: 45 [4700/6658 (71%)]\tLoss: 0.056569\n",
      "Train Epoch: 45 [4800/6658 (72%)]\tLoss: 0.017855\n",
      "Train Epoch: 45 [4900/6658 (74%)]\tLoss: 0.101806\n",
      "Train Epoch: 45 [5000/6658 (75%)]\tLoss: 0.787233\n",
      "Train Epoch: 45 [5100/6658 (77%)]\tLoss: 4.275722\n",
      "Train Epoch: 45 [5200/6658 (78%)]\tLoss: 0.479150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 45 [5300/6658 (80%)]\tLoss: 0.655718\n",
      "Train Epoch: 45 [5400/6658 (81%)]\tLoss: 0.229076\n",
      "Train Epoch: 45 [5500/6658 (83%)]\tLoss: 0.377466\n",
      "Train Epoch: 45 [5600/6658 (84%)]\tLoss: 0.053977\n",
      "Train Epoch: 45 [5700/6658 (86%)]\tLoss: 0.454224\n",
      "Train Epoch: 45 [5800/6658 (87%)]\tLoss: 0.147477\n",
      "Train Epoch: 45 [5900/6658 (89%)]\tLoss: 0.068288\n",
      "Train Epoch: 45 [6000/6658 (90%)]\tLoss: 0.757116\n",
      "Train Epoch: 45 [6100/6658 (92%)]\tLoss: 0.855848\n",
      "Train Epoch: 45 [6200/6658 (93%)]\tLoss: 0.038788\n",
      "Train Epoch: 45 [6300/6658 (95%)]\tLoss: 0.123919\n",
      "Train Epoch: 45 [6400/6658 (96%)]\tLoss: 0.152886\n",
      "Train Epoch: 45 [6500/6658 (98%)]\tLoss: 0.404437\n",
      "Train Epoch: 45 [6600/6658 (99%)]\tLoss: 0.024519\n",
      "train loss average =  0.7355855033494769\n",
      "\n",
      "Test set: Average loss: 0.7182\n",
      "\n",
      "EarlyStopping counter: 9 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1045, 6.0267, 5.9425, 5.9313, 6.0626, 6.0319], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 46 [0/6658 (0%)]\tLoss: 1.099632\n",
      "Train Epoch: 46 [100/6658 (2%)]\tLoss: 0.786212\n",
      "Train Epoch: 46 [200/6658 (3%)]\tLoss: 0.651495\n",
      "Train Epoch: 46 [300/6658 (5%)]\tLoss: 0.064078\n",
      "Train Epoch: 46 [400/6658 (6%)]\tLoss: 0.341361\n",
      "Train Epoch: 46 [500/6658 (8%)]\tLoss: 0.296932\n",
      "Train Epoch: 46 [600/6658 (9%)]\tLoss: 0.097815\n",
      "Train Epoch: 46 [700/6658 (11%)]\tLoss: 0.111580\n",
      "Train Epoch: 46 [800/6658 (12%)]\tLoss: 0.035957\n",
      "Train Epoch: 46 [900/6658 (14%)]\tLoss: 0.305842\n",
      "Train Epoch: 46 [1000/6658 (15%)]\tLoss: 0.362318\n",
      "Train Epoch: 46 [1100/6658 (17%)]\tLoss: 1.016091\n",
      "Train Epoch: 46 [1200/6658 (18%)]\tLoss: 0.001750\n",
      "Train Epoch: 46 [1300/6658 (20%)]\tLoss: 0.397178\n",
      "Train Epoch: 46 [1400/6658 (21%)]\tLoss: 0.573686\n",
      "Train Epoch: 46 [1500/6658 (23%)]\tLoss: 0.157338\n",
      "Train Epoch: 46 [1600/6658 (24%)]\tLoss: 0.503388\n",
      "Train Epoch: 46 [1700/6658 (26%)]\tLoss: 1.008372\n",
      "Train Epoch: 46 [1800/6658 (27%)]\tLoss: 0.412128\n",
      "Train Epoch: 46 [1900/6658 (29%)]\tLoss: 2.565517\n",
      "Train Epoch: 46 [2000/6658 (30%)]\tLoss: 0.961166\n",
      "Train Epoch: 46 [2100/6658 (32%)]\tLoss: 0.202937\n",
      "Train Epoch: 46 [2200/6658 (33%)]\tLoss: 1.650133\n",
      "Train Epoch: 46 [2300/6658 (35%)]\tLoss: 4.541603\n",
      "Train Epoch: 46 [2400/6658 (36%)]\tLoss: 0.011748\n",
      "Train Epoch: 46 [2500/6658 (38%)]\tLoss: 0.078035\n",
      "Train Epoch: 46 [2600/6658 (39%)]\tLoss: 0.205856\n",
      "Train Epoch: 46 [2700/6658 (41%)]\tLoss: 0.141836\n",
      "Train Epoch: 46 [2800/6658 (42%)]\tLoss: 0.026144\n",
      "Train Epoch: 46 [2900/6658 (44%)]\tLoss: 0.787836\n",
      "Train Epoch: 46 [3000/6658 (45%)]\tLoss: 0.002044\n",
      "Train Epoch: 46 [3100/6658 (47%)]\tLoss: 0.000206\n",
      "Train Epoch: 46 [3200/6658 (48%)]\tLoss: 0.058846\n",
      "Train Epoch: 46 [3300/6658 (50%)]\tLoss: 1.381471\n",
      "Train Epoch: 46 [3400/6658 (51%)]\tLoss: 0.805144\n",
      "Train Epoch: 46 [3500/6658 (53%)]\tLoss: 0.106283\n",
      "Train Epoch: 46 [3600/6658 (54%)]\tLoss: 3.973220\n",
      "Train Epoch: 46 [3700/6658 (56%)]\tLoss: 0.280307\n",
      "Train Epoch: 46 [3800/6658 (57%)]\tLoss: 0.000949\n",
      "Train Epoch: 46 [3900/6658 (59%)]\tLoss: 0.358696\n",
      "Train Epoch: 46 [4000/6658 (60%)]\tLoss: 0.518716\n",
      "Train Epoch: 46 [4100/6658 (62%)]\tLoss: 0.598544\n",
      "Train Epoch: 46 [4200/6658 (63%)]\tLoss: 0.002193\n",
      "Train Epoch: 46 [4300/6658 (65%)]\tLoss: 0.437599\n",
      "Train Epoch: 46 [4400/6658 (66%)]\tLoss: 1.653029\n",
      "Train Epoch: 46 [4500/6658 (68%)]\tLoss: 0.741366\n",
      "Train Epoch: 46 [4600/6658 (69%)]\tLoss: 14.487402\n",
      "Train Epoch: 46 [4700/6658 (71%)]\tLoss: 0.134890\n",
      "Train Epoch: 46 [4800/6658 (72%)]\tLoss: 0.009318\n",
      "Train Epoch: 46 [4900/6658 (74%)]\tLoss: 2.234164\n",
      "Train Epoch: 46 [5000/6658 (75%)]\tLoss: 0.059517\n",
      "Train Epoch: 46 [5100/6658 (77%)]\tLoss: 3.172730\n",
      "Train Epoch: 46 [5200/6658 (78%)]\tLoss: 0.723977\n",
      "Train Epoch: 46 [5300/6658 (80%)]\tLoss: 0.825439\n",
      "Train Epoch: 46 [5400/6658 (81%)]\tLoss: 0.000097\n",
      "Train Epoch: 46 [5500/6658 (83%)]\tLoss: 0.195343\n",
      "Train Epoch: 46 [5600/6658 (84%)]\tLoss: 3.508196\n",
      "Train Epoch: 46 [5700/6658 (86%)]\tLoss: 0.244379\n",
      "Train Epoch: 46 [5800/6658 (87%)]\tLoss: 0.812087\n",
      "Train Epoch: 46 [5900/6658 (89%)]\tLoss: 0.048079\n",
      "Train Epoch: 46 [6000/6658 (90%)]\tLoss: 0.009902\n",
      "Train Epoch: 46 [6100/6658 (92%)]\tLoss: 0.114350\n",
      "Train Epoch: 46 [6200/6658 (93%)]\tLoss: 1.177879\n",
      "Train Epoch: 46 [6300/6658 (95%)]\tLoss: 1.338925\n",
      "Train Epoch: 46 [6400/6658 (96%)]\tLoss: 0.137910\n",
      "Train Epoch: 46 [6500/6658 (98%)]\tLoss: 0.006160\n",
      "Train Epoch: 46 [6600/6658 (99%)]\tLoss: 3.754093\n",
      "train loss average =  0.7371467400324296\n",
      "\n",
      "Test set: Average loss: 0.7102\n",
      "\n",
      "EarlyStopping counter: 10 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1065, 6.0281, 5.9411, 5.9298, 6.0646, 6.0324], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 47 [0/6658 (0%)]\tLoss: 0.027255\n",
      "Train Epoch: 47 [100/6658 (2%)]\tLoss: 0.283509\n",
      "Train Epoch: 47 [200/6658 (3%)]\tLoss: 0.200672\n",
      "Train Epoch: 47 [300/6658 (5%)]\tLoss: 0.386653\n",
      "Train Epoch: 47 [400/6658 (6%)]\tLoss: 0.011900\n",
      "Train Epoch: 47 [500/6658 (8%)]\tLoss: 0.499706\n",
      "Train Epoch: 47 [600/6658 (9%)]\tLoss: 7.449646\n",
      "Train Epoch: 47 [700/6658 (11%)]\tLoss: 0.114315\n",
      "Train Epoch: 47 [800/6658 (12%)]\tLoss: 0.691470\n",
      "Train Epoch: 47 [900/6658 (14%)]\tLoss: 0.023103\n",
      "Train Epoch: 47 [1000/6658 (15%)]\tLoss: 1.222320\n",
      "Train Epoch: 47 [1100/6658 (17%)]\tLoss: 0.937307\n",
      "Train Epoch: 47 [1200/6658 (18%)]\tLoss: 3.221645\n",
      "Train Epoch: 47 [1300/6658 (20%)]\tLoss: 0.436386\n",
      "Train Epoch: 47 [1400/6658 (21%)]\tLoss: 0.110809\n",
      "Train Epoch: 47 [1500/6658 (23%)]\tLoss: 2.583555\n",
      "Train Epoch: 47 [1600/6658 (24%)]\tLoss: 0.090966\n",
      "Train Epoch: 47 [1700/6658 (26%)]\tLoss: 0.215425\n",
      "Train Epoch: 47 [1800/6658 (27%)]\tLoss: 0.187174\n",
      "Train Epoch: 47 [1900/6658 (29%)]\tLoss: 0.134188\n",
      "Train Epoch: 47 [2000/6658 (30%)]\tLoss: 0.493378\n",
      "Train Epoch: 47 [2100/6658 (32%)]\tLoss: 0.275886\n",
      "Train Epoch: 47 [2200/6658 (33%)]\tLoss: 0.040591\n",
      "Train Epoch: 47 [2300/6658 (35%)]\tLoss: 0.018280\n",
      "Train Epoch: 47 [2400/6658 (36%)]\tLoss: 0.072879\n",
      "Train Epoch: 47 [2500/6658 (38%)]\tLoss: 0.034994\n",
      "Train Epoch: 47 [2600/6658 (39%)]\tLoss: 0.140927\n",
      "Train Epoch: 47 [2700/6658 (41%)]\tLoss: 0.725130\n",
      "Train Epoch: 47 [2800/6658 (42%)]\tLoss: 0.009987\n",
      "Train Epoch: 47 [2900/6658 (44%)]\tLoss: 0.932769\n",
      "Train Epoch: 47 [3000/6658 (45%)]\tLoss: 0.344936\n",
      "Train Epoch: 47 [3100/6658 (47%)]\tLoss: 0.007778\n",
      "Train Epoch: 47 [3200/6658 (48%)]\tLoss: 0.702255\n",
      "Train Epoch: 47 [3300/6658 (50%)]\tLoss: 0.157330\n",
      "Train Epoch: 47 [3400/6658 (51%)]\tLoss: 0.992355\n",
      "Train Epoch: 47 [3500/6658 (53%)]\tLoss: 0.190774\n",
      "Train Epoch: 47 [3600/6658 (54%)]\tLoss: 0.135587\n",
      "Train Epoch: 47 [3700/6658 (56%)]\tLoss: 1.162981\n",
      "Train Epoch: 47 [3800/6658 (57%)]\tLoss: 0.617982\n",
      "Train Epoch: 47 [3900/6658 (59%)]\tLoss: 0.175352\n",
      "Train Epoch: 47 [4000/6658 (60%)]\tLoss: 0.912553\n",
      "Train Epoch: 47 [4100/6658 (62%)]\tLoss: 0.305790\n",
      "Train Epoch: 47 [4200/6658 (63%)]\tLoss: 0.178814\n",
      "Train Epoch: 47 [4300/6658 (65%)]\tLoss: 0.159480\n",
      "Train Epoch: 47 [4400/6658 (66%)]\tLoss: 0.892383\n",
      "Train Epoch: 47 [4500/6658 (68%)]\tLoss: 0.502301\n",
      "Train Epoch: 47 [4600/6658 (69%)]\tLoss: 0.052279\n",
      "Train Epoch: 47 [4700/6658 (71%)]\tLoss: 0.196873\n",
      "Train Epoch: 47 [4800/6658 (72%)]\tLoss: 0.729425\n",
      "Train Epoch: 47 [4900/6658 (74%)]\tLoss: 0.534649\n",
      "Train Epoch: 47 [5000/6658 (75%)]\tLoss: 0.434896\n",
      "Train Epoch: 47 [5100/6658 (77%)]\tLoss: 0.232440\n",
      "Train Epoch: 47 [5200/6658 (78%)]\tLoss: 11.331809\n",
      "Train Epoch: 47 [5300/6658 (80%)]\tLoss: 0.701258\n",
      "Train Epoch: 47 [5400/6658 (81%)]\tLoss: 0.483678\n",
      "Train Epoch: 47 [5500/6658 (83%)]\tLoss: 0.008759\n",
      "Train Epoch: 47 [5600/6658 (84%)]\tLoss: 0.038980\n",
      "Train Epoch: 47 [5700/6658 (86%)]\tLoss: 0.018810\n",
      "Train Epoch: 47 [5800/6658 (87%)]\tLoss: 0.119384\n",
      "Train Epoch: 47 [5900/6658 (89%)]\tLoss: 1.122724\n",
      "Train Epoch: 47 [6000/6658 (90%)]\tLoss: 0.092988\n",
      "Train Epoch: 47 [6100/6658 (92%)]\tLoss: 0.064406\n",
      "Train Epoch: 47 [6200/6658 (93%)]\tLoss: 0.010533\n",
      "Train Epoch: 47 [6300/6658 (95%)]\tLoss: 0.465105\n",
      "Train Epoch: 47 [6400/6658 (96%)]\tLoss: 0.490260\n",
      "Train Epoch: 47 [6500/6658 (98%)]\tLoss: 0.101372\n",
      "Train Epoch: 47 [6600/6658 (99%)]\tLoss: 0.524561\n",
      "train loss average =  0.7327892726641068\n",
      "\n",
      "Test set: Average loss: 0.7318\n",
      "\n",
      "EarlyStopping counter: 11 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1091, 6.0288, 5.9403, 5.9292, 6.0656, 6.0325], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 48 [0/6658 (0%)]\tLoss: 0.596958\n",
      "Train Epoch: 48 [100/6658 (2%)]\tLoss: 0.710723\n",
      "Train Epoch: 48 [200/6658 (3%)]\tLoss: 0.029314\n",
      "Train Epoch: 48 [300/6658 (5%)]\tLoss: 0.605056\n",
      "Train Epoch: 48 [400/6658 (6%)]\tLoss: 0.849813\n",
      "Train Epoch: 48 [500/6658 (8%)]\tLoss: 1.726899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 48 [600/6658 (9%)]\tLoss: 0.084527\n",
      "Train Epoch: 48 [700/6658 (11%)]\tLoss: 0.005114\n",
      "Train Epoch: 48 [800/6658 (12%)]\tLoss: 0.011055\n",
      "Train Epoch: 48 [900/6658 (14%)]\tLoss: 0.493490\n",
      "Train Epoch: 48 [1000/6658 (15%)]\tLoss: 0.202840\n",
      "Train Epoch: 48 [1100/6658 (17%)]\tLoss: 1.127893\n",
      "Train Epoch: 48 [1200/6658 (18%)]\tLoss: 0.264243\n",
      "Train Epoch: 48 [1300/6658 (20%)]\tLoss: 0.061239\n",
      "Train Epoch: 48 [1400/6658 (21%)]\tLoss: 0.171583\n",
      "Train Epoch: 48 [1500/6658 (23%)]\tLoss: 0.105355\n",
      "Train Epoch: 48 [1600/6658 (24%)]\tLoss: 0.171686\n",
      "Train Epoch: 48 [1700/6658 (26%)]\tLoss: 0.237960\n",
      "Train Epoch: 48 [1800/6658 (27%)]\tLoss: 0.161240\n",
      "Train Epoch: 48 [1900/6658 (29%)]\tLoss: 1.000093\n",
      "Train Epoch: 48 [2000/6658 (30%)]\tLoss: 2.434022\n",
      "Train Epoch: 48 [2100/6658 (32%)]\tLoss: 0.041438\n",
      "Train Epoch: 48 [2200/6658 (33%)]\tLoss: 0.380574\n",
      "Train Epoch: 48 [2300/6658 (35%)]\tLoss: 1.509461\n",
      "Train Epoch: 48 [2400/6658 (36%)]\tLoss: 3.083824\n",
      "Train Epoch: 48 [2500/6658 (38%)]\tLoss: 3.513683\n",
      "Train Epoch: 48 [2600/6658 (39%)]\tLoss: 0.534527\n",
      "Train Epoch: 48 [2700/6658 (41%)]\tLoss: 0.387571\n",
      "Train Epoch: 48 [2800/6658 (42%)]\tLoss: 0.180071\n",
      "Train Epoch: 48 [2900/6658 (44%)]\tLoss: 0.001296\n",
      "Train Epoch: 48 [3000/6658 (45%)]\tLoss: 3.393342\n",
      "Train Epoch: 48 [3100/6658 (47%)]\tLoss: 0.006328\n",
      "Train Epoch: 48 [3200/6658 (48%)]\tLoss: 0.033323\n",
      "Train Epoch: 48 [3300/6658 (50%)]\tLoss: 0.143584\n",
      "Train Epoch: 48 [3400/6658 (51%)]\tLoss: 0.000195\n",
      "Train Epoch: 48 [3500/6658 (53%)]\tLoss: 0.594764\n",
      "Train Epoch: 48 [3600/6658 (54%)]\tLoss: 0.107198\n",
      "Train Epoch: 48 [3700/6658 (56%)]\tLoss: 0.148354\n",
      "Train Epoch: 48 [3800/6658 (57%)]\tLoss: 0.031259\n",
      "Train Epoch: 48 [3900/6658 (59%)]\tLoss: 0.005551\n",
      "Train Epoch: 48 [4000/6658 (60%)]\tLoss: 0.301560\n",
      "Train Epoch: 48 [4100/6658 (62%)]\tLoss: 0.095927\n",
      "Train Epoch: 48 [4200/6658 (63%)]\tLoss: 4.391240\n",
      "Train Epoch: 48 [4300/6658 (65%)]\tLoss: 0.100126\n",
      "Train Epoch: 48 [4400/6658 (66%)]\tLoss: 0.828753\n",
      "Train Epoch: 48 [4500/6658 (68%)]\tLoss: 13.132159\n",
      "Train Epoch: 48 [4600/6658 (69%)]\tLoss: 0.324717\n",
      "Train Epoch: 48 [4700/6658 (71%)]\tLoss: 0.005174\n",
      "Train Epoch: 48 [4800/6658 (72%)]\tLoss: 0.115975\n",
      "Train Epoch: 48 [4900/6658 (74%)]\tLoss: 0.419048\n",
      "Train Epoch: 48 [5000/6658 (75%)]\tLoss: 0.154068\n",
      "Train Epoch: 48 [5100/6658 (77%)]\tLoss: 0.054978\n",
      "Train Epoch: 48 [5200/6658 (78%)]\tLoss: 0.714166\n",
      "Train Epoch: 48 [5300/6658 (80%)]\tLoss: 0.422231\n",
      "Train Epoch: 48 [5400/6658 (81%)]\tLoss: 0.006026\n",
      "Train Epoch: 48 [5500/6658 (83%)]\tLoss: 0.042695\n",
      "Train Epoch: 48 [5600/6658 (84%)]\tLoss: 1.334413\n",
      "Train Epoch: 48 [5700/6658 (86%)]\tLoss: 0.000009\n",
      "Train Epoch: 48 [5800/6658 (87%)]\tLoss: 2.481709\n",
      "Train Epoch: 48 [5900/6658 (89%)]\tLoss: 1.343226\n",
      "Train Epoch: 48 [6000/6658 (90%)]\tLoss: 0.373221\n",
      "Train Epoch: 48 [6100/6658 (92%)]\tLoss: 0.568792\n",
      "Train Epoch: 48 [6200/6658 (93%)]\tLoss: 0.302354\n",
      "Train Epoch: 48 [6300/6658 (95%)]\tLoss: 0.184168\n",
      "Train Epoch: 48 [6400/6658 (96%)]\tLoss: 0.033809\n",
      "Train Epoch: 48 [6500/6658 (98%)]\tLoss: 0.000040\n",
      "Train Epoch: 48 [6600/6658 (99%)]\tLoss: 0.019513\n",
      "train loss average =  0.7330692179447905\n",
      "\n",
      "Test set: Average loss: 0.7215\n",
      "\n",
      "EarlyStopping counter: 12 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1119, 6.0293, 5.9390, 5.9287, 6.0670, 6.0328], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 49 [0/6658 (0%)]\tLoss: 0.217313\n",
      "Train Epoch: 49 [100/6658 (2%)]\tLoss: 0.116321\n",
      "Train Epoch: 49 [200/6658 (3%)]\tLoss: 0.014137\n",
      "Train Epoch: 49 [300/6658 (5%)]\tLoss: 0.000479\n",
      "Train Epoch: 49 [400/6658 (6%)]\tLoss: 0.700868\n",
      "Train Epoch: 49 [500/6658 (8%)]\tLoss: 0.038150\n",
      "Train Epoch: 49 [600/6658 (9%)]\tLoss: 0.259713\n",
      "Train Epoch: 49 [700/6658 (11%)]\tLoss: 0.011776\n",
      "Train Epoch: 49 [800/6658 (12%)]\tLoss: 0.053442\n",
      "Train Epoch: 49 [900/6658 (14%)]\tLoss: 0.009776\n",
      "Train Epoch: 49 [1000/6658 (15%)]\tLoss: 0.748145\n",
      "Train Epoch: 49 [1100/6658 (17%)]\tLoss: 0.819175\n",
      "Train Epoch: 49 [1200/6658 (18%)]\tLoss: 2.066845\n",
      "Train Epoch: 49 [1300/6658 (20%)]\tLoss: 1.726675\n",
      "Train Epoch: 49 [1400/6658 (21%)]\tLoss: 0.089508\n",
      "Train Epoch: 49 [1500/6658 (23%)]\tLoss: 0.084024\n",
      "Train Epoch: 49 [1600/6658 (24%)]\tLoss: 0.144073\n",
      "Train Epoch: 49 [1700/6658 (26%)]\tLoss: 0.107030\n",
      "Train Epoch: 49 [1800/6658 (27%)]\tLoss: 0.098791\n",
      "Train Epoch: 49 [1900/6658 (29%)]\tLoss: 0.765833\n",
      "Train Epoch: 49 [2000/6658 (30%)]\tLoss: 0.491595\n",
      "Train Epoch: 49 [2100/6658 (32%)]\tLoss: 0.109914\n",
      "Train Epoch: 49 [2200/6658 (33%)]\tLoss: 1.485642\n",
      "Train Epoch: 49 [2300/6658 (35%)]\tLoss: 1.889819\n",
      "Train Epoch: 49 [2400/6658 (36%)]\tLoss: 1.269847\n",
      "Train Epoch: 49 [2500/6658 (38%)]\tLoss: 0.235462\n",
      "Train Epoch: 49 [2600/6658 (39%)]\tLoss: 0.017528\n",
      "Train Epoch: 49 [2700/6658 (41%)]\tLoss: 1.180065\n",
      "Train Epoch: 49 [2800/6658 (42%)]\tLoss: 0.819808\n",
      "Train Epoch: 49 [2900/6658 (44%)]\tLoss: 0.451562\n",
      "Train Epoch: 49 [3000/6658 (45%)]\tLoss: 2.003550\n",
      "Train Epoch: 49 [3100/6658 (47%)]\tLoss: 0.165205\n",
      "Train Epoch: 49 [3200/6658 (48%)]\tLoss: 0.259021\n",
      "Train Epoch: 49 [3300/6658 (50%)]\tLoss: 0.395005\n",
      "Train Epoch: 49 [3400/6658 (51%)]\tLoss: 0.177874\n",
      "Train Epoch: 49 [3500/6658 (53%)]\tLoss: 0.049681\n",
      "Train Epoch: 49 [3600/6658 (54%)]\tLoss: 0.000005\n",
      "Train Epoch: 49 [3700/6658 (56%)]\tLoss: 0.021943\n",
      "Train Epoch: 49 [3800/6658 (57%)]\tLoss: 0.920261\n",
      "Train Epoch: 49 [3900/6658 (59%)]\tLoss: 0.170952\n",
      "Train Epoch: 49 [4000/6658 (60%)]\tLoss: 0.018919\n",
      "Train Epoch: 49 [4100/6658 (62%)]\tLoss: 0.200067\n",
      "Train Epoch: 49 [4200/6658 (63%)]\tLoss: 0.970202\n",
      "Train Epoch: 49 [4300/6658 (65%)]\tLoss: 0.625223\n",
      "Train Epoch: 49 [4400/6658 (66%)]\tLoss: 0.016133\n",
      "Train Epoch: 49 [4500/6658 (68%)]\tLoss: 0.237547\n",
      "Train Epoch: 49 [4600/6658 (69%)]\tLoss: 0.155033\n",
      "Train Epoch: 49 [4700/6658 (71%)]\tLoss: 0.095297\n",
      "Train Epoch: 49 [4800/6658 (72%)]\tLoss: 0.578421\n",
      "Train Epoch: 49 [4900/6658 (74%)]\tLoss: 0.000002\n",
      "Train Epoch: 49 [5000/6658 (75%)]\tLoss: 0.533905\n",
      "Train Epoch: 49 [5100/6658 (77%)]\tLoss: 0.363961\n",
      "Train Epoch: 49 [5200/6658 (78%)]\tLoss: 0.109369\n",
      "Train Epoch: 49 [5300/6658 (80%)]\tLoss: 1.515572\n",
      "Train Epoch: 49 [5400/6658 (81%)]\tLoss: 0.220145\n",
      "Train Epoch: 49 [5500/6658 (83%)]\tLoss: 0.254831\n",
      "Train Epoch: 49 [5600/6658 (84%)]\tLoss: 0.609638\n",
      "Train Epoch: 49 [5700/6658 (86%)]\tLoss: 1.417376\n",
      "Train Epoch: 49 [5800/6658 (87%)]\tLoss: 0.311029\n",
      "Train Epoch: 49 [5900/6658 (89%)]\tLoss: 0.446907\n",
      "Train Epoch: 49 [6000/6658 (90%)]\tLoss: 1.354038\n",
      "Train Epoch: 49 [6100/6658 (92%)]\tLoss: 0.576956\n",
      "Train Epoch: 49 [6200/6658 (93%)]\tLoss: 0.253645\n",
      "Train Epoch: 49 [6300/6658 (95%)]\tLoss: 0.292143\n",
      "Train Epoch: 49 [6400/6658 (96%)]\tLoss: 0.192838\n",
      "Train Epoch: 49 [6500/6658 (98%)]\tLoss: 1.272596\n",
      "Train Epoch: 49 [6600/6658 (99%)]\tLoss: 1.184434\n",
      "train loss average =  0.7238352092966244\n",
      "\n",
      "Test set: Average loss: 0.7260\n",
      "\n",
      "EarlyStopping counter: 13 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1136, 6.0297, 5.9368, 5.9275, 6.0687, 6.0335], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 50 [0/6658 (0%)]\tLoss: 0.217478\n",
      "Train Epoch: 50 [100/6658 (2%)]\tLoss: 0.864370\n",
      "Train Epoch: 50 [200/6658 (3%)]\tLoss: 0.073064\n",
      "Train Epoch: 50 [300/6658 (5%)]\tLoss: 0.258601\n",
      "Train Epoch: 50 [400/6658 (6%)]\tLoss: 1.516681\n",
      "Train Epoch: 50 [500/6658 (8%)]\tLoss: 0.564777\n",
      "Train Epoch: 50 [600/6658 (9%)]\tLoss: 0.684369\n",
      "Train Epoch: 50 [700/6658 (11%)]\tLoss: 0.103088\n",
      "Train Epoch: 50 [800/6658 (12%)]\tLoss: 0.261092\n",
      "Train Epoch: 50 [900/6658 (14%)]\tLoss: 1.982257\n",
      "Train Epoch: 50 [1000/6658 (15%)]\tLoss: 1.269742\n",
      "Train Epoch: 50 [1100/6658 (17%)]\tLoss: 0.471774\n",
      "Train Epoch: 50 [1200/6658 (18%)]\tLoss: 1.602701\n",
      "Train Epoch: 50 [1300/6658 (20%)]\tLoss: 1.604334\n",
      "Train Epoch: 50 [1400/6658 (21%)]\tLoss: 0.057660\n",
      "Train Epoch: 50 [1500/6658 (23%)]\tLoss: 0.189976\n",
      "Train Epoch: 50 [1600/6658 (24%)]\tLoss: 0.281988\n",
      "Train Epoch: 50 [1700/6658 (26%)]\tLoss: 1.649921\n",
      "Train Epoch: 50 [1800/6658 (27%)]\tLoss: 0.579823\n",
      "Train Epoch: 50 [1900/6658 (29%)]\tLoss: 0.419299\n",
      "Train Epoch: 50 [2000/6658 (30%)]\tLoss: 3.448367\n",
      "Train Epoch: 50 [2100/6658 (32%)]\tLoss: 0.026568\n",
      "Train Epoch: 50 [2200/6658 (33%)]\tLoss: 0.064561\n",
      "Train Epoch: 50 [2300/6658 (35%)]\tLoss: 0.242391\n",
      "Train Epoch: 50 [2400/6658 (36%)]\tLoss: 0.089759\n",
      "Train Epoch: 50 [2500/6658 (38%)]\tLoss: 0.065820\n",
      "Train Epoch: 50 [2600/6658 (39%)]\tLoss: 1.694696\n",
      "Train Epoch: 50 [2700/6658 (41%)]\tLoss: 4.155415\n",
      "Train Epoch: 50 [2800/6658 (42%)]\tLoss: 0.952798\n",
      "Train Epoch: 50 [2900/6658 (44%)]\tLoss: 0.302956\n",
      "Train Epoch: 50 [3000/6658 (45%)]\tLoss: 0.011292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [3100/6658 (47%)]\tLoss: 0.114026\n",
      "Train Epoch: 50 [3200/6658 (48%)]\tLoss: 1.288183\n",
      "Train Epoch: 50 [3300/6658 (50%)]\tLoss: 0.430593\n",
      "Train Epoch: 50 [3400/6658 (51%)]\tLoss: 0.099666\n",
      "Train Epoch: 50 [3500/6658 (53%)]\tLoss: 0.832640\n",
      "Train Epoch: 50 [3600/6658 (54%)]\tLoss: 0.268336\n",
      "Train Epoch: 50 [3700/6658 (56%)]\tLoss: 0.082083\n",
      "Train Epoch: 50 [3800/6658 (57%)]\tLoss: 1.015821\n",
      "Train Epoch: 50 [3900/6658 (59%)]\tLoss: 0.047544\n",
      "Train Epoch: 50 [4000/6658 (60%)]\tLoss: 0.467024\n",
      "Train Epoch: 50 [4100/6658 (62%)]\tLoss: 0.311729\n",
      "Train Epoch: 50 [4200/6658 (63%)]\tLoss: 0.316507\n",
      "Train Epoch: 50 [4300/6658 (65%)]\tLoss: 0.354737\n",
      "Train Epoch: 50 [4400/6658 (66%)]\tLoss: 0.491745\n",
      "Train Epoch: 50 [4500/6658 (68%)]\tLoss: 1.925765\n",
      "Train Epoch: 50 [4600/6658 (69%)]\tLoss: 0.141989\n",
      "Train Epoch: 50 [4700/6658 (71%)]\tLoss: 0.235082\n",
      "Train Epoch: 50 [4800/6658 (72%)]\tLoss: 0.059366\n",
      "Train Epoch: 50 [4900/6658 (74%)]\tLoss: 1.137087\n",
      "Train Epoch: 50 [5000/6658 (75%)]\tLoss: 0.048129\n",
      "Train Epoch: 50 [5100/6658 (77%)]\tLoss: 0.077439\n",
      "Train Epoch: 50 [5200/6658 (78%)]\tLoss: 0.065316\n",
      "Train Epoch: 50 [5300/6658 (80%)]\tLoss: 0.034171\n",
      "Train Epoch: 50 [5400/6658 (81%)]\tLoss: 0.059775\n",
      "Train Epoch: 50 [5500/6658 (83%)]\tLoss: 12.057229\n",
      "Train Epoch: 50 [5600/6658 (84%)]\tLoss: 0.042691\n",
      "Train Epoch: 50 [5700/6658 (86%)]\tLoss: 3.399404\n",
      "Train Epoch: 50 [5800/6658 (87%)]\tLoss: 0.052657\n",
      "Train Epoch: 50 [5900/6658 (89%)]\tLoss: 1.642507\n",
      "Train Epoch: 50 [6000/6658 (90%)]\tLoss: 1.491377\n",
      "Train Epoch: 50 [6100/6658 (92%)]\tLoss: 0.086793\n",
      "Train Epoch: 50 [6200/6658 (93%)]\tLoss: 0.849052\n",
      "Train Epoch: 50 [6300/6658 (95%)]\tLoss: 0.258572\n",
      "Train Epoch: 50 [6400/6658 (96%)]\tLoss: 0.081688\n",
      "Train Epoch: 50 [6500/6658 (98%)]\tLoss: 0.221573\n",
      "Train Epoch: 50 [6600/6658 (99%)]\tLoss: 0.286923\n",
      "train loss average =  0.7337036156806934\n",
      "\n",
      "Test set: Average loss: 0.7199\n",
      "\n",
      "EarlyStopping counter: 14 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1153, 6.0311, 5.9352, 5.9264, 6.0694, 6.0331], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 51 [0/6658 (0%)]\tLoss: 0.076878\n",
      "Train Epoch: 51 [100/6658 (2%)]\tLoss: 3.274919\n",
      "Train Epoch: 51 [200/6658 (3%)]\tLoss: 6.401983\n",
      "Train Epoch: 51 [300/6658 (5%)]\tLoss: 0.000705\n",
      "Train Epoch: 51 [400/6658 (6%)]\tLoss: 0.145969\n",
      "Train Epoch: 51 [500/6658 (8%)]\tLoss: 0.005292\n",
      "Train Epoch: 51 [600/6658 (9%)]\tLoss: 1.471267\n",
      "Train Epoch: 51 [700/6658 (11%)]\tLoss: 0.179478\n",
      "Train Epoch: 51 [800/6658 (12%)]\tLoss: 0.033191\n",
      "Train Epoch: 51 [900/6658 (14%)]\tLoss: 1.348662\n",
      "Train Epoch: 51 [1000/6658 (15%)]\tLoss: 7.602770\n",
      "Train Epoch: 51 [1100/6658 (17%)]\tLoss: 1.417556\n",
      "Train Epoch: 51 [1200/6658 (18%)]\tLoss: 0.350859\n",
      "Train Epoch: 51 [1300/6658 (20%)]\tLoss: 1.489794\n",
      "Train Epoch: 51 [1400/6658 (21%)]\tLoss: 0.882560\n",
      "Train Epoch: 51 [1500/6658 (23%)]\tLoss: 1.852908\n",
      "Train Epoch: 51 [1600/6658 (24%)]\tLoss: 0.297583\n",
      "Train Epoch: 51 [1700/6658 (26%)]\tLoss: 0.046784\n",
      "Train Epoch: 51 [1800/6658 (27%)]\tLoss: 0.149146\n",
      "Train Epoch: 51 [1900/6658 (29%)]\tLoss: 0.274109\n",
      "Train Epoch: 51 [2000/6658 (30%)]\tLoss: 0.278519\n",
      "Train Epoch: 51 [2100/6658 (32%)]\tLoss: 1.842491\n",
      "Train Epoch: 51 [2200/6658 (33%)]\tLoss: 0.877870\n",
      "Train Epoch: 51 [2300/6658 (35%)]\tLoss: 0.180014\n",
      "Train Epoch: 51 [2400/6658 (36%)]\tLoss: 0.000247\n",
      "Train Epoch: 51 [2500/6658 (38%)]\tLoss: 0.885483\n",
      "Train Epoch: 51 [2600/6658 (39%)]\tLoss: 0.005825\n",
      "Train Epoch: 51 [2700/6658 (41%)]\tLoss: 0.160926\n",
      "Train Epoch: 51 [2800/6658 (42%)]\tLoss: 0.245915\n",
      "Train Epoch: 51 [2900/6658 (44%)]\tLoss: 0.738511\n",
      "Train Epoch: 51 [3000/6658 (45%)]\tLoss: 2.838899\n",
      "Train Epoch: 51 [3100/6658 (47%)]\tLoss: 1.011100\n",
      "Train Epoch: 51 [3200/6658 (48%)]\tLoss: 3.614712\n",
      "Train Epoch: 51 [3300/6658 (50%)]\tLoss: 0.001933\n",
      "Train Epoch: 51 [3400/6658 (51%)]\tLoss: 0.031123\n",
      "Train Epoch: 51 [3500/6658 (53%)]\tLoss: 0.625922\n",
      "Train Epoch: 51 [3600/6658 (54%)]\tLoss: 0.434152\n",
      "Train Epoch: 51 [3700/6658 (56%)]\tLoss: 1.429187\n",
      "Train Epoch: 51 [3800/6658 (57%)]\tLoss: 0.022461\n",
      "Train Epoch: 51 [3900/6658 (59%)]\tLoss: 0.285585\n",
      "Train Epoch: 51 [4000/6658 (60%)]\tLoss: 0.282602\n",
      "Train Epoch: 51 [4100/6658 (62%)]\tLoss: 0.051953\n",
      "Train Epoch: 51 [4200/6658 (63%)]\tLoss: 0.107429\n",
      "Train Epoch: 51 [4300/6658 (65%)]\tLoss: 0.033917\n",
      "Train Epoch: 51 [4400/6658 (66%)]\tLoss: 0.387888\n",
      "Train Epoch: 51 [4500/6658 (68%)]\tLoss: 0.520257\n",
      "Train Epoch: 51 [4600/6658 (69%)]\tLoss: 0.597700\n",
      "Train Epoch: 51 [4700/6658 (71%)]\tLoss: 0.003022\n",
      "Train Epoch: 51 [4800/6658 (72%)]\tLoss: 0.043329\n",
      "Train Epoch: 51 [4900/6658 (74%)]\tLoss: 0.449030\n",
      "Train Epoch: 51 [5000/6658 (75%)]\tLoss: 0.000029\n",
      "Train Epoch: 51 [5100/6658 (77%)]\tLoss: 0.085322\n",
      "Train Epoch: 51 [5200/6658 (78%)]\tLoss: 0.019471\n",
      "Train Epoch: 51 [5300/6658 (80%)]\tLoss: 0.693311\n",
      "Train Epoch: 51 [5400/6658 (81%)]\tLoss: 0.069752\n",
      "Train Epoch: 51 [5500/6658 (83%)]\tLoss: 0.025941\n",
      "Train Epoch: 51 [5600/6658 (84%)]\tLoss: 0.082138\n",
      "Train Epoch: 51 [5700/6658 (86%)]\tLoss: 1.023291\n",
      "Train Epoch: 51 [5800/6658 (87%)]\tLoss: 0.453079\n",
      "Train Epoch: 51 [5900/6658 (89%)]\tLoss: 1.036149\n",
      "Train Epoch: 51 [6000/6658 (90%)]\tLoss: 0.223052\n",
      "Train Epoch: 51 [6100/6658 (92%)]\tLoss: 6.431828\n",
      "Train Epoch: 51 [6200/6658 (93%)]\tLoss: 0.141326\n",
      "Train Epoch: 51 [6300/6658 (95%)]\tLoss: 0.146645\n",
      "Train Epoch: 51 [6400/6658 (96%)]\tLoss: 4.981938\n",
      "Train Epoch: 51 [6500/6658 (98%)]\tLoss: 0.000410\n",
      "Train Epoch: 51 [6600/6658 (99%)]\tLoss: 0.106922\n",
      "train loss average =  0.7358973928417761\n",
      "\n",
      "Test set: Average loss: 0.7029\n",
      "\n",
      "Validation loss decreased (0.704185 --> 0.702905).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.1171, 6.0318, 5.9343, 5.9255, 6.0707, 6.0340], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 52 [0/6658 (0%)]\tLoss: 0.142230\n",
      "Train Epoch: 52 [100/6658 (2%)]\tLoss: 0.065866\n",
      "Train Epoch: 52 [200/6658 (3%)]\tLoss: 0.080910\n",
      "Train Epoch: 52 [300/6658 (5%)]\tLoss: 0.314169\n",
      "Train Epoch: 52 [400/6658 (6%)]\tLoss: 0.025879\n",
      "Train Epoch: 52 [500/6658 (8%)]\tLoss: 0.014678\n",
      "Train Epoch: 52 [600/6658 (9%)]\tLoss: 0.010157\n",
      "Train Epoch: 52 [700/6658 (11%)]\tLoss: 0.011053\n",
      "Train Epoch: 52 [800/6658 (12%)]\tLoss: 0.375149\n",
      "Train Epoch: 52 [900/6658 (14%)]\tLoss: 3.858147\n",
      "Train Epoch: 52 [1000/6658 (15%)]\tLoss: 0.203696\n",
      "Train Epoch: 52 [1100/6658 (17%)]\tLoss: 0.203180\n",
      "Train Epoch: 52 [1200/6658 (18%)]\tLoss: 2.978979\n",
      "Train Epoch: 52 [1300/6658 (20%)]\tLoss: 0.519898\n",
      "Train Epoch: 52 [1400/6658 (21%)]\tLoss: 3.379203\n",
      "Train Epoch: 52 [1500/6658 (23%)]\tLoss: 0.339088\n",
      "Train Epoch: 52 [1600/6658 (24%)]\tLoss: 0.376373\n",
      "Train Epoch: 52 [1700/6658 (26%)]\tLoss: 0.421411\n",
      "Train Epoch: 52 [1800/6658 (27%)]\tLoss: 0.469336\n",
      "Train Epoch: 52 [1900/6658 (29%)]\tLoss: 0.013644\n",
      "Train Epoch: 52 [2000/6658 (30%)]\tLoss: 0.896750\n",
      "Train Epoch: 52 [2100/6658 (32%)]\tLoss: 0.014244\n",
      "Train Epoch: 52 [2200/6658 (33%)]\tLoss: 0.235386\n",
      "Train Epoch: 52 [2300/6658 (35%)]\tLoss: 0.071618\n",
      "Train Epoch: 52 [2400/6658 (36%)]\tLoss: 0.057580\n",
      "Train Epoch: 52 [2500/6658 (38%)]\tLoss: 0.165771\n",
      "Train Epoch: 52 [2600/6658 (39%)]\tLoss: 1.471153\n",
      "Train Epoch: 52 [2700/6658 (41%)]\tLoss: 5.275724\n",
      "Train Epoch: 52 [2800/6658 (42%)]\tLoss: 0.911499\n",
      "Train Epoch: 52 [2900/6658 (44%)]\tLoss: 0.017878\n",
      "Train Epoch: 52 [3000/6658 (45%)]\tLoss: 0.014560\n",
      "Train Epoch: 52 [3100/6658 (47%)]\tLoss: 1.402771\n",
      "Train Epoch: 52 [3200/6658 (48%)]\tLoss: 0.092553\n",
      "Train Epoch: 52 [3300/6658 (50%)]\tLoss: 1.833200\n",
      "Train Epoch: 52 [3400/6658 (51%)]\tLoss: 0.589265\n",
      "Train Epoch: 52 [3500/6658 (53%)]\tLoss: 0.000850\n",
      "Train Epoch: 52 [3600/6658 (54%)]\tLoss: 0.213316\n",
      "Train Epoch: 52 [3700/6658 (56%)]\tLoss: 0.358836\n",
      "Train Epoch: 52 [3800/6658 (57%)]\tLoss: 0.778826\n",
      "Train Epoch: 52 [3900/6658 (59%)]\tLoss: 0.284468\n",
      "Train Epoch: 52 [4000/6658 (60%)]\tLoss: 0.017567\n",
      "Train Epoch: 52 [4100/6658 (62%)]\tLoss: 0.059002\n",
      "Train Epoch: 52 [4200/6658 (63%)]\tLoss: 1.370816\n",
      "Train Epoch: 52 [4300/6658 (65%)]\tLoss: 0.471755\n",
      "Train Epoch: 52 [4400/6658 (66%)]\tLoss: 0.172359\n",
      "Train Epoch: 52 [4500/6658 (68%)]\tLoss: 0.463539\n",
      "Train Epoch: 52 [4600/6658 (69%)]\tLoss: 0.018633\n",
      "Train Epoch: 52 [4700/6658 (71%)]\tLoss: 0.573789\n",
      "Train Epoch: 52 [4800/6658 (72%)]\tLoss: 0.260031\n",
      "Train Epoch: 52 [4900/6658 (74%)]\tLoss: 0.019003\n",
      "Train Epoch: 52 [5000/6658 (75%)]\tLoss: 0.832225\n",
      "Train Epoch: 52 [5100/6658 (77%)]\tLoss: 1.174818\n",
      "Train Epoch: 52 [5200/6658 (78%)]\tLoss: 2.883361\n",
      "Train Epoch: 52 [5300/6658 (80%)]\tLoss: 0.336603\n",
      "Train Epoch: 52 [5400/6658 (81%)]\tLoss: 0.014256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 52 [5500/6658 (83%)]\tLoss: 0.449463\n",
      "Train Epoch: 52 [5600/6658 (84%)]\tLoss: 1.405257\n",
      "Train Epoch: 52 [5700/6658 (86%)]\tLoss: 0.090782\n",
      "Train Epoch: 52 [5800/6658 (87%)]\tLoss: 0.032388\n",
      "Train Epoch: 52 [5900/6658 (89%)]\tLoss: 0.115788\n",
      "Train Epoch: 52 [6000/6658 (90%)]\tLoss: 0.004795\n",
      "Train Epoch: 52 [6100/6658 (92%)]\tLoss: 0.136583\n",
      "Train Epoch: 52 [6200/6658 (93%)]\tLoss: 0.558378\n",
      "Train Epoch: 52 [6300/6658 (95%)]\tLoss: 1.449524\n",
      "Train Epoch: 52 [6400/6658 (96%)]\tLoss: 0.050688\n",
      "Train Epoch: 52 [6500/6658 (98%)]\tLoss: 0.532528\n",
      "Train Epoch: 52 [6600/6658 (99%)]\tLoss: 0.002578\n",
      "train loss average =  0.7313366239197596\n",
      "\n",
      "Test set: Average loss: 0.7085\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1197, 6.0322, 5.9342, 5.9241, 6.0724, 6.0345], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 53 [0/6658 (0%)]\tLoss: 0.235087\n",
      "Train Epoch: 53 [100/6658 (2%)]\tLoss: 0.284843\n",
      "Train Epoch: 53 [200/6658 (3%)]\tLoss: 0.183490\n",
      "Train Epoch: 53 [300/6658 (5%)]\tLoss: 0.074034\n",
      "Train Epoch: 53 [400/6658 (6%)]\tLoss: 0.092349\n",
      "Train Epoch: 53 [500/6658 (8%)]\tLoss: 0.248459\n",
      "Train Epoch: 53 [600/6658 (9%)]\tLoss: 0.135515\n",
      "Train Epoch: 53 [700/6658 (11%)]\tLoss: 0.252165\n",
      "Train Epoch: 53 [800/6658 (12%)]\tLoss: 0.269776\n",
      "Train Epoch: 53 [900/6658 (14%)]\tLoss: 0.189576\n",
      "Train Epoch: 53 [1000/6658 (15%)]\tLoss: 0.027294\n",
      "Train Epoch: 53 [1100/6658 (17%)]\tLoss: 0.382696\n",
      "Train Epoch: 53 [1200/6658 (18%)]\tLoss: 2.437765\n",
      "Train Epoch: 53 [1300/6658 (20%)]\tLoss: 0.514309\n",
      "Train Epoch: 53 [1400/6658 (21%)]\tLoss: 0.715650\n",
      "Train Epoch: 53 [1500/6658 (23%)]\tLoss: 1.585377\n",
      "Train Epoch: 53 [1600/6658 (24%)]\tLoss: 0.052671\n",
      "Train Epoch: 53 [1700/6658 (26%)]\tLoss: 0.006615\n",
      "Train Epoch: 53 [1800/6658 (27%)]\tLoss: 0.394712\n",
      "Train Epoch: 53 [1900/6658 (29%)]\tLoss: 0.123444\n",
      "Train Epoch: 53 [2000/6658 (30%)]\tLoss: 0.141134\n",
      "Train Epoch: 53 [2100/6658 (32%)]\tLoss: 0.528394\n",
      "Train Epoch: 53 [2200/6658 (33%)]\tLoss: 0.465122\n",
      "Train Epoch: 53 [2300/6658 (35%)]\tLoss: 1.948428\n",
      "Train Epoch: 53 [2400/6658 (36%)]\tLoss: 0.025063\n",
      "Train Epoch: 53 [2500/6658 (38%)]\tLoss: 0.300514\n",
      "Train Epoch: 53 [2600/6658 (39%)]\tLoss: 2.787359\n",
      "Train Epoch: 53 [2700/6658 (41%)]\tLoss: 0.077300\n",
      "Train Epoch: 53 [2800/6658 (42%)]\tLoss: 0.024392\n",
      "Train Epoch: 53 [2900/6658 (44%)]\tLoss: 1.085810\n",
      "Train Epoch: 53 [3000/6658 (45%)]\tLoss: 0.001389\n",
      "Train Epoch: 53 [3100/6658 (47%)]\tLoss: 0.039825\n",
      "Train Epoch: 53 [3200/6658 (48%)]\tLoss: 0.415612\n",
      "Train Epoch: 53 [3300/6658 (50%)]\tLoss: 0.135017\n",
      "Train Epoch: 53 [3400/6658 (51%)]\tLoss: 8.249874\n",
      "Train Epoch: 53 [3500/6658 (53%)]\tLoss: 0.170710\n",
      "Train Epoch: 53 [3600/6658 (54%)]\tLoss: 3.691292\n",
      "Train Epoch: 53 [3700/6658 (56%)]\tLoss: 1.337358\n",
      "Train Epoch: 53 [3800/6658 (57%)]\tLoss: 0.169437\n",
      "Train Epoch: 53 [3900/6658 (59%)]\tLoss: 0.447924\n",
      "Train Epoch: 53 [4000/6658 (60%)]\tLoss: 1.254830\n",
      "Train Epoch: 53 [4100/6658 (62%)]\tLoss: 0.145000\n",
      "Train Epoch: 53 [4200/6658 (63%)]\tLoss: 0.075762\n",
      "Train Epoch: 53 [4300/6658 (65%)]\tLoss: 0.018854\n",
      "Train Epoch: 53 [4400/6658 (66%)]\tLoss: 0.051300\n",
      "Train Epoch: 53 [4500/6658 (68%)]\tLoss: 0.025149\n",
      "Train Epoch: 53 [4600/6658 (69%)]\tLoss: 0.021717\n",
      "Train Epoch: 53 [4700/6658 (71%)]\tLoss: 0.136102\n",
      "Train Epoch: 53 [4800/6658 (72%)]\tLoss: 5.400725\n",
      "Train Epoch: 53 [4900/6658 (74%)]\tLoss: 0.080396\n",
      "Train Epoch: 53 [5000/6658 (75%)]\tLoss: 0.807591\n",
      "Train Epoch: 53 [5100/6658 (77%)]\tLoss: 0.014262\n",
      "Train Epoch: 53 [5200/6658 (78%)]\tLoss: 0.277788\n",
      "Train Epoch: 53 [5300/6658 (80%)]\tLoss: 0.215896\n",
      "Train Epoch: 53 [5400/6658 (81%)]\tLoss: 0.015556\n",
      "Train Epoch: 53 [5500/6658 (83%)]\tLoss: 1.145054\n",
      "Train Epoch: 53 [5600/6658 (84%)]\tLoss: 0.131260\n",
      "Train Epoch: 53 [5700/6658 (86%)]\tLoss: 0.247278\n",
      "Train Epoch: 53 [5800/6658 (87%)]\tLoss: 0.073899\n",
      "Train Epoch: 53 [5900/6658 (89%)]\tLoss: 0.488344\n",
      "Train Epoch: 53 [6000/6658 (90%)]\tLoss: 0.001450\n",
      "Train Epoch: 53 [6100/6658 (92%)]\tLoss: 2.809826\n",
      "Train Epoch: 53 [6200/6658 (93%)]\tLoss: 0.283770\n",
      "Train Epoch: 53 [6300/6658 (95%)]\tLoss: 0.016549\n",
      "Train Epoch: 53 [6400/6658 (96%)]\tLoss: 0.000973\n",
      "Train Epoch: 53 [6500/6658 (98%)]\tLoss: 0.585185\n",
      "Train Epoch: 53 [6600/6658 (99%)]\tLoss: 0.126157\n",
      "train loss average =  0.733932687686797\n",
      "\n",
      "Test set: Average loss: 0.7156\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1221, 6.0333, 5.9334, 5.9231, 6.0739, 6.0347], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 54 [0/6658 (0%)]\tLoss: 1.757682\n",
      "Train Epoch: 54 [100/6658 (2%)]\tLoss: 2.616627\n",
      "Train Epoch: 54 [200/6658 (3%)]\tLoss: 0.425057\n",
      "Train Epoch: 54 [300/6658 (5%)]\tLoss: 0.024390\n",
      "Train Epoch: 54 [400/6658 (6%)]\tLoss: 0.037833\n",
      "Train Epoch: 54 [500/6658 (8%)]\tLoss: 0.706040\n",
      "Train Epoch: 54 [600/6658 (9%)]\tLoss: 1.991495\n",
      "Train Epoch: 54 [700/6658 (11%)]\tLoss: 0.047426\n",
      "Train Epoch: 54 [800/6658 (12%)]\tLoss: 1.026639\n",
      "Train Epoch: 54 [900/6658 (14%)]\tLoss: 0.459134\n",
      "Train Epoch: 54 [1000/6658 (15%)]\tLoss: 0.116635\n",
      "Train Epoch: 54 [1100/6658 (17%)]\tLoss: 0.004274\n",
      "Train Epoch: 54 [1200/6658 (18%)]\tLoss: 1.013951\n",
      "Train Epoch: 54 [1300/6658 (20%)]\tLoss: 0.570268\n",
      "Train Epoch: 54 [1400/6658 (21%)]\tLoss: 1.402485\n",
      "Train Epoch: 54 [1500/6658 (23%)]\tLoss: 0.365725\n",
      "Train Epoch: 54 [1600/6658 (24%)]\tLoss: 2.539135\n",
      "Train Epoch: 54 [1700/6658 (26%)]\tLoss: 0.043714\n",
      "Train Epoch: 54 [1800/6658 (27%)]\tLoss: 0.295659\n",
      "Train Epoch: 54 [1900/6658 (29%)]\tLoss: 0.074704\n",
      "Train Epoch: 54 [2000/6658 (30%)]\tLoss: 0.946962\n",
      "Train Epoch: 54 [2100/6658 (32%)]\tLoss: 1.114354\n",
      "Train Epoch: 54 [2200/6658 (33%)]\tLoss: 0.131095\n",
      "Train Epoch: 54 [2300/6658 (35%)]\tLoss: 0.122506\n",
      "Train Epoch: 54 [2400/6658 (36%)]\tLoss: 0.114884\n",
      "Train Epoch: 54 [2500/6658 (38%)]\tLoss: 0.378933\n",
      "Train Epoch: 54 [2600/6658 (39%)]\tLoss: 0.359963\n",
      "Train Epoch: 54 [2700/6658 (41%)]\tLoss: 0.432237\n",
      "Train Epoch: 54 [2800/6658 (42%)]\tLoss: 0.478847\n",
      "Train Epoch: 54 [2900/6658 (44%)]\tLoss: 1.596974\n",
      "Train Epoch: 54 [3000/6658 (45%)]\tLoss: 10.416141\n",
      "Train Epoch: 54 [3100/6658 (47%)]\tLoss: 0.025402\n",
      "Train Epoch: 54 [3200/6658 (48%)]\tLoss: 0.116499\n",
      "Train Epoch: 54 [3300/6658 (50%)]\tLoss: 0.018817\n",
      "Train Epoch: 54 [3400/6658 (51%)]\tLoss: 1.640684\n",
      "Train Epoch: 54 [3500/6658 (53%)]\tLoss: 0.324635\n",
      "Train Epoch: 54 [3600/6658 (54%)]\tLoss: 0.184657\n",
      "Train Epoch: 54 [3700/6658 (56%)]\tLoss: 0.392697\n",
      "Train Epoch: 54 [3800/6658 (57%)]\tLoss: 0.147784\n",
      "Train Epoch: 54 [3900/6658 (59%)]\tLoss: 0.012394\n",
      "Train Epoch: 54 [4000/6658 (60%)]\tLoss: 0.002659\n",
      "Train Epoch: 54 [4100/6658 (62%)]\tLoss: 0.938776\n",
      "Train Epoch: 54 [4200/6658 (63%)]\tLoss: 0.014166\n",
      "Train Epoch: 54 [4300/6658 (65%)]\tLoss: 0.262793\n",
      "Train Epoch: 54 [4400/6658 (66%)]\tLoss: 1.052801\n",
      "Train Epoch: 54 [4500/6658 (68%)]\tLoss: 2.803693\n",
      "Train Epoch: 54 [4600/6658 (69%)]\tLoss: 1.711955\n",
      "Train Epoch: 54 [4700/6658 (71%)]\tLoss: 0.001327\n",
      "Train Epoch: 54 [4800/6658 (72%)]\tLoss: 0.891074\n",
      "Train Epoch: 54 [4900/6658 (74%)]\tLoss: 0.025184\n",
      "Train Epoch: 54 [5000/6658 (75%)]\tLoss: 0.014205\n",
      "Train Epoch: 54 [5100/6658 (77%)]\tLoss: 0.238850\n",
      "Train Epoch: 54 [5200/6658 (78%)]\tLoss: 0.460675\n",
      "Train Epoch: 54 [5300/6658 (80%)]\tLoss: 0.420066\n",
      "Train Epoch: 54 [5400/6658 (81%)]\tLoss: 0.006598\n",
      "Train Epoch: 54 [5500/6658 (83%)]\tLoss: 0.171298\n",
      "Train Epoch: 54 [5600/6658 (84%)]\tLoss: 0.235080\n",
      "Train Epoch: 54 [5700/6658 (86%)]\tLoss: 0.395419\n",
      "Train Epoch: 54 [5800/6658 (87%)]\tLoss: 0.128444\n",
      "Train Epoch: 54 [5900/6658 (89%)]\tLoss: 0.561637\n",
      "Train Epoch: 54 [6000/6658 (90%)]\tLoss: 0.002347\n",
      "Train Epoch: 54 [6100/6658 (92%)]\tLoss: 0.320350\n",
      "Train Epoch: 54 [6200/6658 (93%)]\tLoss: 0.000900\n",
      "Train Epoch: 54 [6300/6658 (95%)]\tLoss: 0.145698\n",
      "Train Epoch: 54 [6400/6658 (96%)]\tLoss: 0.093370\n",
      "Train Epoch: 54 [6500/6658 (98%)]\tLoss: 0.046770\n",
      "Train Epoch: 54 [6600/6658 (99%)]\tLoss: 0.229196\n",
      "train loss average =  0.7338418427696862\n",
      "\n",
      "Test set: Average loss: 0.7016\n",
      "\n",
      "Validation loss decreased (0.702905 --> 0.701613).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.1245, 6.0338, 5.9315, 5.9222, 6.0750, 6.0351], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 55 [0/6658 (0%)]\tLoss: 0.031945\n",
      "Train Epoch: 55 [100/6658 (2%)]\tLoss: 0.223063\n",
      "Train Epoch: 55 [200/6658 (3%)]\tLoss: 3.357302\n",
      "Train Epoch: 55 [300/6658 (5%)]\tLoss: 0.055199\n",
      "Train Epoch: 55 [400/6658 (6%)]\tLoss: 1.227285\n",
      "Train Epoch: 55 [500/6658 (8%)]\tLoss: 0.627631\n",
      "Train Epoch: 55 [600/6658 (9%)]\tLoss: 0.105278\n",
      "Train Epoch: 55 [700/6658 (11%)]\tLoss: 0.031571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 55 [800/6658 (12%)]\tLoss: 0.000079\n",
      "Train Epoch: 55 [900/6658 (14%)]\tLoss: 0.014517\n",
      "Train Epoch: 55 [1000/6658 (15%)]\tLoss: 0.002806\n",
      "Train Epoch: 55 [1100/6658 (17%)]\tLoss: 1.563144\n",
      "Train Epoch: 55 [1200/6658 (18%)]\tLoss: 1.364799\n",
      "Train Epoch: 55 [1300/6658 (20%)]\tLoss: 0.695704\n",
      "Train Epoch: 55 [1400/6658 (21%)]\tLoss: 0.035165\n",
      "Train Epoch: 55 [1500/6658 (23%)]\tLoss: 0.179305\n",
      "Train Epoch: 55 [1600/6658 (24%)]\tLoss: 0.051277\n",
      "Train Epoch: 55 [1700/6658 (26%)]\tLoss: 0.148976\n",
      "Train Epoch: 55 [1800/6658 (27%)]\tLoss: 0.320293\n",
      "Train Epoch: 55 [1900/6658 (29%)]\tLoss: 0.132634\n",
      "Train Epoch: 55 [2000/6658 (30%)]\tLoss: 0.247958\n",
      "Train Epoch: 55 [2100/6658 (32%)]\tLoss: 0.209713\n",
      "Train Epoch: 55 [2200/6658 (33%)]\tLoss: 0.001351\n",
      "Train Epoch: 55 [2300/6658 (35%)]\tLoss: 0.009020\n",
      "Train Epoch: 55 [2400/6658 (36%)]\tLoss: 0.087839\n",
      "Train Epoch: 55 [2500/6658 (38%)]\tLoss: 5.948292\n",
      "Train Epoch: 55 [2600/6658 (39%)]\tLoss: 0.517269\n",
      "Train Epoch: 55 [2700/6658 (41%)]\tLoss: 0.008265\n",
      "Train Epoch: 55 [2800/6658 (42%)]\tLoss: 1.945632\n",
      "Train Epoch: 55 [2900/6658 (44%)]\tLoss: 1.349956\n",
      "Train Epoch: 55 [3000/6658 (45%)]\tLoss: 1.443823\n",
      "Train Epoch: 55 [3100/6658 (47%)]\tLoss: 0.309225\n",
      "Train Epoch: 55 [3200/6658 (48%)]\tLoss: 0.021434\n",
      "Train Epoch: 55 [3300/6658 (50%)]\tLoss: 1.592452\n",
      "Train Epoch: 55 [3400/6658 (51%)]\tLoss: 0.490406\n",
      "Train Epoch: 55 [3500/6658 (53%)]\tLoss: 0.142312\n",
      "Train Epoch: 55 [3600/6658 (54%)]\tLoss: 0.185705\n",
      "Train Epoch: 55 [3700/6658 (56%)]\tLoss: 0.000494\n",
      "Train Epoch: 55 [3800/6658 (57%)]\tLoss: 0.322032\n",
      "Train Epoch: 55 [3900/6658 (59%)]\tLoss: 0.585041\n",
      "Train Epoch: 55 [4000/6658 (60%)]\tLoss: 3.750888\n",
      "Train Epoch: 55 [4100/6658 (62%)]\tLoss: 0.237500\n",
      "Train Epoch: 55 [4200/6658 (63%)]\tLoss: 0.473809\n",
      "Train Epoch: 55 [4300/6658 (65%)]\tLoss: 0.139252\n",
      "Train Epoch: 55 [4400/6658 (66%)]\tLoss: 1.310637\n",
      "Train Epoch: 55 [4500/6658 (68%)]\tLoss: 0.181535\n",
      "Train Epoch: 55 [4600/6658 (69%)]\tLoss: 0.387806\n",
      "Train Epoch: 55 [4700/6658 (71%)]\tLoss: 0.009801\n",
      "Train Epoch: 55 [4800/6658 (72%)]\tLoss: 0.278272\n",
      "Train Epoch: 55 [4900/6658 (74%)]\tLoss: 0.036601\n",
      "Train Epoch: 55 [5000/6658 (75%)]\tLoss: 0.085257\n",
      "Train Epoch: 55 [5100/6658 (77%)]\tLoss: 0.332957\n",
      "Train Epoch: 55 [5200/6658 (78%)]\tLoss: 0.017838\n",
      "Train Epoch: 55 [5300/6658 (80%)]\tLoss: 3.768164\n",
      "Train Epoch: 55 [5400/6658 (81%)]\tLoss: 1.280996\n",
      "Train Epoch: 55 [5500/6658 (83%)]\tLoss: 0.576238\n",
      "Train Epoch: 55 [5600/6658 (84%)]\tLoss: 0.000410\n",
      "Train Epoch: 55 [5700/6658 (86%)]\tLoss: 0.593489\n",
      "Train Epoch: 55 [5800/6658 (87%)]\tLoss: 1.001502\n",
      "Train Epoch: 55 [5900/6658 (89%)]\tLoss: 1.849547\n",
      "Train Epoch: 55 [6000/6658 (90%)]\tLoss: 0.676624\n",
      "Train Epoch: 55 [6100/6658 (92%)]\tLoss: 0.102917\n",
      "Train Epoch: 55 [6200/6658 (93%)]\tLoss: 0.023182\n",
      "Train Epoch: 55 [6300/6658 (95%)]\tLoss: 0.541339\n",
      "Train Epoch: 55 [6400/6658 (96%)]\tLoss: 0.040276\n",
      "Train Epoch: 55 [6500/6658 (98%)]\tLoss: 0.244237\n",
      "Train Epoch: 55 [6600/6658 (99%)]\tLoss: 0.122642\n",
      "train loss average =  0.7381671856147914\n",
      "\n",
      "Test set: Average loss: 0.7105\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1270, 6.0350, 5.9304, 5.9204, 6.0765, 6.0357], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 56 [0/6658 (0%)]\tLoss: 0.084068\n",
      "Train Epoch: 56 [100/6658 (2%)]\tLoss: 0.021304\n",
      "Train Epoch: 56 [200/6658 (3%)]\tLoss: 0.173017\n",
      "Train Epoch: 56 [300/6658 (5%)]\tLoss: 2.948921\n",
      "Train Epoch: 56 [400/6658 (6%)]\tLoss: 0.003552\n",
      "Train Epoch: 56 [500/6658 (8%)]\tLoss: 0.540001\n",
      "Train Epoch: 56 [600/6658 (9%)]\tLoss: 0.970960\n",
      "Train Epoch: 56 [700/6658 (11%)]\tLoss: 0.079789\n",
      "Train Epoch: 56 [800/6658 (12%)]\tLoss: 0.409062\n",
      "Train Epoch: 56 [900/6658 (14%)]\tLoss: 1.456035\n",
      "Train Epoch: 56 [1000/6658 (15%)]\tLoss: 0.709800\n",
      "Train Epoch: 56 [1100/6658 (17%)]\tLoss: 4.474138\n",
      "Train Epoch: 56 [1200/6658 (18%)]\tLoss: 0.161680\n",
      "Train Epoch: 56 [1300/6658 (20%)]\tLoss: 0.295063\n",
      "Train Epoch: 56 [1400/6658 (21%)]\tLoss: 0.191810\n",
      "Train Epoch: 56 [1500/6658 (23%)]\tLoss: 1.148978\n",
      "Train Epoch: 56 [1600/6658 (24%)]\tLoss: 1.857147\n",
      "Train Epoch: 56 [1700/6658 (26%)]\tLoss: 0.536779\n",
      "Train Epoch: 56 [1800/6658 (27%)]\tLoss: 2.284315\n",
      "Train Epoch: 56 [1900/6658 (29%)]\tLoss: 0.941896\n",
      "Train Epoch: 56 [2000/6658 (30%)]\tLoss: 1.024182\n",
      "Train Epoch: 56 [2100/6658 (32%)]\tLoss: 0.094155\n",
      "Train Epoch: 56 [2200/6658 (33%)]\tLoss: 0.101508\n",
      "Train Epoch: 56 [2300/6658 (35%)]\tLoss: 1.059728\n",
      "Train Epoch: 56 [2400/6658 (36%)]\tLoss: 0.578219\n",
      "Train Epoch: 56 [2500/6658 (38%)]\tLoss: 0.126429\n",
      "Train Epoch: 56 [2600/6658 (39%)]\tLoss: 0.119876\n",
      "Train Epoch: 56 [2700/6658 (41%)]\tLoss: 1.108046\n",
      "Train Epoch: 56 [2800/6658 (42%)]\tLoss: 0.212214\n",
      "Train Epoch: 56 [2900/6658 (44%)]\tLoss: 0.250510\n",
      "Train Epoch: 56 [3000/6658 (45%)]\tLoss: 0.099404\n",
      "Train Epoch: 56 [3100/6658 (47%)]\tLoss: 0.007628\n",
      "Train Epoch: 56 [3200/6658 (48%)]\tLoss: 0.451978\n",
      "Train Epoch: 56 [3300/6658 (50%)]\tLoss: 0.682756\n",
      "Train Epoch: 56 [3400/6658 (51%)]\tLoss: 0.000069\n",
      "Train Epoch: 56 [3500/6658 (53%)]\tLoss: 0.124086\n",
      "Train Epoch: 56 [3600/6658 (54%)]\tLoss: 0.268344\n",
      "Train Epoch: 56 [3700/6658 (56%)]\tLoss: 0.190094\n",
      "Train Epoch: 56 [3800/6658 (57%)]\tLoss: 0.095863\n",
      "Train Epoch: 56 [3900/6658 (59%)]\tLoss: 4.745337\n",
      "Train Epoch: 56 [4000/6658 (60%)]\tLoss: 1.299145\n",
      "Train Epoch: 56 [4100/6658 (62%)]\tLoss: 0.090065\n",
      "Train Epoch: 56 [4200/6658 (63%)]\tLoss: 0.347911\n",
      "Train Epoch: 56 [4300/6658 (65%)]\tLoss: 0.211454\n",
      "Train Epoch: 56 [4400/6658 (66%)]\tLoss: 6.244089\n",
      "Train Epoch: 56 [4500/6658 (68%)]\tLoss: 0.392939\n",
      "Train Epoch: 56 [4600/6658 (69%)]\tLoss: 0.122381\n",
      "Train Epoch: 56 [4700/6658 (71%)]\tLoss: 0.457407\n",
      "Train Epoch: 56 [4800/6658 (72%)]\tLoss: 0.134185\n",
      "Train Epoch: 56 [4900/6658 (74%)]\tLoss: 0.459347\n",
      "Train Epoch: 56 [5000/6658 (75%)]\tLoss: 0.022506\n",
      "Train Epoch: 56 [5100/6658 (77%)]\tLoss: 0.088017\n",
      "Train Epoch: 56 [5200/6658 (78%)]\tLoss: 0.881950\n",
      "Train Epoch: 56 [5300/6658 (80%)]\tLoss: 0.069993\n",
      "Train Epoch: 56 [5400/6658 (81%)]\tLoss: 1.264753\n",
      "Train Epoch: 56 [5500/6658 (83%)]\tLoss: 0.117754\n",
      "Train Epoch: 56 [5600/6658 (84%)]\tLoss: 0.214677\n",
      "Train Epoch: 56 [5700/6658 (86%)]\tLoss: 0.376455\n",
      "Train Epoch: 56 [5800/6658 (87%)]\tLoss: 0.468627\n",
      "Train Epoch: 56 [5900/6658 (89%)]\tLoss: 0.217789\n",
      "Train Epoch: 56 [6000/6658 (90%)]\tLoss: 0.019730\n",
      "Train Epoch: 56 [6100/6658 (92%)]\tLoss: 0.174391\n",
      "Train Epoch: 56 [6200/6658 (93%)]\tLoss: 1.051378\n",
      "Train Epoch: 56 [6300/6658 (95%)]\tLoss: 1.065560\n",
      "Train Epoch: 56 [6400/6658 (96%)]\tLoss: 0.731794\n",
      "Train Epoch: 56 [6500/6658 (98%)]\tLoss: 0.000198\n",
      "Train Epoch: 56 [6600/6658 (99%)]\tLoss: 1.499159\n",
      "train loss average =  0.7318897442964748\n",
      "\n",
      "Test set: Average loss: 0.7139\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1302, 6.0362, 5.9286, 5.9195, 6.0776, 6.0357], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 57 [0/6658 (0%)]\tLoss: 0.086522\n",
      "Train Epoch: 57 [100/6658 (2%)]\tLoss: 0.020154\n",
      "Train Epoch: 57 [200/6658 (3%)]\tLoss: 0.851531\n",
      "Train Epoch: 57 [300/6658 (5%)]\tLoss: 0.013720\n",
      "Train Epoch: 57 [400/6658 (6%)]\tLoss: 0.355456\n",
      "Train Epoch: 57 [500/6658 (8%)]\tLoss: 0.518736\n",
      "Train Epoch: 57 [600/6658 (9%)]\tLoss: 0.611166\n",
      "Train Epoch: 57 [700/6658 (11%)]\tLoss: 0.364194\n",
      "Train Epoch: 57 [800/6658 (12%)]\tLoss: 0.627436\n",
      "Train Epoch: 57 [900/6658 (14%)]\tLoss: 0.232079\n",
      "Train Epoch: 57 [1000/6658 (15%)]\tLoss: 0.393961\n",
      "Train Epoch: 57 [1100/6658 (17%)]\tLoss: 1.151298\n",
      "Train Epoch: 57 [1200/6658 (18%)]\tLoss: 0.032773\n",
      "Train Epoch: 57 [1300/6658 (20%)]\tLoss: 0.105391\n",
      "Train Epoch: 57 [1400/6658 (21%)]\tLoss: 0.003859\n",
      "Train Epoch: 57 [1500/6658 (23%)]\tLoss: 0.373247\n",
      "Train Epoch: 57 [1600/6658 (24%)]\tLoss: 1.686653\n",
      "Train Epoch: 57 [1700/6658 (26%)]\tLoss: 0.080382\n",
      "Train Epoch: 57 [1800/6658 (27%)]\tLoss: 0.253228\n",
      "Train Epoch: 57 [1900/6658 (29%)]\tLoss: 0.067469\n",
      "Train Epoch: 57 [2000/6658 (30%)]\tLoss: 0.027050\n",
      "Train Epoch: 57 [2100/6658 (32%)]\tLoss: 0.292414\n",
      "Train Epoch: 57 [2200/6658 (33%)]\tLoss: 0.728163\n",
      "Train Epoch: 57 [2300/6658 (35%)]\tLoss: 0.091826\n",
      "Train Epoch: 57 [2400/6658 (36%)]\tLoss: 0.257484\n",
      "Train Epoch: 57 [2500/6658 (38%)]\tLoss: 0.053115\n",
      "Train Epoch: 57 [2600/6658 (39%)]\tLoss: 1.239652\n",
      "Train Epoch: 57 [2700/6658 (41%)]\tLoss: 0.674414\n",
      "Train Epoch: 57 [2800/6658 (42%)]\tLoss: 0.071920\n",
      "Train Epoch: 57 [2900/6658 (44%)]\tLoss: 0.451266\n",
      "Train Epoch: 57 [3000/6658 (45%)]\tLoss: 0.113372\n",
      "Train Epoch: 57 [3100/6658 (47%)]\tLoss: 0.211597\n",
      "Train Epoch: 57 [3200/6658 (48%)]\tLoss: 0.016307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 57 [3300/6658 (50%)]\tLoss: 0.297843\n",
      "Train Epoch: 57 [3400/6658 (51%)]\tLoss: 0.159682\n",
      "Train Epoch: 57 [3500/6658 (53%)]\tLoss: 0.497810\n",
      "Train Epoch: 57 [3600/6658 (54%)]\tLoss: 0.533984\n",
      "Train Epoch: 57 [3700/6658 (56%)]\tLoss: 0.216286\n",
      "Train Epoch: 57 [3800/6658 (57%)]\tLoss: 0.184265\n",
      "Train Epoch: 57 [3900/6658 (59%)]\tLoss: 0.011924\n",
      "Train Epoch: 57 [4000/6658 (60%)]\tLoss: 0.003647\n",
      "Train Epoch: 57 [4100/6658 (62%)]\tLoss: 4.684865\n",
      "Train Epoch: 57 [4200/6658 (63%)]\tLoss: 0.131081\n",
      "Train Epoch: 57 [4300/6658 (65%)]\tLoss: 0.031121\n",
      "Train Epoch: 57 [4400/6658 (66%)]\tLoss: 0.478070\n",
      "Train Epoch: 57 [4500/6658 (68%)]\tLoss: 2.551946\n",
      "Train Epoch: 57 [4600/6658 (69%)]\tLoss: 1.143656\n",
      "Train Epoch: 57 [4700/6658 (71%)]\tLoss: 0.412662\n",
      "Train Epoch: 57 [4800/6658 (72%)]\tLoss: 0.009783\n",
      "Train Epoch: 57 [4900/6658 (74%)]\tLoss: 0.230195\n",
      "Train Epoch: 57 [5000/6658 (75%)]\tLoss: 0.012781\n",
      "Train Epoch: 57 [5100/6658 (77%)]\tLoss: 0.447148\n",
      "Train Epoch: 57 [5200/6658 (78%)]\tLoss: 0.000861\n",
      "Train Epoch: 57 [5300/6658 (80%)]\tLoss: 0.598332\n",
      "Train Epoch: 57 [5400/6658 (81%)]\tLoss: 0.119598\n",
      "Train Epoch: 57 [5500/6658 (83%)]\tLoss: 0.337770\n",
      "Train Epoch: 57 [5600/6658 (84%)]\tLoss: 1.498735\n",
      "Train Epoch: 57 [5700/6658 (86%)]\tLoss: 0.033751\n",
      "Train Epoch: 57 [5800/6658 (87%)]\tLoss: 0.120430\n",
      "Train Epoch: 57 [5900/6658 (89%)]\tLoss: 0.442757\n",
      "Train Epoch: 57 [6000/6658 (90%)]\tLoss: 0.290649\n",
      "Train Epoch: 57 [6100/6658 (92%)]\tLoss: 0.024957\n",
      "Train Epoch: 57 [6200/6658 (93%)]\tLoss: 2.738863\n",
      "Train Epoch: 57 [6300/6658 (95%)]\tLoss: 0.432877\n",
      "Train Epoch: 57 [6400/6658 (96%)]\tLoss: 0.040486\n",
      "Train Epoch: 57 [6500/6658 (98%)]\tLoss: 0.085328\n",
      "Train Epoch: 57 [6600/6658 (99%)]\tLoss: 0.005861\n",
      "train loss average =  0.7300845626852019\n",
      "\n",
      "Test set: Average loss: 0.7128\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1327, 6.0369, 5.9272, 5.9187, 6.0792, 6.0363], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 58 [0/6658 (0%)]\tLoss: 0.665096\n",
      "Train Epoch: 58 [100/6658 (2%)]\tLoss: 0.009788\n",
      "Train Epoch: 58 [200/6658 (3%)]\tLoss: 0.134808\n",
      "Train Epoch: 58 [300/6658 (5%)]\tLoss: 0.286929\n",
      "Train Epoch: 58 [400/6658 (6%)]\tLoss: 0.001991\n",
      "Train Epoch: 58 [500/6658 (8%)]\tLoss: 0.757358\n",
      "Train Epoch: 58 [600/6658 (9%)]\tLoss: 0.042497\n",
      "Train Epoch: 58 [700/6658 (11%)]\tLoss: 0.070186\n",
      "Train Epoch: 58 [800/6658 (12%)]\tLoss: 1.161121\n",
      "Train Epoch: 58 [900/6658 (14%)]\tLoss: 0.591955\n",
      "Train Epoch: 58 [1000/6658 (15%)]\tLoss: 0.017982\n",
      "Train Epoch: 58 [1100/6658 (17%)]\tLoss: 0.669010\n",
      "Train Epoch: 58 [1200/6658 (18%)]\tLoss: 0.001238\n",
      "Train Epoch: 58 [1300/6658 (20%)]\tLoss: 0.960287\n",
      "Train Epoch: 58 [1400/6658 (21%)]\tLoss: 2.892953\n",
      "Train Epoch: 58 [1500/6658 (23%)]\tLoss: 0.000345\n",
      "Train Epoch: 58 [1600/6658 (24%)]\tLoss: 0.436537\n",
      "Train Epoch: 58 [1700/6658 (26%)]\tLoss: 0.092500\n",
      "Train Epoch: 58 [1800/6658 (27%)]\tLoss: 0.210840\n",
      "Train Epoch: 58 [1900/6658 (29%)]\tLoss: 0.005804\n",
      "Train Epoch: 58 [2000/6658 (30%)]\tLoss: 0.010321\n",
      "Train Epoch: 58 [2100/6658 (32%)]\tLoss: 0.002527\n",
      "Train Epoch: 58 [2200/6658 (33%)]\tLoss: 0.334535\n",
      "Train Epoch: 58 [2300/6658 (35%)]\tLoss: 0.649134\n",
      "Train Epoch: 58 [2400/6658 (36%)]\tLoss: 0.306675\n",
      "Train Epoch: 58 [2500/6658 (38%)]\tLoss: 11.139941\n",
      "Train Epoch: 58 [2600/6658 (39%)]\tLoss: 0.005093\n",
      "Train Epoch: 58 [2700/6658 (41%)]\tLoss: 0.024584\n",
      "Train Epoch: 58 [2800/6658 (42%)]\tLoss: 0.079272\n",
      "Train Epoch: 58 [2900/6658 (44%)]\tLoss: 0.074493\n",
      "Train Epoch: 58 [3000/6658 (45%)]\tLoss: 0.071082\n",
      "Train Epoch: 58 [3100/6658 (47%)]\tLoss: 3.736426\n",
      "Train Epoch: 58 [3200/6658 (48%)]\tLoss: 0.004328\n",
      "Train Epoch: 58 [3300/6658 (50%)]\tLoss: 0.000030\n",
      "Train Epoch: 58 [3400/6658 (51%)]\tLoss: 0.046398\n",
      "Train Epoch: 58 [3500/6658 (53%)]\tLoss: 0.024908\n",
      "Train Epoch: 58 [3600/6658 (54%)]\tLoss: 0.024401\n",
      "Train Epoch: 58 [3700/6658 (56%)]\tLoss: 0.288940\n",
      "Train Epoch: 58 [3800/6658 (57%)]\tLoss: 0.091655\n",
      "Train Epoch: 58 [3900/6658 (59%)]\tLoss: 0.392085\n",
      "Train Epoch: 58 [4000/6658 (60%)]\tLoss: 0.308302\n",
      "Train Epoch: 58 [4100/6658 (62%)]\tLoss: 0.387540\n",
      "Train Epoch: 58 [4200/6658 (63%)]\tLoss: 0.458398\n",
      "Train Epoch: 58 [4300/6658 (65%)]\tLoss: 0.017450\n",
      "Train Epoch: 58 [4400/6658 (66%)]\tLoss: 0.140362\n",
      "Train Epoch: 58 [4500/6658 (68%)]\tLoss: 0.000327\n",
      "Train Epoch: 58 [4600/6658 (69%)]\tLoss: 0.019949\n",
      "Train Epoch: 58 [4700/6658 (71%)]\tLoss: 0.004934\n",
      "Train Epoch: 58 [4800/6658 (72%)]\tLoss: 13.514304\n",
      "Train Epoch: 58 [4900/6658 (74%)]\tLoss: 0.000030\n",
      "Train Epoch: 58 [5000/6658 (75%)]\tLoss: 0.353112\n",
      "Train Epoch: 58 [5100/6658 (77%)]\tLoss: 0.613454\n",
      "Train Epoch: 58 [5200/6658 (78%)]\tLoss: 0.000024\n",
      "Train Epoch: 58 [5300/6658 (80%)]\tLoss: 0.042510\n",
      "Train Epoch: 58 [5400/6658 (81%)]\tLoss: 0.019607\n",
      "Train Epoch: 58 [5500/6658 (83%)]\tLoss: 0.206742\n",
      "Train Epoch: 58 [5600/6658 (84%)]\tLoss: 0.222999\n",
      "Train Epoch: 58 [5700/6658 (86%)]\tLoss: 1.364735\n",
      "Train Epoch: 58 [5800/6658 (87%)]\tLoss: 0.046762\n",
      "Train Epoch: 58 [5900/6658 (89%)]\tLoss: 0.354196\n",
      "Train Epoch: 58 [6000/6658 (90%)]\tLoss: 0.070167\n",
      "Train Epoch: 58 [6100/6658 (92%)]\tLoss: 1.046115\n",
      "Train Epoch: 58 [6200/6658 (93%)]\tLoss: 1.002608\n",
      "Train Epoch: 58 [6300/6658 (95%)]\tLoss: 0.166990\n",
      "Train Epoch: 58 [6400/6658 (96%)]\tLoss: 0.475616\n",
      "Train Epoch: 58 [6500/6658 (98%)]\tLoss: 0.024413\n",
      "Train Epoch: 58 [6600/6658 (99%)]\tLoss: 0.116170\n",
      "train loss average =  0.7296251086586564\n",
      "\n",
      "Test set: Average loss: 0.7103\n",
      "\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1347, 6.0385, 5.9262, 5.9174, 6.0807, 6.0372], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 59 [0/6658 (0%)]\tLoss: 0.017693\n",
      "Train Epoch: 59 [100/6658 (2%)]\tLoss: 0.087638\n",
      "Train Epoch: 59 [200/6658 (3%)]\tLoss: 0.010246\n",
      "Train Epoch: 59 [300/6658 (5%)]\tLoss: 1.595469\n",
      "Train Epoch: 59 [400/6658 (6%)]\tLoss: 0.386175\n",
      "Train Epoch: 59 [500/6658 (8%)]\tLoss: 0.172294\n",
      "Train Epoch: 59 [600/6658 (9%)]\tLoss: 0.014799\n",
      "Train Epoch: 59 [700/6658 (11%)]\tLoss: 0.025870\n",
      "Train Epoch: 59 [800/6658 (12%)]\tLoss: 0.496199\n",
      "Train Epoch: 59 [900/6658 (14%)]\tLoss: 0.132169\n",
      "Train Epoch: 59 [1000/6658 (15%)]\tLoss: 0.545870\n",
      "Train Epoch: 59 [1100/6658 (17%)]\tLoss: 0.230932\n",
      "Train Epoch: 59 [1200/6658 (18%)]\tLoss: 0.575969\n",
      "Train Epoch: 59 [1300/6658 (20%)]\tLoss: 0.473792\n",
      "Train Epoch: 59 [1400/6658 (21%)]\tLoss: 0.862460\n",
      "Train Epoch: 59 [1500/6658 (23%)]\tLoss: 0.539642\n",
      "Train Epoch: 59 [1600/6658 (24%)]\tLoss: 1.236455\n",
      "Train Epoch: 59 [1700/6658 (26%)]\tLoss: 0.923361\n",
      "Train Epoch: 59 [1800/6658 (27%)]\tLoss: 0.246956\n",
      "Train Epoch: 59 [1900/6658 (29%)]\tLoss: 0.000607\n",
      "Train Epoch: 59 [2000/6658 (30%)]\tLoss: 0.475833\n",
      "Train Epoch: 59 [2100/6658 (32%)]\tLoss: 0.322204\n",
      "Train Epoch: 59 [2200/6658 (33%)]\tLoss: 0.659468\n",
      "Train Epoch: 59 [2300/6658 (35%)]\tLoss: 2.102576\n",
      "Train Epoch: 59 [2400/6658 (36%)]\tLoss: 0.052066\n",
      "Train Epoch: 59 [2500/6658 (38%)]\tLoss: 0.008129\n",
      "Train Epoch: 59 [2600/6658 (39%)]\tLoss: 0.113473\n",
      "Train Epoch: 59 [2700/6658 (41%)]\tLoss: 0.134842\n",
      "Train Epoch: 59 [2800/6658 (42%)]\tLoss: 0.179675\n",
      "Train Epoch: 59 [2900/6658 (44%)]\tLoss: 0.129450\n",
      "Train Epoch: 59 [3000/6658 (45%)]\tLoss: 0.766609\n",
      "Train Epoch: 59 [3100/6658 (47%)]\tLoss: 0.876424\n",
      "Train Epoch: 59 [3200/6658 (48%)]\tLoss: 0.821267\n",
      "Train Epoch: 59 [3300/6658 (50%)]\tLoss: 0.105230\n",
      "Train Epoch: 59 [3400/6658 (51%)]\tLoss: 1.523090\n",
      "Train Epoch: 59 [3500/6658 (53%)]\tLoss: 0.106864\n",
      "Train Epoch: 59 [3600/6658 (54%)]\tLoss: 1.491198\n",
      "Train Epoch: 59 [3700/6658 (56%)]\tLoss: 0.203873\n",
      "Train Epoch: 59 [3800/6658 (57%)]\tLoss: 0.065706\n",
      "Train Epoch: 59 [3900/6658 (59%)]\tLoss: 0.447702\n",
      "Train Epoch: 59 [4000/6658 (60%)]\tLoss: 4.000807\n",
      "Train Epoch: 59 [4100/6658 (62%)]\tLoss: 0.465675\n",
      "Train Epoch: 59 [4200/6658 (63%)]\tLoss: 4.216550\n",
      "Train Epoch: 59 [4300/6658 (65%)]\tLoss: 0.012951\n",
      "Train Epoch: 59 [4400/6658 (66%)]\tLoss: 5.666298\n",
      "Train Epoch: 59 [4500/6658 (68%)]\tLoss: 0.305544\n",
      "Train Epoch: 59 [4600/6658 (69%)]\tLoss: 0.660493\n",
      "Train Epoch: 59 [4700/6658 (71%)]\tLoss: 0.340413\n",
      "Train Epoch: 59 [4800/6658 (72%)]\tLoss: 5.691654\n",
      "Train Epoch: 59 [4900/6658 (74%)]\tLoss: 0.048338\n",
      "Train Epoch: 59 [5000/6658 (75%)]\tLoss: 0.122615\n",
      "Train Epoch: 59 [5100/6658 (77%)]\tLoss: 10.250483\n",
      "Train Epoch: 59 [5200/6658 (78%)]\tLoss: 1.557250\n",
      "Train Epoch: 59 [5300/6658 (80%)]\tLoss: 0.614449\n",
      "Train Epoch: 59 [5400/6658 (81%)]\tLoss: 2.303921\n",
      "Train Epoch: 59 [5500/6658 (83%)]\tLoss: 0.165139\n",
      "Train Epoch: 59 [5600/6658 (84%)]\tLoss: 0.101983\n",
      "Train Epoch: 59 [5700/6658 (86%)]\tLoss: 0.315486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 59 [5800/6658 (87%)]\tLoss: 0.315428\n",
      "Train Epoch: 59 [5900/6658 (89%)]\tLoss: 1.711085\n",
      "Train Epoch: 59 [6000/6658 (90%)]\tLoss: 5.728524\n",
      "Train Epoch: 59 [6100/6658 (92%)]\tLoss: 1.811577\n",
      "Train Epoch: 59 [6200/6658 (93%)]\tLoss: 0.066622\n",
      "Train Epoch: 59 [6300/6658 (95%)]\tLoss: 0.059599\n",
      "Train Epoch: 59 [6400/6658 (96%)]\tLoss: 0.146047\n",
      "Train Epoch: 59 [6500/6658 (98%)]\tLoss: 0.000179\n",
      "Train Epoch: 59 [6600/6658 (99%)]\tLoss: 0.667311\n",
      "train loss average =  0.7325057688458656\n",
      "\n",
      "Test set: Average loss: 0.7185\n",
      "\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1371, 6.0391, 5.9245, 5.9160, 6.0817, 6.0374], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 60 [0/6658 (0%)]\tLoss: 0.726261\n",
      "Train Epoch: 60 [100/6658 (2%)]\tLoss: 0.005299\n",
      "Train Epoch: 60 [200/6658 (3%)]\tLoss: 0.006749\n",
      "Train Epoch: 60 [300/6658 (5%)]\tLoss: 0.898562\n",
      "Train Epoch: 60 [400/6658 (6%)]\tLoss: 0.277655\n",
      "Train Epoch: 60 [500/6658 (8%)]\tLoss: 0.080830\n",
      "Train Epoch: 60 [600/6658 (9%)]\tLoss: 1.797726\n",
      "Train Epoch: 60 [700/6658 (11%)]\tLoss: 0.002913\n",
      "Train Epoch: 60 [800/6658 (12%)]\tLoss: 0.009665\n",
      "Train Epoch: 60 [900/6658 (14%)]\tLoss: 1.182186\n",
      "Train Epoch: 60 [1000/6658 (15%)]\tLoss: 0.019810\n",
      "Train Epoch: 60 [1100/6658 (17%)]\tLoss: 0.082903\n",
      "Train Epoch: 60 [1200/6658 (18%)]\tLoss: 1.084464\n",
      "Train Epoch: 60 [1300/6658 (20%)]\tLoss: 0.006118\n",
      "Train Epoch: 60 [1400/6658 (21%)]\tLoss: 0.211330\n",
      "Train Epoch: 60 [1500/6658 (23%)]\tLoss: 0.149754\n",
      "Train Epoch: 60 [1600/6658 (24%)]\tLoss: 0.008818\n",
      "Train Epoch: 60 [1700/6658 (26%)]\tLoss: 1.570445\n",
      "Train Epoch: 60 [1800/6658 (27%)]\tLoss: 0.002183\n",
      "Train Epoch: 60 [1900/6658 (29%)]\tLoss: 2.283770\n",
      "Train Epoch: 60 [2000/6658 (30%)]\tLoss: 0.207503\n",
      "Train Epoch: 60 [2100/6658 (32%)]\tLoss: 0.603725\n",
      "Train Epoch: 60 [2200/6658 (33%)]\tLoss: 0.183006\n",
      "Train Epoch: 60 [2300/6658 (35%)]\tLoss: 0.177969\n",
      "Train Epoch: 60 [2400/6658 (36%)]\tLoss: 0.000371\n",
      "Train Epoch: 60 [2500/6658 (38%)]\tLoss: 1.960401\n",
      "Train Epoch: 60 [2600/6658 (39%)]\tLoss: 0.054333\n",
      "Train Epoch: 60 [2700/6658 (41%)]\tLoss: 0.459492\n",
      "Train Epoch: 60 [2800/6658 (42%)]\tLoss: 0.721080\n",
      "Train Epoch: 60 [2900/6658 (44%)]\tLoss: 0.436488\n",
      "Train Epoch: 60 [3000/6658 (45%)]\tLoss: 0.238029\n",
      "Train Epoch: 60 [3100/6658 (47%)]\tLoss: 0.849124\n",
      "Train Epoch: 60 [3200/6658 (48%)]\tLoss: 0.188188\n",
      "Train Epoch: 60 [3300/6658 (50%)]\tLoss: 1.601609\n",
      "Train Epoch: 60 [3400/6658 (51%)]\tLoss: 0.760769\n",
      "Train Epoch: 60 [3500/6658 (53%)]\tLoss: 0.025684\n",
      "Train Epoch: 60 [3600/6658 (54%)]\tLoss: 1.159210\n",
      "Train Epoch: 60 [3700/6658 (56%)]\tLoss: 0.126068\n",
      "Train Epoch: 60 [3800/6658 (57%)]\tLoss: 3.623139\n",
      "Train Epoch: 60 [3900/6658 (59%)]\tLoss: 0.333488\n",
      "Train Epoch: 60 [4000/6658 (60%)]\tLoss: 5.613344\n",
      "Train Epoch: 60 [4100/6658 (62%)]\tLoss: 3.685158\n",
      "Train Epoch: 60 [4200/6658 (63%)]\tLoss: 3.620567\n",
      "Train Epoch: 60 [4300/6658 (65%)]\tLoss: 0.205739\n",
      "Train Epoch: 60 [4400/6658 (66%)]\tLoss: 0.117097\n",
      "Train Epoch: 60 [4500/6658 (68%)]\tLoss: 0.460527\n",
      "Train Epoch: 60 [4600/6658 (69%)]\tLoss: 0.037593\n",
      "Train Epoch: 60 [4700/6658 (71%)]\tLoss: 0.039665\n",
      "Train Epoch: 60 [4800/6658 (72%)]\tLoss: 0.175842\n",
      "Train Epoch: 60 [4900/6658 (74%)]\tLoss: 0.430898\n",
      "Train Epoch: 60 [5000/6658 (75%)]\tLoss: 0.913568\n",
      "Train Epoch: 60 [5100/6658 (77%)]\tLoss: 0.064206\n",
      "Train Epoch: 60 [5200/6658 (78%)]\tLoss: 0.205948\n",
      "Train Epoch: 60 [5300/6658 (80%)]\tLoss: 0.067323\n",
      "Train Epoch: 60 [5400/6658 (81%)]\tLoss: 0.270166\n",
      "Train Epoch: 60 [5500/6658 (83%)]\tLoss: 0.111132\n",
      "Train Epoch: 60 [5600/6658 (84%)]\tLoss: 0.060021\n",
      "Train Epoch: 60 [5700/6658 (86%)]\tLoss: 0.031512\n",
      "Train Epoch: 60 [5800/6658 (87%)]\tLoss: 0.112150\n",
      "Train Epoch: 60 [5900/6658 (89%)]\tLoss: 0.401161\n",
      "Train Epoch: 60 [6000/6658 (90%)]\tLoss: 0.067701\n",
      "Train Epoch: 60 [6100/6658 (92%)]\tLoss: 0.306023\n",
      "Train Epoch: 60 [6200/6658 (93%)]\tLoss: 0.179380\n",
      "Train Epoch: 60 [6300/6658 (95%)]\tLoss: 0.067622\n",
      "Train Epoch: 60 [6400/6658 (96%)]\tLoss: 0.001839\n",
      "Train Epoch: 60 [6500/6658 (98%)]\tLoss: 0.591545\n",
      "Train Epoch: 60 [6600/6658 (99%)]\tLoss: 0.105963\n",
      "train loss average =  0.7321041351580951\n",
      "\n",
      "Test set: Average loss: 0.7015\n",
      "\n",
      "Validation loss decreased (0.701613 --> 0.701479).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.1389, 6.0401, 5.9236, 5.9154, 6.0832, 6.0382], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 61 [0/6658 (0%)]\tLoss: 0.001602\n",
      "Train Epoch: 61 [100/6658 (2%)]\tLoss: 0.441910\n",
      "Train Epoch: 61 [200/6658 (3%)]\tLoss: 1.884905\n",
      "Train Epoch: 61 [300/6658 (5%)]\tLoss: 0.146344\n",
      "Train Epoch: 61 [400/6658 (6%)]\tLoss: 1.756699\n",
      "Train Epoch: 61 [500/6658 (8%)]\tLoss: 0.172274\n",
      "Train Epoch: 61 [600/6658 (9%)]\tLoss: 2.810952\n",
      "Train Epoch: 61 [700/6658 (11%)]\tLoss: 0.528964\n",
      "Train Epoch: 61 [800/6658 (12%)]\tLoss: 0.001443\n",
      "Train Epoch: 61 [900/6658 (14%)]\tLoss: 0.016620\n",
      "Train Epoch: 61 [1000/6658 (15%)]\tLoss: 0.223529\n",
      "Train Epoch: 61 [1100/6658 (17%)]\tLoss: 3.144778\n",
      "Train Epoch: 61 [1200/6658 (18%)]\tLoss: 0.159154\n",
      "Train Epoch: 61 [1300/6658 (20%)]\tLoss: 0.253020\n",
      "Train Epoch: 61 [1400/6658 (21%)]\tLoss: 6.190299\n",
      "Train Epoch: 61 [1500/6658 (23%)]\tLoss: 0.173012\n",
      "Train Epoch: 61 [1600/6658 (24%)]\tLoss: 0.363928\n",
      "Train Epoch: 61 [1700/6658 (26%)]\tLoss: 0.279482\n",
      "Train Epoch: 61 [1800/6658 (27%)]\tLoss: 0.000031\n",
      "Train Epoch: 61 [1900/6658 (29%)]\tLoss: 0.001374\n",
      "Train Epoch: 61 [2000/6658 (30%)]\tLoss: 1.105750\n",
      "Train Epoch: 61 [2100/6658 (32%)]\tLoss: 1.539427\n",
      "Train Epoch: 61 [2200/6658 (33%)]\tLoss: 0.000748\n",
      "Train Epoch: 61 [2300/6658 (35%)]\tLoss: 3.916348\n",
      "Train Epoch: 61 [2400/6658 (36%)]\tLoss: 0.009292\n",
      "Train Epoch: 61 [2500/6658 (38%)]\tLoss: 1.056800\n",
      "Train Epoch: 61 [2600/6658 (39%)]\tLoss: 0.058574\n",
      "Train Epoch: 61 [2700/6658 (41%)]\tLoss: 0.109742\n",
      "Train Epoch: 61 [2800/6658 (42%)]\tLoss: 0.007467\n",
      "Train Epoch: 61 [2900/6658 (44%)]\tLoss: 0.291793\n",
      "Train Epoch: 61 [3000/6658 (45%)]\tLoss: 0.356328\n",
      "Train Epoch: 61 [3100/6658 (47%)]\tLoss: 0.618401\n",
      "Train Epoch: 61 [3200/6658 (48%)]\tLoss: 1.128308\n",
      "Train Epoch: 61 [3300/6658 (50%)]\tLoss: 3.153568\n",
      "Train Epoch: 61 [3400/6658 (51%)]\tLoss: 0.006794\n",
      "Train Epoch: 61 [3500/6658 (53%)]\tLoss: 2.863844\n",
      "Train Epoch: 61 [3600/6658 (54%)]\tLoss: 0.849617\n",
      "Train Epoch: 61 [3700/6658 (56%)]\tLoss: 0.173972\n",
      "Train Epoch: 61 [3800/6658 (57%)]\tLoss: 0.014538\n",
      "Train Epoch: 61 [3900/6658 (59%)]\tLoss: 0.454240\n",
      "Train Epoch: 61 [4000/6658 (60%)]\tLoss: 0.001792\n",
      "Train Epoch: 61 [4100/6658 (62%)]\tLoss: 0.067251\n",
      "Train Epoch: 61 [4200/6658 (63%)]\tLoss: 0.005542\n",
      "Train Epoch: 61 [4300/6658 (65%)]\tLoss: 0.036304\n",
      "Train Epoch: 61 [4400/6658 (66%)]\tLoss: 0.477822\n",
      "Train Epoch: 61 [4500/6658 (68%)]\tLoss: 0.291503\n",
      "Train Epoch: 61 [4600/6658 (69%)]\tLoss: 0.004724\n",
      "Train Epoch: 61 [4700/6658 (71%)]\tLoss: 0.163252\n",
      "Train Epoch: 61 [4800/6658 (72%)]\tLoss: 0.358225\n",
      "Train Epoch: 61 [4900/6658 (74%)]\tLoss: 0.451274\n",
      "Train Epoch: 61 [5000/6658 (75%)]\tLoss: 0.645062\n",
      "Train Epoch: 61 [5100/6658 (77%)]\tLoss: 0.255664\n",
      "Train Epoch: 61 [5200/6658 (78%)]\tLoss: 0.035969\n",
      "Train Epoch: 61 [5300/6658 (80%)]\tLoss: 0.265330\n",
      "Train Epoch: 61 [5400/6658 (81%)]\tLoss: 0.084456\n",
      "Train Epoch: 61 [5500/6658 (83%)]\tLoss: 0.855886\n",
      "Train Epoch: 61 [5600/6658 (84%)]\tLoss: 0.000174\n",
      "Train Epoch: 61 [5700/6658 (86%)]\tLoss: 0.012784\n",
      "Train Epoch: 61 [5800/6658 (87%)]\tLoss: 0.044882\n",
      "Train Epoch: 61 [5900/6658 (89%)]\tLoss: 0.545002\n",
      "Train Epoch: 61 [6000/6658 (90%)]\tLoss: 0.016710\n",
      "Train Epoch: 61 [6100/6658 (92%)]\tLoss: 0.678925\n",
      "Train Epoch: 61 [6200/6658 (93%)]\tLoss: 0.209787\n",
      "Train Epoch: 61 [6300/6658 (95%)]\tLoss: 2.195464\n",
      "Train Epoch: 61 [6400/6658 (96%)]\tLoss: 1.305575\n",
      "Train Epoch: 61 [6500/6658 (98%)]\tLoss: 0.022868\n",
      "Train Epoch: 61 [6600/6658 (99%)]\tLoss: 2.430133\n",
      "train loss average =  0.7226871935335002\n",
      "\n",
      "Test set: Average loss: 0.7125\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1413, 6.0404, 5.9234, 5.9149, 6.0845, 6.0380], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 62 [0/6658 (0%)]\tLoss: 0.926377\n",
      "Train Epoch: 62 [100/6658 (2%)]\tLoss: 0.794643\n",
      "Train Epoch: 62 [200/6658 (3%)]\tLoss: 0.144726\n",
      "Train Epoch: 62 [300/6658 (5%)]\tLoss: 0.566224\n",
      "Train Epoch: 62 [400/6658 (6%)]\tLoss: 0.022701\n",
      "Train Epoch: 62 [500/6658 (8%)]\tLoss: 0.197608\n",
      "Train Epoch: 62 [600/6658 (9%)]\tLoss: 0.312755\n",
      "Train Epoch: 62 [700/6658 (11%)]\tLoss: 0.634097\n",
      "Train Epoch: 62 [800/6658 (12%)]\tLoss: 0.245648\n",
      "Train Epoch: 62 [900/6658 (14%)]\tLoss: 0.082546\n",
      "Train Epoch: 62 [1000/6658 (15%)]\tLoss: 0.965215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 62 [1100/6658 (17%)]\tLoss: 7.999719\n",
      "Train Epoch: 62 [1200/6658 (18%)]\tLoss: 0.023643\n",
      "Train Epoch: 62 [1300/6658 (20%)]\tLoss: 0.924736\n",
      "Train Epoch: 62 [1400/6658 (21%)]\tLoss: 0.066593\n",
      "Train Epoch: 62 [1500/6658 (23%)]\tLoss: 0.072157\n",
      "Train Epoch: 62 [1600/6658 (24%)]\tLoss: 0.330457\n",
      "Train Epoch: 62 [1700/6658 (26%)]\tLoss: 1.093942\n",
      "Train Epoch: 62 [1800/6658 (27%)]\tLoss: 0.270212\n",
      "Train Epoch: 62 [1900/6658 (29%)]\tLoss: 1.791559\n",
      "Train Epoch: 62 [2000/6658 (30%)]\tLoss: 0.156374\n",
      "Train Epoch: 62 [2100/6658 (32%)]\tLoss: 1.149654\n",
      "Train Epoch: 62 [2200/6658 (33%)]\tLoss: 0.346677\n",
      "Train Epoch: 62 [2300/6658 (35%)]\tLoss: 0.406860\n",
      "Train Epoch: 62 [2400/6658 (36%)]\tLoss: 2.489412\n",
      "Train Epoch: 62 [2500/6658 (38%)]\tLoss: 0.006798\n",
      "Train Epoch: 62 [2600/6658 (39%)]\tLoss: 0.012518\n",
      "Train Epoch: 62 [2700/6658 (41%)]\tLoss: 0.670726\n",
      "Train Epoch: 62 [2800/6658 (42%)]\tLoss: 1.846021\n",
      "Train Epoch: 62 [2900/6658 (44%)]\tLoss: 0.064292\n",
      "Train Epoch: 62 [3000/6658 (45%)]\tLoss: 0.074825\n",
      "Train Epoch: 62 [3100/6658 (47%)]\tLoss: 0.346396\n",
      "Train Epoch: 62 [3200/6658 (48%)]\tLoss: 0.099088\n",
      "Train Epoch: 62 [3300/6658 (50%)]\tLoss: 0.117450\n",
      "Train Epoch: 62 [3400/6658 (51%)]\tLoss: 0.161545\n",
      "Train Epoch: 62 [3500/6658 (53%)]\tLoss: 0.216140\n",
      "Train Epoch: 62 [3600/6658 (54%)]\tLoss: 0.122180\n",
      "Train Epoch: 62 [3700/6658 (56%)]\tLoss: 0.012444\n",
      "Train Epoch: 62 [3800/6658 (57%)]\tLoss: 0.198156\n",
      "Train Epoch: 62 [3900/6658 (59%)]\tLoss: 0.032158\n",
      "Train Epoch: 62 [4000/6658 (60%)]\tLoss: 0.149057\n",
      "Train Epoch: 62 [4100/6658 (62%)]\tLoss: 0.111368\n",
      "Train Epoch: 62 [4200/6658 (63%)]\tLoss: 0.000106\n",
      "Train Epoch: 62 [4300/6658 (65%)]\tLoss: 0.660040\n",
      "Train Epoch: 62 [4400/6658 (66%)]\tLoss: 0.715110\n",
      "Train Epoch: 62 [4500/6658 (68%)]\tLoss: 0.548698\n",
      "Train Epoch: 62 [4600/6658 (69%)]\tLoss: 0.101094\n",
      "Train Epoch: 62 [4700/6658 (71%)]\tLoss: 0.033652\n",
      "Train Epoch: 62 [4800/6658 (72%)]\tLoss: 0.357402\n",
      "Train Epoch: 62 [4900/6658 (74%)]\tLoss: 2.620391\n",
      "Train Epoch: 62 [5000/6658 (75%)]\tLoss: 1.238808\n",
      "Train Epoch: 62 [5100/6658 (77%)]\tLoss: 0.112983\n",
      "Train Epoch: 62 [5200/6658 (78%)]\tLoss: 0.000006\n",
      "Train Epoch: 62 [5300/6658 (80%)]\tLoss: 2.568176\n",
      "Train Epoch: 62 [5400/6658 (81%)]\tLoss: 0.094342\n",
      "Train Epoch: 62 [5500/6658 (83%)]\tLoss: 0.665280\n",
      "Train Epoch: 62 [5600/6658 (84%)]\tLoss: 0.970234\n",
      "Train Epoch: 62 [5700/6658 (86%)]\tLoss: 9.435148\n",
      "Train Epoch: 62 [5800/6658 (87%)]\tLoss: 0.439829\n",
      "Train Epoch: 62 [5900/6658 (89%)]\tLoss: 0.452280\n",
      "Train Epoch: 62 [6000/6658 (90%)]\tLoss: 0.000680\n",
      "Train Epoch: 62 [6100/6658 (92%)]\tLoss: 0.897412\n",
      "Train Epoch: 62 [6200/6658 (93%)]\tLoss: 0.149727\n",
      "Train Epoch: 62 [6300/6658 (95%)]\tLoss: 0.018916\n",
      "Train Epoch: 62 [6400/6658 (96%)]\tLoss: 0.210316\n",
      "Train Epoch: 62 [6500/6658 (98%)]\tLoss: 0.205423\n",
      "Train Epoch: 62 [6600/6658 (99%)]\tLoss: 0.032218\n",
      "train loss average =  0.7282945256851191\n",
      "\n",
      "Test set: Average loss: 0.7125\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1426, 6.0410, 5.9218, 5.9140, 6.0856, 6.0383], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 63 [0/6658 (0%)]\tLoss: 0.933368\n",
      "Train Epoch: 63 [100/6658 (2%)]\tLoss: 0.051462\n",
      "Train Epoch: 63 [200/6658 (3%)]\tLoss: 0.016713\n",
      "Train Epoch: 63 [300/6658 (5%)]\tLoss: 0.106886\n",
      "Train Epoch: 63 [400/6658 (6%)]\tLoss: 0.088340\n",
      "Train Epoch: 63 [500/6658 (8%)]\tLoss: 22.635681\n",
      "Train Epoch: 63 [600/6658 (9%)]\tLoss: 0.549365\n",
      "Train Epoch: 63 [700/6658 (11%)]\tLoss: 0.013988\n",
      "Train Epoch: 63 [800/6658 (12%)]\tLoss: 2.988211\n",
      "Train Epoch: 63 [900/6658 (14%)]\tLoss: 0.036984\n",
      "Train Epoch: 63 [1000/6658 (15%)]\tLoss: 2.619620\n",
      "Train Epoch: 63 [1100/6658 (17%)]\tLoss: 1.876533\n",
      "Train Epoch: 63 [1200/6658 (18%)]\tLoss: 0.015340\n",
      "Train Epoch: 63 [1300/6658 (20%)]\tLoss: 0.031544\n",
      "Train Epoch: 63 [1400/6658 (21%)]\tLoss: 0.700279\n",
      "Train Epoch: 63 [1500/6658 (23%)]\tLoss: 0.442096\n",
      "Train Epoch: 63 [1600/6658 (24%)]\tLoss: 0.126117\n",
      "Train Epoch: 63 [1700/6658 (26%)]\tLoss: 0.693467\n",
      "Train Epoch: 63 [1800/6658 (27%)]\tLoss: 3.186007\n",
      "Train Epoch: 63 [1900/6658 (29%)]\tLoss: 0.006024\n",
      "Train Epoch: 63 [2000/6658 (30%)]\tLoss: 0.228123\n",
      "Train Epoch: 63 [2100/6658 (32%)]\tLoss: 0.910240\n",
      "Train Epoch: 63 [2200/6658 (33%)]\tLoss: 0.051830\n",
      "Train Epoch: 63 [2300/6658 (35%)]\tLoss: 0.098356\n",
      "Train Epoch: 63 [2400/6658 (36%)]\tLoss: 0.347669\n",
      "Train Epoch: 63 [2500/6658 (38%)]\tLoss: 0.302209\n",
      "Train Epoch: 63 [2600/6658 (39%)]\tLoss: 1.000860\n",
      "Train Epoch: 63 [2700/6658 (41%)]\tLoss: 1.375659\n",
      "Train Epoch: 63 [2800/6658 (42%)]\tLoss: 0.233058\n",
      "Train Epoch: 63 [2900/6658 (44%)]\tLoss: 0.239137\n",
      "Train Epoch: 63 [3000/6658 (45%)]\tLoss: 0.080364\n",
      "Train Epoch: 63 [3100/6658 (47%)]\tLoss: 1.795857\n",
      "Train Epoch: 63 [3200/6658 (48%)]\tLoss: 0.107277\n",
      "Train Epoch: 63 [3300/6658 (50%)]\tLoss: 2.026359\n",
      "Train Epoch: 63 [3400/6658 (51%)]\tLoss: 0.084299\n",
      "Train Epoch: 63 [3500/6658 (53%)]\tLoss: 0.342255\n",
      "Train Epoch: 63 [3600/6658 (54%)]\tLoss: 0.007484\n",
      "Train Epoch: 63 [3700/6658 (56%)]\tLoss: 1.143553\n",
      "Train Epoch: 63 [3800/6658 (57%)]\tLoss: 1.396067\n",
      "Train Epoch: 63 [3900/6658 (59%)]\tLoss: 0.589421\n",
      "Train Epoch: 63 [4000/6658 (60%)]\tLoss: 0.438456\n",
      "Train Epoch: 63 [4100/6658 (62%)]\tLoss: 0.005042\n",
      "Train Epoch: 63 [4200/6658 (63%)]\tLoss: 0.273161\n",
      "Train Epoch: 63 [4300/6658 (65%)]\tLoss: 0.384780\n",
      "Train Epoch: 63 [4400/6658 (66%)]\tLoss: 0.623647\n",
      "Train Epoch: 63 [4500/6658 (68%)]\tLoss: 0.776426\n",
      "Train Epoch: 63 [4600/6658 (69%)]\tLoss: 0.074133\n",
      "Train Epoch: 63 [4700/6658 (71%)]\tLoss: 0.000715\n",
      "Train Epoch: 63 [4800/6658 (72%)]\tLoss: 0.422024\n",
      "Train Epoch: 63 [4900/6658 (74%)]\tLoss: 0.261184\n",
      "Train Epoch: 63 [5000/6658 (75%)]\tLoss: 0.315166\n",
      "Train Epoch: 63 [5100/6658 (77%)]\tLoss: 0.212778\n",
      "Train Epoch: 63 [5200/6658 (78%)]\tLoss: 0.311904\n",
      "Train Epoch: 63 [5300/6658 (80%)]\tLoss: 0.018064\n",
      "Train Epoch: 63 [5400/6658 (81%)]\tLoss: 0.185019\n",
      "Train Epoch: 63 [5500/6658 (83%)]\tLoss: 0.293296\n",
      "Train Epoch: 63 [5600/6658 (84%)]\tLoss: 0.086377\n",
      "Train Epoch: 63 [5700/6658 (86%)]\tLoss: 0.043532\n",
      "Train Epoch: 63 [5800/6658 (87%)]\tLoss: 0.366184\n",
      "Train Epoch: 63 [5900/6658 (89%)]\tLoss: 0.150093\n",
      "Train Epoch: 63 [6000/6658 (90%)]\tLoss: 0.153920\n",
      "Train Epoch: 63 [6100/6658 (92%)]\tLoss: 0.641212\n",
      "Train Epoch: 63 [6200/6658 (93%)]\tLoss: 0.737878\n",
      "Train Epoch: 63 [6300/6658 (95%)]\tLoss: 0.001939\n",
      "Train Epoch: 63 [6400/6658 (96%)]\tLoss: 0.434340\n",
      "Train Epoch: 63 [6500/6658 (98%)]\tLoss: 3.349724\n",
      "Train Epoch: 63 [6600/6658 (99%)]\tLoss: 0.691972\n",
      "train loss average =  0.7280391492217033\n",
      "\n",
      "Test set: Average loss: 0.7211\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1451, 6.0413, 5.9209, 5.9130, 6.0865, 6.0381], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 64 [0/6658 (0%)]\tLoss: 0.004424\n",
      "Train Epoch: 64 [100/6658 (2%)]\tLoss: 0.005936\n",
      "Train Epoch: 64 [200/6658 (3%)]\tLoss: 0.652260\n",
      "Train Epoch: 64 [300/6658 (5%)]\tLoss: 0.419107\n",
      "Train Epoch: 64 [400/6658 (6%)]\tLoss: 0.007945\n",
      "Train Epoch: 64 [500/6658 (8%)]\tLoss: 0.005172\n",
      "Train Epoch: 64 [600/6658 (9%)]\tLoss: 1.792880\n",
      "Train Epoch: 64 [700/6658 (11%)]\tLoss: 0.447042\n",
      "Train Epoch: 64 [800/6658 (12%)]\tLoss: 0.118769\n",
      "Train Epoch: 64 [900/6658 (14%)]\tLoss: 5.586749\n",
      "Train Epoch: 64 [1000/6658 (15%)]\tLoss: 0.003121\n",
      "Train Epoch: 64 [1100/6658 (17%)]\tLoss: 0.447841\n",
      "Train Epoch: 64 [1200/6658 (18%)]\tLoss: 0.093230\n",
      "Train Epoch: 64 [1300/6658 (20%)]\tLoss: 0.908155\n",
      "Train Epoch: 64 [1400/6658 (21%)]\tLoss: 0.333193\n",
      "Train Epoch: 64 [1500/6658 (23%)]\tLoss: 0.200664\n",
      "Train Epoch: 64 [1600/6658 (24%)]\tLoss: 0.083150\n",
      "Train Epoch: 64 [1700/6658 (26%)]\tLoss: 0.952842\n",
      "Train Epoch: 64 [1800/6658 (27%)]\tLoss: 1.501463\n",
      "Train Epoch: 64 [1900/6658 (29%)]\tLoss: 0.000077\n",
      "Train Epoch: 64 [2000/6658 (30%)]\tLoss: 0.365947\n",
      "Train Epoch: 64 [2100/6658 (32%)]\tLoss: 0.001053\n",
      "Train Epoch: 64 [2200/6658 (33%)]\tLoss: 1.234625\n",
      "Train Epoch: 64 [2300/6658 (35%)]\tLoss: 0.071896\n",
      "Train Epoch: 64 [2400/6658 (36%)]\tLoss: 0.328474\n",
      "Train Epoch: 64 [2500/6658 (38%)]\tLoss: 0.543776\n",
      "Train Epoch: 64 [2600/6658 (39%)]\tLoss: 0.008157\n",
      "Train Epoch: 64 [2700/6658 (41%)]\tLoss: 0.388765\n",
      "Train Epoch: 64 [2800/6658 (42%)]\tLoss: 2.405811\n",
      "Train Epoch: 64 [2900/6658 (44%)]\tLoss: 0.575711\n",
      "Train Epoch: 64 [3000/6658 (45%)]\tLoss: 0.599059\n",
      "Train Epoch: 64 [3100/6658 (47%)]\tLoss: 0.018909\n",
      "Train Epoch: 64 [3200/6658 (48%)]\tLoss: 0.001730\n",
      "Train Epoch: 64 [3300/6658 (50%)]\tLoss: 2.201986\n",
      "Train Epoch: 64 [3400/6658 (51%)]\tLoss: 0.004326\n",
      "Train Epoch: 64 [3500/6658 (53%)]\tLoss: 0.070274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 64 [3600/6658 (54%)]\tLoss: 0.051684\n",
      "Train Epoch: 64 [3700/6658 (56%)]\tLoss: 0.577850\n",
      "Train Epoch: 64 [3800/6658 (57%)]\tLoss: 1.279754\n",
      "Train Epoch: 64 [3900/6658 (59%)]\tLoss: 0.558699\n",
      "Train Epoch: 64 [4000/6658 (60%)]\tLoss: 0.725702\n",
      "Train Epoch: 64 [4100/6658 (62%)]\tLoss: 0.160825\n",
      "Train Epoch: 64 [4200/6658 (63%)]\tLoss: 0.086658\n",
      "Train Epoch: 64 [4300/6658 (65%)]\tLoss: 0.510064\n",
      "Train Epoch: 64 [4400/6658 (66%)]\tLoss: 0.022657\n",
      "Train Epoch: 64 [4500/6658 (68%)]\tLoss: 0.706010\n",
      "Train Epoch: 64 [4600/6658 (69%)]\tLoss: 0.063162\n",
      "Train Epoch: 64 [4700/6658 (71%)]\tLoss: 1.087854\n",
      "Train Epoch: 64 [4800/6658 (72%)]\tLoss: 0.047871\n",
      "Train Epoch: 64 [4900/6658 (74%)]\tLoss: 0.906636\n",
      "Train Epoch: 64 [5000/6658 (75%)]\tLoss: 1.107776\n",
      "Train Epoch: 64 [5100/6658 (77%)]\tLoss: 0.695584\n",
      "Train Epoch: 64 [5200/6658 (78%)]\tLoss: 0.612616\n",
      "Train Epoch: 64 [5300/6658 (80%)]\tLoss: 7.645853\n",
      "Train Epoch: 64 [5400/6658 (81%)]\tLoss: 0.661732\n",
      "Train Epoch: 64 [5500/6658 (83%)]\tLoss: 0.166474\n",
      "Train Epoch: 64 [5600/6658 (84%)]\tLoss: 0.644614\n",
      "Train Epoch: 64 [5700/6658 (86%)]\tLoss: 0.190594\n",
      "Train Epoch: 64 [5800/6658 (87%)]\tLoss: 0.378892\n",
      "Train Epoch: 64 [5900/6658 (89%)]\tLoss: 0.169447\n",
      "Train Epoch: 64 [6000/6658 (90%)]\tLoss: 0.098923\n",
      "Train Epoch: 64 [6100/6658 (92%)]\tLoss: 1.312524\n",
      "Train Epoch: 64 [6200/6658 (93%)]\tLoss: 0.899068\n",
      "Train Epoch: 64 [6300/6658 (95%)]\tLoss: 1.202166\n",
      "Train Epoch: 64 [6400/6658 (96%)]\tLoss: 0.470681\n",
      "Train Epoch: 64 [6500/6658 (98%)]\tLoss: 0.314276\n",
      "Train Epoch: 64 [6600/6658 (99%)]\tLoss: 0.185997\n",
      "train loss average =  0.7236966649630556\n",
      "\n",
      "Test set: Average loss: 0.7204\n",
      "\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1481, 6.0417, 5.9201, 5.9120, 6.0883, 6.0388], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 65 [0/6658 (0%)]\tLoss: 0.000027\n",
      "Train Epoch: 65 [100/6658 (2%)]\tLoss: 0.071756\n",
      "Train Epoch: 65 [200/6658 (3%)]\tLoss: 0.000711\n",
      "Train Epoch: 65 [300/6658 (5%)]\tLoss: 2.057464\n",
      "Train Epoch: 65 [400/6658 (6%)]\tLoss: 2.751224\n",
      "Train Epoch: 65 [500/6658 (8%)]\tLoss: 0.000763\n",
      "Train Epoch: 65 [600/6658 (9%)]\tLoss: 1.247755\n",
      "Train Epoch: 65 [700/6658 (11%)]\tLoss: 0.519727\n",
      "Train Epoch: 65 [800/6658 (12%)]\tLoss: 0.277179\n",
      "Train Epoch: 65 [900/6658 (14%)]\tLoss: 1.314988\n",
      "Train Epoch: 65 [1000/6658 (15%)]\tLoss: 0.488156\n",
      "Train Epoch: 65 [1100/6658 (17%)]\tLoss: 0.233359\n",
      "Train Epoch: 65 [1200/6658 (18%)]\tLoss: 1.329323\n",
      "Train Epoch: 65 [1300/6658 (20%)]\tLoss: 0.401403\n",
      "Train Epoch: 65 [1400/6658 (21%)]\tLoss: 1.402678\n",
      "Train Epoch: 65 [1500/6658 (23%)]\tLoss: 0.002599\n",
      "Train Epoch: 65 [1600/6658 (24%)]\tLoss: 0.747210\n",
      "Train Epoch: 65 [1700/6658 (26%)]\tLoss: 0.367569\n",
      "Train Epoch: 65 [1800/6658 (27%)]\tLoss: 0.163914\n",
      "Train Epoch: 65 [1900/6658 (29%)]\tLoss: 0.013068\n",
      "Train Epoch: 65 [2000/6658 (30%)]\tLoss: 0.530290\n",
      "Train Epoch: 65 [2100/6658 (32%)]\tLoss: 1.102835\n",
      "Train Epoch: 65 [2200/6658 (33%)]\tLoss: 0.415538\n",
      "Train Epoch: 65 [2300/6658 (35%)]\tLoss: 29.388689\n",
      "Train Epoch: 65 [2400/6658 (36%)]\tLoss: 0.194700\n",
      "Train Epoch: 65 [2500/6658 (38%)]\tLoss: 0.301801\n",
      "Train Epoch: 65 [2600/6658 (39%)]\tLoss: 1.967537\n",
      "Train Epoch: 65 [2700/6658 (41%)]\tLoss: 0.099006\n",
      "Train Epoch: 65 [2800/6658 (42%)]\tLoss: 0.025865\n",
      "Train Epoch: 65 [2900/6658 (44%)]\tLoss: 0.018124\n",
      "Train Epoch: 65 [3000/6658 (45%)]\tLoss: 0.444879\n",
      "Train Epoch: 65 [3100/6658 (47%)]\tLoss: 0.571033\n",
      "Train Epoch: 65 [3200/6658 (48%)]\tLoss: 0.124603\n",
      "Train Epoch: 65 [3300/6658 (50%)]\tLoss: 2.778554\n",
      "Train Epoch: 65 [3400/6658 (51%)]\tLoss: 1.000998\n",
      "Train Epoch: 65 [3500/6658 (53%)]\tLoss: 0.162680\n",
      "Train Epoch: 65 [3600/6658 (54%)]\tLoss: 0.900589\n",
      "Train Epoch: 65 [3700/6658 (56%)]\tLoss: 1.392993\n",
      "Train Epoch: 65 [3800/6658 (57%)]\tLoss: 0.550835\n",
      "Train Epoch: 65 [3900/6658 (59%)]\tLoss: 0.005783\n",
      "Train Epoch: 65 [4000/6658 (60%)]\tLoss: 0.091891\n",
      "Train Epoch: 65 [4100/6658 (62%)]\tLoss: 1.396044\n",
      "Train Epoch: 65 [4200/6658 (63%)]\tLoss: 1.098064\n",
      "Train Epoch: 65 [4300/6658 (65%)]\tLoss: 0.305166\n",
      "Train Epoch: 65 [4400/6658 (66%)]\tLoss: 0.403542\n",
      "Train Epoch: 65 [4500/6658 (68%)]\tLoss: 0.194812\n",
      "Train Epoch: 65 [4600/6658 (69%)]\tLoss: 3.301204\n",
      "Train Epoch: 65 [4700/6658 (71%)]\tLoss: 2.593257\n",
      "Train Epoch: 65 [4800/6658 (72%)]\tLoss: 0.137081\n",
      "Train Epoch: 65 [4900/6658 (74%)]\tLoss: 2.564199\n",
      "Train Epoch: 65 [5000/6658 (75%)]\tLoss: 0.340386\n",
      "Train Epoch: 65 [5100/6658 (77%)]\tLoss: 0.842205\n",
      "Train Epoch: 65 [5200/6658 (78%)]\tLoss: 0.774735\n",
      "Train Epoch: 65 [5300/6658 (80%)]\tLoss: 0.019935\n",
      "Train Epoch: 65 [5400/6658 (81%)]\tLoss: 0.203120\n",
      "Train Epoch: 65 [5500/6658 (83%)]\tLoss: 0.312141\n",
      "Train Epoch: 65 [5600/6658 (84%)]\tLoss: 0.625948\n",
      "Train Epoch: 65 [5700/6658 (86%)]\tLoss: 0.138763\n",
      "Train Epoch: 65 [5800/6658 (87%)]\tLoss: 0.228530\n",
      "Train Epoch: 65 [5900/6658 (89%)]\tLoss: 1.740817\n",
      "Train Epoch: 65 [6000/6658 (90%)]\tLoss: 0.114458\n",
      "Train Epoch: 65 [6100/6658 (92%)]\tLoss: 1.987923\n",
      "Train Epoch: 65 [6200/6658 (93%)]\tLoss: 0.004022\n",
      "Train Epoch: 65 [6300/6658 (95%)]\tLoss: 0.375566\n",
      "Train Epoch: 65 [6400/6658 (96%)]\tLoss: 0.049817\n",
      "Train Epoch: 65 [6500/6658 (98%)]\tLoss: 0.647956\n",
      "Train Epoch: 65 [6600/6658 (99%)]\tLoss: 0.015285\n",
      "train loss average =  0.7286202224028429\n",
      "\n",
      "Test set: Average loss: 0.7029\n",
      "\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1499, 6.0427, 5.9190, 5.9110, 6.0890, 6.0391], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 66 [0/6658 (0%)]\tLoss: 7.340008\n",
      "Train Epoch: 66 [100/6658 (2%)]\tLoss: 2.147677\n",
      "Train Epoch: 66 [200/6658 (3%)]\tLoss: 0.618291\n",
      "Train Epoch: 66 [300/6658 (5%)]\tLoss: 0.114248\n",
      "Train Epoch: 66 [400/6658 (6%)]\tLoss: 2.188618\n",
      "Train Epoch: 66 [500/6658 (8%)]\tLoss: 0.041155\n",
      "Train Epoch: 66 [600/6658 (9%)]\tLoss: 0.142775\n",
      "Train Epoch: 66 [700/6658 (11%)]\tLoss: 0.120275\n",
      "Train Epoch: 66 [800/6658 (12%)]\tLoss: 2.611803\n",
      "Train Epoch: 66 [900/6658 (14%)]\tLoss: 0.199566\n",
      "Train Epoch: 66 [1000/6658 (15%)]\tLoss: 0.010070\n",
      "Train Epoch: 66 [1100/6658 (17%)]\tLoss: 0.490021\n",
      "Train Epoch: 66 [1200/6658 (18%)]\tLoss: 1.110160\n",
      "Train Epoch: 66 [1300/6658 (20%)]\tLoss: 0.027549\n",
      "Train Epoch: 66 [1400/6658 (21%)]\tLoss: 0.327665\n",
      "Train Epoch: 66 [1500/6658 (23%)]\tLoss: 0.074403\n",
      "Train Epoch: 66 [1600/6658 (24%)]\tLoss: 2.095301\n",
      "Train Epoch: 66 [1700/6658 (26%)]\tLoss: 0.133793\n",
      "Train Epoch: 66 [1800/6658 (27%)]\tLoss: 0.042409\n",
      "Train Epoch: 66 [1900/6658 (29%)]\tLoss: 0.163486\n",
      "Train Epoch: 66 [2000/6658 (30%)]\tLoss: 0.028776\n",
      "Train Epoch: 66 [2100/6658 (32%)]\tLoss: 0.067294\n",
      "Train Epoch: 66 [2200/6658 (33%)]\tLoss: 0.807850\n",
      "Train Epoch: 66 [2300/6658 (35%)]\tLoss: 0.614139\n",
      "Train Epoch: 66 [2400/6658 (36%)]\tLoss: 0.547027\n",
      "Train Epoch: 66 [2500/6658 (38%)]\tLoss: 0.398239\n",
      "Train Epoch: 66 [2600/6658 (39%)]\tLoss: 0.350650\n",
      "Train Epoch: 66 [2700/6658 (41%)]\tLoss: 0.203561\n",
      "Train Epoch: 66 [2800/6658 (42%)]\tLoss: 0.939024\n",
      "Train Epoch: 66 [2900/6658 (44%)]\tLoss: 0.103756\n",
      "Train Epoch: 66 [3000/6658 (45%)]\tLoss: 0.001817\n",
      "Train Epoch: 66 [3100/6658 (47%)]\tLoss: 0.748737\n",
      "Train Epoch: 66 [3200/6658 (48%)]\tLoss: 0.072379\n",
      "Train Epoch: 66 [3300/6658 (50%)]\tLoss: 0.272299\n",
      "Train Epoch: 66 [3400/6658 (51%)]\tLoss: 2.146446\n",
      "Train Epoch: 66 [3500/6658 (53%)]\tLoss: 0.005531\n",
      "Train Epoch: 66 [3600/6658 (54%)]\tLoss: 0.032432\n",
      "Train Epoch: 66 [3700/6658 (56%)]\tLoss: 0.013407\n",
      "Train Epoch: 66 [3800/6658 (57%)]\tLoss: 0.085494\n",
      "Train Epoch: 66 [3900/6658 (59%)]\tLoss: 0.017122\n",
      "Train Epoch: 66 [4000/6658 (60%)]\tLoss: 0.027103\n",
      "Train Epoch: 66 [4100/6658 (62%)]\tLoss: 0.016560\n",
      "Train Epoch: 66 [4200/6658 (63%)]\tLoss: 0.153428\n",
      "Train Epoch: 66 [4300/6658 (65%)]\tLoss: 0.090913\n",
      "Train Epoch: 66 [4400/6658 (66%)]\tLoss: 0.028446\n",
      "Train Epoch: 66 [4500/6658 (68%)]\tLoss: 0.120781\n",
      "Train Epoch: 66 [4600/6658 (69%)]\tLoss: 0.278906\n",
      "Train Epoch: 66 [4700/6658 (71%)]\tLoss: 0.114762\n",
      "Train Epoch: 66 [4800/6658 (72%)]\tLoss: 0.648452\n",
      "Train Epoch: 66 [4900/6658 (74%)]\tLoss: 0.780651\n",
      "Train Epoch: 66 [5000/6658 (75%)]\tLoss: 0.228092\n",
      "Train Epoch: 66 [5100/6658 (77%)]\tLoss: 5.804965\n",
      "Train Epoch: 66 [5200/6658 (78%)]\tLoss: 0.156887\n",
      "Train Epoch: 66 [5300/6658 (80%)]\tLoss: 0.456433\n",
      "Train Epoch: 66 [5400/6658 (81%)]\tLoss: 0.022497\n",
      "Train Epoch: 66 [5500/6658 (83%)]\tLoss: 2.643538\n",
      "Train Epoch: 66 [5600/6658 (84%)]\tLoss: 0.179877\n",
      "Train Epoch: 66 [5700/6658 (86%)]\tLoss: 0.165954\n",
      "Train Epoch: 66 [5800/6658 (87%)]\tLoss: 0.106029\n",
      "Train Epoch: 66 [5900/6658 (89%)]\tLoss: 0.477521\n",
      "Train Epoch: 66 [6000/6658 (90%)]\tLoss: 1.199036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 66 [6100/6658 (92%)]\tLoss: 4.527062\n",
      "Train Epoch: 66 [6200/6658 (93%)]\tLoss: 0.010902\n",
      "Train Epoch: 66 [6300/6658 (95%)]\tLoss: 0.001579\n",
      "Train Epoch: 66 [6400/6658 (96%)]\tLoss: 0.688237\n",
      "Train Epoch: 66 [6500/6658 (98%)]\tLoss: 0.039955\n",
      "Train Epoch: 66 [6600/6658 (99%)]\tLoss: 0.015237\n",
      "train loss average =  0.7271248637722597\n",
      "\n",
      "Test set: Average loss: 0.7133\n",
      "\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1519, 6.0435, 5.9181, 5.9098, 6.0903, 6.0390], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 67 [0/6658 (0%)]\tLoss: 1.214304\n",
      "Train Epoch: 67 [100/6658 (2%)]\tLoss: 0.070821\n",
      "Train Epoch: 67 [200/6658 (3%)]\tLoss: 0.245735\n",
      "Train Epoch: 67 [300/6658 (5%)]\tLoss: 0.831441\n",
      "Train Epoch: 67 [400/6658 (6%)]\tLoss: 0.932705\n",
      "Train Epoch: 67 [500/6658 (8%)]\tLoss: 0.308219\n",
      "Train Epoch: 67 [600/6658 (9%)]\tLoss: 0.496760\n",
      "Train Epoch: 67 [700/6658 (11%)]\tLoss: 0.309910\n",
      "Train Epoch: 67 [800/6658 (12%)]\tLoss: 0.001478\n",
      "Train Epoch: 67 [900/6658 (14%)]\tLoss: 0.227873\n",
      "Train Epoch: 67 [1000/6658 (15%)]\tLoss: 0.206908\n",
      "Train Epoch: 67 [1100/6658 (17%)]\tLoss: 0.439969\n",
      "Train Epoch: 67 [1200/6658 (18%)]\tLoss: 0.115496\n",
      "Train Epoch: 67 [1300/6658 (20%)]\tLoss: 0.063014\n",
      "Train Epoch: 67 [1400/6658 (21%)]\tLoss: 1.662921\n",
      "Train Epoch: 67 [1500/6658 (23%)]\tLoss: 0.057682\n",
      "Train Epoch: 67 [1600/6658 (24%)]\tLoss: 0.464282\n",
      "Train Epoch: 67 [1700/6658 (26%)]\tLoss: 0.093497\n",
      "Train Epoch: 67 [1800/6658 (27%)]\tLoss: 0.131785\n",
      "Train Epoch: 67 [1900/6658 (29%)]\tLoss: 0.204824\n",
      "Train Epoch: 67 [2000/6658 (30%)]\tLoss: 0.065680\n",
      "Train Epoch: 67 [2100/6658 (32%)]\tLoss: 0.115818\n",
      "Train Epoch: 67 [2200/6658 (33%)]\tLoss: 0.118911\n",
      "Train Epoch: 67 [2300/6658 (35%)]\tLoss: 0.068154\n",
      "Train Epoch: 67 [2400/6658 (36%)]\tLoss: 0.007014\n",
      "Train Epoch: 67 [2500/6658 (38%)]\tLoss: 1.313176\n",
      "Train Epoch: 67 [2600/6658 (39%)]\tLoss: 0.016490\n",
      "Train Epoch: 67 [2700/6658 (41%)]\tLoss: 0.177861\n",
      "Train Epoch: 67 [2800/6658 (42%)]\tLoss: 0.108040\n",
      "Train Epoch: 67 [2900/6658 (44%)]\tLoss: 0.532020\n",
      "Train Epoch: 67 [3000/6658 (45%)]\tLoss: 0.812822\n",
      "Train Epoch: 67 [3100/6658 (47%)]\tLoss: 2.206572\n",
      "Train Epoch: 67 [3200/6658 (48%)]\tLoss: 0.869102\n",
      "Train Epoch: 67 [3300/6658 (50%)]\tLoss: 0.006002\n",
      "Train Epoch: 67 [3400/6658 (51%)]\tLoss: 0.000329\n",
      "Train Epoch: 67 [3500/6658 (53%)]\tLoss: 0.322214\n",
      "Train Epoch: 67 [3600/6658 (54%)]\tLoss: 1.927588\n",
      "Train Epoch: 67 [3700/6658 (56%)]\tLoss: 0.048406\n",
      "Train Epoch: 67 [3800/6658 (57%)]\tLoss: 0.114418\n",
      "Train Epoch: 67 [3900/6658 (59%)]\tLoss: 0.771015\n",
      "Train Epoch: 67 [4000/6658 (60%)]\tLoss: 0.644830\n",
      "Train Epoch: 67 [4100/6658 (62%)]\tLoss: 0.044704\n",
      "Train Epoch: 67 [4200/6658 (63%)]\tLoss: 1.449021\n",
      "Train Epoch: 67 [4300/6658 (65%)]\tLoss: 0.018121\n",
      "Train Epoch: 67 [4400/6658 (66%)]\tLoss: 0.796703\n",
      "Train Epoch: 67 [4500/6658 (68%)]\tLoss: 0.120024\n",
      "Train Epoch: 67 [4600/6658 (69%)]\tLoss: 0.364909\n",
      "Train Epoch: 67 [4700/6658 (71%)]\tLoss: 0.000686\n",
      "Train Epoch: 67 [4800/6658 (72%)]\tLoss: 0.226876\n",
      "Train Epoch: 67 [4900/6658 (74%)]\tLoss: 1.696890\n",
      "Train Epoch: 67 [5000/6658 (75%)]\tLoss: 0.012495\n",
      "Train Epoch: 67 [5100/6658 (77%)]\tLoss: 0.500104\n",
      "Train Epoch: 67 [5200/6658 (78%)]\tLoss: 0.373623\n",
      "Train Epoch: 67 [5300/6658 (80%)]\tLoss: 1.917454\n",
      "Train Epoch: 67 [5400/6658 (81%)]\tLoss: 0.278852\n",
      "Train Epoch: 67 [5500/6658 (83%)]\tLoss: 0.258976\n",
      "Train Epoch: 67 [5600/6658 (84%)]\tLoss: 0.879780\n",
      "Train Epoch: 67 [5700/6658 (86%)]\tLoss: 0.631173\n",
      "Train Epoch: 67 [5800/6658 (87%)]\tLoss: 0.194052\n",
      "Train Epoch: 67 [5900/6658 (89%)]\tLoss: 0.016458\n",
      "Train Epoch: 67 [6000/6658 (90%)]\tLoss: 0.312514\n",
      "Train Epoch: 67 [6100/6658 (92%)]\tLoss: 0.613917\n",
      "Train Epoch: 67 [6200/6658 (93%)]\tLoss: 0.453948\n",
      "Train Epoch: 67 [6300/6658 (95%)]\tLoss: 0.188978\n",
      "Train Epoch: 67 [6400/6658 (96%)]\tLoss: 0.424785\n",
      "Train Epoch: 67 [6500/6658 (98%)]\tLoss: 0.055352\n",
      "Train Epoch: 67 [6600/6658 (99%)]\tLoss: 0.013080\n",
      "train loss average =  0.7317276400744253\n",
      "\n",
      "Test set: Average loss: 0.7512\n",
      "\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1540, 6.0440, 5.9167, 5.9091, 6.0922, 6.0397], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 68 [0/6658 (0%)]\tLoss: 3.017896\n",
      "Train Epoch: 68 [100/6658 (2%)]\tLoss: 0.065124\n",
      "Train Epoch: 68 [200/6658 (3%)]\tLoss: 1.342678\n",
      "Train Epoch: 68 [300/6658 (5%)]\tLoss: 0.318908\n",
      "Train Epoch: 68 [400/6658 (6%)]\tLoss: 0.031473\n",
      "Train Epoch: 68 [500/6658 (8%)]\tLoss: 0.597865\n",
      "Train Epoch: 68 [600/6658 (9%)]\tLoss: 0.646797\n",
      "Train Epoch: 68 [700/6658 (11%)]\tLoss: 7.380914\n",
      "Train Epoch: 68 [800/6658 (12%)]\tLoss: 0.598960\n",
      "Train Epoch: 68 [900/6658 (14%)]\tLoss: 0.054360\n",
      "Train Epoch: 68 [1000/6658 (15%)]\tLoss: 0.032222\n",
      "Train Epoch: 68 [1100/6658 (17%)]\tLoss: 0.037954\n",
      "Train Epoch: 68 [1200/6658 (18%)]\tLoss: 0.038395\n",
      "Train Epoch: 68 [1300/6658 (20%)]\tLoss: 0.359939\n",
      "Train Epoch: 68 [1400/6658 (21%)]\tLoss: 0.179754\n",
      "Train Epoch: 68 [1500/6658 (23%)]\tLoss: 0.309884\n",
      "Train Epoch: 68 [1600/6658 (24%)]\tLoss: 0.687890\n",
      "Train Epoch: 68 [1700/6658 (26%)]\tLoss: 4.060548\n",
      "Train Epoch: 68 [1800/6658 (27%)]\tLoss: 0.105587\n",
      "Train Epoch: 68 [1900/6658 (29%)]\tLoss: 0.095782\n",
      "Train Epoch: 68 [2000/6658 (30%)]\tLoss: 0.032086\n",
      "Train Epoch: 68 [2100/6658 (32%)]\tLoss: 0.155782\n",
      "Train Epoch: 68 [2200/6658 (33%)]\tLoss: 0.053117\n",
      "Train Epoch: 68 [2300/6658 (35%)]\tLoss: 0.310413\n",
      "Train Epoch: 68 [2400/6658 (36%)]\tLoss: 0.052698\n",
      "Train Epoch: 68 [2500/6658 (38%)]\tLoss: 2.495102\n",
      "Train Epoch: 68 [2600/6658 (39%)]\tLoss: 0.264680\n",
      "Train Epoch: 68 [2700/6658 (41%)]\tLoss: 0.295458\n",
      "Train Epoch: 68 [2800/6658 (42%)]\tLoss: 0.968401\n",
      "Train Epoch: 68 [2900/6658 (44%)]\tLoss: 0.045436\n",
      "Train Epoch: 68 [3000/6658 (45%)]\tLoss: 2.146447\n",
      "Train Epoch: 68 [3100/6658 (47%)]\tLoss: 0.347627\n",
      "Train Epoch: 68 [3200/6658 (48%)]\tLoss: 0.047882\n",
      "Train Epoch: 68 [3300/6658 (50%)]\tLoss: 1.923192\n",
      "Train Epoch: 68 [3400/6658 (51%)]\tLoss: 0.183746\n",
      "Train Epoch: 68 [3500/6658 (53%)]\tLoss: 0.015394\n",
      "Train Epoch: 68 [3600/6658 (54%)]\tLoss: 0.049752\n",
      "Train Epoch: 68 [3700/6658 (56%)]\tLoss: 0.166102\n",
      "Train Epoch: 68 [3800/6658 (57%)]\tLoss: 1.266951\n",
      "Train Epoch: 68 [3900/6658 (59%)]\tLoss: 0.883071\n",
      "Train Epoch: 68 [4000/6658 (60%)]\tLoss: 0.317794\n",
      "Train Epoch: 68 [4100/6658 (62%)]\tLoss: 0.497986\n",
      "Train Epoch: 68 [4200/6658 (63%)]\tLoss: 0.141099\n",
      "Train Epoch: 68 [4300/6658 (65%)]\tLoss: 0.720488\n",
      "Train Epoch: 68 [4400/6658 (66%)]\tLoss: 0.000540\n",
      "Train Epoch: 68 [4500/6658 (68%)]\tLoss: 0.804601\n",
      "Train Epoch: 68 [4600/6658 (69%)]\tLoss: 1.890401\n",
      "Train Epoch: 68 [4700/6658 (71%)]\tLoss: 0.159113\n",
      "Train Epoch: 68 [4800/6658 (72%)]\tLoss: 0.000149\n",
      "Train Epoch: 68 [4900/6658 (74%)]\tLoss: 1.231962\n",
      "Train Epoch: 68 [5000/6658 (75%)]\tLoss: 0.020971\n",
      "Train Epoch: 68 [5100/6658 (77%)]\tLoss: 0.784587\n",
      "Train Epoch: 68 [5200/6658 (78%)]\tLoss: 0.000860\n",
      "Train Epoch: 68 [5300/6658 (80%)]\tLoss: 1.624602\n",
      "Train Epoch: 68 [5400/6658 (81%)]\tLoss: 0.635797\n",
      "Train Epoch: 68 [5500/6658 (83%)]\tLoss: 1.687853\n",
      "Train Epoch: 68 [5600/6658 (84%)]\tLoss: 0.223106\n",
      "Train Epoch: 68 [5700/6658 (86%)]\tLoss: 0.133630\n",
      "Train Epoch: 68 [5800/6658 (87%)]\tLoss: 1.252614\n",
      "Train Epoch: 68 [5900/6658 (89%)]\tLoss: 0.000890\n",
      "Train Epoch: 68 [6000/6658 (90%)]\tLoss: 0.717487\n",
      "Train Epoch: 68 [6100/6658 (92%)]\tLoss: 0.475117\n",
      "Train Epoch: 68 [6200/6658 (93%)]\tLoss: 0.621241\n",
      "Train Epoch: 68 [6300/6658 (95%)]\tLoss: 0.003578\n",
      "Train Epoch: 68 [6400/6658 (96%)]\tLoss: 0.213851\n",
      "Train Epoch: 68 [6500/6658 (98%)]\tLoss: 0.565648\n",
      "Train Epoch: 68 [6600/6658 (99%)]\tLoss: 0.180751\n",
      "train loss average =  0.7294132281125808\n",
      "\n",
      "Test set: Average loss: 0.7133\n",
      "\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1558, 6.0447, 5.9143, 5.9074, 6.0931, 6.0401], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 69 [0/6658 (0%)]\tLoss: 0.259993\n",
      "Train Epoch: 69 [100/6658 (2%)]\tLoss: 0.002479\n",
      "Train Epoch: 69 [200/6658 (3%)]\tLoss: 0.797395\n",
      "Train Epoch: 69 [300/6658 (5%)]\tLoss: 0.818146\n",
      "Train Epoch: 69 [400/6658 (6%)]\tLoss: 0.258714\n",
      "Train Epoch: 69 [500/6658 (8%)]\tLoss: 0.291563\n",
      "Train Epoch: 69 [600/6658 (9%)]\tLoss: 0.134723\n",
      "Train Epoch: 69 [700/6658 (11%)]\tLoss: 0.201684\n",
      "Train Epoch: 69 [800/6658 (12%)]\tLoss: 2.742959\n",
      "Train Epoch: 69 [900/6658 (14%)]\tLoss: 0.116358\n",
      "Train Epoch: 69 [1000/6658 (15%)]\tLoss: 0.354735\n",
      "Train Epoch: 69 [1100/6658 (17%)]\tLoss: 0.762924\n",
      "Train Epoch: 69 [1200/6658 (18%)]\tLoss: 0.008497\n",
      "Train Epoch: 69 [1300/6658 (20%)]\tLoss: 0.000665\n",
      "Train Epoch: 69 [1400/6658 (21%)]\tLoss: 0.024756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 69 [1500/6658 (23%)]\tLoss: 0.453598\n",
      "Train Epoch: 69 [1600/6658 (24%)]\tLoss: 1.167050\n",
      "Train Epoch: 69 [1700/6658 (26%)]\tLoss: 8.063619\n",
      "Train Epoch: 69 [1800/6658 (27%)]\tLoss: 0.035171\n",
      "Train Epoch: 69 [1900/6658 (29%)]\tLoss: 0.267381\n",
      "Train Epoch: 69 [2000/6658 (30%)]\tLoss: 0.927912\n",
      "Train Epoch: 69 [2100/6658 (32%)]\tLoss: 0.024304\n",
      "Train Epoch: 69 [2200/6658 (33%)]\tLoss: 0.235011\n",
      "Train Epoch: 69 [2300/6658 (35%)]\tLoss: 0.883148\n",
      "Train Epoch: 69 [2400/6658 (36%)]\tLoss: 0.883231\n",
      "Train Epoch: 69 [2500/6658 (38%)]\tLoss: 0.894832\n",
      "Train Epoch: 69 [2600/6658 (39%)]\tLoss: 1.100875\n",
      "Train Epoch: 69 [2700/6658 (41%)]\tLoss: 0.029564\n",
      "Train Epoch: 69 [2800/6658 (42%)]\tLoss: 0.182168\n",
      "Train Epoch: 69 [2900/6658 (44%)]\tLoss: 2.009359\n",
      "Train Epoch: 69 [3000/6658 (45%)]\tLoss: 0.144351\n",
      "Train Epoch: 69 [3100/6658 (47%)]\tLoss: 0.066471\n",
      "Train Epoch: 69 [3200/6658 (48%)]\tLoss: 0.755151\n",
      "Train Epoch: 69 [3300/6658 (50%)]\tLoss: 0.016989\n",
      "Train Epoch: 69 [3400/6658 (51%)]\tLoss: 0.543408\n",
      "Train Epoch: 69 [3500/6658 (53%)]\tLoss: 0.818087\n",
      "Train Epoch: 69 [3600/6658 (54%)]\tLoss: 0.494554\n",
      "Train Epoch: 69 [3700/6658 (56%)]\tLoss: 0.172636\n",
      "Train Epoch: 69 [3800/6658 (57%)]\tLoss: 0.000182\n",
      "Train Epoch: 69 [3900/6658 (59%)]\tLoss: 0.225709\n",
      "Train Epoch: 69 [4000/6658 (60%)]\tLoss: 0.082098\n",
      "Train Epoch: 69 [4100/6658 (62%)]\tLoss: 0.551980\n",
      "Train Epoch: 69 [4200/6658 (63%)]\tLoss: 2.983921\n",
      "Train Epoch: 69 [4300/6658 (65%)]\tLoss: 0.096208\n",
      "Train Epoch: 69 [4400/6658 (66%)]\tLoss: 0.062893\n",
      "Train Epoch: 69 [4500/6658 (68%)]\tLoss: 0.109191\n",
      "Train Epoch: 69 [4600/6658 (69%)]\tLoss: 0.005020\n",
      "Train Epoch: 69 [4700/6658 (71%)]\tLoss: 2.016248\n",
      "Train Epoch: 69 [4800/6658 (72%)]\tLoss: 1.163030\n",
      "Train Epoch: 69 [4900/6658 (74%)]\tLoss: 0.456332\n",
      "Train Epoch: 69 [5000/6658 (75%)]\tLoss: 0.000379\n",
      "Train Epoch: 69 [5100/6658 (77%)]\tLoss: 0.160926\n",
      "Train Epoch: 69 [5200/6658 (78%)]\tLoss: 0.254811\n",
      "Train Epoch: 69 [5300/6658 (80%)]\tLoss: 0.261331\n",
      "Train Epoch: 69 [5400/6658 (81%)]\tLoss: 0.000308\n",
      "Train Epoch: 69 [5500/6658 (83%)]\tLoss: 0.090172\n",
      "Train Epoch: 69 [5600/6658 (84%)]\tLoss: 0.002738\n",
      "Train Epoch: 69 [5700/6658 (86%)]\tLoss: 0.276785\n",
      "Train Epoch: 69 [5800/6658 (87%)]\tLoss: 0.148162\n",
      "Train Epoch: 69 [5900/6658 (89%)]\tLoss: 0.280803\n",
      "Train Epoch: 69 [6000/6658 (90%)]\tLoss: 1.979010\n",
      "Train Epoch: 69 [6100/6658 (92%)]\tLoss: 0.365643\n",
      "Train Epoch: 69 [6200/6658 (93%)]\tLoss: 0.066587\n",
      "Train Epoch: 69 [6300/6658 (95%)]\tLoss: 0.205027\n",
      "Train Epoch: 69 [6400/6658 (96%)]\tLoss: 0.564955\n",
      "Train Epoch: 69 [6500/6658 (98%)]\tLoss: 0.119446\n",
      "Train Epoch: 69 [6600/6658 (99%)]\tLoss: 0.385274\n",
      "train loss average =  0.7321196733425384\n",
      "\n",
      "Test set: Average loss: 0.7150\n",
      "\n",
      "EarlyStopping counter: 9 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1572, 6.0451, 5.9126, 5.9073, 6.0947, 6.0403], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 70 [0/6658 (0%)]\tLoss: 0.219677\n",
      "Train Epoch: 70 [100/6658 (2%)]\tLoss: 0.004416\n",
      "Train Epoch: 70 [200/6658 (3%)]\tLoss: 0.018771\n",
      "Train Epoch: 70 [300/6658 (5%)]\tLoss: 2.203579\n",
      "Train Epoch: 70 [400/6658 (6%)]\tLoss: 2.483516\n",
      "Train Epoch: 70 [500/6658 (8%)]\tLoss: 0.767344\n",
      "Train Epoch: 70 [600/6658 (9%)]\tLoss: 0.323407\n",
      "Train Epoch: 70 [700/6658 (11%)]\tLoss: 0.166880\n",
      "Train Epoch: 70 [800/6658 (12%)]\tLoss: 0.193380\n",
      "Train Epoch: 70 [900/6658 (14%)]\tLoss: 1.064569\n",
      "Train Epoch: 70 [1000/6658 (15%)]\tLoss: 0.403221\n",
      "Train Epoch: 70 [1100/6658 (17%)]\tLoss: 1.236934\n",
      "Train Epoch: 70 [1200/6658 (18%)]\tLoss: 0.020821\n",
      "Train Epoch: 70 [1300/6658 (20%)]\tLoss: 0.352967\n",
      "Train Epoch: 70 [1400/6658 (21%)]\tLoss: 0.014359\n",
      "Train Epoch: 70 [1500/6658 (23%)]\tLoss: 0.209142\n",
      "Train Epoch: 70 [1600/6658 (24%)]\tLoss: 0.013029\n",
      "Train Epoch: 70 [1700/6658 (26%)]\tLoss: 0.026897\n",
      "Train Epoch: 70 [1800/6658 (27%)]\tLoss: 0.188039\n",
      "Train Epoch: 70 [1900/6658 (29%)]\tLoss: 11.660264\n",
      "Train Epoch: 70 [2000/6658 (30%)]\tLoss: 0.086128\n",
      "Train Epoch: 70 [2100/6658 (32%)]\tLoss: 0.382314\n",
      "Train Epoch: 70 [2200/6658 (33%)]\tLoss: 1.265442\n",
      "Train Epoch: 70 [2300/6658 (35%)]\tLoss: 1.100455\n",
      "Train Epoch: 70 [2400/6658 (36%)]\tLoss: 0.012989\n",
      "Train Epoch: 70 [2500/6658 (38%)]\tLoss: 0.004401\n",
      "Train Epoch: 70 [2600/6658 (39%)]\tLoss: 0.731470\n",
      "Train Epoch: 70 [2700/6658 (41%)]\tLoss: 1.568773\n",
      "Train Epoch: 70 [2800/6658 (42%)]\tLoss: 0.034411\n",
      "Train Epoch: 70 [2900/6658 (44%)]\tLoss: 0.761117\n",
      "Train Epoch: 70 [3000/6658 (45%)]\tLoss: 1.125977\n",
      "Train Epoch: 70 [3100/6658 (47%)]\tLoss: 0.499932\n",
      "Train Epoch: 70 [3200/6658 (48%)]\tLoss: 0.008437\n",
      "Train Epoch: 70 [3300/6658 (50%)]\tLoss: 7.160332\n",
      "Train Epoch: 70 [3400/6658 (51%)]\tLoss: 0.012665\n",
      "Train Epoch: 70 [3500/6658 (53%)]\tLoss: 0.227787\n",
      "Train Epoch: 70 [3600/6658 (54%)]\tLoss: 0.847629\n",
      "Train Epoch: 70 [3700/6658 (56%)]\tLoss: 2.622936\n",
      "Train Epoch: 70 [3800/6658 (57%)]\tLoss: 4.148129\n",
      "Train Epoch: 70 [3900/6658 (59%)]\tLoss: 0.011312\n",
      "Train Epoch: 70 [4000/6658 (60%)]\tLoss: 0.333484\n",
      "Train Epoch: 70 [4100/6658 (62%)]\tLoss: 2.298571\n",
      "Train Epoch: 70 [4200/6658 (63%)]\tLoss: 0.568225\n",
      "Train Epoch: 70 [4300/6658 (65%)]\tLoss: 0.021867\n",
      "Train Epoch: 70 [4400/6658 (66%)]\tLoss: 5.587780\n",
      "Train Epoch: 70 [4500/6658 (68%)]\tLoss: 0.308098\n",
      "Train Epoch: 70 [4600/6658 (69%)]\tLoss: 0.700384\n",
      "Train Epoch: 70 [4700/6658 (71%)]\tLoss: 0.049657\n",
      "Train Epoch: 70 [4800/6658 (72%)]\tLoss: 0.907901\n",
      "Train Epoch: 70 [4900/6658 (74%)]\tLoss: 2.125627\n",
      "Train Epoch: 70 [5000/6658 (75%)]\tLoss: 0.049747\n",
      "Train Epoch: 70 [5100/6658 (77%)]\tLoss: 0.198382\n",
      "Train Epoch: 70 [5200/6658 (78%)]\tLoss: 0.005729\n",
      "Train Epoch: 70 [5300/6658 (80%)]\tLoss: 0.015063\n",
      "Train Epoch: 70 [5400/6658 (81%)]\tLoss: 0.266883\n",
      "Train Epoch: 70 [5500/6658 (83%)]\tLoss: 0.093189\n",
      "Train Epoch: 70 [5600/6658 (84%)]\tLoss: 1.371536\n",
      "Train Epoch: 70 [5700/6658 (86%)]\tLoss: 0.025090\n",
      "Train Epoch: 70 [5800/6658 (87%)]\tLoss: 1.509647\n",
      "Train Epoch: 70 [5900/6658 (89%)]\tLoss: 0.008486\n",
      "Train Epoch: 70 [6000/6658 (90%)]\tLoss: 0.117011\n",
      "Train Epoch: 70 [6100/6658 (92%)]\tLoss: 0.353417\n",
      "Train Epoch: 70 [6200/6658 (93%)]\tLoss: 0.110475\n",
      "Train Epoch: 70 [6300/6658 (95%)]\tLoss: 0.947730\n",
      "Train Epoch: 70 [6400/6658 (96%)]\tLoss: 0.021937\n",
      "Train Epoch: 70 [6500/6658 (98%)]\tLoss: 0.137007\n",
      "Train Epoch: 70 [6600/6658 (99%)]\tLoss: 0.347953\n",
      "train loss average =  0.7269915883004503\n",
      "\n",
      "Test set: Average loss: 0.7006\n",
      "\n",
      "Validation loss decreased (0.701479 --> 0.700575).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.1596, 6.0455, 5.9103, 5.9057, 6.0960, 6.0410], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 71 [0/6658 (0%)]\tLoss: 1.233918\n",
      "Train Epoch: 71 [100/6658 (2%)]\tLoss: 0.321920\n",
      "Train Epoch: 71 [200/6658 (3%)]\tLoss: 0.122832\n",
      "Train Epoch: 71 [300/6658 (5%)]\tLoss: 0.061626\n",
      "Train Epoch: 71 [400/6658 (6%)]\tLoss: 0.194678\n",
      "Train Epoch: 71 [500/6658 (8%)]\tLoss: 10.157422\n",
      "Train Epoch: 71 [600/6658 (9%)]\tLoss: 0.411714\n",
      "Train Epoch: 71 [700/6658 (11%)]\tLoss: 0.890482\n",
      "Train Epoch: 71 [800/6658 (12%)]\tLoss: 0.026025\n",
      "Train Epoch: 71 [900/6658 (14%)]\tLoss: 0.697628\n",
      "Train Epoch: 71 [1000/6658 (15%)]\tLoss: 0.027658\n",
      "Train Epoch: 71 [1100/6658 (17%)]\tLoss: 0.000023\n",
      "Train Epoch: 71 [1200/6658 (18%)]\tLoss: 0.001680\n",
      "Train Epoch: 71 [1300/6658 (20%)]\tLoss: 0.079126\n",
      "Train Epoch: 71 [1400/6658 (21%)]\tLoss: 0.075493\n",
      "Train Epoch: 71 [1500/6658 (23%)]\tLoss: 0.273334\n",
      "Train Epoch: 71 [1600/6658 (24%)]\tLoss: 0.993393\n",
      "Train Epoch: 71 [1700/6658 (26%)]\tLoss: 3.277692\n",
      "Train Epoch: 71 [1800/6658 (27%)]\tLoss: 0.602112\n",
      "Train Epoch: 71 [1900/6658 (29%)]\tLoss: 0.015468\n",
      "Train Epoch: 71 [2000/6658 (30%)]\tLoss: 0.008733\n",
      "Train Epoch: 71 [2100/6658 (32%)]\tLoss: 1.157774\n",
      "Train Epoch: 71 [2200/6658 (33%)]\tLoss: 0.013710\n",
      "Train Epoch: 71 [2300/6658 (35%)]\tLoss: 0.287714\n",
      "Train Epoch: 71 [2400/6658 (36%)]\tLoss: 0.002099\n",
      "Train Epoch: 71 [2500/6658 (38%)]\tLoss: 0.059777\n",
      "Train Epoch: 71 [2600/6658 (39%)]\tLoss: 0.117642\n",
      "Train Epoch: 71 [2700/6658 (41%)]\tLoss: 0.098697\n",
      "Train Epoch: 71 [2800/6658 (42%)]\tLoss: 1.114321\n",
      "Train Epoch: 71 [2900/6658 (44%)]\tLoss: 11.742674\n",
      "Train Epoch: 71 [3000/6658 (45%)]\tLoss: 0.764904\n",
      "Train Epoch: 71 [3100/6658 (47%)]\tLoss: 0.242799\n",
      "Train Epoch: 71 [3200/6658 (48%)]\tLoss: 0.056652\n",
      "Train Epoch: 71 [3300/6658 (50%)]\tLoss: 0.028663\n",
      "Train Epoch: 71 [3400/6658 (51%)]\tLoss: 0.000888\n",
      "Train Epoch: 71 [3500/6658 (53%)]\tLoss: 0.180616\n",
      "Train Epoch: 71 [3600/6658 (54%)]\tLoss: 2.897085\n",
      "Train Epoch: 71 [3700/6658 (56%)]\tLoss: 0.135529\n",
      "Train Epoch: 71 [3800/6658 (57%)]\tLoss: 0.706083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 71 [3900/6658 (59%)]\tLoss: 0.008407\n",
      "Train Epoch: 71 [4000/6658 (60%)]\tLoss: 1.395965\n",
      "Train Epoch: 71 [4100/6658 (62%)]\tLoss: 0.129098\n",
      "Train Epoch: 71 [4200/6658 (63%)]\tLoss: 0.152036\n",
      "Train Epoch: 71 [4300/6658 (65%)]\tLoss: 0.259519\n",
      "Train Epoch: 71 [4400/6658 (66%)]\tLoss: 0.025037\n",
      "Train Epoch: 71 [4500/6658 (68%)]\tLoss: 0.220824\n",
      "Train Epoch: 71 [4600/6658 (69%)]\tLoss: 0.403501\n",
      "Train Epoch: 71 [4700/6658 (71%)]\tLoss: 0.001836\n",
      "Train Epoch: 71 [4800/6658 (72%)]\tLoss: 0.244864\n",
      "Train Epoch: 71 [4900/6658 (74%)]\tLoss: 0.000983\n",
      "Train Epoch: 71 [5000/6658 (75%)]\tLoss: 0.000773\n",
      "Train Epoch: 71 [5100/6658 (77%)]\tLoss: 0.774692\n",
      "Train Epoch: 71 [5200/6658 (78%)]\tLoss: 1.540560\n",
      "Train Epoch: 71 [5300/6658 (80%)]\tLoss: 0.072574\n",
      "Train Epoch: 71 [5400/6658 (81%)]\tLoss: 0.085840\n",
      "Train Epoch: 71 [5500/6658 (83%)]\tLoss: 0.312354\n",
      "Train Epoch: 71 [5600/6658 (84%)]\tLoss: 0.749416\n",
      "Train Epoch: 71 [5700/6658 (86%)]\tLoss: 0.069083\n",
      "Train Epoch: 71 [5800/6658 (87%)]\tLoss: 0.489235\n",
      "Train Epoch: 71 [5900/6658 (89%)]\tLoss: 0.024440\n",
      "Train Epoch: 71 [6000/6658 (90%)]\tLoss: 1.608394\n",
      "Train Epoch: 71 [6100/6658 (92%)]\tLoss: 0.021696\n",
      "Train Epoch: 71 [6200/6658 (93%)]\tLoss: 0.000322\n",
      "Train Epoch: 71 [6300/6658 (95%)]\tLoss: 0.059541\n",
      "Train Epoch: 71 [6400/6658 (96%)]\tLoss: 0.201656\n",
      "Train Epoch: 71 [6500/6658 (98%)]\tLoss: 0.033585\n",
      "Train Epoch: 71 [6600/6658 (99%)]\tLoss: 0.636352\n",
      "train loss average =  0.7322253724912364\n",
      "\n",
      "Test set: Average loss: 0.7118\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1622, 6.0459, 5.9081, 5.9047, 6.0974, 6.0414], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 72 [0/6658 (0%)]\tLoss: 0.947208\n",
      "Train Epoch: 72 [100/6658 (2%)]\tLoss: 0.010566\n",
      "Train Epoch: 72 [200/6658 (3%)]\tLoss: 0.430607\n",
      "Train Epoch: 72 [300/6658 (5%)]\tLoss: 1.577371\n",
      "Train Epoch: 72 [400/6658 (6%)]\tLoss: 3.145967\n",
      "Train Epoch: 72 [500/6658 (8%)]\tLoss: 4.526406\n",
      "Train Epoch: 72 [600/6658 (9%)]\tLoss: 0.045724\n",
      "Train Epoch: 72 [700/6658 (11%)]\tLoss: 1.851457\n",
      "Train Epoch: 72 [800/6658 (12%)]\tLoss: 0.371206\n",
      "Train Epoch: 72 [900/6658 (14%)]\tLoss: 0.205338\n",
      "Train Epoch: 72 [1000/6658 (15%)]\tLoss: 0.019604\n",
      "Train Epoch: 72 [1100/6658 (17%)]\tLoss: 0.001234\n",
      "Train Epoch: 72 [1200/6658 (18%)]\tLoss: 0.009486\n",
      "Train Epoch: 72 [1300/6658 (20%)]\tLoss: 0.186903\n",
      "Train Epoch: 72 [1400/6658 (21%)]\tLoss: 1.567296\n",
      "Train Epoch: 72 [1500/6658 (23%)]\tLoss: 0.000439\n",
      "Train Epoch: 72 [1600/6658 (24%)]\tLoss: 0.069490\n",
      "Train Epoch: 72 [1700/6658 (26%)]\tLoss: 0.038016\n",
      "Train Epoch: 72 [1800/6658 (27%)]\tLoss: 0.077006\n",
      "Train Epoch: 72 [1900/6658 (29%)]\tLoss: 0.113807\n",
      "Train Epoch: 72 [2000/6658 (30%)]\tLoss: 0.125805\n",
      "Train Epoch: 72 [2100/6658 (32%)]\tLoss: 1.132096\n",
      "Train Epoch: 72 [2200/6658 (33%)]\tLoss: 0.000003\n",
      "Train Epoch: 72 [2300/6658 (35%)]\tLoss: 2.469845\n",
      "Train Epoch: 72 [2400/6658 (36%)]\tLoss: 0.568879\n",
      "Train Epoch: 72 [2500/6658 (38%)]\tLoss: 0.030066\n",
      "Train Epoch: 72 [2600/6658 (39%)]\tLoss: 0.211217\n",
      "Train Epoch: 72 [2700/6658 (41%)]\tLoss: 0.110894\n",
      "Train Epoch: 72 [2800/6658 (42%)]\tLoss: 0.136076\n",
      "Train Epoch: 72 [2900/6658 (44%)]\tLoss: 2.271012\n",
      "Train Epoch: 72 [3000/6658 (45%)]\tLoss: 0.033813\n",
      "Train Epoch: 72 [3100/6658 (47%)]\tLoss: 0.172186\n",
      "Train Epoch: 72 [3200/6658 (48%)]\tLoss: 0.163252\n",
      "Train Epoch: 72 [3300/6658 (50%)]\tLoss: 0.042059\n",
      "Train Epoch: 72 [3400/6658 (51%)]\tLoss: 0.524317\n",
      "Train Epoch: 72 [3500/6658 (53%)]\tLoss: 0.321792\n",
      "Train Epoch: 72 [3600/6658 (54%)]\tLoss: 0.081114\n",
      "Train Epoch: 72 [3700/6658 (56%)]\tLoss: 0.013657\n",
      "Train Epoch: 72 [3800/6658 (57%)]\tLoss: 0.268519\n",
      "Train Epoch: 72 [3900/6658 (59%)]\tLoss: 0.063898\n",
      "Train Epoch: 72 [4000/6658 (60%)]\tLoss: 0.716155\n",
      "Train Epoch: 72 [4100/6658 (62%)]\tLoss: 1.031480\n",
      "Train Epoch: 72 [4200/6658 (63%)]\tLoss: 0.266474\n",
      "Train Epoch: 72 [4300/6658 (65%)]\tLoss: 0.726459\n",
      "Train Epoch: 72 [4400/6658 (66%)]\tLoss: 0.140617\n",
      "Train Epoch: 72 [4500/6658 (68%)]\tLoss: 0.020240\n",
      "Train Epoch: 72 [4600/6658 (69%)]\tLoss: 0.168326\n",
      "Train Epoch: 72 [4700/6658 (71%)]\tLoss: 0.000473\n",
      "Train Epoch: 72 [4800/6658 (72%)]\tLoss: 0.442472\n",
      "Train Epoch: 72 [4900/6658 (74%)]\tLoss: 0.050220\n",
      "Train Epoch: 72 [5000/6658 (75%)]\tLoss: 1.237058\n",
      "Train Epoch: 72 [5100/6658 (77%)]\tLoss: 2.853710\n",
      "Train Epoch: 72 [5200/6658 (78%)]\tLoss: 0.092355\n",
      "Train Epoch: 72 [5300/6658 (80%)]\tLoss: 0.049681\n",
      "Train Epoch: 72 [5400/6658 (81%)]\tLoss: 0.458597\n",
      "Train Epoch: 72 [5500/6658 (83%)]\tLoss: 1.396786\n",
      "Train Epoch: 72 [5600/6658 (84%)]\tLoss: 0.318561\n",
      "Train Epoch: 72 [5700/6658 (86%)]\tLoss: 2.206519\n",
      "Train Epoch: 72 [5800/6658 (87%)]\tLoss: 0.040018\n",
      "Train Epoch: 72 [5900/6658 (89%)]\tLoss: 0.277624\n",
      "Train Epoch: 72 [6000/6658 (90%)]\tLoss: 0.073587\n",
      "Train Epoch: 72 [6100/6658 (92%)]\tLoss: 0.249969\n",
      "Train Epoch: 72 [6200/6658 (93%)]\tLoss: 0.267300\n",
      "Train Epoch: 72 [6300/6658 (95%)]\tLoss: 1.284060\n",
      "Train Epoch: 72 [6400/6658 (96%)]\tLoss: 0.000932\n",
      "Train Epoch: 72 [6500/6658 (98%)]\tLoss: 0.059117\n",
      "Train Epoch: 72 [6600/6658 (99%)]\tLoss: 0.173326\n",
      "train loss average =  0.7262452786585375\n",
      "\n",
      "Test set: Average loss: 0.7090\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1642, 6.0467, 5.9064, 5.9047, 6.0988, 6.0415], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 73 [0/6658 (0%)]\tLoss: 0.275541\n",
      "Train Epoch: 73 [100/6658 (2%)]\tLoss: 0.008275\n",
      "Train Epoch: 73 [200/6658 (3%)]\tLoss: 0.127070\n",
      "Train Epoch: 73 [300/6658 (5%)]\tLoss: 0.495565\n",
      "Train Epoch: 73 [400/6658 (6%)]\tLoss: 0.156306\n",
      "Train Epoch: 73 [500/6658 (8%)]\tLoss: 1.320148\n",
      "Train Epoch: 73 [600/6658 (9%)]\tLoss: 0.344340\n",
      "Train Epoch: 73 [700/6658 (11%)]\tLoss: 0.005516\n",
      "Train Epoch: 73 [800/6658 (12%)]\tLoss: 0.011210\n",
      "Train Epoch: 73 [900/6658 (14%)]\tLoss: 5.795669\n",
      "Train Epoch: 73 [1000/6658 (15%)]\tLoss: 0.244480\n",
      "Train Epoch: 73 [1100/6658 (17%)]\tLoss: 0.201725\n",
      "Train Epoch: 73 [1200/6658 (18%)]\tLoss: 0.235573\n",
      "Train Epoch: 73 [1300/6658 (20%)]\tLoss: 0.001391\n",
      "Train Epoch: 73 [1400/6658 (21%)]\tLoss: 0.121957\n",
      "Train Epoch: 73 [1500/6658 (23%)]\tLoss: 0.410792\n",
      "Train Epoch: 73 [1600/6658 (24%)]\tLoss: 0.546582\n",
      "Train Epoch: 73 [1700/6658 (26%)]\tLoss: 0.000494\n",
      "Train Epoch: 73 [1800/6658 (27%)]\tLoss: 0.389136\n",
      "Train Epoch: 73 [1900/6658 (29%)]\tLoss: 0.008575\n",
      "Train Epoch: 73 [2000/6658 (30%)]\tLoss: 0.528371\n",
      "Train Epoch: 73 [2100/6658 (32%)]\tLoss: 0.125783\n",
      "Train Epoch: 73 [2200/6658 (33%)]\tLoss: 0.058539\n",
      "Train Epoch: 73 [2300/6658 (35%)]\tLoss: 0.418988\n",
      "Train Epoch: 73 [2400/6658 (36%)]\tLoss: 0.629178\n",
      "Train Epoch: 73 [2500/6658 (38%)]\tLoss: 1.353126\n",
      "Train Epoch: 73 [2600/6658 (39%)]\tLoss: 0.466688\n",
      "Train Epoch: 73 [2700/6658 (41%)]\tLoss: 0.095633\n",
      "Train Epoch: 73 [2800/6658 (42%)]\tLoss: 0.127878\n",
      "Train Epoch: 73 [2900/6658 (44%)]\tLoss: 0.234141\n",
      "Train Epoch: 73 [3000/6658 (45%)]\tLoss: 0.139537\n",
      "Train Epoch: 73 [3100/6658 (47%)]\tLoss: 0.315647\n",
      "Train Epoch: 73 [3200/6658 (48%)]\tLoss: 1.309040\n",
      "Train Epoch: 73 [3300/6658 (50%)]\tLoss: 0.476454\n",
      "Train Epoch: 73 [3400/6658 (51%)]\tLoss: 16.096714\n",
      "Train Epoch: 73 [3500/6658 (53%)]\tLoss: 0.281438\n",
      "Train Epoch: 73 [3600/6658 (54%)]\tLoss: 0.073076\n",
      "Train Epoch: 73 [3700/6658 (56%)]\tLoss: 0.229786\n",
      "Train Epoch: 73 [3800/6658 (57%)]\tLoss: 0.048352\n",
      "Train Epoch: 73 [3900/6658 (59%)]\tLoss: 10.260969\n",
      "Train Epoch: 73 [4000/6658 (60%)]\tLoss: 0.644840\n",
      "Train Epoch: 73 [4100/6658 (62%)]\tLoss: 0.141939\n",
      "Train Epoch: 73 [4200/6658 (63%)]\tLoss: 0.291609\n",
      "Train Epoch: 73 [4300/6658 (65%)]\tLoss: 0.906823\n",
      "Train Epoch: 73 [4400/6658 (66%)]\tLoss: 0.715620\n",
      "Train Epoch: 73 [4500/6658 (68%)]\tLoss: 5.399396\n",
      "Train Epoch: 73 [4600/6658 (69%)]\tLoss: 0.524271\n",
      "Train Epoch: 73 [4700/6658 (71%)]\tLoss: 0.004052\n",
      "Train Epoch: 73 [4800/6658 (72%)]\tLoss: 0.309066\n",
      "Train Epoch: 73 [4900/6658 (74%)]\tLoss: 0.634994\n",
      "Train Epoch: 73 [5000/6658 (75%)]\tLoss: 0.495405\n",
      "Train Epoch: 73 [5100/6658 (77%)]\tLoss: 0.948784\n",
      "Train Epoch: 73 [5200/6658 (78%)]\tLoss: 0.667070\n",
      "Train Epoch: 73 [5300/6658 (80%)]\tLoss: 1.999680\n",
      "Train Epoch: 73 [5400/6658 (81%)]\tLoss: 0.128321\n",
      "Train Epoch: 73 [5500/6658 (83%)]\tLoss: 0.739876\n",
      "Train Epoch: 73 [5600/6658 (84%)]\tLoss: 0.200004\n",
      "Train Epoch: 73 [5700/6658 (86%)]\tLoss: 0.077958\n",
      "Train Epoch: 73 [5800/6658 (87%)]\tLoss: 0.711655\n",
      "Train Epoch: 73 [5900/6658 (89%)]\tLoss: 0.045933\n",
      "Train Epoch: 73 [6000/6658 (90%)]\tLoss: 0.252223\n",
      "Train Epoch: 73 [6100/6658 (92%)]\tLoss: 0.361664\n",
      "Train Epoch: 73 [6200/6658 (93%)]\tLoss: 2.017558\n",
      "Train Epoch: 73 [6300/6658 (95%)]\tLoss: 0.078801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 73 [6400/6658 (96%)]\tLoss: 0.634703\n",
      "Train Epoch: 73 [6500/6658 (98%)]\tLoss: 0.148269\n",
      "Train Epoch: 73 [6600/6658 (99%)]\tLoss: 0.104698\n",
      "train loss average =  0.7235828838570968\n",
      "\n",
      "Test set: Average loss: 0.7195\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1670, 6.0479, 5.9054, 5.9049, 6.1004, 6.0420], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 74 [0/6658 (0%)]\tLoss: 0.470616\n",
      "Train Epoch: 74 [100/6658 (2%)]\tLoss: 1.374761\n",
      "Train Epoch: 74 [200/6658 (3%)]\tLoss: 0.314211\n",
      "Train Epoch: 74 [300/6658 (5%)]\tLoss: 1.305590\n",
      "Train Epoch: 74 [400/6658 (6%)]\tLoss: 0.089203\n",
      "Train Epoch: 74 [500/6658 (8%)]\tLoss: 0.071047\n",
      "Train Epoch: 74 [600/6658 (9%)]\tLoss: 0.159291\n",
      "Train Epoch: 74 [700/6658 (11%)]\tLoss: 4.256826\n",
      "Train Epoch: 74 [800/6658 (12%)]\tLoss: 0.169445\n",
      "Train Epoch: 74 [900/6658 (14%)]\tLoss: 0.604052\n",
      "Train Epoch: 74 [1000/6658 (15%)]\tLoss: 0.305453\n",
      "Train Epoch: 74 [1100/6658 (17%)]\tLoss: 0.013434\n",
      "Train Epoch: 74 [1200/6658 (18%)]\tLoss: 0.028660\n",
      "Train Epoch: 74 [1300/6658 (20%)]\tLoss: 0.909507\n",
      "Train Epoch: 74 [1400/6658 (21%)]\tLoss: 0.002391\n",
      "Train Epoch: 74 [1500/6658 (23%)]\tLoss: 0.709384\n",
      "Train Epoch: 74 [1600/6658 (24%)]\tLoss: 0.475829\n",
      "Train Epoch: 74 [1700/6658 (26%)]\tLoss: 0.130738\n",
      "Train Epoch: 74 [1800/6658 (27%)]\tLoss: 0.410338\n",
      "Train Epoch: 74 [1900/6658 (29%)]\tLoss: 1.957076\n",
      "Train Epoch: 74 [2000/6658 (30%)]\tLoss: 0.757252\n",
      "Train Epoch: 74 [2100/6658 (32%)]\tLoss: 0.206770\n",
      "Train Epoch: 74 [2200/6658 (33%)]\tLoss: 0.370291\n",
      "Train Epoch: 74 [2300/6658 (35%)]\tLoss: 1.205663\n",
      "Train Epoch: 74 [2400/6658 (36%)]\tLoss: 0.173916\n",
      "Train Epoch: 74 [2500/6658 (38%)]\tLoss: 1.170023\n",
      "Train Epoch: 74 [2600/6658 (39%)]\tLoss: 0.093253\n",
      "Train Epoch: 74 [2700/6658 (41%)]\tLoss: 0.378939\n",
      "Train Epoch: 74 [2800/6658 (42%)]\tLoss: 0.245761\n",
      "Train Epoch: 74 [2900/6658 (44%)]\tLoss: 4.888840\n",
      "Train Epoch: 74 [3000/6658 (45%)]\tLoss: 0.487824\n",
      "Train Epoch: 74 [3100/6658 (47%)]\tLoss: 0.495091\n",
      "Train Epoch: 74 [3200/6658 (48%)]\tLoss: 3.328716\n",
      "Train Epoch: 74 [3300/6658 (50%)]\tLoss: 0.120758\n",
      "Train Epoch: 74 [3400/6658 (51%)]\tLoss: 1.071131\n",
      "Train Epoch: 74 [3500/6658 (53%)]\tLoss: 0.516916\n",
      "Train Epoch: 74 [3600/6658 (54%)]\tLoss: 0.954799\n",
      "Train Epoch: 74 [3700/6658 (56%)]\tLoss: 0.065633\n",
      "Train Epoch: 74 [3800/6658 (57%)]\tLoss: 0.047699\n",
      "Train Epoch: 74 [3900/6658 (59%)]\tLoss: 0.186336\n",
      "Train Epoch: 74 [4000/6658 (60%)]\tLoss: 0.032921\n",
      "Train Epoch: 74 [4100/6658 (62%)]\tLoss: 4.890630\n",
      "Train Epoch: 74 [4200/6658 (63%)]\tLoss: 0.203879\n",
      "Train Epoch: 74 [4300/6658 (65%)]\tLoss: 0.480428\n",
      "Train Epoch: 74 [4400/6658 (66%)]\tLoss: 0.000366\n",
      "Train Epoch: 74 [4500/6658 (68%)]\tLoss: 0.013275\n",
      "Train Epoch: 74 [4600/6658 (69%)]\tLoss: 1.135832\n",
      "Train Epoch: 74 [4700/6658 (71%)]\tLoss: 0.039763\n",
      "Train Epoch: 74 [4800/6658 (72%)]\tLoss: 0.559049\n",
      "Train Epoch: 74 [4900/6658 (74%)]\tLoss: 0.095128\n",
      "Train Epoch: 74 [5000/6658 (75%)]\tLoss: 0.417124\n",
      "Train Epoch: 74 [5100/6658 (77%)]\tLoss: 2.069809\n",
      "Train Epoch: 74 [5200/6658 (78%)]\tLoss: 0.001750\n",
      "Train Epoch: 74 [5300/6658 (80%)]\tLoss: 0.528860\n",
      "Train Epoch: 74 [5400/6658 (81%)]\tLoss: 0.350864\n",
      "Train Epoch: 74 [5500/6658 (83%)]\tLoss: 0.116588\n",
      "Train Epoch: 74 [5600/6658 (84%)]\tLoss: 0.018361\n",
      "Train Epoch: 74 [5700/6658 (86%)]\tLoss: 0.307145\n",
      "Train Epoch: 74 [5800/6658 (87%)]\tLoss: 0.071800\n",
      "Train Epoch: 74 [5900/6658 (89%)]\tLoss: 0.148184\n",
      "Train Epoch: 74 [6000/6658 (90%)]\tLoss: 0.865513\n",
      "Train Epoch: 74 [6100/6658 (92%)]\tLoss: 0.128109\n",
      "Train Epoch: 74 [6200/6658 (93%)]\tLoss: 0.040005\n",
      "Train Epoch: 74 [6300/6658 (95%)]\tLoss: 0.044206\n",
      "Train Epoch: 74 [6400/6658 (96%)]\tLoss: 0.331730\n",
      "Train Epoch: 74 [6500/6658 (98%)]\tLoss: 1.342860\n",
      "Train Epoch: 74 [6600/6658 (99%)]\tLoss: 0.001135\n",
      "train loss average =  0.7273983761873305\n",
      "\n",
      "Test set: Average loss: 0.7847\n",
      "\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1686, 6.0479, 5.9039, 5.9039, 6.1019, 6.0420], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 75 [0/6658 (0%)]\tLoss: 2.108195\n",
      "Train Epoch: 75 [100/6658 (2%)]\tLoss: 0.770278\n",
      "Train Epoch: 75 [200/6658 (3%)]\tLoss: 0.370483\n",
      "Train Epoch: 75 [300/6658 (5%)]\tLoss: 0.318457\n",
      "Train Epoch: 75 [400/6658 (6%)]\tLoss: 1.292384\n",
      "Train Epoch: 75 [500/6658 (8%)]\tLoss: 2.218060\n",
      "Train Epoch: 75 [600/6658 (9%)]\tLoss: 0.129751\n",
      "Train Epoch: 75 [700/6658 (11%)]\tLoss: 0.052349\n",
      "Train Epoch: 75 [800/6658 (12%)]\tLoss: 0.521079\n",
      "Train Epoch: 75 [900/6658 (14%)]\tLoss: 0.025879\n",
      "Train Epoch: 75 [1000/6658 (15%)]\tLoss: 0.036820\n",
      "Train Epoch: 75 [1100/6658 (17%)]\tLoss: 0.263660\n",
      "Train Epoch: 75 [1200/6658 (18%)]\tLoss: 0.064875\n",
      "Train Epoch: 75 [1300/6658 (20%)]\tLoss: 0.015504\n",
      "Train Epoch: 75 [1400/6658 (21%)]\tLoss: 0.530090\n",
      "Train Epoch: 75 [1500/6658 (23%)]\tLoss: 0.048441\n",
      "Train Epoch: 75 [1600/6658 (24%)]\tLoss: 0.143009\n",
      "Train Epoch: 75 [1700/6658 (26%)]\tLoss: 0.327788\n",
      "Train Epoch: 75 [1800/6658 (27%)]\tLoss: 2.222919\n",
      "Train Epoch: 75 [1900/6658 (29%)]\tLoss: 1.831860\n",
      "Train Epoch: 75 [2000/6658 (30%)]\tLoss: 0.012647\n",
      "Train Epoch: 75 [2100/6658 (32%)]\tLoss: 1.279361\n",
      "Train Epoch: 75 [2200/6658 (33%)]\tLoss: 0.039985\n",
      "Train Epoch: 75 [2300/6658 (35%)]\tLoss: 0.013929\n",
      "Train Epoch: 75 [2400/6658 (36%)]\tLoss: 0.003114\n",
      "Train Epoch: 75 [2500/6658 (38%)]\tLoss: 0.114057\n",
      "Train Epoch: 75 [2600/6658 (39%)]\tLoss: 0.080169\n",
      "Train Epoch: 75 [2700/6658 (41%)]\tLoss: 0.037253\n",
      "Train Epoch: 75 [2800/6658 (42%)]\tLoss: 1.100882\n",
      "Train Epoch: 75 [2900/6658 (44%)]\tLoss: 0.540229\n",
      "Train Epoch: 75 [3000/6658 (45%)]\tLoss: 0.214297\n",
      "Train Epoch: 75 [3100/6658 (47%)]\tLoss: 0.158401\n",
      "Train Epoch: 75 [3200/6658 (48%)]\tLoss: 0.318959\n",
      "Train Epoch: 75 [3300/6658 (50%)]\tLoss: 0.138781\n",
      "Train Epoch: 75 [3400/6658 (51%)]\tLoss: 0.291020\n",
      "Train Epoch: 75 [3500/6658 (53%)]\tLoss: 0.039277\n",
      "Train Epoch: 75 [3600/6658 (54%)]\tLoss: 0.014940\n",
      "Train Epoch: 75 [3700/6658 (56%)]\tLoss: 0.341095\n",
      "Train Epoch: 75 [3800/6658 (57%)]\tLoss: 2.135764\n",
      "Train Epoch: 75 [3900/6658 (59%)]\tLoss: 2.806200\n",
      "Train Epoch: 75 [4000/6658 (60%)]\tLoss: 0.000384\n",
      "Train Epoch: 75 [4100/6658 (62%)]\tLoss: 0.041684\n",
      "Train Epoch: 75 [4200/6658 (63%)]\tLoss: 18.715714\n",
      "Train Epoch: 75 [4300/6658 (65%)]\tLoss: 0.200499\n",
      "Train Epoch: 75 [4400/6658 (66%)]\tLoss: 0.006859\n",
      "Train Epoch: 75 [4500/6658 (68%)]\tLoss: 3.603916\n",
      "Train Epoch: 75 [4600/6658 (69%)]\tLoss: 0.332096\n",
      "Train Epoch: 75 [4700/6658 (71%)]\tLoss: 15.370334\n",
      "Train Epoch: 75 [4800/6658 (72%)]\tLoss: 0.352272\n",
      "Train Epoch: 75 [4900/6658 (74%)]\tLoss: 0.059916\n",
      "Train Epoch: 75 [5000/6658 (75%)]\tLoss: 1.294663\n",
      "Train Epoch: 75 [5100/6658 (77%)]\tLoss: 0.268609\n",
      "Train Epoch: 75 [5200/6658 (78%)]\tLoss: 0.953072\n",
      "Train Epoch: 75 [5300/6658 (80%)]\tLoss: 0.000794\n",
      "Train Epoch: 75 [5400/6658 (81%)]\tLoss: 0.112833\n",
      "Train Epoch: 75 [5500/6658 (83%)]\tLoss: 0.187845\n",
      "Train Epoch: 75 [5600/6658 (84%)]\tLoss: 0.108532\n",
      "Train Epoch: 75 [5700/6658 (86%)]\tLoss: 0.035028\n",
      "Train Epoch: 75 [5800/6658 (87%)]\tLoss: 0.105153\n",
      "Train Epoch: 75 [5900/6658 (89%)]\tLoss: 5.448662\n",
      "Train Epoch: 75 [6000/6658 (90%)]\tLoss: 0.049376\n",
      "Train Epoch: 75 [6100/6658 (92%)]\tLoss: 0.102514\n",
      "Train Epoch: 75 [6200/6658 (93%)]\tLoss: 1.304838\n",
      "Train Epoch: 75 [6300/6658 (95%)]\tLoss: 0.389270\n",
      "Train Epoch: 75 [6400/6658 (96%)]\tLoss: 0.552129\n",
      "Train Epoch: 75 [6500/6658 (98%)]\tLoss: 2.324319\n",
      "Train Epoch: 75 [6600/6658 (99%)]\tLoss: 0.240743\n",
      "train loss average =  0.7281809202154162\n",
      "\n",
      "Test set: Average loss: 0.7087\n",
      "\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1710, 6.0492, 5.9026, 5.9033, 6.1034, 6.0420], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 76 [0/6658 (0%)]\tLoss: 0.312020\n",
      "Train Epoch: 76 [100/6658 (2%)]\tLoss: 0.077430\n",
      "Train Epoch: 76 [200/6658 (3%)]\tLoss: 0.011130\n",
      "Train Epoch: 76 [300/6658 (5%)]\tLoss: 0.008636\n",
      "Train Epoch: 76 [400/6658 (6%)]\tLoss: 0.014596\n",
      "Train Epoch: 76 [500/6658 (8%)]\tLoss: 0.012661\n",
      "Train Epoch: 76 [600/6658 (9%)]\tLoss: 1.453271\n",
      "Train Epoch: 76 [700/6658 (11%)]\tLoss: 0.389741\n",
      "Train Epoch: 76 [800/6658 (12%)]\tLoss: 1.207141\n",
      "Train Epoch: 76 [900/6658 (14%)]\tLoss: 0.401289\n",
      "Train Epoch: 76 [1000/6658 (15%)]\tLoss: 4.186039\n",
      "Train Epoch: 76 [1100/6658 (17%)]\tLoss: 0.341987\n",
      "Train Epoch: 76 [1200/6658 (18%)]\tLoss: 0.245881\n",
      "Train Epoch: 76 [1300/6658 (20%)]\tLoss: 0.035168\n",
      "Train Epoch: 76 [1400/6658 (21%)]\tLoss: 0.143036\n",
      "Train Epoch: 76 [1500/6658 (23%)]\tLoss: 0.023227\n",
      "Train Epoch: 76 [1600/6658 (24%)]\tLoss: 0.229249\n",
      "Train Epoch: 76 [1700/6658 (26%)]\tLoss: 1.566653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 76 [1800/6658 (27%)]\tLoss: 0.313026\n",
      "Train Epoch: 76 [1900/6658 (29%)]\tLoss: 0.197281\n",
      "Train Epoch: 76 [2000/6658 (30%)]\tLoss: 2.757830\n",
      "Train Epoch: 76 [2100/6658 (32%)]\tLoss: 0.292354\n",
      "Train Epoch: 76 [2200/6658 (33%)]\tLoss: 0.214249\n",
      "Train Epoch: 76 [2300/6658 (35%)]\tLoss: 0.977321\n",
      "Train Epoch: 76 [2400/6658 (36%)]\tLoss: 0.278613\n",
      "Train Epoch: 76 [2500/6658 (38%)]\tLoss: 0.458115\n",
      "Train Epoch: 76 [2600/6658 (39%)]\tLoss: 0.492357\n",
      "Train Epoch: 76 [2700/6658 (41%)]\tLoss: 0.053853\n",
      "Train Epoch: 76 [2800/6658 (42%)]\tLoss: 0.790605\n",
      "Train Epoch: 76 [2900/6658 (44%)]\tLoss: 0.005733\n",
      "Train Epoch: 76 [3000/6658 (45%)]\tLoss: 0.044171\n",
      "Train Epoch: 76 [3100/6658 (47%)]\tLoss: 0.263501\n",
      "Train Epoch: 76 [3200/6658 (48%)]\tLoss: 0.515770\n",
      "Train Epoch: 76 [3300/6658 (50%)]\tLoss: 0.160995\n",
      "Train Epoch: 76 [3400/6658 (51%)]\tLoss: 0.145414\n",
      "Train Epoch: 76 [3500/6658 (53%)]\tLoss: 0.039492\n",
      "Train Epoch: 76 [3600/6658 (54%)]\tLoss: 0.059110\n",
      "Train Epoch: 76 [3700/6658 (56%)]\tLoss: 0.368653\n",
      "Train Epoch: 76 [3800/6658 (57%)]\tLoss: 0.006296\n",
      "Train Epoch: 76 [3900/6658 (59%)]\tLoss: 0.355498\n",
      "Train Epoch: 76 [4000/6658 (60%)]\tLoss: 0.284082\n",
      "Train Epoch: 76 [4100/6658 (62%)]\tLoss: 0.137123\n",
      "Train Epoch: 76 [4200/6658 (63%)]\tLoss: 0.144352\n",
      "Train Epoch: 76 [4300/6658 (65%)]\tLoss: 0.152439\n",
      "Train Epoch: 76 [4400/6658 (66%)]\tLoss: 1.012461\n",
      "Train Epoch: 76 [4500/6658 (68%)]\tLoss: 0.000141\n",
      "Train Epoch: 76 [4600/6658 (69%)]\tLoss: 0.004279\n",
      "Train Epoch: 76 [4700/6658 (71%)]\tLoss: 1.382968\n",
      "Train Epoch: 76 [4800/6658 (72%)]\tLoss: 0.309020\n",
      "Train Epoch: 76 [4900/6658 (74%)]\tLoss: 0.844056\n",
      "Train Epoch: 76 [5000/6658 (75%)]\tLoss: 0.467940\n",
      "Train Epoch: 76 [5100/6658 (77%)]\tLoss: 0.945611\n",
      "Train Epoch: 76 [5200/6658 (78%)]\tLoss: 0.239119\n",
      "Train Epoch: 76 [5300/6658 (80%)]\tLoss: 0.003554\n",
      "Train Epoch: 76 [5400/6658 (81%)]\tLoss: 0.370498\n",
      "Train Epoch: 76 [5500/6658 (83%)]\tLoss: 0.132533\n",
      "Train Epoch: 76 [5600/6658 (84%)]\tLoss: 0.872932\n",
      "Train Epoch: 76 [5700/6658 (86%)]\tLoss: 0.010247\n",
      "Train Epoch: 76 [5800/6658 (87%)]\tLoss: 0.010139\n",
      "Train Epoch: 76 [5900/6658 (89%)]\tLoss: 0.183044\n",
      "Train Epoch: 76 [6000/6658 (90%)]\tLoss: 0.216638\n",
      "Train Epoch: 76 [6100/6658 (92%)]\tLoss: 0.207703\n",
      "Train Epoch: 76 [6200/6658 (93%)]\tLoss: 0.637203\n",
      "Train Epoch: 76 [6300/6658 (95%)]\tLoss: 10.179516\n",
      "Train Epoch: 76 [6400/6658 (96%)]\tLoss: 0.228305\n",
      "Train Epoch: 76 [6500/6658 (98%)]\tLoss: 0.435321\n",
      "Train Epoch: 76 [6600/6658 (99%)]\tLoss: 0.304467\n",
      "train loss average =  0.7322687874780386\n",
      "\n",
      "Test set: Average loss: 0.6982\n",
      "\n",
      "Validation loss decreased (0.700575 --> 0.698242).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.1728, 6.0491, 5.9019, 5.9027, 6.1047, 6.0420], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 77 [0/6658 (0%)]\tLoss: 0.203084\n",
      "Train Epoch: 77 [100/6658 (2%)]\tLoss: 0.010819\n",
      "Train Epoch: 77 [200/6658 (3%)]\tLoss: 0.171349\n",
      "Train Epoch: 77 [300/6658 (5%)]\tLoss: 0.025130\n",
      "Train Epoch: 77 [400/6658 (6%)]\tLoss: 0.003489\n",
      "Train Epoch: 77 [500/6658 (8%)]\tLoss: 0.069128\n",
      "Train Epoch: 77 [600/6658 (9%)]\tLoss: 0.168363\n",
      "Train Epoch: 77 [700/6658 (11%)]\tLoss: 0.600858\n",
      "Train Epoch: 77 [800/6658 (12%)]\tLoss: 0.198173\n",
      "Train Epoch: 77 [900/6658 (14%)]\tLoss: 0.988848\n",
      "Train Epoch: 77 [1000/6658 (15%)]\tLoss: 0.452538\n",
      "Train Epoch: 77 [1100/6658 (17%)]\tLoss: 0.065698\n",
      "Train Epoch: 77 [1200/6658 (18%)]\tLoss: 1.227544\n",
      "Train Epoch: 77 [1300/6658 (20%)]\tLoss: 0.016405\n",
      "Train Epoch: 77 [1400/6658 (21%)]\tLoss: 2.169354\n",
      "Train Epoch: 77 [1500/6658 (23%)]\tLoss: 0.217171\n",
      "Train Epoch: 77 [1600/6658 (24%)]\tLoss: 0.055437\n",
      "Train Epoch: 77 [1700/6658 (26%)]\tLoss: 0.010884\n",
      "Train Epoch: 77 [1800/6658 (27%)]\tLoss: 0.113166\n",
      "Train Epoch: 77 [1900/6658 (29%)]\tLoss: 0.121151\n",
      "Train Epoch: 77 [2000/6658 (30%)]\tLoss: 30.404705\n",
      "Train Epoch: 77 [2100/6658 (32%)]\tLoss: 0.148989\n",
      "Train Epoch: 77 [2200/6658 (33%)]\tLoss: 0.021811\n",
      "Train Epoch: 77 [2300/6658 (35%)]\tLoss: 0.566355\n",
      "Train Epoch: 77 [2400/6658 (36%)]\tLoss: 0.407346\n",
      "Train Epoch: 77 [2500/6658 (38%)]\tLoss: 0.105310\n",
      "Train Epoch: 77 [2600/6658 (39%)]\tLoss: 0.101040\n",
      "Train Epoch: 77 [2700/6658 (41%)]\tLoss: 0.000003\n",
      "Train Epoch: 77 [2800/6658 (42%)]\tLoss: 0.027792\n",
      "Train Epoch: 77 [2900/6658 (44%)]\tLoss: 3.754531\n",
      "Train Epoch: 77 [3000/6658 (45%)]\tLoss: 0.139306\n",
      "Train Epoch: 77 [3100/6658 (47%)]\tLoss: 0.000520\n",
      "Train Epoch: 77 [3200/6658 (48%)]\tLoss: 0.038194\n",
      "Train Epoch: 77 [3300/6658 (50%)]\tLoss: 0.300206\n",
      "Train Epoch: 77 [3400/6658 (51%)]\tLoss: 0.592273\n",
      "Train Epoch: 77 [3500/6658 (53%)]\tLoss: 0.298842\n",
      "Train Epoch: 77 [3600/6658 (54%)]\tLoss: 0.080684\n",
      "Train Epoch: 77 [3700/6658 (56%)]\tLoss: 0.424111\n",
      "Train Epoch: 77 [3800/6658 (57%)]\tLoss: 0.246222\n",
      "Train Epoch: 77 [3900/6658 (59%)]\tLoss: 0.363996\n",
      "Train Epoch: 77 [4000/6658 (60%)]\tLoss: 0.412594\n",
      "Train Epoch: 77 [4100/6658 (62%)]\tLoss: 0.150588\n",
      "Train Epoch: 77 [4200/6658 (63%)]\tLoss: 0.566167\n",
      "Train Epoch: 77 [4300/6658 (65%)]\tLoss: 0.382646\n",
      "Train Epoch: 77 [4400/6658 (66%)]\tLoss: 0.626728\n",
      "Train Epoch: 77 [4500/6658 (68%)]\tLoss: 0.228300\n",
      "Train Epoch: 77 [4600/6658 (69%)]\tLoss: 1.188494\n",
      "Train Epoch: 77 [4700/6658 (71%)]\tLoss: 0.811017\n",
      "Train Epoch: 77 [4800/6658 (72%)]\tLoss: 1.237798\n",
      "Train Epoch: 77 [4900/6658 (74%)]\tLoss: 0.198947\n",
      "Train Epoch: 77 [5000/6658 (75%)]\tLoss: 0.151797\n",
      "Train Epoch: 77 [5100/6658 (77%)]\tLoss: 0.043074\n",
      "Train Epoch: 77 [5200/6658 (78%)]\tLoss: 1.285043\n",
      "Train Epoch: 77 [5300/6658 (80%)]\tLoss: 0.325470\n",
      "Train Epoch: 77 [5400/6658 (81%)]\tLoss: 0.040464\n",
      "Train Epoch: 77 [5500/6658 (83%)]\tLoss: 0.232960\n",
      "Train Epoch: 77 [5600/6658 (84%)]\tLoss: 0.775511\n",
      "Train Epoch: 77 [5700/6658 (86%)]\tLoss: 1.144719\n",
      "Train Epoch: 77 [5800/6658 (87%)]\tLoss: 0.043447\n",
      "Train Epoch: 77 [5900/6658 (89%)]\tLoss: 1.212480\n",
      "Train Epoch: 77 [6000/6658 (90%)]\tLoss: 0.298578\n",
      "Train Epoch: 77 [6100/6658 (92%)]\tLoss: 0.558338\n",
      "Train Epoch: 77 [6200/6658 (93%)]\tLoss: 0.310007\n",
      "Train Epoch: 77 [6300/6658 (95%)]\tLoss: 0.329442\n",
      "Train Epoch: 77 [6400/6658 (96%)]\tLoss: 0.000084\n",
      "Train Epoch: 77 [6500/6658 (98%)]\tLoss: 1.189057\n",
      "Train Epoch: 77 [6600/6658 (99%)]\tLoss: 0.003830\n",
      "train loss average =  0.7294535609200817\n",
      "\n",
      "Test set: Average loss: 0.7189\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1754, 6.0495, 5.9020, 5.9024, 6.1062, 6.0425], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 78 [0/6658 (0%)]\tLoss: 0.649926\n",
      "Train Epoch: 78 [100/6658 (2%)]\tLoss: 0.456168\n",
      "Train Epoch: 78 [200/6658 (3%)]\tLoss: 1.498194\n",
      "Train Epoch: 78 [300/6658 (5%)]\tLoss: 0.786665\n",
      "Train Epoch: 78 [400/6658 (6%)]\tLoss: 0.845378\n",
      "Train Epoch: 78 [500/6658 (8%)]\tLoss: 0.075801\n",
      "Train Epoch: 78 [600/6658 (9%)]\tLoss: 2.288072\n",
      "Train Epoch: 78 [700/6658 (11%)]\tLoss: 0.186277\n",
      "Train Epoch: 78 [800/6658 (12%)]\tLoss: 0.438803\n",
      "Train Epoch: 78 [900/6658 (14%)]\tLoss: 0.070378\n",
      "Train Epoch: 78 [1000/6658 (15%)]\tLoss: 0.102776\n",
      "Train Epoch: 78 [1100/6658 (17%)]\tLoss: 1.017205\n",
      "Train Epoch: 78 [1200/6658 (18%)]\tLoss: 0.479283\n",
      "Train Epoch: 78 [1300/6658 (20%)]\tLoss: 0.844701\n",
      "Train Epoch: 78 [1400/6658 (21%)]\tLoss: 0.583833\n",
      "Train Epoch: 78 [1500/6658 (23%)]\tLoss: 0.366651\n",
      "Train Epoch: 78 [1600/6658 (24%)]\tLoss: 0.384792\n",
      "Train Epoch: 78 [1700/6658 (26%)]\tLoss: 0.611047\n",
      "Train Epoch: 78 [1800/6658 (27%)]\tLoss: 0.054973\n",
      "Train Epoch: 78 [1900/6658 (29%)]\tLoss: 0.035452\n",
      "Train Epoch: 78 [2000/6658 (30%)]\tLoss: 0.445773\n",
      "Train Epoch: 78 [2100/6658 (32%)]\tLoss: 1.583324\n",
      "Train Epoch: 78 [2200/6658 (33%)]\tLoss: 0.282043\n",
      "Train Epoch: 78 [2300/6658 (35%)]\tLoss: 0.055098\n",
      "Train Epoch: 78 [2400/6658 (36%)]\tLoss: 0.936164\n",
      "Train Epoch: 78 [2500/6658 (38%)]\tLoss: 1.054084\n",
      "Train Epoch: 78 [2600/6658 (39%)]\tLoss: 0.548070\n",
      "Train Epoch: 78 [2700/6658 (41%)]\tLoss: 0.107813\n",
      "Train Epoch: 78 [2800/6658 (42%)]\tLoss: 0.003484\n",
      "Train Epoch: 78 [2900/6658 (44%)]\tLoss: 1.025291\n",
      "Train Epoch: 78 [3000/6658 (45%)]\tLoss: 0.005028\n",
      "Train Epoch: 78 [3100/6658 (47%)]\tLoss: 0.345565\n",
      "Train Epoch: 78 [3200/6658 (48%)]\tLoss: 0.042945\n",
      "Train Epoch: 78 [3300/6658 (50%)]\tLoss: 0.056255\n",
      "Train Epoch: 78 [3400/6658 (51%)]\tLoss: 0.008904\n",
      "Train Epoch: 78 [3500/6658 (53%)]\tLoss: 0.671226\n",
      "Train Epoch: 78 [3600/6658 (54%)]\tLoss: 0.162712\n",
      "Train Epoch: 78 [3700/6658 (56%)]\tLoss: 0.039536\n",
      "Train Epoch: 78 [3800/6658 (57%)]\tLoss: 1.122531\n",
      "Train Epoch: 78 [3900/6658 (59%)]\tLoss: 0.167736\n",
      "Train Epoch: 78 [4000/6658 (60%)]\tLoss: 0.067401\n",
      "Train Epoch: 78 [4100/6658 (62%)]\tLoss: 0.109210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 78 [4200/6658 (63%)]\tLoss: 0.416115\n",
      "Train Epoch: 78 [4300/6658 (65%)]\tLoss: 0.108324\n",
      "Train Epoch: 78 [4400/6658 (66%)]\tLoss: 2.347315\n",
      "Train Epoch: 78 [4500/6658 (68%)]\tLoss: 0.530511\n",
      "Train Epoch: 78 [4600/6658 (69%)]\tLoss: 0.000449\n",
      "Train Epoch: 78 [4700/6658 (71%)]\tLoss: 0.100271\n",
      "Train Epoch: 78 [4800/6658 (72%)]\tLoss: 0.112872\n",
      "Train Epoch: 78 [4900/6658 (74%)]\tLoss: 0.161169\n",
      "Train Epoch: 78 [5000/6658 (75%)]\tLoss: 0.018443\n",
      "Train Epoch: 78 [5100/6658 (77%)]\tLoss: 0.290254\n",
      "Train Epoch: 78 [5200/6658 (78%)]\tLoss: 0.331818\n",
      "Train Epoch: 78 [5300/6658 (80%)]\tLoss: 1.017191\n",
      "Train Epoch: 78 [5400/6658 (81%)]\tLoss: 0.531350\n",
      "Train Epoch: 78 [5500/6658 (83%)]\tLoss: 2.423493\n",
      "Train Epoch: 78 [5600/6658 (84%)]\tLoss: 0.046950\n",
      "Train Epoch: 78 [5700/6658 (86%)]\tLoss: 1.460352\n",
      "Train Epoch: 78 [5800/6658 (87%)]\tLoss: 0.000001\n",
      "Train Epoch: 78 [5900/6658 (89%)]\tLoss: 0.035262\n",
      "Train Epoch: 78 [6000/6658 (90%)]\tLoss: 0.015111\n",
      "Train Epoch: 78 [6100/6658 (92%)]\tLoss: 1.867988\n",
      "Train Epoch: 78 [6200/6658 (93%)]\tLoss: 0.341452\n",
      "Train Epoch: 78 [6300/6658 (95%)]\tLoss: 0.015069\n",
      "Train Epoch: 78 [6400/6658 (96%)]\tLoss: 0.000027\n",
      "Train Epoch: 78 [6500/6658 (98%)]\tLoss: 0.009468\n",
      "Train Epoch: 78 [6600/6658 (99%)]\tLoss: 0.185270\n",
      "train loss average =  0.7235622718206305\n",
      "\n",
      "Test set: Average loss: 0.7168\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1771, 6.0507, 5.9007, 5.9010, 6.1076, 6.0428], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 79 [0/6658 (0%)]\tLoss: 0.658215\n",
      "Train Epoch: 79 [100/6658 (2%)]\tLoss: 2.314687\n",
      "Train Epoch: 79 [200/6658 (3%)]\tLoss: 0.005165\n",
      "Train Epoch: 79 [300/6658 (5%)]\tLoss: 0.000694\n",
      "Train Epoch: 79 [400/6658 (6%)]\tLoss: 0.052526\n",
      "Train Epoch: 79 [500/6658 (8%)]\tLoss: 0.011826\n",
      "Train Epoch: 79 [600/6658 (9%)]\tLoss: 0.121451\n",
      "Train Epoch: 79 [700/6658 (11%)]\tLoss: 0.033349\n",
      "Train Epoch: 79 [800/6658 (12%)]\tLoss: 0.308339\n",
      "Train Epoch: 79 [900/6658 (14%)]\tLoss: 0.602503\n",
      "Train Epoch: 79 [1000/6658 (15%)]\tLoss: 0.547486\n",
      "Train Epoch: 79 [1100/6658 (17%)]\tLoss: 0.188900\n",
      "Train Epoch: 79 [1200/6658 (18%)]\tLoss: 0.015178\n",
      "Train Epoch: 79 [1300/6658 (20%)]\tLoss: 0.119078\n",
      "Train Epoch: 79 [1400/6658 (21%)]\tLoss: 0.511901\n",
      "Train Epoch: 79 [1500/6658 (23%)]\tLoss: 0.253426\n",
      "Train Epoch: 79 [1600/6658 (24%)]\tLoss: 0.095043\n",
      "Train Epoch: 79 [1700/6658 (26%)]\tLoss: 0.213160\n",
      "Train Epoch: 79 [1800/6658 (27%)]\tLoss: 0.116841\n",
      "Train Epoch: 79 [1900/6658 (29%)]\tLoss: 1.078586\n",
      "Train Epoch: 79 [2000/6658 (30%)]\tLoss: 0.555591\n",
      "Train Epoch: 79 [2100/6658 (32%)]\tLoss: 0.057021\n",
      "Train Epoch: 79 [2200/6658 (33%)]\tLoss: 0.072841\n",
      "Train Epoch: 79 [2300/6658 (35%)]\tLoss: 0.710806\n",
      "Train Epoch: 79 [2400/6658 (36%)]\tLoss: 0.636933\n",
      "Train Epoch: 79 [2500/6658 (38%)]\tLoss: 2.830443\n",
      "Train Epoch: 79 [2600/6658 (39%)]\tLoss: 0.842259\n",
      "Train Epoch: 79 [2700/6658 (41%)]\tLoss: 12.365293\n",
      "Train Epoch: 79 [2800/6658 (42%)]\tLoss: 0.036910\n",
      "Train Epoch: 79 [2900/6658 (44%)]\tLoss: 0.090949\n",
      "Train Epoch: 79 [3000/6658 (45%)]\tLoss: 2.214977\n",
      "Train Epoch: 79 [3100/6658 (47%)]\tLoss: 0.227385\n",
      "Train Epoch: 79 [3200/6658 (48%)]\tLoss: 1.535733\n",
      "Train Epoch: 79 [3300/6658 (50%)]\tLoss: 0.031928\n",
      "Train Epoch: 79 [3400/6658 (51%)]\tLoss: 0.377256\n",
      "Train Epoch: 79 [3500/6658 (53%)]\tLoss: 0.157924\n",
      "Train Epoch: 79 [3600/6658 (54%)]\tLoss: 0.552536\n",
      "Train Epoch: 79 [3700/6658 (56%)]\tLoss: 0.185316\n",
      "Train Epoch: 79 [3800/6658 (57%)]\tLoss: 0.292902\n",
      "Train Epoch: 79 [3900/6658 (59%)]\tLoss: 0.543530\n",
      "Train Epoch: 79 [4000/6658 (60%)]\tLoss: 0.376170\n",
      "Train Epoch: 79 [4100/6658 (62%)]\tLoss: 0.844097\n",
      "Train Epoch: 79 [4200/6658 (63%)]\tLoss: 0.233868\n",
      "Train Epoch: 79 [4300/6658 (65%)]\tLoss: 0.260794\n",
      "Train Epoch: 79 [4400/6658 (66%)]\tLoss: 0.042129\n",
      "Train Epoch: 79 [4500/6658 (68%)]\tLoss: 0.048934\n",
      "Train Epoch: 79 [4600/6658 (69%)]\tLoss: 0.019369\n",
      "Train Epoch: 79 [4700/6658 (71%)]\tLoss: 0.689649\n",
      "Train Epoch: 79 [4800/6658 (72%)]\tLoss: 5.054346\n",
      "Train Epoch: 79 [4900/6658 (74%)]\tLoss: 1.596561\n",
      "Train Epoch: 79 [5000/6658 (75%)]\tLoss: 3.588575\n",
      "Train Epoch: 79 [5100/6658 (77%)]\tLoss: 0.003814\n",
      "Train Epoch: 79 [5200/6658 (78%)]\tLoss: 1.921983\n",
      "Train Epoch: 79 [5300/6658 (80%)]\tLoss: 0.476687\n",
      "Train Epoch: 79 [5400/6658 (81%)]\tLoss: 0.650771\n",
      "Train Epoch: 79 [5500/6658 (83%)]\tLoss: 0.976591\n",
      "Train Epoch: 79 [5600/6658 (84%)]\tLoss: 0.089241\n",
      "Train Epoch: 79 [5700/6658 (86%)]\tLoss: 2.874835\n",
      "Train Epoch: 79 [5800/6658 (87%)]\tLoss: 0.622554\n",
      "Train Epoch: 79 [5900/6658 (89%)]\tLoss: 0.197785\n",
      "Train Epoch: 79 [6000/6658 (90%)]\tLoss: 0.142528\n",
      "Train Epoch: 79 [6100/6658 (92%)]\tLoss: 0.470006\n",
      "Train Epoch: 79 [6200/6658 (93%)]\tLoss: 0.247764\n",
      "Train Epoch: 79 [6300/6658 (95%)]\tLoss: 0.587151\n",
      "Train Epoch: 79 [6400/6658 (96%)]\tLoss: 2.940952\n",
      "Train Epoch: 79 [6500/6658 (98%)]\tLoss: 0.879493\n",
      "Train Epoch: 79 [6600/6658 (99%)]\tLoss: 0.017065\n",
      "train loss average =  0.7350179970948781\n",
      "\n",
      "Test set: Average loss: 0.7101\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1796, 6.0516, 5.9002, 5.9007, 6.1089, 6.0429], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 80 [0/6658 (0%)]\tLoss: 0.568876\n",
      "Train Epoch: 80 [100/6658 (2%)]\tLoss: 0.003395\n",
      "Train Epoch: 80 [200/6658 (3%)]\tLoss: 0.259226\n",
      "Train Epoch: 80 [300/6658 (5%)]\tLoss: 1.503774\n",
      "Train Epoch: 80 [400/6658 (6%)]\tLoss: 0.122248\n",
      "Train Epoch: 80 [500/6658 (8%)]\tLoss: 0.001589\n",
      "Train Epoch: 80 [600/6658 (9%)]\tLoss: 0.027995\n",
      "Train Epoch: 80 [700/6658 (11%)]\tLoss: 0.242421\n",
      "Train Epoch: 80 [800/6658 (12%)]\tLoss: 0.134025\n",
      "Train Epoch: 80 [900/6658 (14%)]\tLoss: 0.909570\n",
      "Train Epoch: 80 [1000/6658 (15%)]\tLoss: 0.307673\n",
      "Train Epoch: 80 [1100/6658 (17%)]\tLoss: 0.371966\n",
      "Train Epoch: 80 [1200/6658 (18%)]\tLoss: 0.217575\n",
      "Train Epoch: 80 [1300/6658 (20%)]\tLoss: 3.138593\n",
      "Train Epoch: 80 [1400/6658 (21%)]\tLoss: 0.088151\n",
      "Train Epoch: 80 [1500/6658 (23%)]\tLoss: 0.304618\n",
      "Train Epoch: 80 [1600/6658 (24%)]\tLoss: 0.069317\n",
      "Train Epoch: 80 [1700/6658 (26%)]\tLoss: 0.783038\n",
      "Train Epoch: 80 [1800/6658 (27%)]\tLoss: 0.432520\n",
      "Train Epoch: 80 [1900/6658 (29%)]\tLoss: 0.122624\n",
      "Train Epoch: 80 [2000/6658 (30%)]\tLoss: 0.041217\n",
      "Train Epoch: 80 [2100/6658 (32%)]\tLoss: 6.974861\n",
      "Train Epoch: 80 [2200/6658 (33%)]\tLoss: 0.876424\n",
      "Train Epoch: 80 [2300/6658 (35%)]\tLoss: 0.359951\n",
      "Train Epoch: 80 [2400/6658 (36%)]\tLoss: 0.038939\n",
      "Train Epoch: 80 [2500/6658 (38%)]\tLoss: 0.465156\n",
      "Train Epoch: 80 [2600/6658 (39%)]\tLoss: 0.030642\n",
      "Train Epoch: 80 [2700/6658 (41%)]\tLoss: 0.717366\n",
      "Train Epoch: 80 [2800/6658 (42%)]\tLoss: 0.100875\n",
      "Train Epoch: 80 [2900/6658 (44%)]\tLoss: 0.693250\n",
      "Train Epoch: 80 [3000/6658 (45%)]\tLoss: 0.899468\n",
      "Train Epoch: 80 [3100/6658 (47%)]\tLoss: 0.189456\n",
      "Train Epoch: 80 [3200/6658 (48%)]\tLoss: 0.277118\n",
      "Train Epoch: 80 [3300/6658 (50%)]\tLoss: 0.973009\n",
      "Train Epoch: 80 [3400/6658 (51%)]\tLoss: 1.576352\n",
      "Train Epoch: 80 [3500/6658 (53%)]\tLoss: 0.939434\n",
      "Train Epoch: 80 [3600/6658 (54%)]\tLoss: 0.293175\n",
      "Train Epoch: 80 [3700/6658 (56%)]\tLoss: 5.606218\n",
      "Train Epoch: 80 [3800/6658 (57%)]\tLoss: 0.466483\n",
      "Train Epoch: 80 [3900/6658 (59%)]\tLoss: 0.088432\n",
      "Train Epoch: 80 [4000/6658 (60%)]\tLoss: 0.015918\n",
      "Train Epoch: 80 [4100/6658 (62%)]\tLoss: 0.150410\n",
      "Train Epoch: 80 [4200/6658 (63%)]\tLoss: 0.791053\n",
      "Train Epoch: 80 [4300/6658 (65%)]\tLoss: 0.966876\n",
      "Train Epoch: 80 [4400/6658 (66%)]\tLoss: 0.000276\n",
      "Train Epoch: 80 [4500/6658 (68%)]\tLoss: 0.139220\n",
      "Train Epoch: 80 [4600/6658 (69%)]\tLoss: 0.689182\n",
      "Train Epoch: 80 [4700/6658 (71%)]\tLoss: 10.399401\n",
      "Train Epoch: 80 [4800/6658 (72%)]\tLoss: 1.535720\n",
      "Train Epoch: 80 [4900/6658 (74%)]\tLoss: 1.039795\n",
      "Train Epoch: 80 [5000/6658 (75%)]\tLoss: 0.018383\n",
      "Train Epoch: 80 [5100/6658 (77%)]\tLoss: 0.161069\n",
      "Train Epoch: 80 [5200/6658 (78%)]\tLoss: 0.059180\n",
      "Train Epoch: 80 [5300/6658 (80%)]\tLoss: 0.482175\n",
      "Train Epoch: 80 [5400/6658 (81%)]\tLoss: 1.376903\n",
      "Train Epoch: 80 [5500/6658 (83%)]\tLoss: 2.023088\n",
      "Train Epoch: 80 [5600/6658 (84%)]\tLoss: 0.010588\n",
      "Train Epoch: 80 [5700/6658 (86%)]\tLoss: 0.817005\n",
      "Train Epoch: 80 [5800/6658 (87%)]\tLoss: 3.550059\n",
      "Train Epoch: 80 [5900/6658 (89%)]\tLoss: 0.158479\n",
      "Train Epoch: 80 [6000/6658 (90%)]\tLoss: 0.234696\n",
      "Train Epoch: 80 [6100/6658 (92%)]\tLoss: 2.251674\n",
      "Train Epoch: 80 [6200/6658 (93%)]\tLoss: 0.016448\n",
      "Train Epoch: 80 [6300/6658 (95%)]\tLoss: 0.002936\n",
      "Train Epoch: 80 [6400/6658 (96%)]\tLoss: 0.568394\n",
      "Train Epoch: 80 [6500/6658 (98%)]\tLoss: 0.161653\n",
      "Train Epoch: 80 [6600/6658 (99%)]\tLoss: 0.002927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss average =  0.7262006886182287\n",
      "\n",
      "Test set: Average loss: 0.7238\n",
      "\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1816, 6.0519, 5.8987, 5.8999, 6.1108, 6.0434], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 81 [0/6658 (0%)]\tLoss: 4.504148\n",
      "Train Epoch: 81 [100/6658 (2%)]\tLoss: 0.517463\n",
      "Train Epoch: 81 [200/6658 (3%)]\tLoss: 0.305446\n",
      "Train Epoch: 81 [300/6658 (5%)]\tLoss: 0.057435\n",
      "Train Epoch: 81 [400/6658 (6%)]\tLoss: 0.001410\n",
      "Train Epoch: 81 [500/6658 (8%)]\tLoss: 0.054049\n",
      "Train Epoch: 81 [600/6658 (9%)]\tLoss: 0.481620\n",
      "Train Epoch: 81 [700/6658 (11%)]\tLoss: 0.036126\n",
      "Train Epoch: 81 [800/6658 (12%)]\tLoss: 0.646132\n",
      "Train Epoch: 81 [900/6658 (14%)]\tLoss: 4.305756\n",
      "Train Epoch: 81 [1000/6658 (15%)]\tLoss: 0.144591\n",
      "Train Epoch: 81 [1100/6658 (17%)]\tLoss: 0.317834\n",
      "Train Epoch: 81 [1200/6658 (18%)]\tLoss: 0.255357\n",
      "Train Epoch: 81 [1300/6658 (20%)]\tLoss: 0.392532\n",
      "Train Epoch: 81 [1400/6658 (21%)]\tLoss: 0.012986\n",
      "Train Epoch: 81 [1500/6658 (23%)]\tLoss: 0.283350\n",
      "Train Epoch: 81 [1600/6658 (24%)]\tLoss: 15.790303\n",
      "Train Epoch: 81 [1700/6658 (26%)]\tLoss: 0.016872\n",
      "Train Epoch: 81 [1800/6658 (27%)]\tLoss: 4.707154\n",
      "Train Epoch: 81 [1900/6658 (29%)]\tLoss: 0.063296\n",
      "Train Epoch: 81 [2000/6658 (30%)]\tLoss: 0.145216\n",
      "Train Epoch: 81 [2100/6658 (32%)]\tLoss: 0.954139\n",
      "Train Epoch: 81 [2200/6658 (33%)]\tLoss: 0.587448\n",
      "Train Epoch: 81 [2300/6658 (35%)]\tLoss: 0.084457\n",
      "Train Epoch: 81 [2400/6658 (36%)]\tLoss: 0.002105\n",
      "Train Epoch: 81 [2500/6658 (38%)]\tLoss: 0.036977\n",
      "Train Epoch: 81 [2600/6658 (39%)]\tLoss: 0.127809\n",
      "Train Epoch: 81 [2700/6658 (41%)]\tLoss: 0.025814\n",
      "Train Epoch: 81 [2800/6658 (42%)]\tLoss: 0.863573\n",
      "Train Epoch: 81 [2900/6658 (44%)]\tLoss: 0.001107\n",
      "Train Epoch: 81 [3000/6658 (45%)]\tLoss: 1.220192\n",
      "Train Epoch: 81 [3100/6658 (47%)]\tLoss: 0.213673\n",
      "Train Epoch: 81 [3200/6658 (48%)]\tLoss: 0.004228\n",
      "Train Epoch: 81 [3300/6658 (50%)]\tLoss: 0.000008\n",
      "Train Epoch: 81 [3400/6658 (51%)]\tLoss: 14.707193\n",
      "Train Epoch: 81 [3500/6658 (53%)]\tLoss: 0.021633\n",
      "Train Epoch: 81 [3600/6658 (54%)]\tLoss: 0.040713\n",
      "Train Epoch: 81 [3700/6658 (56%)]\tLoss: 0.015955\n",
      "Train Epoch: 81 [3800/6658 (57%)]\tLoss: 1.827486\n",
      "Train Epoch: 81 [3900/6658 (59%)]\tLoss: 0.163380\n",
      "Train Epoch: 81 [4000/6658 (60%)]\tLoss: 0.125687\n",
      "Train Epoch: 81 [4100/6658 (62%)]\tLoss: 0.010190\n",
      "Train Epoch: 81 [4200/6658 (63%)]\tLoss: 1.014566\n",
      "Train Epoch: 81 [4300/6658 (65%)]\tLoss: 0.454776\n",
      "Train Epoch: 81 [4400/6658 (66%)]\tLoss: 0.011609\n",
      "Train Epoch: 81 [4500/6658 (68%)]\tLoss: 0.615075\n",
      "Train Epoch: 81 [4600/6658 (69%)]\tLoss: 2.472068\n",
      "Train Epoch: 81 [4700/6658 (71%)]\tLoss: 0.923388\n",
      "Train Epoch: 81 [4800/6658 (72%)]\tLoss: 0.027528\n",
      "Train Epoch: 81 [4900/6658 (74%)]\tLoss: 0.099714\n",
      "Train Epoch: 81 [5000/6658 (75%)]\tLoss: 0.339743\n",
      "Train Epoch: 81 [5100/6658 (77%)]\tLoss: 0.400871\n",
      "Train Epoch: 81 [5200/6658 (78%)]\tLoss: 0.000004\n",
      "Train Epoch: 81 [5300/6658 (80%)]\tLoss: 0.045041\n",
      "Train Epoch: 81 [5400/6658 (81%)]\tLoss: 0.596236\n",
      "Train Epoch: 81 [5500/6658 (83%)]\tLoss: 1.497760\n",
      "Train Epoch: 81 [5600/6658 (84%)]\tLoss: 0.903780\n",
      "Train Epoch: 81 [5700/6658 (86%)]\tLoss: 0.038606\n",
      "Train Epoch: 81 [5800/6658 (87%)]\tLoss: 1.126578\n",
      "Train Epoch: 81 [5900/6658 (89%)]\tLoss: 0.569983\n",
      "Train Epoch: 81 [6000/6658 (90%)]\tLoss: 0.169471\n",
      "Train Epoch: 81 [6100/6658 (92%)]\tLoss: 0.267738\n",
      "Train Epoch: 81 [6200/6658 (93%)]\tLoss: 0.517154\n",
      "Train Epoch: 81 [6300/6658 (95%)]\tLoss: 5.007144\n",
      "Train Epoch: 81 [6400/6658 (96%)]\tLoss: 0.313089\n",
      "Train Epoch: 81 [6500/6658 (98%)]\tLoss: 0.375122\n",
      "Train Epoch: 81 [6600/6658 (99%)]\tLoss: 0.170860\n",
      "train loss average =  0.7215685433993635\n",
      "\n",
      "Test set: Average loss: 0.7194\n",
      "\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1837, 6.0523, 5.8982, 5.8985, 6.1130, 6.0441], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 82 [0/6658 (0%)]\tLoss: 0.039050\n",
      "Train Epoch: 82 [100/6658 (2%)]\tLoss: 1.260432\n",
      "Train Epoch: 82 [200/6658 (3%)]\tLoss: 0.012596\n",
      "Train Epoch: 82 [300/6658 (5%)]\tLoss: 0.004846\n",
      "Train Epoch: 82 [400/6658 (6%)]\tLoss: 0.075622\n",
      "Train Epoch: 82 [500/6658 (8%)]\tLoss: 0.935861\n",
      "Train Epoch: 82 [600/6658 (9%)]\tLoss: 0.372498\n",
      "Train Epoch: 82 [700/6658 (11%)]\tLoss: 1.748395\n",
      "Train Epoch: 82 [800/6658 (12%)]\tLoss: 0.117284\n",
      "Train Epoch: 82 [900/6658 (14%)]\tLoss: 0.218693\n",
      "Train Epoch: 82 [1000/6658 (15%)]\tLoss: 0.445293\n",
      "Train Epoch: 82 [1100/6658 (17%)]\tLoss: 0.289477\n",
      "Train Epoch: 82 [1200/6658 (18%)]\tLoss: 3.231138\n",
      "Train Epoch: 82 [1300/6658 (20%)]\tLoss: 0.194686\n",
      "Train Epoch: 82 [1400/6658 (21%)]\tLoss: 7.844406\n",
      "Train Epoch: 82 [1500/6658 (23%)]\tLoss: 0.753231\n",
      "Train Epoch: 82 [1600/6658 (24%)]\tLoss: 0.042248\n",
      "Train Epoch: 82 [1700/6658 (26%)]\tLoss: 0.413691\n",
      "Train Epoch: 82 [1800/6658 (27%)]\tLoss: 0.090227\n",
      "Train Epoch: 82 [1900/6658 (29%)]\tLoss: 0.076691\n",
      "Train Epoch: 82 [2000/6658 (30%)]\tLoss: 0.523296\n",
      "Train Epoch: 82 [2100/6658 (32%)]\tLoss: 1.897223\n",
      "Train Epoch: 82 [2200/6658 (33%)]\tLoss: 0.270036\n",
      "Train Epoch: 82 [2300/6658 (35%)]\tLoss: 0.220229\n",
      "Train Epoch: 82 [2400/6658 (36%)]\tLoss: 0.270624\n",
      "Train Epoch: 82 [2500/6658 (38%)]\tLoss: 0.465085\n",
      "Train Epoch: 82 [2600/6658 (39%)]\tLoss: 0.000021\n",
      "Train Epoch: 82 [2700/6658 (41%)]\tLoss: 0.000780\n",
      "Train Epoch: 82 [2800/6658 (42%)]\tLoss: 0.519066\n",
      "Train Epoch: 82 [2900/6658 (44%)]\tLoss: 0.731838\n",
      "Train Epoch: 82 [3000/6658 (45%)]\tLoss: 3.013224\n",
      "Train Epoch: 82 [3100/6658 (47%)]\tLoss: 0.007952\n",
      "Train Epoch: 82 [3200/6658 (48%)]\tLoss: 0.486384\n",
      "Train Epoch: 82 [3300/6658 (50%)]\tLoss: 0.240856\n",
      "Train Epoch: 82 [3400/6658 (51%)]\tLoss: 0.000462\n",
      "Train Epoch: 82 [3500/6658 (53%)]\tLoss: 0.027775\n",
      "Train Epoch: 82 [3600/6658 (54%)]\tLoss: 0.033557\n",
      "Train Epoch: 82 [3700/6658 (56%)]\tLoss: 0.981785\n",
      "Train Epoch: 82 [3800/6658 (57%)]\tLoss: 3.259985\n",
      "Train Epoch: 82 [3900/6658 (59%)]\tLoss: 4.733717\n",
      "Train Epoch: 82 [4000/6658 (60%)]\tLoss: 0.332389\n",
      "Train Epoch: 82 [4100/6658 (62%)]\tLoss: 0.858096\n",
      "Train Epoch: 82 [4200/6658 (63%)]\tLoss: 0.087551\n",
      "Train Epoch: 82 [4300/6658 (65%)]\tLoss: 0.696838\n",
      "Train Epoch: 82 [4400/6658 (66%)]\tLoss: 0.021692\n",
      "Train Epoch: 82 [4500/6658 (68%)]\tLoss: 0.520280\n",
      "Train Epoch: 82 [4600/6658 (69%)]\tLoss: 0.381025\n",
      "Train Epoch: 82 [4700/6658 (71%)]\tLoss: 4.029518\n",
      "Train Epoch: 82 [4800/6658 (72%)]\tLoss: 0.010941\n",
      "Train Epoch: 82 [4900/6658 (74%)]\tLoss: 1.880236\n",
      "Train Epoch: 82 [5000/6658 (75%)]\tLoss: 0.036793\n",
      "Train Epoch: 82 [5100/6658 (77%)]\tLoss: 0.172402\n",
      "Train Epoch: 82 [5200/6658 (78%)]\tLoss: 0.321510\n",
      "Train Epoch: 82 [5300/6658 (80%)]\tLoss: 0.185861\n",
      "Train Epoch: 82 [5400/6658 (81%)]\tLoss: 0.048323\n",
      "Train Epoch: 82 [5500/6658 (83%)]\tLoss: 0.057861\n",
      "Train Epoch: 82 [5600/6658 (84%)]\tLoss: 0.244426\n",
      "Train Epoch: 82 [5700/6658 (86%)]\tLoss: 0.415186\n",
      "Train Epoch: 82 [5800/6658 (87%)]\tLoss: 0.003112\n",
      "Train Epoch: 82 [5900/6658 (89%)]\tLoss: 0.277102\n",
      "Train Epoch: 82 [6000/6658 (90%)]\tLoss: 0.332624\n",
      "Train Epoch: 82 [6100/6658 (92%)]\tLoss: 0.110841\n",
      "Train Epoch: 82 [6200/6658 (93%)]\tLoss: 0.033370\n",
      "Train Epoch: 82 [6300/6658 (95%)]\tLoss: 1.034770\n",
      "Train Epoch: 82 [6400/6658 (96%)]\tLoss: 0.076655\n",
      "Train Epoch: 82 [6500/6658 (98%)]\tLoss: 0.009583\n",
      "Train Epoch: 82 [6600/6658 (99%)]\tLoss: 0.534346\n",
      "train loss average =  0.7263659111339344\n",
      "\n",
      "Test set: Average loss: 0.6967\n",
      "\n",
      "Validation loss decreased (0.698242 --> 0.696673).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.1858, 6.0539, 5.8975, 5.8984, 6.1147, 6.0437], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 83 [0/6658 (0%)]\tLoss: 0.119382\n",
      "Train Epoch: 83 [100/6658 (2%)]\tLoss: 1.332146\n",
      "Train Epoch: 83 [200/6658 (3%)]\tLoss: 0.012328\n",
      "Train Epoch: 83 [300/6658 (5%)]\tLoss: 0.249408\n",
      "Train Epoch: 83 [400/6658 (6%)]\tLoss: 1.270138\n",
      "Train Epoch: 83 [500/6658 (8%)]\tLoss: 0.068503\n",
      "Train Epoch: 83 [600/6658 (9%)]\tLoss: 0.023890\n",
      "Train Epoch: 83 [700/6658 (11%)]\tLoss: 0.027100\n",
      "Train Epoch: 83 [800/6658 (12%)]\tLoss: 0.171393\n",
      "Train Epoch: 83 [900/6658 (14%)]\tLoss: 0.049167\n",
      "Train Epoch: 83 [1000/6658 (15%)]\tLoss: 0.226954\n",
      "Train Epoch: 83 [1100/6658 (17%)]\tLoss: 0.650213\n",
      "Train Epoch: 83 [1200/6658 (18%)]\tLoss: 2.336338\n",
      "Train Epoch: 83 [1300/6658 (20%)]\tLoss: 0.283459\n",
      "Train Epoch: 83 [1400/6658 (21%)]\tLoss: 0.372616\n",
      "Train Epoch: 83 [1500/6658 (23%)]\tLoss: 0.999933\n",
      "Train Epoch: 83 [1600/6658 (24%)]\tLoss: 4.138320\n",
      "Train Epoch: 83 [1700/6658 (26%)]\tLoss: 0.001131\n",
      "Train Epoch: 83 [1800/6658 (27%)]\tLoss: 0.024031\n",
      "Train Epoch: 83 [1900/6658 (29%)]\tLoss: 2.229182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 83 [2000/6658 (30%)]\tLoss: 0.065216\n",
      "Train Epoch: 83 [2100/6658 (32%)]\tLoss: 0.073441\n",
      "Train Epoch: 83 [2200/6658 (33%)]\tLoss: 0.884489\n",
      "Train Epoch: 83 [2300/6658 (35%)]\tLoss: 0.045753\n",
      "Train Epoch: 83 [2400/6658 (36%)]\tLoss: 0.348305\n",
      "Train Epoch: 83 [2500/6658 (38%)]\tLoss: 0.885510\n",
      "Train Epoch: 83 [2600/6658 (39%)]\tLoss: 0.207432\n",
      "Train Epoch: 83 [2700/6658 (41%)]\tLoss: 0.014064\n",
      "Train Epoch: 83 [2800/6658 (42%)]\tLoss: 0.240968\n",
      "Train Epoch: 83 [2900/6658 (44%)]\tLoss: 0.010555\n",
      "Train Epoch: 83 [3000/6658 (45%)]\tLoss: 0.142968\n",
      "Train Epoch: 83 [3100/6658 (47%)]\tLoss: 8.363317\n",
      "Train Epoch: 83 [3200/6658 (48%)]\tLoss: 0.097620\n",
      "Train Epoch: 83 [3300/6658 (50%)]\tLoss: 0.002487\n",
      "Train Epoch: 83 [3400/6658 (51%)]\tLoss: 1.214030\n",
      "Train Epoch: 83 [3500/6658 (53%)]\tLoss: 5.794003\n",
      "Train Epoch: 83 [3600/6658 (54%)]\tLoss: 0.278797\n",
      "Train Epoch: 83 [3700/6658 (56%)]\tLoss: 0.005626\n",
      "Train Epoch: 83 [3800/6658 (57%)]\tLoss: 0.000396\n",
      "Train Epoch: 83 [3900/6658 (59%)]\tLoss: 0.610916\n",
      "Train Epoch: 83 [4000/6658 (60%)]\tLoss: 1.179780\n",
      "Train Epoch: 83 [4100/6658 (62%)]\tLoss: 2.019171\n",
      "Train Epoch: 83 [4200/6658 (63%)]\tLoss: 0.230076\n",
      "Train Epoch: 83 [4300/6658 (65%)]\tLoss: 0.042341\n",
      "Train Epoch: 83 [4400/6658 (66%)]\tLoss: 0.122047\n",
      "Train Epoch: 83 [4500/6658 (68%)]\tLoss: 0.003346\n",
      "Train Epoch: 83 [4600/6658 (69%)]\tLoss: 0.117217\n",
      "Train Epoch: 83 [4700/6658 (71%)]\tLoss: 0.360135\n",
      "Train Epoch: 83 [4800/6658 (72%)]\tLoss: 0.012851\n",
      "Train Epoch: 83 [4900/6658 (74%)]\tLoss: 1.945263\n",
      "Train Epoch: 83 [5000/6658 (75%)]\tLoss: 0.858916\n",
      "Train Epoch: 83 [5100/6658 (77%)]\tLoss: 0.162796\n",
      "Train Epoch: 83 [5200/6658 (78%)]\tLoss: 0.250202\n",
      "Train Epoch: 83 [5300/6658 (80%)]\tLoss: 0.193731\n",
      "Train Epoch: 83 [5400/6658 (81%)]\tLoss: 0.029768\n",
      "Train Epoch: 83 [5500/6658 (83%)]\tLoss: 0.009412\n",
      "Train Epoch: 83 [5600/6658 (84%)]\tLoss: 1.165345\n",
      "Train Epoch: 83 [5700/6658 (86%)]\tLoss: 0.316851\n",
      "Train Epoch: 83 [5800/6658 (87%)]\tLoss: 0.424618\n",
      "Train Epoch: 83 [5900/6658 (89%)]\tLoss: 0.708803\n",
      "Train Epoch: 83 [6000/6658 (90%)]\tLoss: 2.201885\n",
      "Train Epoch: 83 [6100/6658 (92%)]\tLoss: 1.785526\n",
      "Train Epoch: 83 [6200/6658 (93%)]\tLoss: 0.095534\n",
      "Train Epoch: 83 [6300/6658 (95%)]\tLoss: 0.000016\n",
      "Train Epoch: 83 [6400/6658 (96%)]\tLoss: 0.040523\n",
      "Train Epoch: 83 [6500/6658 (98%)]\tLoss: 0.311626\n",
      "Train Epoch: 83 [6600/6658 (99%)]\tLoss: 0.686008\n",
      "train loss average =  0.7298766248299974\n",
      "\n",
      "Test set: Average loss: 0.7079\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1874, 6.0550, 5.8968, 5.8986, 6.1161, 6.0443], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 84 [0/6658 (0%)]\tLoss: 0.156099\n",
      "Train Epoch: 84 [100/6658 (2%)]\tLoss: 0.257755\n",
      "Train Epoch: 84 [200/6658 (3%)]\tLoss: 0.731940\n",
      "Train Epoch: 84 [300/6658 (5%)]\tLoss: 0.620939\n",
      "Train Epoch: 84 [400/6658 (6%)]\tLoss: 4.369811\n",
      "Train Epoch: 84 [500/6658 (8%)]\tLoss: 6.533304\n",
      "Train Epoch: 84 [600/6658 (9%)]\tLoss: 1.219986\n",
      "Train Epoch: 84 [700/6658 (11%)]\tLoss: 1.150837\n",
      "Train Epoch: 84 [800/6658 (12%)]\tLoss: 0.198338\n",
      "Train Epoch: 84 [900/6658 (14%)]\tLoss: 2.531989\n",
      "Train Epoch: 84 [1000/6658 (15%)]\tLoss: 0.269260\n",
      "Train Epoch: 84 [1100/6658 (17%)]\tLoss: 0.981918\n",
      "Train Epoch: 84 [1200/6658 (18%)]\tLoss: 0.033843\n",
      "Train Epoch: 84 [1300/6658 (20%)]\tLoss: 3.340253\n",
      "Train Epoch: 84 [1400/6658 (21%)]\tLoss: 0.535142\n",
      "Train Epoch: 84 [1500/6658 (23%)]\tLoss: 0.816800\n",
      "Train Epoch: 84 [1600/6658 (24%)]\tLoss: 0.275616\n",
      "Train Epoch: 84 [1700/6658 (26%)]\tLoss: 0.650230\n",
      "Train Epoch: 84 [1800/6658 (27%)]\tLoss: 0.122619\n",
      "Train Epoch: 84 [1900/6658 (29%)]\tLoss: 0.175240\n",
      "Train Epoch: 84 [2000/6658 (30%)]\tLoss: 2.667538\n",
      "Train Epoch: 84 [2100/6658 (32%)]\tLoss: 0.787163\n",
      "Train Epoch: 84 [2200/6658 (33%)]\tLoss: 0.728482\n",
      "Train Epoch: 84 [2300/6658 (35%)]\tLoss: 1.322103\n",
      "Train Epoch: 84 [2400/6658 (36%)]\tLoss: 0.002465\n",
      "Train Epoch: 84 [2500/6658 (38%)]\tLoss: 0.590674\n",
      "Train Epoch: 84 [2600/6658 (39%)]\tLoss: 0.159171\n",
      "Train Epoch: 84 [2700/6658 (41%)]\tLoss: 0.713252\n",
      "Train Epoch: 84 [2800/6658 (42%)]\tLoss: 0.569821\n",
      "Train Epoch: 84 [2900/6658 (44%)]\tLoss: 3.656764\n",
      "Train Epoch: 84 [3000/6658 (45%)]\tLoss: 0.189759\n",
      "Train Epoch: 84 [3100/6658 (47%)]\tLoss: 0.040894\n",
      "Train Epoch: 84 [3200/6658 (48%)]\tLoss: 0.346514\n",
      "Train Epoch: 84 [3300/6658 (50%)]\tLoss: 0.199489\n",
      "Train Epoch: 84 [3400/6658 (51%)]\tLoss: 0.190608\n",
      "Train Epoch: 84 [3500/6658 (53%)]\tLoss: 0.389321\n",
      "Train Epoch: 84 [3600/6658 (54%)]\tLoss: 0.048822\n",
      "Train Epoch: 84 [3700/6658 (56%)]\tLoss: 0.458710\n",
      "Train Epoch: 84 [3800/6658 (57%)]\tLoss: 0.005459\n",
      "Train Epoch: 84 [3900/6658 (59%)]\tLoss: 1.307460\n",
      "Train Epoch: 84 [4000/6658 (60%)]\tLoss: 0.329268\n",
      "Train Epoch: 84 [4100/6658 (62%)]\tLoss: 0.037619\n",
      "Train Epoch: 84 [4200/6658 (63%)]\tLoss: 0.092717\n",
      "Train Epoch: 84 [4300/6658 (65%)]\tLoss: 0.762921\n",
      "Train Epoch: 84 [4400/6658 (66%)]\tLoss: 2.239982\n",
      "Train Epoch: 84 [4500/6658 (68%)]\tLoss: 0.635620\n",
      "Train Epoch: 84 [4600/6658 (69%)]\tLoss: 0.289314\n",
      "Train Epoch: 84 [4700/6658 (71%)]\tLoss: 0.816296\n",
      "Train Epoch: 84 [4800/6658 (72%)]\tLoss: 0.010927\n",
      "Train Epoch: 84 [4900/6658 (74%)]\tLoss: 1.010952\n",
      "Train Epoch: 84 [5000/6658 (75%)]\tLoss: 0.039696\n",
      "Train Epoch: 84 [5100/6658 (77%)]\tLoss: 0.009291\n",
      "Train Epoch: 84 [5200/6658 (78%)]\tLoss: 0.221404\n",
      "Train Epoch: 84 [5300/6658 (80%)]\tLoss: 0.792460\n",
      "Train Epoch: 84 [5400/6658 (81%)]\tLoss: 0.159950\n",
      "Train Epoch: 84 [5500/6658 (83%)]\tLoss: 0.734703\n",
      "Train Epoch: 84 [5600/6658 (84%)]\tLoss: 0.104493\n",
      "Train Epoch: 84 [5700/6658 (86%)]\tLoss: 1.106701\n",
      "Train Epoch: 84 [5800/6658 (87%)]\tLoss: 0.472641\n",
      "Train Epoch: 84 [5900/6658 (89%)]\tLoss: 0.084066\n",
      "Train Epoch: 84 [6000/6658 (90%)]\tLoss: 0.565524\n",
      "Train Epoch: 84 [6100/6658 (92%)]\tLoss: 0.222397\n",
      "Train Epoch: 84 [6200/6658 (93%)]\tLoss: 0.273151\n",
      "Train Epoch: 84 [6300/6658 (95%)]\tLoss: 0.379727\n",
      "Train Epoch: 84 [6400/6658 (96%)]\tLoss: 0.374203\n",
      "Train Epoch: 84 [6500/6658 (98%)]\tLoss: 1.010875\n",
      "Train Epoch: 84 [6600/6658 (99%)]\tLoss: 7.473400\n",
      "train loss average =  0.7225237095663203\n",
      "\n",
      "Test set: Average loss: 0.7210\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1899, 6.0562, 5.8958, 5.8968, 6.1176, 6.0441], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 85 [0/6658 (0%)]\tLoss: 0.001324\n",
      "Train Epoch: 85 [100/6658 (2%)]\tLoss: 1.369005\n",
      "Train Epoch: 85 [200/6658 (3%)]\tLoss: 0.744033\n",
      "Train Epoch: 85 [300/6658 (5%)]\tLoss: 0.107987\n",
      "Train Epoch: 85 [400/6658 (6%)]\tLoss: 0.082493\n",
      "Train Epoch: 85 [500/6658 (8%)]\tLoss: 0.012678\n",
      "Train Epoch: 85 [600/6658 (9%)]\tLoss: 0.000633\n",
      "Train Epoch: 85 [700/6658 (11%)]\tLoss: 0.166357\n",
      "Train Epoch: 85 [800/6658 (12%)]\tLoss: 14.626836\n",
      "Train Epoch: 85 [900/6658 (14%)]\tLoss: 0.002031\n",
      "Train Epoch: 85 [1000/6658 (15%)]\tLoss: 0.439865\n",
      "Train Epoch: 85 [1100/6658 (17%)]\tLoss: 1.171782\n",
      "Train Epoch: 85 [1200/6658 (18%)]\tLoss: 0.037540\n",
      "Train Epoch: 85 [1300/6658 (20%)]\tLoss: 0.530619\n",
      "Train Epoch: 85 [1400/6658 (21%)]\tLoss: 0.047899\n",
      "Train Epoch: 85 [1500/6658 (23%)]\tLoss: 1.572881\n",
      "Train Epoch: 85 [1600/6658 (24%)]\tLoss: 0.052300\n",
      "Train Epoch: 85 [1700/6658 (26%)]\tLoss: 0.378197\n",
      "Train Epoch: 85 [1800/6658 (27%)]\tLoss: 0.260115\n",
      "Train Epoch: 85 [1900/6658 (29%)]\tLoss: 1.501177\n",
      "Train Epoch: 85 [2000/6658 (30%)]\tLoss: 0.766435\n",
      "Train Epoch: 85 [2100/6658 (32%)]\tLoss: 0.540893\n",
      "Train Epoch: 85 [2200/6658 (33%)]\tLoss: 0.223758\n",
      "Train Epoch: 85 [2300/6658 (35%)]\tLoss: 1.302786\n",
      "Train Epoch: 85 [2400/6658 (36%)]\tLoss: 0.183483\n",
      "Train Epoch: 85 [2500/6658 (38%)]\tLoss: 0.568310\n",
      "Train Epoch: 85 [2600/6658 (39%)]\tLoss: 0.075370\n",
      "Train Epoch: 85 [2700/6658 (41%)]\tLoss: 0.556634\n",
      "Train Epoch: 85 [2800/6658 (42%)]\tLoss: 0.790076\n",
      "Train Epoch: 85 [2900/6658 (44%)]\tLoss: 0.343711\n",
      "Train Epoch: 85 [3000/6658 (45%)]\tLoss: 2.487754\n",
      "Train Epoch: 85 [3100/6658 (47%)]\tLoss: 2.067841\n",
      "Train Epoch: 85 [3200/6658 (48%)]\tLoss: 0.209817\n",
      "Train Epoch: 85 [3300/6658 (50%)]\tLoss: 0.018523\n",
      "Train Epoch: 85 [3400/6658 (51%)]\tLoss: 0.069835\n",
      "Train Epoch: 85 [3500/6658 (53%)]\tLoss: 2.265358\n",
      "Train Epoch: 85 [3600/6658 (54%)]\tLoss: 0.330805\n",
      "Train Epoch: 85 [3700/6658 (56%)]\tLoss: 0.147135\n",
      "Train Epoch: 85 [3800/6658 (57%)]\tLoss: 0.047410\n",
      "Train Epoch: 85 [3900/6658 (59%)]\tLoss: 0.019987\n",
      "Train Epoch: 85 [4000/6658 (60%)]\tLoss: 0.186776\n",
      "Train Epoch: 85 [4100/6658 (62%)]\tLoss: 0.026264\n",
      "Train Epoch: 85 [4200/6658 (63%)]\tLoss: 0.348494\n",
      "Train Epoch: 85 [4300/6658 (65%)]\tLoss: 0.002005\n",
      "Train Epoch: 85 [4400/6658 (66%)]\tLoss: 0.000699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 85 [4500/6658 (68%)]\tLoss: 0.223326\n",
      "Train Epoch: 85 [4600/6658 (69%)]\tLoss: 0.176247\n",
      "Train Epoch: 85 [4700/6658 (71%)]\tLoss: 0.010313\n",
      "Train Epoch: 85 [4800/6658 (72%)]\tLoss: 0.718292\n",
      "Train Epoch: 85 [4900/6658 (74%)]\tLoss: 1.668918\n",
      "Train Epoch: 85 [5000/6658 (75%)]\tLoss: 0.295735\n",
      "Train Epoch: 85 [5100/6658 (77%)]\tLoss: 0.231221\n",
      "Train Epoch: 85 [5200/6658 (78%)]\tLoss: 0.494113\n",
      "Train Epoch: 85 [5300/6658 (80%)]\tLoss: 3.444376\n",
      "Train Epoch: 85 [5400/6658 (81%)]\tLoss: 0.261261\n",
      "Train Epoch: 85 [5500/6658 (83%)]\tLoss: 0.011366\n",
      "Train Epoch: 85 [5600/6658 (84%)]\tLoss: 0.663275\n",
      "Train Epoch: 85 [5700/6658 (86%)]\tLoss: 1.313292\n",
      "Train Epoch: 85 [5800/6658 (87%)]\tLoss: 11.440670\n",
      "Train Epoch: 85 [5900/6658 (89%)]\tLoss: 0.655914\n",
      "Train Epoch: 85 [6000/6658 (90%)]\tLoss: 0.002752\n",
      "Train Epoch: 85 [6100/6658 (92%)]\tLoss: 0.000962\n",
      "Train Epoch: 85 [6200/6658 (93%)]\tLoss: 0.642312\n",
      "Train Epoch: 85 [6300/6658 (95%)]\tLoss: 0.469012\n",
      "Train Epoch: 85 [6400/6658 (96%)]\tLoss: 0.086526\n",
      "Train Epoch: 85 [6500/6658 (98%)]\tLoss: 1.989487\n",
      "Train Epoch: 85 [6600/6658 (99%)]\tLoss: 2.014979\n",
      "train loss average =  0.7296390546872921\n",
      "\n",
      "Test set: Average loss: 0.7168\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1918, 6.0571, 5.8943, 5.8964, 6.1196, 6.0444], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 86 [0/6658 (0%)]\tLoss: 1.230325\n",
      "Train Epoch: 86 [100/6658 (2%)]\tLoss: 0.000304\n",
      "Train Epoch: 86 [200/6658 (3%)]\tLoss: 0.251294\n",
      "Train Epoch: 86 [300/6658 (5%)]\tLoss: 2.321476\n",
      "Train Epoch: 86 [400/6658 (6%)]\tLoss: 1.268173\n",
      "Train Epoch: 86 [500/6658 (8%)]\tLoss: 0.754122\n",
      "Train Epoch: 86 [600/6658 (9%)]\tLoss: 0.710098\n",
      "Train Epoch: 86 [700/6658 (11%)]\tLoss: 1.294433\n",
      "Train Epoch: 86 [800/6658 (12%)]\tLoss: 0.868095\n",
      "Train Epoch: 86 [900/6658 (14%)]\tLoss: 1.167563\n",
      "Train Epoch: 86 [1000/6658 (15%)]\tLoss: 0.475168\n",
      "Train Epoch: 86 [1100/6658 (17%)]\tLoss: 0.205776\n",
      "Train Epoch: 86 [1200/6658 (18%)]\tLoss: 1.229035\n",
      "Train Epoch: 86 [1300/6658 (20%)]\tLoss: 0.244558\n",
      "Train Epoch: 86 [1400/6658 (21%)]\tLoss: 1.725845\n",
      "Train Epoch: 86 [1500/6658 (23%)]\tLoss: 1.215596\n",
      "Train Epoch: 86 [1600/6658 (24%)]\tLoss: 0.046409\n",
      "Train Epoch: 86 [1700/6658 (26%)]\tLoss: 0.014918\n",
      "Train Epoch: 86 [1800/6658 (27%)]\tLoss: 1.077029\n",
      "Train Epoch: 86 [1900/6658 (29%)]\tLoss: 0.011398\n",
      "Train Epoch: 86 [2000/6658 (30%)]\tLoss: 2.860950\n",
      "Train Epoch: 86 [2100/6658 (32%)]\tLoss: 0.320076\n",
      "Train Epoch: 86 [2200/6658 (33%)]\tLoss: 0.214758\n",
      "Train Epoch: 86 [2300/6658 (35%)]\tLoss: 0.004222\n",
      "Train Epoch: 86 [2400/6658 (36%)]\tLoss: 0.074063\n",
      "Train Epoch: 86 [2500/6658 (38%)]\tLoss: 0.368163\n",
      "Train Epoch: 86 [2600/6658 (39%)]\tLoss: 0.365654\n",
      "Train Epoch: 86 [2700/6658 (41%)]\tLoss: 0.265418\n",
      "Train Epoch: 86 [2800/6658 (42%)]\tLoss: 0.015344\n",
      "Train Epoch: 86 [2900/6658 (44%)]\tLoss: 0.099252\n",
      "Train Epoch: 86 [3000/6658 (45%)]\tLoss: 0.488426\n",
      "Train Epoch: 86 [3100/6658 (47%)]\tLoss: 0.196277\n",
      "Train Epoch: 86 [3200/6658 (48%)]\tLoss: 0.177735\n",
      "Train Epoch: 86 [3300/6658 (50%)]\tLoss: 0.068153\n",
      "Train Epoch: 86 [3400/6658 (51%)]\tLoss: 0.000582\n",
      "Train Epoch: 86 [3500/6658 (53%)]\tLoss: 0.067418\n",
      "Train Epoch: 86 [3600/6658 (54%)]\tLoss: 1.284852\n",
      "Train Epoch: 86 [3700/6658 (56%)]\tLoss: 0.026985\n",
      "Train Epoch: 86 [3800/6658 (57%)]\tLoss: 0.162538\n",
      "Train Epoch: 86 [3900/6658 (59%)]\tLoss: 0.701091\n",
      "Train Epoch: 86 [4000/6658 (60%)]\tLoss: 0.008830\n",
      "Train Epoch: 86 [4100/6658 (62%)]\tLoss: 0.110853\n",
      "Train Epoch: 86 [4200/6658 (63%)]\tLoss: 1.568667\n",
      "Train Epoch: 86 [4300/6658 (65%)]\tLoss: 0.111121\n",
      "Train Epoch: 86 [4400/6658 (66%)]\tLoss: 0.366025\n",
      "Train Epoch: 86 [4500/6658 (68%)]\tLoss: 0.091643\n",
      "Train Epoch: 86 [4600/6658 (69%)]\tLoss: 1.828013\n",
      "Train Epoch: 86 [4700/6658 (71%)]\tLoss: 0.884070\n",
      "Train Epoch: 86 [4800/6658 (72%)]\tLoss: 0.165801\n",
      "Train Epoch: 86 [4900/6658 (74%)]\tLoss: 0.152727\n",
      "Train Epoch: 86 [5000/6658 (75%)]\tLoss: 0.640946\n",
      "Train Epoch: 86 [5100/6658 (77%)]\tLoss: 0.999152\n",
      "Train Epoch: 86 [5200/6658 (78%)]\tLoss: 0.197154\n",
      "Train Epoch: 86 [5300/6658 (80%)]\tLoss: 0.126962\n",
      "Train Epoch: 86 [5400/6658 (81%)]\tLoss: 0.092258\n",
      "Train Epoch: 86 [5500/6658 (83%)]\tLoss: 0.028275\n",
      "Train Epoch: 86 [5600/6658 (84%)]\tLoss: 0.073520\n",
      "Train Epoch: 86 [5700/6658 (86%)]\tLoss: 0.027987\n",
      "Train Epoch: 86 [5800/6658 (87%)]\tLoss: 6.668250\n",
      "Train Epoch: 86 [5900/6658 (89%)]\tLoss: 0.680638\n",
      "Train Epoch: 86 [6000/6658 (90%)]\tLoss: 0.236248\n",
      "Train Epoch: 86 [6100/6658 (92%)]\tLoss: 0.679216\n",
      "Train Epoch: 86 [6200/6658 (93%)]\tLoss: 1.010277\n",
      "Train Epoch: 86 [6300/6658 (95%)]\tLoss: 0.653689\n",
      "Train Epoch: 86 [6400/6658 (96%)]\tLoss: 0.681063\n",
      "Train Epoch: 86 [6500/6658 (98%)]\tLoss: 0.192872\n",
      "Train Epoch: 86 [6600/6658 (99%)]\tLoss: 0.056507\n",
      "train loss average =  0.7281441683862758\n",
      "\n",
      "Test set: Average loss: 0.7141\n",
      "\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1934, 6.0574, 5.8937, 5.8963, 6.1210, 6.0444], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 87 [0/6658 (0%)]\tLoss: 0.002380\n",
      "Train Epoch: 87 [100/6658 (2%)]\tLoss: 0.201810\n",
      "Train Epoch: 87 [200/6658 (3%)]\tLoss: 0.865099\n",
      "Train Epoch: 87 [300/6658 (5%)]\tLoss: 1.725430\n",
      "Train Epoch: 87 [400/6658 (6%)]\tLoss: 2.042122\n",
      "Train Epoch: 87 [500/6658 (8%)]\tLoss: 0.671137\n",
      "Train Epoch: 87 [600/6658 (9%)]\tLoss: 0.013005\n",
      "Train Epoch: 87 [700/6658 (11%)]\tLoss: 0.040863\n",
      "Train Epoch: 87 [800/6658 (12%)]\tLoss: 2.049535\n",
      "Train Epoch: 87 [900/6658 (14%)]\tLoss: 0.854678\n",
      "Train Epoch: 87 [1000/6658 (15%)]\tLoss: 0.090888\n",
      "Train Epoch: 87 [1100/6658 (17%)]\tLoss: 0.921541\n",
      "Train Epoch: 87 [1200/6658 (18%)]\tLoss: 0.798080\n",
      "Train Epoch: 87 [1300/6658 (20%)]\tLoss: 1.894430\n",
      "Train Epoch: 87 [1400/6658 (21%)]\tLoss: 0.170134\n",
      "Train Epoch: 87 [1500/6658 (23%)]\tLoss: 0.107308\n",
      "Train Epoch: 87 [1600/6658 (24%)]\tLoss: 0.213936\n",
      "Train Epoch: 87 [1700/6658 (26%)]\tLoss: 0.467281\n",
      "Train Epoch: 87 [1800/6658 (27%)]\tLoss: 0.004470\n",
      "Train Epoch: 87 [1900/6658 (29%)]\tLoss: 0.044542\n",
      "Train Epoch: 87 [2000/6658 (30%)]\tLoss: 0.018047\n",
      "Train Epoch: 87 [2100/6658 (32%)]\tLoss: 0.465725\n",
      "Train Epoch: 87 [2200/6658 (33%)]\tLoss: 0.291261\n",
      "Train Epoch: 87 [2300/6658 (35%)]\tLoss: 0.374171\n",
      "Train Epoch: 87 [2400/6658 (36%)]\tLoss: 0.082123\n",
      "Train Epoch: 87 [2500/6658 (38%)]\tLoss: 0.121455\n",
      "Train Epoch: 87 [2600/6658 (39%)]\tLoss: 0.106864\n",
      "Train Epoch: 87 [2700/6658 (41%)]\tLoss: 0.176008\n",
      "Train Epoch: 87 [2800/6658 (42%)]\tLoss: 0.040871\n",
      "Train Epoch: 87 [2900/6658 (44%)]\tLoss: 0.811457\n",
      "Train Epoch: 87 [3000/6658 (45%)]\tLoss: 0.278508\n",
      "Train Epoch: 87 [3100/6658 (47%)]\tLoss: 0.116430\n",
      "Train Epoch: 87 [3200/6658 (48%)]\tLoss: 0.033997\n",
      "Train Epoch: 87 [3300/6658 (50%)]\tLoss: 5.066369\n",
      "Train Epoch: 87 [3400/6658 (51%)]\tLoss: 0.737787\n",
      "Train Epoch: 87 [3500/6658 (53%)]\tLoss: 0.146368\n",
      "Train Epoch: 87 [3600/6658 (54%)]\tLoss: 1.760490\n",
      "Train Epoch: 87 [3700/6658 (56%)]\tLoss: 0.415707\n",
      "Train Epoch: 87 [3800/6658 (57%)]\tLoss: 1.121362\n",
      "Train Epoch: 87 [3900/6658 (59%)]\tLoss: 0.024685\n",
      "Train Epoch: 87 [4000/6658 (60%)]\tLoss: 1.035997\n",
      "Train Epoch: 87 [4100/6658 (62%)]\tLoss: 0.024353\n",
      "Train Epoch: 87 [4200/6658 (63%)]\tLoss: 0.003952\n",
      "Train Epoch: 87 [4300/6658 (65%)]\tLoss: 2.360960\n",
      "Train Epoch: 87 [4400/6658 (66%)]\tLoss: 0.266075\n",
      "Train Epoch: 87 [4500/6658 (68%)]\tLoss: 0.715596\n",
      "Train Epoch: 87 [4600/6658 (69%)]\tLoss: 1.227666\n",
      "Train Epoch: 87 [4700/6658 (71%)]\tLoss: 0.035453\n",
      "Train Epoch: 87 [4800/6658 (72%)]\tLoss: 0.002662\n",
      "Train Epoch: 87 [4900/6658 (74%)]\tLoss: 0.533000\n",
      "Train Epoch: 87 [5000/6658 (75%)]\tLoss: 1.028443\n",
      "Train Epoch: 87 [5100/6658 (77%)]\tLoss: 0.714742\n",
      "Train Epoch: 87 [5200/6658 (78%)]\tLoss: 0.000368\n",
      "Train Epoch: 87 [5300/6658 (80%)]\tLoss: 0.002597\n",
      "Train Epoch: 87 [5400/6658 (81%)]\tLoss: 0.232112\n",
      "Train Epoch: 87 [5500/6658 (83%)]\tLoss: 0.389750\n",
      "Train Epoch: 87 [5600/6658 (84%)]\tLoss: 0.507152\n",
      "Train Epoch: 87 [5700/6658 (86%)]\tLoss: 0.370472\n",
      "Train Epoch: 87 [5800/6658 (87%)]\tLoss: 0.001308\n",
      "Train Epoch: 87 [5900/6658 (89%)]\tLoss: 0.285245\n",
      "Train Epoch: 87 [6000/6658 (90%)]\tLoss: 0.061087\n",
      "Train Epoch: 87 [6100/6658 (92%)]\tLoss: 6.611329\n",
      "Train Epoch: 87 [6200/6658 (93%)]\tLoss: 0.145023\n",
      "Train Epoch: 87 [6300/6658 (95%)]\tLoss: 0.024326\n",
      "Train Epoch: 87 [6400/6658 (96%)]\tLoss: 0.425043\n",
      "Train Epoch: 87 [6500/6658 (98%)]\tLoss: 0.001262\n",
      "Train Epoch: 87 [6600/6658 (99%)]\tLoss: 2.293036\n",
      "train loss average =  0.7301695905731401\n",
      "\n",
      "Test set: Average loss: 0.7122\n",
      "\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1946, 6.0588, 5.8926, 5.8956, 6.1230, 6.0451], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 88 [0/6658 (0%)]\tLoss: 0.429575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 88 [100/6658 (2%)]\tLoss: 1.399268\n",
      "Train Epoch: 88 [200/6658 (3%)]\tLoss: 0.051473\n",
      "Train Epoch: 88 [300/6658 (5%)]\tLoss: 0.175939\n",
      "Train Epoch: 88 [400/6658 (6%)]\tLoss: 0.028521\n",
      "Train Epoch: 88 [500/6658 (8%)]\tLoss: 0.403835\n",
      "Train Epoch: 88 [600/6658 (9%)]\tLoss: 0.260996\n",
      "Train Epoch: 88 [700/6658 (11%)]\tLoss: 8.148536\n",
      "Train Epoch: 88 [800/6658 (12%)]\tLoss: 0.002020\n",
      "Train Epoch: 88 [900/6658 (14%)]\tLoss: 0.004362\n",
      "Train Epoch: 88 [1000/6658 (15%)]\tLoss: 0.109746\n",
      "Train Epoch: 88 [1100/6658 (17%)]\tLoss: 0.002448\n",
      "Train Epoch: 88 [1200/6658 (18%)]\tLoss: 0.189107\n",
      "Train Epoch: 88 [1300/6658 (20%)]\tLoss: 0.635925\n",
      "Train Epoch: 88 [1400/6658 (21%)]\tLoss: 5.881243\n",
      "Train Epoch: 88 [1500/6658 (23%)]\tLoss: 0.270987\n",
      "Train Epoch: 88 [1600/6658 (24%)]\tLoss: 0.045324\n",
      "Train Epoch: 88 [1700/6658 (26%)]\tLoss: 0.472067\n",
      "Train Epoch: 88 [1800/6658 (27%)]\tLoss: 0.011093\n",
      "Train Epoch: 88 [1900/6658 (29%)]\tLoss: 0.748505\n",
      "Train Epoch: 88 [2000/6658 (30%)]\tLoss: 1.099254\n",
      "Train Epoch: 88 [2100/6658 (32%)]\tLoss: 4.175127\n",
      "Train Epoch: 88 [2200/6658 (33%)]\tLoss: 1.756891\n",
      "Train Epoch: 88 [2300/6658 (35%)]\tLoss: 0.003898\n",
      "Train Epoch: 88 [2400/6658 (36%)]\tLoss: 0.040610\n",
      "Train Epoch: 88 [2500/6658 (38%)]\tLoss: 0.003861\n",
      "Train Epoch: 88 [2600/6658 (39%)]\tLoss: 0.155187\n",
      "Train Epoch: 88 [2700/6658 (41%)]\tLoss: 0.066451\n",
      "Train Epoch: 88 [2800/6658 (42%)]\tLoss: 1.451584\n",
      "Train Epoch: 88 [2900/6658 (44%)]\tLoss: 0.024318\n",
      "Train Epoch: 88 [3000/6658 (45%)]\tLoss: 0.170594\n",
      "Train Epoch: 88 [3100/6658 (47%)]\tLoss: 0.424422\n",
      "Train Epoch: 88 [3200/6658 (48%)]\tLoss: 0.009438\n",
      "Train Epoch: 88 [3300/6658 (50%)]\tLoss: 0.001100\n",
      "Train Epoch: 88 [3400/6658 (51%)]\tLoss: 0.207758\n",
      "Train Epoch: 88 [3500/6658 (53%)]\tLoss: 0.381694\n",
      "Train Epoch: 88 [3600/6658 (54%)]\tLoss: 1.009886\n",
      "Train Epoch: 88 [3700/6658 (56%)]\tLoss: 0.788508\n",
      "Train Epoch: 88 [3800/6658 (57%)]\tLoss: 17.753599\n",
      "Train Epoch: 88 [3900/6658 (59%)]\tLoss: 0.082375\n",
      "Train Epoch: 88 [4000/6658 (60%)]\tLoss: 0.192629\n",
      "Train Epoch: 88 [4100/6658 (62%)]\tLoss: 0.426482\n",
      "Train Epoch: 88 [4200/6658 (63%)]\tLoss: 0.013871\n",
      "Train Epoch: 88 [4300/6658 (65%)]\tLoss: 0.278692\n",
      "Train Epoch: 88 [4400/6658 (66%)]\tLoss: 1.205551\n",
      "Train Epoch: 88 [4500/6658 (68%)]\tLoss: 0.678588\n",
      "Train Epoch: 88 [4600/6658 (69%)]\tLoss: 0.379630\n",
      "Train Epoch: 88 [4700/6658 (71%)]\tLoss: 0.276998\n",
      "Train Epoch: 88 [4800/6658 (72%)]\tLoss: 0.025877\n",
      "Train Epoch: 88 [4900/6658 (74%)]\tLoss: 0.994290\n",
      "Train Epoch: 88 [5000/6658 (75%)]\tLoss: 0.290674\n",
      "Train Epoch: 88 [5100/6658 (77%)]\tLoss: 1.074660\n",
      "Train Epoch: 88 [5200/6658 (78%)]\tLoss: 0.417145\n",
      "Train Epoch: 88 [5300/6658 (80%)]\tLoss: 0.402427\n",
      "Train Epoch: 88 [5400/6658 (81%)]\tLoss: 0.577061\n",
      "Train Epoch: 88 [5500/6658 (83%)]\tLoss: 1.496637\n",
      "Train Epoch: 88 [5600/6658 (84%)]\tLoss: 0.006879\n",
      "Train Epoch: 88 [5700/6658 (86%)]\tLoss: 3.575174\n",
      "Train Epoch: 88 [5800/6658 (87%)]\tLoss: 0.352954\n",
      "Train Epoch: 88 [5900/6658 (89%)]\tLoss: 0.188641\n",
      "Train Epoch: 88 [6000/6658 (90%)]\tLoss: 0.129301\n",
      "Train Epoch: 88 [6100/6658 (92%)]\tLoss: 0.328300\n",
      "Train Epoch: 88 [6200/6658 (93%)]\tLoss: 0.000610\n",
      "Train Epoch: 88 [6300/6658 (95%)]\tLoss: 2.926509\n",
      "Train Epoch: 88 [6400/6658 (96%)]\tLoss: 0.041099\n",
      "Train Epoch: 88 [6500/6658 (98%)]\tLoss: 0.095509\n",
      "Train Epoch: 88 [6600/6658 (99%)]\tLoss: 0.554509\n",
      "train loss average =  0.7223321873882596\n",
      "\n",
      "Test set: Average loss: 0.7022\n",
      "\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1967, 6.0606, 5.8921, 5.8949, 6.1245, 6.0450], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 89 [0/6658 (0%)]\tLoss: 0.275693\n",
      "Train Epoch: 89 [100/6658 (2%)]\tLoss: 0.529992\n",
      "Train Epoch: 89 [200/6658 (3%)]\tLoss: 0.103005\n",
      "Train Epoch: 89 [300/6658 (5%)]\tLoss: 0.107895\n",
      "Train Epoch: 89 [400/6658 (6%)]\tLoss: 0.233561\n",
      "Train Epoch: 89 [500/6658 (8%)]\tLoss: 0.075976\n",
      "Train Epoch: 89 [600/6658 (9%)]\tLoss: 0.057108\n",
      "Train Epoch: 89 [700/6658 (11%)]\tLoss: 0.081638\n",
      "Train Epoch: 89 [800/6658 (12%)]\tLoss: 0.642640\n",
      "Train Epoch: 89 [900/6658 (14%)]\tLoss: 0.279572\n",
      "Train Epoch: 89 [1000/6658 (15%)]\tLoss: 0.370264\n",
      "Train Epoch: 89 [1100/6658 (17%)]\tLoss: 0.262900\n",
      "Train Epoch: 89 [1200/6658 (18%)]\tLoss: 0.044850\n",
      "Train Epoch: 89 [1300/6658 (20%)]\tLoss: 0.012891\n",
      "Train Epoch: 89 [1400/6658 (21%)]\tLoss: 0.002145\n",
      "Train Epoch: 89 [1500/6658 (23%)]\tLoss: 0.990669\n",
      "Train Epoch: 89 [1600/6658 (24%)]\tLoss: 0.711529\n",
      "Train Epoch: 89 [1700/6658 (26%)]\tLoss: 0.396510\n",
      "Train Epoch: 89 [1800/6658 (27%)]\tLoss: 0.551138\n",
      "Train Epoch: 89 [1900/6658 (29%)]\tLoss: 0.316520\n",
      "Train Epoch: 89 [2000/6658 (30%)]\tLoss: 0.418682\n",
      "Train Epoch: 89 [2100/6658 (32%)]\tLoss: 0.861984\n",
      "Train Epoch: 89 [2200/6658 (33%)]\tLoss: 4.844997\n",
      "Train Epoch: 89 [2300/6658 (35%)]\tLoss: 0.182886\n",
      "Train Epoch: 89 [2400/6658 (36%)]\tLoss: 2.683757\n",
      "Train Epoch: 89 [2500/6658 (38%)]\tLoss: 0.047852\n",
      "Train Epoch: 89 [2600/6658 (39%)]\tLoss: 1.351233\n",
      "Train Epoch: 89 [2700/6658 (41%)]\tLoss: 0.027662\n",
      "Train Epoch: 89 [2800/6658 (42%)]\tLoss: 0.087431\n",
      "Train Epoch: 89 [2900/6658 (44%)]\tLoss: 0.063139\n",
      "Train Epoch: 89 [3000/6658 (45%)]\tLoss: 0.000931\n",
      "Train Epoch: 89 [3100/6658 (47%)]\tLoss: 0.390831\n",
      "Train Epoch: 89 [3200/6658 (48%)]\tLoss: 0.045853\n",
      "Train Epoch: 89 [3300/6658 (50%)]\tLoss: 1.321956\n",
      "Train Epoch: 89 [3400/6658 (51%)]\tLoss: 0.796435\n",
      "Train Epoch: 89 [3500/6658 (53%)]\tLoss: 0.162327\n",
      "Train Epoch: 89 [3600/6658 (54%)]\tLoss: 1.201739\n",
      "Train Epoch: 89 [3700/6658 (56%)]\tLoss: 0.716304\n",
      "Train Epoch: 89 [3800/6658 (57%)]\tLoss: 1.428522\n",
      "Train Epoch: 89 [3900/6658 (59%)]\tLoss: 0.807881\n",
      "Train Epoch: 89 [4000/6658 (60%)]\tLoss: 0.052682\n",
      "Train Epoch: 89 [4100/6658 (62%)]\tLoss: 0.433505\n",
      "Train Epoch: 89 [4200/6658 (63%)]\tLoss: 0.503960\n",
      "Train Epoch: 89 [4300/6658 (65%)]\tLoss: 0.024894\n",
      "Train Epoch: 89 [4400/6658 (66%)]\tLoss: 0.239392\n",
      "Train Epoch: 89 [4500/6658 (68%)]\tLoss: 0.366369\n",
      "Train Epoch: 89 [4600/6658 (69%)]\tLoss: 0.104642\n",
      "Train Epoch: 89 [4700/6658 (71%)]\tLoss: 0.020369\n",
      "Train Epoch: 89 [4800/6658 (72%)]\tLoss: 0.000001\n",
      "Train Epoch: 89 [4900/6658 (74%)]\tLoss: 0.161592\n",
      "Train Epoch: 89 [5000/6658 (75%)]\tLoss: 0.554703\n",
      "Train Epoch: 89 [5100/6658 (77%)]\tLoss: 2.514164\n",
      "Train Epoch: 89 [5200/6658 (78%)]\tLoss: 0.015867\n",
      "Train Epoch: 89 [5300/6658 (80%)]\tLoss: 0.051843\n",
      "Train Epoch: 89 [5400/6658 (81%)]\tLoss: 0.007926\n",
      "Train Epoch: 89 [5500/6658 (83%)]\tLoss: 2.021540\n",
      "Train Epoch: 89 [5600/6658 (84%)]\tLoss: 0.366344\n",
      "Train Epoch: 89 [5700/6658 (86%)]\tLoss: 1.749316\n",
      "Train Epoch: 89 [5800/6658 (87%)]\tLoss: 2.936939\n",
      "Train Epoch: 89 [5900/6658 (89%)]\tLoss: 0.225430\n",
      "Train Epoch: 89 [6000/6658 (90%)]\tLoss: 0.691473\n",
      "Train Epoch: 89 [6100/6658 (92%)]\tLoss: 0.084060\n",
      "Train Epoch: 89 [6200/6658 (93%)]\tLoss: 0.272589\n",
      "Train Epoch: 89 [6300/6658 (95%)]\tLoss: 0.111295\n",
      "Train Epoch: 89 [6400/6658 (96%)]\tLoss: 0.527767\n",
      "Train Epoch: 89 [6500/6658 (98%)]\tLoss: 0.101929\n",
      "Train Epoch: 89 [6600/6658 (99%)]\tLoss: 0.184323\n",
      "train loss average =  0.722623371121396\n",
      "\n",
      "Test set: Average loss: 0.7097\n",
      "\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.1982, 6.0613, 5.8914, 5.8941, 6.1259, 6.0450], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 90 [0/6658 (0%)]\tLoss: 0.321648\n",
      "Train Epoch: 90 [100/6658 (2%)]\tLoss: 1.188599\n",
      "Train Epoch: 90 [200/6658 (3%)]\tLoss: 0.346381\n",
      "Train Epoch: 90 [300/6658 (5%)]\tLoss: 0.320660\n",
      "Train Epoch: 90 [400/6658 (6%)]\tLoss: 0.151683\n",
      "Train Epoch: 90 [500/6658 (8%)]\tLoss: 5.650230\n",
      "Train Epoch: 90 [600/6658 (9%)]\tLoss: 0.534997\n",
      "Train Epoch: 90 [700/6658 (11%)]\tLoss: 0.041586\n",
      "Train Epoch: 90 [800/6658 (12%)]\tLoss: 1.406492\n",
      "Train Epoch: 90 [900/6658 (14%)]\tLoss: 0.011404\n",
      "Train Epoch: 90 [1000/6658 (15%)]\tLoss: 0.990181\n",
      "Train Epoch: 90 [1100/6658 (17%)]\tLoss: 4.052918\n",
      "Train Epoch: 90 [1200/6658 (18%)]\tLoss: 0.105338\n",
      "Train Epoch: 90 [1300/6658 (20%)]\tLoss: 0.268558\n",
      "Train Epoch: 90 [1400/6658 (21%)]\tLoss: 0.273761\n",
      "Train Epoch: 90 [1500/6658 (23%)]\tLoss: 0.070443\n",
      "Train Epoch: 90 [1600/6658 (24%)]\tLoss: 3.465816\n",
      "Train Epoch: 90 [1700/6658 (26%)]\tLoss: 0.926908\n",
      "Train Epoch: 90 [1800/6658 (27%)]\tLoss: 0.221306\n",
      "Train Epoch: 90 [1900/6658 (29%)]\tLoss: 0.301850\n",
      "Train Epoch: 90 [2000/6658 (30%)]\tLoss: 0.139183\n",
      "Train Epoch: 90 [2100/6658 (32%)]\tLoss: 0.088636\n",
      "Train Epoch: 90 [2200/6658 (33%)]\tLoss: 1.276733\n",
      "Train Epoch: 90 [2300/6658 (35%)]\tLoss: 0.193207\n",
      "Train Epoch: 90 [2400/6658 (36%)]\tLoss: 1.651797\n",
      "Train Epoch: 90 [2500/6658 (38%)]\tLoss: 0.544442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 90 [2600/6658 (39%)]\tLoss: 0.005159\n",
      "Train Epoch: 90 [2700/6658 (41%)]\tLoss: 0.892439\n",
      "Train Epoch: 90 [2800/6658 (42%)]\tLoss: 0.038137\n",
      "Train Epoch: 90 [2900/6658 (44%)]\tLoss: 1.060126\n",
      "Train Epoch: 90 [3000/6658 (45%)]\tLoss: 0.268128\n",
      "Train Epoch: 90 [3100/6658 (47%)]\tLoss: 0.092858\n",
      "Train Epoch: 90 [3200/6658 (48%)]\tLoss: 0.245366\n",
      "Train Epoch: 90 [3300/6658 (50%)]\tLoss: 0.186100\n",
      "Train Epoch: 90 [3400/6658 (51%)]\tLoss: 0.094441\n",
      "Train Epoch: 90 [3500/6658 (53%)]\tLoss: 0.202620\n",
      "Train Epoch: 90 [3600/6658 (54%)]\tLoss: 0.894278\n",
      "Train Epoch: 90 [3700/6658 (56%)]\tLoss: 0.154877\n",
      "Train Epoch: 90 [3800/6658 (57%)]\tLoss: 0.325613\n",
      "Train Epoch: 90 [3900/6658 (59%)]\tLoss: 0.000431\n",
      "Train Epoch: 90 [4000/6658 (60%)]\tLoss: 1.451817\n",
      "Train Epoch: 90 [4100/6658 (62%)]\tLoss: 3.600150\n",
      "Train Epoch: 90 [4200/6658 (63%)]\tLoss: 0.061620\n",
      "Train Epoch: 90 [4300/6658 (65%)]\tLoss: 1.374021\n",
      "Train Epoch: 90 [4400/6658 (66%)]\tLoss: 0.104748\n",
      "Train Epoch: 90 [4500/6658 (68%)]\tLoss: 1.013060\n",
      "Train Epoch: 90 [4600/6658 (69%)]\tLoss: 4.610545\n",
      "Train Epoch: 90 [4700/6658 (71%)]\tLoss: 1.702512\n",
      "Train Epoch: 90 [4800/6658 (72%)]\tLoss: 0.223375\n",
      "Train Epoch: 90 [4900/6658 (74%)]\tLoss: 0.299080\n",
      "Train Epoch: 90 [5000/6658 (75%)]\tLoss: 0.014393\n",
      "Train Epoch: 90 [5100/6658 (77%)]\tLoss: 0.046986\n",
      "Train Epoch: 90 [5200/6658 (78%)]\tLoss: 0.295191\n",
      "Train Epoch: 90 [5300/6658 (80%)]\tLoss: 0.499150\n",
      "Train Epoch: 90 [5400/6658 (81%)]\tLoss: 9.056275\n",
      "Train Epoch: 90 [5500/6658 (83%)]\tLoss: 0.295821\n",
      "Train Epoch: 90 [5600/6658 (84%)]\tLoss: 0.003760\n",
      "Train Epoch: 90 [5700/6658 (86%)]\tLoss: 0.430404\n",
      "Train Epoch: 90 [5800/6658 (87%)]\tLoss: 0.001848\n",
      "Train Epoch: 90 [5900/6658 (89%)]\tLoss: 0.009631\n",
      "Train Epoch: 90 [6000/6658 (90%)]\tLoss: 0.492638\n",
      "Train Epoch: 90 [6100/6658 (92%)]\tLoss: 0.026350\n",
      "Train Epoch: 90 [6200/6658 (93%)]\tLoss: 0.271239\n",
      "Train Epoch: 90 [6300/6658 (95%)]\tLoss: 0.136276\n",
      "Train Epoch: 90 [6400/6658 (96%)]\tLoss: 0.619426\n",
      "Train Epoch: 90 [6500/6658 (98%)]\tLoss: 0.022078\n",
      "Train Epoch: 90 [6600/6658 (99%)]\tLoss: 0.540405\n",
      "train loss average =  0.7306031139094247\n",
      "\n",
      "Test set: Average loss: 0.6953\n",
      "\n",
      "Validation loss decreased (0.696673 --> 0.695293).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.2005, 6.0621, 5.8908, 5.8921, 6.1272, 6.0449], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 91 [0/6658 (0%)]\tLoss: 0.869978\n",
      "Train Epoch: 91 [100/6658 (2%)]\tLoss: 0.210138\n",
      "Train Epoch: 91 [200/6658 (3%)]\tLoss: 0.000471\n",
      "Train Epoch: 91 [300/6658 (5%)]\tLoss: 0.450592\n",
      "Train Epoch: 91 [400/6658 (6%)]\tLoss: 0.189360\n",
      "Train Epoch: 91 [500/6658 (8%)]\tLoss: 0.019709\n",
      "Train Epoch: 91 [600/6658 (9%)]\tLoss: 1.137281\n",
      "Train Epoch: 91 [700/6658 (11%)]\tLoss: 2.303502\n",
      "Train Epoch: 91 [800/6658 (12%)]\tLoss: 0.289993\n",
      "Train Epoch: 91 [900/6658 (14%)]\tLoss: 0.117554\n",
      "Train Epoch: 91 [1000/6658 (15%)]\tLoss: 1.140270\n",
      "Train Epoch: 91 [1100/6658 (17%)]\tLoss: 0.356949\n",
      "Train Epoch: 91 [1200/6658 (18%)]\tLoss: 0.364593\n",
      "Train Epoch: 91 [1300/6658 (20%)]\tLoss: 0.900149\n",
      "Train Epoch: 91 [1400/6658 (21%)]\tLoss: 0.342669\n",
      "Train Epoch: 91 [1500/6658 (23%)]\tLoss: 0.057019\n",
      "Train Epoch: 91 [1600/6658 (24%)]\tLoss: 0.637603\n",
      "Train Epoch: 91 [1700/6658 (26%)]\tLoss: 0.001787\n",
      "Train Epoch: 91 [1800/6658 (27%)]\tLoss: 1.249292\n",
      "Train Epoch: 91 [1900/6658 (29%)]\tLoss: 0.001562\n",
      "Train Epoch: 91 [2000/6658 (30%)]\tLoss: 0.033021\n",
      "Train Epoch: 91 [2100/6658 (32%)]\tLoss: 0.275366\n",
      "Train Epoch: 91 [2200/6658 (33%)]\tLoss: 0.078052\n",
      "Train Epoch: 91 [2300/6658 (35%)]\tLoss: 0.052256\n",
      "Train Epoch: 91 [2400/6658 (36%)]\tLoss: 0.319470\n",
      "Train Epoch: 91 [2500/6658 (38%)]\tLoss: 0.010444\n",
      "Train Epoch: 91 [2600/6658 (39%)]\tLoss: 0.639893\n",
      "Train Epoch: 91 [2700/6658 (41%)]\tLoss: 0.246382\n",
      "Train Epoch: 91 [2800/6658 (42%)]\tLoss: 0.045122\n",
      "Train Epoch: 91 [2900/6658 (44%)]\tLoss: 0.238071\n",
      "Train Epoch: 91 [3000/6658 (45%)]\tLoss: 0.303082\n",
      "Train Epoch: 91 [3100/6658 (47%)]\tLoss: 0.172340\n",
      "Train Epoch: 91 [3200/6658 (48%)]\tLoss: 0.951924\n",
      "Train Epoch: 91 [3300/6658 (50%)]\tLoss: 0.083633\n",
      "Train Epoch: 91 [3400/6658 (51%)]\tLoss: 0.000090\n",
      "Train Epoch: 91 [3500/6658 (53%)]\tLoss: 0.017303\n",
      "Train Epoch: 91 [3600/6658 (54%)]\tLoss: 0.000969\n",
      "Train Epoch: 91 [3700/6658 (56%)]\tLoss: 0.060371\n",
      "Train Epoch: 91 [3800/6658 (57%)]\tLoss: 0.041539\n",
      "Train Epoch: 91 [3900/6658 (59%)]\tLoss: 0.004342\n",
      "Train Epoch: 91 [4000/6658 (60%)]\tLoss: 0.500905\n",
      "Train Epoch: 91 [4100/6658 (62%)]\tLoss: 1.573863\n",
      "Train Epoch: 91 [4200/6658 (63%)]\tLoss: 3.227839\n",
      "Train Epoch: 91 [4300/6658 (65%)]\tLoss: 0.036993\n",
      "Train Epoch: 91 [4400/6658 (66%)]\tLoss: 1.971758\n",
      "Train Epoch: 91 [4500/6658 (68%)]\tLoss: 0.128933\n",
      "Train Epoch: 91 [4600/6658 (69%)]\tLoss: 0.644884\n",
      "Train Epoch: 91 [4700/6658 (71%)]\tLoss: 0.033339\n",
      "Train Epoch: 91 [4800/6658 (72%)]\tLoss: 0.483829\n",
      "Train Epoch: 91 [4900/6658 (74%)]\tLoss: 0.079554\n",
      "Train Epoch: 91 [5000/6658 (75%)]\tLoss: 0.053632\n",
      "Train Epoch: 91 [5100/6658 (77%)]\tLoss: 0.148217\n",
      "Train Epoch: 91 [5200/6658 (78%)]\tLoss: 0.050547\n",
      "Train Epoch: 91 [5300/6658 (80%)]\tLoss: 0.164745\n",
      "Train Epoch: 91 [5400/6658 (81%)]\tLoss: 0.088697\n",
      "Train Epoch: 91 [5500/6658 (83%)]\tLoss: 0.326263\n",
      "Train Epoch: 91 [5600/6658 (84%)]\tLoss: 0.001086\n",
      "Train Epoch: 91 [5700/6658 (86%)]\tLoss: 0.021341\n",
      "Train Epoch: 91 [5800/6658 (87%)]\tLoss: 0.035001\n",
      "Train Epoch: 91 [5900/6658 (89%)]\tLoss: 0.009459\n",
      "Train Epoch: 91 [6000/6658 (90%)]\tLoss: 0.513918\n",
      "Train Epoch: 91 [6100/6658 (92%)]\tLoss: 0.434073\n",
      "Train Epoch: 91 [6200/6658 (93%)]\tLoss: 0.421831\n",
      "Train Epoch: 91 [6300/6658 (95%)]\tLoss: 0.030308\n",
      "Train Epoch: 91 [6400/6658 (96%)]\tLoss: 0.248208\n",
      "Train Epoch: 91 [6500/6658 (98%)]\tLoss: 0.020172\n",
      "Train Epoch: 91 [6600/6658 (99%)]\tLoss: 0.436174\n",
      "train loss average =  0.7241465427816394\n",
      "\n",
      "Test set: Average loss: 0.7016\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2020, 6.0626, 5.8896, 5.8915, 6.1288, 6.0451], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 92 [0/6658 (0%)]\tLoss: 1.329181\n",
      "Train Epoch: 92 [100/6658 (2%)]\tLoss: 0.595873\n",
      "Train Epoch: 92 [200/6658 (3%)]\tLoss: 1.194550\n",
      "Train Epoch: 92 [300/6658 (5%)]\tLoss: 0.174275\n",
      "Train Epoch: 92 [400/6658 (6%)]\tLoss: 0.980558\n",
      "Train Epoch: 92 [500/6658 (8%)]\tLoss: 0.062466\n",
      "Train Epoch: 92 [600/6658 (9%)]\tLoss: 0.003576\n",
      "Train Epoch: 92 [700/6658 (11%)]\tLoss: 0.361091\n",
      "Train Epoch: 92 [800/6658 (12%)]\tLoss: 2.376738\n",
      "Train Epoch: 92 [900/6658 (14%)]\tLoss: 0.488905\n",
      "Train Epoch: 92 [1000/6658 (15%)]\tLoss: 0.130295\n",
      "Train Epoch: 92 [1100/6658 (17%)]\tLoss: 0.337819\n",
      "Train Epoch: 92 [1200/6658 (18%)]\tLoss: 0.524434\n",
      "Train Epoch: 92 [1300/6658 (20%)]\tLoss: 0.490418\n",
      "Train Epoch: 92 [1400/6658 (21%)]\tLoss: 0.267535\n",
      "Train Epoch: 92 [1500/6658 (23%)]\tLoss: 0.284482\n",
      "Train Epoch: 92 [1600/6658 (24%)]\tLoss: 0.242201\n",
      "Train Epoch: 92 [1700/6658 (26%)]\tLoss: 0.832772\n",
      "Train Epoch: 92 [1800/6658 (27%)]\tLoss: 0.018317\n",
      "Train Epoch: 92 [1900/6658 (29%)]\tLoss: 1.665671\n",
      "Train Epoch: 92 [2000/6658 (30%)]\tLoss: 0.000111\n",
      "Train Epoch: 92 [2100/6658 (32%)]\tLoss: 0.029587\n",
      "Train Epoch: 92 [2200/6658 (33%)]\tLoss: 0.093332\n",
      "Train Epoch: 92 [2300/6658 (35%)]\tLoss: 1.341104\n",
      "Train Epoch: 92 [2400/6658 (36%)]\tLoss: 0.559257\n",
      "Train Epoch: 92 [2500/6658 (38%)]\tLoss: 0.111745\n",
      "Train Epoch: 92 [2600/6658 (39%)]\tLoss: 0.147219\n",
      "Train Epoch: 92 [2700/6658 (41%)]\tLoss: 0.243124\n",
      "Train Epoch: 92 [2800/6658 (42%)]\tLoss: 1.448299\n",
      "Train Epoch: 92 [2900/6658 (44%)]\tLoss: 0.593823\n",
      "Train Epoch: 92 [3000/6658 (45%)]\tLoss: 0.232878\n",
      "Train Epoch: 92 [3100/6658 (47%)]\tLoss: 0.830838\n",
      "Train Epoch: 92 [3200/6658 (48%)]\tLoss: 6.612347\n",
      "Train Epoch: 92 [3300/6658 (50%)]\tLoss: 0.154739\n",
      "Train Epoch: 92 [3400/6658 (51%)]\tLoss: 0.276851\n",
      "Train Epoch: 92 [3500/6658 (53%)]\tLoss: 0.090155\n",
      "Train Epoch: 92 [3600/6658 (54%)]\tLoss: 0.000468\n",
      "Train Epoch: 92 [3700/6658 (56%)]\tLoss: 1.726944\n",
      "Train Epoch: 92 [3800/6658 (57%)]\tLoss: 0.567888\n",
      "Train Epoch: 92 [3900/6658 (59%)]\tLoss: 0.052860\n",
      "Train Epoch: 92 [4000/6658 (60%)]\tLoss: 0.006078\n",
      "Train Epoch: 92 [4100/6658 (62%)]\tLoss: 0.017947\n",
      "Train Epoch: 92 [4200/6658 (63%)]\tLoss: 0.021434\n",
      "Train Epoch: 92 [4300/6658 (65%)]\tLoss: 0.003174\n",
      "Train Epoch: 92 [4400/6658 (66%)]\tLoss: 0.003104\n",
      "Train Epoch: 92 [4500/6658 (68%)]\tLoss: 0.374018\n",
      "Train Epoch: 92 [4600/6658 (69%)]\tLoss: 0.344625\n",
      "Train Epoch: 92 [4700/6658 (71%)]\tLoss: 0.405323\n",
      "Train Epoch: 92 [4800/6658 (72%)]\tLoss: 2.305537\n",
      "Train Epoch: 92 [4900/6658 (74%)]\tLoss: 43.063366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 92 [5000/6658 (75%)]\tLoss: 0.219993\n",
      "Train Epoch: 92 [5100/6658 (77%)]\tLoss: 0.003117\n",
      "Train Epoch: 92 [5200/6658 (78%)]\tLoss: 1.040967\n",
      "Train Epoch: 92 [5300/6658 (80%)]\tLoss: 0.051702\n",
      "Train Epoch: 92 [5400/6658 (81%)]\tLoss: 1.860230\n",
      "Train Epoch: 92 [5500/6658 (83%)]\tLoss: 0.254557\n",
      "Train Epoch: 92 [5600/6658 (84%)]\tLoss: 0.165928\n",
      "Train Epoch: 92 [5700/6658 (86%)]\tLoss: 0.455264\n",
      "Train Epoch: 92 [5800/6658 (87%)]\tLoss: 0.073982\n",
      "Train Epoch: 92 [5900/6658 (89%)]\tLoss: 0.379307\n",
      "Train Epoch: 92 [6000/6658 (90%)]\tLoss: 0.071365\n",
      "Train Epoch: 92 [6100/6658 (92%)]\tLoss: 0.002517\n",
      "Train Epoch: 92 [6200/6658 (93%)]\tLoss: 2.200839\n",
      "Train Epoch: 92 [6300/6658 (95%)]\tLoss: 0.078345\n",
      "Train Epoch: 92 [6400/6658 (96%)]\tLoss: 0.043754\n",
      "Train Epoch: 92 [6500/6658 (98%)]\tLoss: 0.104942\n",
      "Train Epoch: 92 [6600/6658 (99%)]\tLoss: 0.467312\n",
      "train loss average =  0.725006447630826\n",
      "\n",
      "Test set: Average loss: 0.7060\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2048, 6.0634, 5.8885, 5.8912, 6.1308, 6.0457], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 93 [0/6658 (0%)]\tLoss: 0.144815\n",
      "Train Epoch: 93 [100/6658 (2%)]\tLoss: 0.016523\n",
      "Train Epoch: 93 [200/6658 (3%)]\tLoss: 0.400901\n",
      "Train Epoch: 93 [300/6658 (5%)]\tLoss: 0.423056\n",
      "Train Epoch: 93 [400/6658 (6%)]\tLoss: 1.653416\n",
      "Train Epoch: 93 [500/6658 (8%)]\tLoss: 0.145119\n",
      "Train Epoch: 93 [600/6658 (9%)]\tLoss: 0.000042\n",
      "Train Epoch: 93 [700/6658 (11%)]\tLoss: 0.075321\n",
      "Train Epoch: 93 [800/6658 (12%)]\tLoss: 0.017088\n",
      "Train Epoch: 93 [900/6658 (14%)]\tLoss: 0.007175\n",
      "Train Epoch: 93 [1000/6658 (15%)]\tLoss: 0.069260\n",
      "Train Epoch: 93 [1100/6658 (17%)]\tLoss: 0.104130\n",
      "Train Epoch: 93 [1200/6658 (18%)]\tLoss: 0.375960\n",
      "Train Epoch: 93 [1300/6658 (20%)]\tLoss: 0.049396\n",
      "Train Epoch: 93 [1400/6658 (21%)]\tLoss: 0.083497\n",
      "Train Epoch: 93 [1500/6658 (23%)]\tLoss: 0.672692\n",
      "Train Epoch: 93 [1600/6658 (24%)]\tLoss: 0.432631\n",
      "Train Epoch: 93 [1700/6658 (26%)]\tLoss: 0.005018\n",
      "Train Epoch: 93 [1800/6658 (27%)]\tLoss: 0.190070\n",
      "Train Epoch: 93 [1900/6658 (29%)]\tLoss: 0.149766\n",
      "Train Epoch: 93 [2000/6658 (30%)]\tLoss: 0.001413\n",
      "Train Epoch: 93 [2100/6658 (32%)]\tLoss: 0.226467\n",
      "Train Epoch: 93 [2200/6658 (33%)]\tLoss: 0.763554\n",
      "Train Epoch: 93 [2300/6658 (35%)]\tLoss: 0.004540\n",
      "Train Epoch: 93 [2400/6658 (36%)]\tLoss: 0.015433\n",
      "Train Epoch: 93 [2500/6658 (38%)]\tLoss: 2.332687\n",
      "Train Epoch: 93 [2600/6658 (39%)]\tLoss: 0.533978\n",
      "Train Epoch: 93 [2700/6658 (41%)]\tLoss: 0.120491\n",
      "Train Epoch: 93 [2800/6658 (42%)]\tLoss: 0.084824\n",
      "Train Epoch: 93 [2900/6658 (44%)]\tLoss: 0.077626\n",
      "Train Epoch: 93 [3000/6658 (45%)]\tLoss: 0.692007\n",
      "Train Epoch: 93 [3100/6658 (47%)]\tLoss: 0.023274\n",
      "Train Epoch: 93 [3200/6658 (48%)]\tLoss: 0.005720\n",
      "Train Epoch: 93 [3300/6658 (50%)]\tLoss: 0.352297\n",
      "Train Epoch: 93 [3400/6658 (51%)]\tLoss: 0.003268\n",
      "Train Epoch: 93 [3500/6658 (53%)]\tLoss: 0.002208\n",
      "Train Epoch: 93 [3600/6658 (54%)]\tLoss: 0.049877\n",
      "Train Epoch: 93 [3700/6658 (56%)]\tLoss: 0.235058\n",
      "Train Epoch: 93 [3800/6658 (57%)]\tLoss: 0.081622\n",
      "Train Epoch: 93 [3900/6658 (59%)]\tLoss: 0.044341\n",
      "Train Epoch: 93 [4000/6658 (60%)]\tLoss: 0.855740\n",
      "Train Epoch: 93 [4100/6658 (62%)]\tLoss: 0.751835\n",
      "Train Epoch: 93 [4200/6658 (63%)]\tLoss: 0.057544\n",
      "Train Epoch: 93 [4300/6658 (65%)]\tLoss: 0.282833\n",
      "Train Epoch: 93 [4400/6658 (66%)]\tLoss: 0.218092\n",
      "Train Epoch: 93 [4500/6658 (68%)]\tLoss: 0.262633\n",
      "Train Epoch: 93 [4600/6658 (69%)]\tLoss: 0.216524\n",
      "Train Epoch: 93 [4700/6658 (71%)]\tLoss: 0.243319\n",
      "Train Epoch: 93 [4800/6658 (72%)]\tLoss: 0.016428\n",
      "Train Epoch: 93 [4900/6658 (74%)]\tLoss: 0.104441\n",
      "Train Epoch: 93 [5000/6658 (75%)]\tLoss: 0.066998\n",
      "Train Epoch: 93 [5100/6658 (77%)]\tLoss: 0.011954\n",
      "Train Epoch: 93 [5200/6658 (78%)]\tLoss: 0.051511\n",
      "Train Epoch: 93 [5300/6658 (80%)]\tLoss: 2.009719\n",
      "Train Epoch: 93 [5400/6658 (81%)]\tLoss: 0.000738\n",
      "Train Epoch: 93 [5500/6658 (83%)]\tLoss: 0.789707\n",
      "Train Epoch: 93 [5600/6658 (84%)]\tLoss: 1.320517\n",
      "Train Epoch: 93 [5700/6658 (86%)]\tLoss: 0.224044\n",
      "Train Epoch: 93 [5800/6658 (87%)]\tLoss: 0.127163\n",
      "Train Epoch: 93 [5900/6658 (89%)]\tLoss: 0.012389\n",
      "Train Epoch: 93 [6000/6658 (90%)]\tLoss: 0.696281\n",
      "Train Epoch: 93 [6100/6658 (92%)]\tLoss: 1.370629\n",
      "Train Epoch: 93 [6200/6658 (93%)]\tLoss: 0.466396\n",
      "Train Epoch: 93 [6300/6658 (95%)]\tLoss: 1.892702\n",
      "Train Epoch: 93 [6400/6658 (96%)]\tLoss: 0.016238\n",
      "Train Epoch: 93 [6500/6658 (98%)]\tLoss: 0.136903\n",
      "Train Epoch: 93 [6600/6658 (99%)]\tLoss: 3.070518\n",
      "train loss average =  0.722381009005958\n",
      "\n",
      "Test set: Average loss: 0.6928\n",
      "\n",
      "Validation loss decreased (0.695293 --> 0.692768).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.2063, 6.0647, 5.8882, 5.8908, 6.1330, 6.0455], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 94 [0/6658 (0%)]\tLoss: 0.476090\n",
      "Train Epoch: 94 [100/6658 (2%)]\tLoss: 1.025381\n",
      "Train Epoch: 94 [200/6658 (3%)]\tLoss: 0.303157\n",
      "Train Epoch: 94 [300/6658 (5%)]\tLoss: 3.630331\n",
      "Train Epoch: 94 [400/6658 (6%)]\tLoss: 0.056337\n",
      "Train Epoch: 94 [500/6658 (8%)]\tLoss: 0.128391\n",
      "Train Epoch: 94 [600/6658 (9%)]\tLoss: 4.294854\n",
      "Train Epoch: 94 [700/6658 (11%)]\tLoss: 0.003662\n",
      "Train Epoch: 94 [800/6658 (12%)]\tLoss: 0.804110\n",
      "Train Epoch: 94 [900/6658 (14%)]\tLoss: 0.000001\n",
      "Train Epoch: 94 [1000/6658 (15%)]\tLoss: 0.307672\n",
      "Train Epoch: 94 [1100/6658 (17%)]\tLoss: 0.678890\n",
      "Train Epoch: 94 [1200/6658 (18%)]\tLoss: 0.615399\n",
      "Train Epoch: 94 [1300/6658 (20%)]\tLoss: 0.208923\n",
      "Train Epoch: 94 [1400/6658 (21%)]\tLoss: 0.035008\n",
      "Train Epoch: 94 [1500/6658 (23%)]\tLoss: 0.706904\n",
      "Train Epoch: 94 [1600/6658 (24%)]\tLoss: 0.002572\n",
      "Train Epoch: 94 [1700/6658 (26%)]\tLoss: 0.453364\n",
      "Train Epoch: 94 [1800/6658 (27%)]\tLoss: 0.125904\n",
      "Train Epoch: 94 [1900/6658 (29%)]\tLoss: 0.020351\n",
      "Train Epoch: 94 [2000/6658 (30%)]\tLoss: 0.000316\n",
      "Train Epoch: 94 [2100/6658 (32%)]\tLoss: 0.196380\n",
      "Train Epoch: 94 [2200/6658 (33%)]\tLoss: 0.306366\n",
      "Train Epoch: 94 [2300/6658 (35%)]\tLoss: 1.720016\n",
      "Train Epoch: 94 [2400/6658 (36%)]\tLoss: 0.110617\n",
      "Train Epoch: 94 [2500/6658 (38%)]\tLoss: 0.709241\n",
      "Train Epoch: 94 [2600/6658 (39%)]\tLoss: 0.004306\n",
      "Train Epoch: 94 [2700/6658 (41%)]\tLoss: 0.124230\n",
      "Train Epoch: 94 [2800/6658 (42%)]\tLoss: 0.057671\n",
      "Train Epoch: 94 [2900/6658 (44%)]\tLoss: 0.639347\n",
      "Train Epoch: 94 [3000/6658 (45%)]\tLoss: 0.004189\n",
      "Train Epoch: 94 [3100/6658 (47%)]\tLoss: 0.096727\n",
      "Train Epoch: 94 [3200/6658 (48%)]\tLoss: 0.049837\n",
      "Train Epoch: 94 [3300/6658 (50%)]\tLoss: 6.310185\n",
      "Train Epoch: 94 [3400/6658 (51%)]\tLoss: 0.174658\n",
      "Train Epoch: 94 [3500/6658 (53%)]\tLoss: 0.290826\n",
      "Train Epoch: 94 [3600/6658 (54%)]\tLoss: 0.954686\n",
      "Train Epoch: 94 [3700/6658 (56%)]\tLoss: 0.001076\n",
      "Train Epoch: 94 [3800/6658 (57%)]\tLoss: 0.003484\n",
      "Train Epoch: 94 [3900/6658 (59%)]\tLoss: 1.835334\n",
      "Train Epoch: 94 [4000/6658 (60%)]\tLoss: 0.034563\n",
      "Train Epoch: 94 [4100/6658 (62%)]\tLoss: 0.029294\n",
      "Train Epoch: 94 [4200/6658 (63%)]\tLoss: 1.336199\n",
      "Train Epoch: 94 [4300/6658 (65%)]\tLoss: 0.086646\n",
      "Train Epoch: 94 [4400/6658 (66%)]\tLoss: 0.014857\n",
      "Train Epoch: 94 [4500/6658 (68%)]\tLoss: 0.003257\n",
      "Train Epoch: 94 [4600/6658 (69%)]\tLoss: 0.025411\n",
      "Train Epoch: 94 [4700/6658 (71%)]\tLoss: 0.241819\n",
      "Train Epoch: 94 [4800/6658 (72%)]\tLoss: 0.353562\n",
      "Train Epoch: 94 [4900/6658 (74%)]\tLoss: 0.025554\n",
      "Train Epoch: 94 [5000/6658 (75%)]\tLoss: 1.498836\n",
      "Train Epoch: 94 [5100/6658 (77%)]\tLoss: 1.259167\n",
      "Train Epoch: 94 [5200/6658 (78%)]\tLoss: 0.011616\n",
      "Train Epoch: 94 [5300/6658 (80%)]\tLoss: 0.627505\n",
      "Train Epoch: 94 [5400/6658 (81%)]\tLoss: 0.004870\n",
      "Train Epoch: 94 [5500/6658 (83%)]\tLoss: 2.219642\n",
      "Train Epoch: 94 [5600/6658 (84%)]\tLoss: 0.311650\n",
      "Train Epoch: 94 [5700/6658 (86%)]\tLoss: 8.969364\n",
      "Train Epoch: 94 [5800/6658 (87%)]\tLoss: 0.003164\n",
      "Train Epoch: 94 [5900/6658 (89%)]\tLoss: 0.083824\n",
      "Train Epoch: 94 [6000/6658 (90%)]\tLoss: 0.075827\n",
      "Train Epoch: 94 [6100/6658 (92%)]\tLoss: 0.213088\n",
      "Train Epoch: 94 [6200/6658 (93%)]\tLoss: 0.841897\n",
      "Train Epoch: 94 [6300/6658 (95%)]\tLoss: 0.073003\n",
      "Train Epoch: 94 [6400/6658 (96%)]\tLoss: 1.385538\n",
      "Train Epoch: 94 [6500/6658 (98%)]\tLoss: 0.147948\n",
      "Train Epoch: 94 [6600/6658 (99%)]\tLoss: 0.003699\n",
      "train loss average =  0.7205767507807374\n",
      "\n",
      "Test set: Average loss: 0.7013\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2083, 6.0647, 5.8872, 5.8909, 6.1343, 6.0459], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 95 [0/6658 (0%)]\tLoss: 0.813529\n",
      "Train Epoch: 95 [100/6658 (2%)]\tLoss: 0.036271\n",
      "Train Epoch: 95 [200/6658 (3%)]\tLoss: 0.002154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 95 [300/6658 (5%)]\tLoss: 0.056132\n",
      "Train Epoch: 95 [400/6658 (6%)]\tLoss: 0.567495\n",
      "Train Epoch: 95 [500/6658 (8%)]\tLoss: 0.072571\n",
      "Train Epoch: 95 [600/6658 (9%)]\tLoss: 0.602338\n",
      "Train Epoch: 95 [700/6658 (11%)]\tLoss: 0.211891\n",
      "Train Epoch: 95 [800/6658 (12%)]\tLoss: 0.142469\n",
      "Train Epoch: 95 [900/6658 (14%)]\tLoss: 0.059244\n",
      "Train Epoch: 95 [1000/6658 (15%)]\tLoss: 0.164671\n",
      "Train Epoch: 95 [1100/6658 (17%)]\tLoss: 0.032906\n",
      "Train Epoch: 95 [1200/6658 (18%)]\tLoss: 0.071466\n",
      "Train Epoch: 95 [1300/6658 (20%)]\tLoss: 3.575171\n",
      "Train Epoch: 95 [1400/6658 (21%)]\tLoss: 0.100043\n",
      "Train Epoch: 95 [1500/6658 (23%)]\tLoss: 0.099511\n",
      "Train Epoch: 95 [1600/6658 (24%)]\tLoss: 0.252713\n",
      "Train Epoch: 95 [1700/6658 (26%)]\tLoss: 0.006418\n",
      "Train Epoch: 95 [1800/6658 (27%)]\tLoss: 0.016529\n",
      "Train Epoch: 95 [1900/6658 (29%)]\tLoss: 0.188778\n",
      "Train Epoch: 95 [2000/6658 (30%)]\tLoss: 0.118622\n",
      "Train Epoch: 95 [2100/6658 (32%)]\tLoss: 0.544471\n",
      "Train Epoch: 95 [2200/6658 (33%)]\tLoss: 0.066454\n",
      "Train Epoch: 95 [2300/6658 (35%)]\tLoss: 1.250218\n",
      "Train Epoch: 95 [2400/6658 (36%)]\tLoss: 0.193256\n",
      "Train Epoch: 95 [2500/6658 (38%)]\tLoss: 0.008003\n",
      "Train Epoch: 95 [2600/6658 (39%)]\tLoss: 0.381721\n",
      "Train Epoch: 95 [2700/6658 (41%)]\tLoss: 0.890215\n",
      "Train Epoch: 95 [2800/6658 (42%)]\tLoss: 0.137663\n",
      "Train Epoch: 95 [2900/6658 (44%)]\tLoss: 0.000133\n",
      "Train Epoch: 95 [3000/6658 (45%)]\tLoss: 0.100023\n",
      "Train Epoch: 95 [3100/6658 (47%)]\tLoss: 1.690998\n",
      "Train Epoch: 95 [3200/6658 (48%)]\tLoss: 0.812416\n",
      "Train Epoch: 95 [3300/6658 (50%)]\tLoss: 0.663161\n",
      "Train Epoch: 95 [3400/6658 (51%)]\tLoss: 0.172231\n",
      "Train Epoch: 95 [3500/6658 (53%)]\tLoss: 3.527025\n",
      "Train Epoch: 95 [3600/6658 (54%)]\tLoss: 0.125335\n",
      "Train Epoch: 95 [3700/6658 (56%)]\tLoss: 0.148870\n",
      "Train Epoch: 95 [3800/6658 (57%)]\tLoss: 0.004563\n",
      "Train Epoch: 95 [3900/6658 (59%)]\tLoss: 0.000553\n",
      "Train Epoch: 95 [4000/6658 (60%)]\tLoss: 0.357169\n",
      "Train Epoch: 95 [4100/6658 (62%)]\tLoss: 0.139586\n",
      "Train Epoch: 95 [4200/6658 (63%)]\tLoss: 0.114366\n",
      "Train Epoch: 95 [4300/6658 (65%)]\tLoss: 0.212177\n",
      "Train Epoch: 95 [4400/6658 (66%)]\tLoss: 0.270760\n",
      "Train Epoch: 95 [4500/6658 (68%)]\tLoss: 0.284744\n",
      "Train Epoch: 95 [4600/6658 (69%)]\tLoss: 0.061442\n",
      "Train Epoch: 95 [4700/6658 (71%)]\tLoss: 0.669808\n",
      "Train Epoch: 95 [4800/6658 (72%)]\tLoss: 4.638442\n",
      "Train Epoch: 95 [4900/6658 (74%)]\tLoss: 0.001148\n",
      "Train Epoch: 95 [5000/6658 (75%)]\tLoss: 4.388507\n",
      "Train Epoch: 95 [5100/6658 (77%)]\tLoss: 1.700114\n",
      "Train Epoch: 95 [5200/6658 (78%)]\tLoss: 0.055280\n",
      "Train Epoch: 95 [5300/6658 (80%)]\tLoss: 0.005184\n",
      "Train Epoch: 95 [5400/6658 (81%)]\tLoss: 0.549637\n",
      "Train Epoch: 95 [5500/6658 (83%)]\tLoss: 0.178943\n",
      "Train Epoch: 95 [5600/6658 (84%)]\tLoss: 0.268967\n",
      "Train Epoch: 95 [5700/6658 (86%)]\tLoss: 0.475460\n",
      "Train Epoch: 95 [5800/6658 (87%)]\tLoss: 0.000028\n",
      "Train Epoch: 95 [5900/6658 (89%)]\tLoss: 0.230466\n",
      "Train Epoch: 95 [6000/6658 (90%)]\tLoss: 0.004181\n",
      "Train Epoch: 95 [6100/6658 (92%)]\tLoss: 0.065379\n",
      "Train Epoch: 95 [6200/6658 (93%)]\tLoss: 0.071910\n",
      "Train Epoch: 95 [6300/6658 (95%)]\tLoss: 0.109563\n",
      "Train Epoch: 95 [6400/6658 (96%)]\tLoss: 0.142358\n",
      "Train Epoch: 95 [6500/6658 (98%)]\tLoss: 1.736876\n",
      "Train Epoch: 95 [6600/6658 (99%)]\tLoss: 0.011555\n",
      "train loss average =  0.7265001077105698\n",
      "\n",
      "Test set: Average loss: 0.7064\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2099, 6.0639, 5.8853, 5.8907, 6.1351, 6.0458], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 96 [0/6658 (0%)]\tLoss: 0.475409\n",
      "Train Epoch: 96 [100/6658 (2%)]\tLoss: 0.130577\n",
      "Train Epoch: 96 [200/6658 (3%)]\tLoss: 1.494173\n",
      "Train Epoch: 96 [300/6658 (5%)]\tLoss: 0.389529\n",
      "Train Epoch: 96 [400/6658 (6%)]\tLoss: 0.216587\n",
      "Train Epoch: 96 [500/6658 (8%)]\tLoss: 0.410045\n",
      "Train Epoch: 96 [600/6658 (9%)]\tLoss: 0.046062\n",
      "Train Epoch: 96 [700/6658 (11%)]\tLoss: 0.486168\n",
      "Train Epoch: 96 [800/6658 (12%)]\tLoss: 0.001694\n",
      "Train Epoch: 96 [900/6658 (14%)]\tLoss: 0.074745\n",
      "Train Epoch: 96 [1000/6658 (15%)]\tLoss: 0.019191\n",
      "Train Epoch: 96 [1100/6658 (17%)]\tLoss: 0.563851\n",
      "Train Epoch: 96 [1200/6658 (18%)]\tLoss: 0.030426\n",
      "Train Epoch: 96 [1300/6658 (20%)]\tLoss: 0.034792\n",
      "Train Epoch: 96 [1400/6658 (21%)]\tLoss: 1.240628\n",
      "Train Epoch: 96 [1500/6658 (23%)]\tLoss: 0.005013\n",
      "Train Epoch: 96 [1600/6658 (24%)]\tLoss: 0.119103\n",
      "Train Epoch: 96 [1700/6658 (26%)]\tLoss: 0.072312\n",
      "Train Epoch: 96 [1800/6658 (27%)]\tLoss: 1.549245\n",
      "Train Epoch: 96 [1900/6658 (29%)]\tLoss: 1.557190\n",
      "Train Epoch: 96 [2000/6658 (30%)]\tLoss: 0.262223\n",
      "Train Epoch: 96 [2100/6658 (32%)]\tLoss: 0.577012\n",
      "Train Epoch: 96 [2200/6658 (33%)]\tLoss: 0.063699\n",
      "Train Epoch: 96 [2300/6658 (35%)]\tLoss: 1.125436\n",
      "Train Epoch: 96 [2400/6658 (36%)]\tLoss: 2.029835\n",
      "Train Epoch: 96 [2500/6658 (38%)]\tLoss: 0.375157\n",
      "Train Epoch: 96 [2600/6658 (39%)]\tLoss: 0.455012\n",
      "Train Epoch: 96 [2700/6658 (41%)]\tLoss: 0.000117\n",
      "Train Epoch: 96 [2800/6658 (42%)]\tLoss: 0.034587\n",
      "Train Epoch: 96 [2900/6658 (44%)]\tLoss: 0.248437\n",
      "Train Epoch: 96 [3000/6658 (45%)]\tLoss: 0.009238\n",
      "Train Epoch: 96 [3100/6658 (47%)]\tLoss: 0.631642\n",
      "Train Epoch: 96 [3200/6658 (48%)]\tLoss: 0.494960\n",
      "Train Epoch: 96 [3300/6658 (50%)]\tLoss: 0.202347\n",
      "Train Epoch: 96 [3400/6658 (51%)]\tLoss: 0.360307\n",
      "Train Epoch: 96 [3500/6658 (53%)]\tLoss: 0.062793\n",
      "Train Epoch: 96 [3600/6658 (54%)]\tLoss: 0.000085\n",
      "Train Epoch: 96 [3700/6658 (56%)]\tLoss: 1.157882\n",
      "Train Epoch: 96 [3800/6658 (57%)]\tLoss: 1.255931\n",
      "Train Epoch: 96 [3900/6658 (59%)]\tLoss: 0.221188\n",
      "Train Epoch: 96 [4000/6658 (60%)]\tLoss: 0.348329\n",
      "Train Epoch: 96 [4100/6658 (62%)]\tLoss: 0.991437\n",
      "Train Epoch: 96 [4200/6658 (63%)]\tLoss: 0.257296\n",
      "Train Epoch: 96 [4300/6658 (65%)]\tLoss: 0.326326\n",
      "Train Epoch: 96 [4400/6658 (66%)]\tLoss: 0.233723\n",
      "Train Epoch: 96 [4500/6658 (68%)]\tLoss: 0.771782\n",
      "Train Epoch: 96 [4600/6658 (69%)]\tLoss: 0.564534\n",
      "Train Epoch: 96 [4700/6658 (71%)]\tLoss: 0.063856\n",
      "Train Epoch: 96 [4800/6658 (72%)]\tLoss: 1.293050\n",
      "Train Epoch: 96 [4900/6658 (74%)]\tLoss: 0.379130\n",
      "Train Epoch: 96 [5000/6658 (75%)]\tLoss: 0.461525\n",
      "Train Epoch: 96 [5100/6658 (77%)]\tLoss: 1.194918\n",
      "Train Epoch: 96 [5200/6658 (78%)]\tLoss: 0.001004\n",
      "Train Epoch: 96 [5300/6658 (80%)]\tLoss: 4.413352\n",
      "Train Epoch: 96 [5400/6658 (81%)]\tLoss: 0.623487\n",
      "Train Epoch: 96 [5500/6658 (83%)]\tLoss: 0.580311\n",
      "Train Epoch: 96 [5600/6658 (84%)]\tLoss: 0.027325\n",
      "Train Epoch: 96 [5700/6658 (86%)]\tLoss: 0.026453\n",
      "Train Epoch: 96 [5800/6658 (87%)]\tLoss: 0.482740\n",
      "Train Epoch: 96 [5900/6658 (89%)]\tLoss: 0.258423\n",
      "Train Epoch: 96 [6000/6658 (90%)]\tLoss: 0.053045\n",
      "Train Epoch: 96 [6100/6658 (92%)]\tLoss: 0.203937\n",
      "Train Epoch: 96 [6200/6658 (93%)]\tLoss: 0.746326\n",
      "Train Epoch: 96 [6300/6658 (95%)]\tLoss: 0.244015\n",
      "Train Epoch: 96 [6400/6658 (96%)]\tLoss: 0.572143\n",
      "Train Epoch: 96 [6500/6658 (98%)]\tLoss: 0.055298\n",
      "Train Epoch: 96 [6600/6658 (99%)]\tLoss: 0.331507\n",
      "train loss average =  0.725654409692941\n",
      "\n",
      "Test set: Average loss: 0.7023\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2115, 6.0642, 5.8846, 5.8900, 6.1371, 6.0461], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 97 [0/6658 (0%)]\tLoss: 0.004222\n",
      "Train Epoch: 97 [100/6658 (2%)]\tLoss: 0.038084\n",
      "Train Epoch: 97 [200/6658 (3%)]\tLoss: 0.846578\n",
      "Train Epoch: 97 [300/6658 (5%)]\tLoss: 3.758203\n",
      "Train Epoch: 97 [400/6658 (6%)]\tLoss: 0.195630\n",
      "Train Epoch: 97 [500/6658 (8%)]\tLoss: 0.262550\n",
      "Train Epoch: 97 [600/6658 (9%)]\tLoss: 0.102893\n",
      "Train Epoch: 97 [700/6658 (11%)]\tLoss: 1.780872\n",
      "Train Epoch: 97 [800/6658 (12%)]\tLoss: 0.011422\n",
      "Train Epoch: 97 [900/6658 (14%)]\tLoss: 0.038409\n",
      "Train Epoch: 97 [1000/6658 (15%)]\tLoss: 0.055246\n",
      "Train Epoch: 97 [1100/6658 (17%)]\tLoss: 0.151885\n",
      "Train Epoch: 97 [1200/6658 (18%)]\tLoss: 0.408983\n",
      "Train Epoch: 97 [1300/6658 (20%)]\tLoss: 0.340216\n",
      "Train Epoch: 97 [1400/6658 (21%)]\tLoss: 0.402674\n",
      "Train Epoch: 97 [1500/6658 (23%)]\tLoss: 0.007828\n",
      "Train Epoch: 97 [1600/6658 (24%)]\tLoss: 10.247187\n",
      "Train Epoch: 97 [1700/6658 (26%)]\tLoss: 3.065003\n",
      "Train Epoch: 97 [1800/6658 (27%)]\tLoss: 3.282331\n",
      "Train Epoch: 97 [1900/6658 (29%)]\tLoss: 0.011671\n",
      "Train Epoch: 97 [2000/6658 (30%)]\tLoss: 0.040102\n",
      "Train Epoch: 97 [2100/6658 (32%)]\tLoss: 2.134335\n",
      "Train Epoch: 97 [2200/6658 (33%)]\tLoss: 0.036739\n",
      "Train Epoch: 97 [2300/6658 (35%)]\tLoss: 0.087201\n",
      "Train Epoch: 97 [2400/6658 (36%)]\tLoss: 0.211223\n",
      "Train Epoch: 97 [2500/6658 (38%)]\tLoss: 0.021357\n",
      "Train Epoch: 97 [2600/6658 (39%)]\tLoss: 0.891623\n",
      "Train Epoch: 97 [2700/6658 (41%)]\tLoss: 0.640950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 97 [2800/6658 (42%)]\tLoss: 1.505251\n",
      "Train Epoch: 97 [2900/6658 (44%)]\tLoss: 0.939489\n",
      "Train Epoch: 97 [3000/6658 (45%)]\tLoss: 1.210955\n",
      "Train Epoch: 97 [3100/6658 (47%)]\tLoss: 3.170550\n",
      "Train Epoch: 97 [3200/6658 (48%)]\tLoss: 4.216420\n",
      "Train Epoch: 97 [3300/6658 (50%)]\tLoss: 0.447574\n",
      "Train Epoch: 97 [3400/6658 (51%)]\tLoss: 0.265408\n",
      "Train Epoch: 97 [3500/6658 (53%)]\tLoss: 0.392837\n",
      "Train Epoch: 97 [3600/6658 (54%)]\tLoss: 2.181271\n",
      "Train Epoch: 97 [3700/6658 (56%)]\tLoss: 0.004085\n",
      "Train Epoch: 97 [3800/6658 (57%)]\tLoss: 0.484150\n",
      "Train Epoch: 97 [3900/6658 (59%)]\tLoss: 0.524524\n",
      "Train Epoch: 97 [4000/6658 (60%)]\tLoss: 0.019777\n",
      "Train Epoch: 97 [4100/6658 (62%)]\tLoss: 0.803338\n",
      "Train Epoch: 97 [4200/6658 (63%)]\tLoss: 0.391371\n",
      "Train Epoch: 97 [4300/6658 (65%)]\tLoss: 2.310033\n",
      "Train Epoch: 97 [4400/6658 (66%)]\tLoss: 0.000451\n",
      "Train Epoch: 97 [4500/6658 (68%)]\tLoss: 0.288365\n",
      "Train Epoch: 97 [4600/6658 (69%)]\tLoss: 0.045655\n",
      "Train Epoch: 97 [4700/6658 (71%)]\tLoss: 0.032951\n",
      "Train Epoch: 97 [4800/6658 (72%)]\tLoss: 0.505025\n",
      "Train Epoch: 97 [4900/6658 (74%)]\tLoss: 0.042749\n",
      "Train Epoch: 97 [5000/6658 (75%)]\tLoss: 0.005042\n",
      "Train Epoch: 97 [5100/6658 (77%)]\tLoss: 0.075262\n",
      "Train Epoch: 97 [5200/6658 (78%)]\tLoss: 0.013466\n",
      "Train Epoch: 97 [5300/6658 (80%)]\tLoss: 0.321988\n",
      "Train Epoch: 97 [5400/6658 (81%)]\tLoss: 0.117554\n",
      "Train Epoch: 97 [5500/6658 (83%)]\tLoss: 0.092626\n",
      "Train Epoch: 97 [5600/6658 (84%)]\tLoss: 0.654699\n",
      "Train Epoch: 97 [5700/6658 (86%)]\tLoss: 0.374967\n",
      "Train Epoch: 97 [5800/6658 (87%)]\tLoss: 0.196717\n",
      "Train Epoch: 97 [5900/6658 (89%)]\tLoss: 0.386160\n",
      "Train Epoch: 97 [6000/6658 (90%)]\tLoss: 0.633383\n",
      "Train Epoch: 97 [6100/6658 (92%)]\tLoss: 0.048641\n",
      "Train Epoch: 97 [6200/6658 (93%)]\tLoss: 0.068180\n",
      "Train Epoch: 97 [6300/6658 (95%)]\tLoss: 0.943698\n",
      "Train Epoch: 97 [6400/6658 (96%)]\tLoss: 2.046093\n",
      "Train Epoch: 97 [6500/6658 (98%)]\tLoss: 0.004078\n",
      "Train Epoch: 97 [6600/6658 (99%)]\tLoss: 2.093657\n",
      "train loss average =  0.7209988311446855\n",
      "\n",
      "Test set: Average loss: 0.6935\n",
      "\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2131, 6.0642, 5.8845, 5.8892, 6.1389, 6.0462], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 98 [0/6658 (0%)]\tLoss: 0.138114\n",
      "Train Epoch: 98 [100/6658 (2%)]\tLoss: 1.140009\n",
      "Train Epoch: 98 [200/6658 (3%)]\tLoss: 0.001368\n",
      "Train Epoch: 98 [300/6658 (5%)]\tLoss: 0.000144\n",
      "Train Epoch: 98 [400/6658 (6%)]\tLoss: 0.295273\n",
      "Train Epoch: 98 [500/6658 (8%)]\tLoss: 0.000019\n",
      "Train Epoch: 98 [600/6658 (9%)]\tLoss: 0.412632\n",
      "Train Epoch: 98 [700/6658 (11%)]\tLoss: 0.374748\n",
      "Train Epoch: 98 [800/6658 (12%)]\tLoss: 4.771183\n",
      "Train Epoch: 98 [900/6658 (14%)]\tLoss: 0.028495\n",
      "Train Epoch: 98 [1000/6658 (15%)]\tLoss: 0.190185\n",
      "Train Epoch: 98 [1100/6658 (17%)]\tLoss: 0.107538\n",
      "Train Epoch: 98 [1200/6658 (18%)]\tLoss: 0.285042\n",
      "Train Epoch: 98 [1300/6658 (20%)]\tLoss: 0.151844\n",
      "Train Epoch: 98 [1400/6658 (21%)]\tLoss: 0.392329\n",
      "Train Epoch: 98 [1500/6658 (23%)]\tLoss: 0.104796\n",
      "Train Epoch: 98 [1600/6658 (24%)]\tLoss: 0.002581\n",
      "Train Epoch: 98 [1700/6658 (26%)]\tLoss: 0.099955\n",
      "Train Epoch: 98 [1800/6658 (27%)]\tLoss: 0.445580\n",
      "Train Epoch: 98 [1900/6658 (29%)]\tLoss: 0.457197\n",
      "Train Epoch: 98 [2000/6658 (30%)]\tLoss: 0.403255\n",
      "Train Epoch: 98 [2100/6658 (32%)]\tLoss: 4.146689\n",
      "Train Epoch: 98 [2200/6658 (33%)]\tLoss: 0.584035\n",
      "Train Epoch: 98 [2300/6658 (35%)]\tLoss: 2.904385\n",
      "Train Epoch: 98 [2400/6658 (36%)]\tLoss: 0.045467\n",
      "Train Epoch: 98 [2500/6658 (38%)]\tLoss: 2.065372\n",
      "Train Epoch: 98 [2600/6658 (39%)]\tLoss: 1.689788\n",
      "Train Epoch: 98 [2700/6658 (41%)]\tLoss: 0.129230\n",
      "Train Epoch: 98 [2800/6658 (42%)]\tLoss: 0.504009\n",
      "Train Epoch: 98 [2900/6658 (44%)]\tLoss: 0.568624\n",
      "Train Epoch: 98 [3000/6658 (45%)]\tLoss: 0.094630\n",
      "Train Epoch: 98 [3100/6658 (47%)]\tLoss: 0.037963\n",
      "Train Epoch: 98 [3200/6658 (48%)]\tLoss: 0.078488\n",
      "Train Epoch: 98 [3300/6658 (50%)]\tLoss: 0.329207\n",
      "Train Epoch: 98 [3400/6658 (51%)]\tLoss: 0.077342\n",
      "Train Epoch: 98 [3500/6658 (53%)]\tLoss: 0.009496\n",
      "Train Epoch: 98 [3600/6658 (54%)]\tLoss: 0.107233\n",
      "Train Epoch: 98 [3700/6658 (56%)]\tLoss: 0.421275\n",
      "Train Epoch: 98 [3800/6658 (57%)]\tLoss: 0.201263\n",
      "Train Epoch: 98 [3900/6658 (59%)]\tLoss: 0.038022\n",
      "Train Epoch: 98 [4000/6658 (60%)]\tLoss: 0.202187\n",
      "Train Epoch: 98 [4100/6658 (62%)]\tLoss: 0.857719\n",
      "Train Epoch: 98 [4200/6658 (63%)]\tLoss: 0.000832\n",
      "Train Epoch: 98 [4300/6658 (65%)]\tLoss: 0.208130\n",
      "Train Epoch: 98 [4400/6658 (66%)]\tLoss: 0.000809\n",
      "Train Epoch: 98 [4500/6658 (68%)]\tLoss: 0.241202\n",
      "Train Epoch: 98 [4600/6658 (69%)]\tLoss: 0.627356\n",
      "Train Epoch: 98 [4700/6658 (71%)]\tLoss: 0.445657\n",
      "Train Epoch: 98 [4800/6658 (72%)]\tLoss: 0.225454\n",
      "Train Epoch: 98 [4900/6658 (74%)]\tLoss: 0.242812\n",
      "Train Epoch: 98 [5000/6658 (75%)]\tLoss: 0.359616\n",
      "Train Epoch: 98 [5100/6658 (77%)]\tLoss: 0.036346\n",
      "Train Epoch: 98 [5200/6658 (78%)]\tLoss: 0.033067\n",
      "Train Epoch: 98 [5300/6658 (80%)]\tLoss: 0.179101\n",
      "Train Epoch: 98 [5400/6658 (81%)]\tLoss: 0.804886\n",
      "Train Epoch: 98 [5500/6658 (83%)]\tLoss: 0.090721\n",
      "Train Epoch: 98 [5600/6658 (84%)]\tLoss: 0.104773\n",
      "Train Epoch: 98 [5700/6658 (86%)]\tLoss: 0.369519\n",
      "Train Epoch: 98 [5800/6658 (87%)]\tLoss: 0.033489\n",
      "Train Epoch: 98 [5900/6658 (89%)]\tLoss: 3.225699\n",
      "Train Epoch: 98 [6000/6658 (90%)]\tLoss: 0.140273\n",
      "Train Epoch: 98 [6100/6658 (92%)]\tLoss: 0.419405\n",
      "Train Epoch: 98 [6200/6658 (93%)]\tLoss: 0.539334\n",
      "Train Epoch: 98 [6300/6658 (95%)]\tLoss: 0.005201\n",
      "Train Epoch: 98 [6400/6658 (96%)]\tLoss: 0.731896\n",
      "Train Epoch: 98 [6500/6658 (98%)]\tLoss: 0.019920\n",
      "Train Epoch: 98 [6600/6658 (99%)]\tLoss: 1.107376\n",
      "train loss average =  0.7203893355418991\n",
      "\n",
      "Test set: Average loss: 0.6983\n",
      "\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2144, 6.0640, 5.8836, 5.8885, 6.1403, 6.0466], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 99 [0/6658 (0%)]\tLoss: 1.334770\n",
      "Train Epoch: 99 [100/6658 (2%)]\tLoss: 0.179115\n",
      "Train Epoch: 99 [200/6658 (3%)]\tLoss: 0.000000\n",
      "Train Epoch: 99 [300/6658 (5%)]\tLoss: 0.625894\n",
      "Train Epoch: 99 [400/6658 (6%)]\tLoss: 0.007344\n",
      "Train Epoch: 99 [500/6658 (8%)]\tLoss: 0.030287\n",
      "Train Epoch: 99 [600/6658 (9%)]\tLoss: 0.386749\n",
      "Train Epoch: 99 [700/6658 (11%)]\tLoss: 0.015628\n",
      "Train Epoch: 99 [800/6658 (12%)]\tLoss: 0.587161\n",
      "Train Epoch: 99 [900/6658 (14%)]\tLoss: 1.003653\n",
      "Train Epoch: 99 [1000/6658 (15%)]\tLoss: 0.014418\n",
      "Train Epoch: 99 [1100/6658 (17%)]\tLoss: 0.157228\n",
      "Train Epoch: 99 [1200/6658 (18%)]\tLoss: 0.559165\n",
      "Train Epoch: 99 [1300/6658 (20%)]\tLoss: 0.012873\n",
      "Train Epoch: 99 [1400/6658 (21%)]\tLoss: 0.730429\n",
      "Train Epoch: 99 [1500/6658 (23%)]\tLoss: 0.924760\n",
      "Train Epoch: 99 [1600/6658 (24%)]\tLoss: 0.119051\n",
      "Train Epoch: 99 [1700/6658 (26%)]\tLoss: 2.623305\n",
      "Train Epoch: 99 [1800/6658 (27%)]\tLoss: 0.420006\n",
      "Train Epoch: 99 [1900/6658 (29%)]\tLoss: 0.901314\n",
      "Train Epoch: 99 [2000/6658 (30%)]\tLoss: 1.440284\n",
      "Train Epoch: 99 [2100/6658 (32%)]\tLoss: 0.927367\n",
      "Train Epoch: 99 [2200/6658 (33%)]\tLoss: 0.794805\n",
      "Train Epoch: 99 [2300/6658 (35%)]\tLoss: 0.228760\n",
      "Train Epoch: 99 [2400/6658 (36%)]\tLoss: 0.555807\n",
      "Train Epoch: 99 [2500/6658 (38%)]\tLoss: 0.230141\n",
      "Train Epoch: 99 [2600/6658 (39%)]\tLoss: 0.398694\n",
      "Train Epoch: 99 [2700/6658 (41%)]\tLoss: 0.029753\n",
      "Train Epoch: 99 [2800/6658 (42%)]\tLoss: 0.355904\n",
      "Train Epoch: 99 [2900/6658 (44%)]\tLoss: 0.120267\n",
      "Train Epoch: 99 [3000/6658 (45%)]\tLoss: 0.065334\n",
      "Train Epoch: 99 [3100/6658 (47%)]\tLoss: 0.654057\n",
      "Train Epoch: 99 [3200/6658 (48%)]\tLoss: 0.252597\n",
      "Train Epoch: 99 [3300/6658 (50%)]\tLoss: 0.017728\n",
      "Train Epoch: 99 [3400/6658 (51%)]\tLoss: 3.309336\n",
      "Train Epoch: 99 [3500/6658 (53%)]\tLoss: 0.028951\n",
      "Train Epoch: 99 [3600/6658 (54%)]\tLoss: 0.339284\n",
      "Train Epoch: 99 [3700/6658 (56%)]\tLoss: 0.409722\n",
      "Train Epoch: 99 [3800/6658 (57%)]\tLoss: 0.008502\n",
      "Train Epoch: 99 [3900/6658 (59%)]\tLoss: 0.498639\n",
      "Train Epoch: 99 [4000/6658 (60%)]\tLoss: 0.452127\n",
      "Train Epoch: 99 [4100/6658 (62%)]\tLoss: 0.503353\n",
      "Train Epoch: 99 [4200/6658 (63%)]\tLoss: 0.007003\n",
      "Train Epoch: 99 [4300/6658 (65%)]\tLoss: 0.291134\n",
      "Train Epoch: 99 [4400/6658 (66%)]\tLoss: 1.055704\n",
      "Train Epoch: 99 [4500/6658 (68%)]\tLoss: 0.080381\n",
      "Train Epoch: 99 [4600/6658 (69%)]\tLoss: 0.689627\n",
      "Train Epoch: 99 [4700/6658 (71%)]\tLoss: 0.086274\n",
      "Train Epoch: 99 [4800/6658 (72%)]\tLoss: 0.482911\n",
      "Train Epoch: 99 [4900/6658 (74%)]\tLoss: 0.161389\n",
      "Train Epoch: 99 [5000/6658 (75%)]\tLoss: 0.351454\n",
      "Train Epoch: 99 [5100/6658 (77%)]\tLoss: 0.042003\n",
      "Train Epoch: 99 [5200/6658 (78%)]\tLoss: 0.127220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99 [5300/6658 (80%)]\tLoss: 1.320997\n",
      "Train Epoch: 99 [5400/6658 (81%)]\tLoss: 1.133586\n",
      "Train Epoch: 99 [5500/6658 (83%)]\tLoss: 0.228772\n",
      "Train Epoch: 99 [5600/6658 (84%)]\tLoss: 0.669597\n",
      "Train Epoch: 99 [5700/6658 (86%)]\tLoss: 0.224359\n",
      "Train Epoch: 99 [5800/6658 (87%)]\tLoss: 0.533094\n",
      "Train Epoch: 99 [5900/6658 (89%)]\tLoss: 0.271509\n",
      "Train Epoch: 99 [6000/6658 (90%)]\tLoss: 0.733150\n",
      "Train Epoch: 99 [6100/6658 (92%)]\tLoss: 0.322278\n",
      "Train Epoch: 99 [6200/6658 (93%)]\tLoss: 0.085940\n",
      "Train Epoch: 99 [6300/6658 (95%)]\tLoss: 0.071273\n",
      "Train Epoch: 99 [6400/6658 (96%)]\tLoss: 0.244565\n",
      "Train Epoch: 99 [6500/6658 (98%)]\tLoss: 0.150640\n",
      "Train Epoch: 99 [6600/6658 (99%)]\tLoss: 0.092072\n",
      "train loss average =  0.7234706777413307\n",
      "\n",
      "Test set: Average loss: 0.7038\n",
      "\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2169, 6.0649, 5.8827, 5.8890, 6.1420, 6.0464], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 100 [0/6658 (0%)]\tLoss: 0.450026\n",
      "Train Epoch: 100 [100/6658 (2%)]\tLoss: 0.600894\n",
      "Train Epoch: 100 [200/6658 (3%)]\tLoss: 0.690224\n",
      "Train Epoch: 100 [300/6658 (5%)]\tLoss: 0.241474\n",
      "Train Epoch: 100 [400/6658 (6%)]\tLoss: 4.403666\n",
      "Train Epoch: 100 [500/6658 (8%)]\tLoss: 1.791150\n",
      "Train Epoch: 100 [600/6658 (9%)]\tLoss: 1.144599\n",
      "Train Epoch: 100 [700/6658 (11%)]\tLoss: 0.050277\n",
      "Train Epoch: 100 [800/6658 (12%)]\tLoss: 5.641931\n",
      "Train Epoch: 100 [900/6658 (14%)]\tLoss: 2.093830\n",
      "Train Epoch: 100 [1000/6658 (15%)]\tLoss: 0.107816\n",
      "Train Epoch: 100 [1100/6658 (17%)]\tLoss: 0.425187\n",
      "Train Epoch: 100 [1200/6658 (18%)]\tLoss: 0.210684\n",
      "Train Epoch: 100 [1300/6658 (20%)]\tLoss: 5.205918\n",
      "Train Epoch: 100 [1400/6658 (21%)]\tLoss: 0.218427\n",
      "Train Epoch: 100 [1500/6658 (23%)]\tLoss: 0.116723\n",
      "Train Epoch: 100 [1600/6658 (24%)]\tLoss: 1.413489\n",
      "Train Epoch: 100 [1700/6658 (26%)]\tLoss: 0.017125\n",
      "Train Epoch: 100 [1800/6658 (27%)]\tLoss: 10.763021\n",
      "Train Epoch: 100 [1900/6658 (29%)]\tLoss: 0.106476\n",
      "Train Epoch: 100 [2000/6658 (30%)]\tLoss: 0.173002\n",
      "Train Epoch: 100 [2100/6658 (32%)]\tLoss: 1.584383\n",
      "Train Epoch: 100 [2200/6658 (33%)]\tLoss: 0.015077\n",
      "Train Epoch: 100 [2300/6658 (35%)]\tLoss: 0.280380\n",
      "Train Epoch: 100 [2400/6658 (36%)]\tLoss: 0.332234\n",
      "Train Epoch: 100 [2500/6658 (38%)]\tLoss: 1.329492\n",
      "Train Epoch: 100 [2600/6658 (39%)]\tLoss: 0.486764\n",
      "Train Epoch: 100 [2700/6658 (41%)]\tLoss: 0.195953\n",
      "Train Epoch: 100 [2800/6658 (42%)]\tLoss: 0.926798\n",
      "Train Epoch: 100 [2900/6658 (44%)]\tLoss: 0.884242\n",
      "Train Epoch: 100 [3000/6658 (45%)]\tLoss: 0.093941\n",
      "Train Epoch: 100 [3100/6658 (47%)]\tLoss: 0.512890\n",
      "Train Epoch: 100 [3200/6658 (48%)]\tLoss: 0.507163\n",
      "Train Epoch: 100 [3300/6658 (50%)]\tLoss: 0.043684\n",
      "Train Epoch: 100 [3400/6658 (51%)]\tLoss: 0.057947\n",
      "Train Epoch: 100 [3500/6658 (53%)]\tLoss: 0.244348\n",
      "Train Epoch: 100 [3600/6658 (54%)]\tLoss: 0.000084\n",
      "Train Epoch: 100 [3700/6658 (56%)]\tLoss: 0.017690\n",
      "Train Epoch: 100 [3800/6658 (57%)]\tLoss: 0.019529\n",
      "Train Epoch: 100 [3900/6658 (59%)]\tLoss: 0.451808\n",
      "Train Epoch: 100 [4000/6658 (60%)]\tLoss: 1.735329\n",
      "Train Epoch: 100 [4100/6658 (62%)]\tLoss: 0.163989\n",
      "Train Epoch: 100 [4200/6658 (63%)]\tLoss: 0.005980\n",
      "Train Epoch: 100 [4300/6658 (65%)]\tLoss: 0.063037\n",
      "Train Epoch: 100 [4400/6658 (66%)]\tLoss: 0.510879\n",
      "Train Epoch: 100 [4500/6658 (68%)]\tLoss: 0.040698\n",
      "Train Epoch: 100 [4600/6658 (69%)]\tLoss: 1.070754\n",
      "Train Epoch: 100 [4700/6658 (71%)]\tLoss: 0.004459\n",
      "Train Epoch: 100 [4800/6658 (72%)]\tLoss: 0.515941\n",
      "Train Epoch: 100 [4900/6658 (74%)]\tLoss: 3.302212\n",
      "Train Epoch: 100 [5000/6658 (75%)]\tLoss: 0.242693\n",
      "Train Epoch: 100 [5100/6658 (77%)]\tLoss: 0.387993\n",
      "Train Epoch: 100 [5200/6658 (78%)]\tLoss: 0.050979\n",
      "Train Epoch: 100 [5300/6658 (80%)]\tLoss: 0.020094\n",
      "Train Epoch: 100 [5400/6658 (81%)]\tLoss: 0.000063\n",
      "Train Epoch: 100 [5500/6658 (83%)]\tLoss: 0.090512\n",
      "Train Epoch: 100 [5600/6658 (84%)]\tLoss: 0.230602\n",
      "Train Epoch: 100 [5700/6658 (86%)]\tLoss: 0.073008\n",
      "Train Epoch: 100 [5800/6658 (87%)]\tLoss: 1.185265\n",
      "Train Epoch: 100 [5900/6658 (89%)]\tLoss: 0.460268\n",
      "Train Epoch: 100 [6000/6658 (90%)]\tLoss: 0.118494\n",
      "Train Epoch: 100 [6100/6658 (92%)]\tLoss: 0.131434\n",
      "Train Epoch: 100 [6200/6658 (93%)]\tLoss: 0.004414\n",
      "Train Epoch: 100 [6300/6658 (95%)]\tLoss: 2.509179\n",
      "Train Epoch: 100 [6400/6658 (96%)]\tLoss: 0.299001\n",
      "Train Epoch: 100 [6500/6658 (98%)]\tLoss: 0.737797\n",
      "Train Epoch: 100 [6600/6658 (99%)]\tLoss: 0.433615\n",
      "train loss average =  0.7227181976968452\n",
      "\n",
      "Test set: Average loss: 0.7157\n",
      "\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2188, 6.0654, 5.8821, 5.8871, 6.1431, 6.0465], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 101 [0/6658 (0%)]\tLoss: 0.050854\n",
      "Train Epoch: 101 [100/6658 (2%)]\tLoss: 0.162691\n",
      "Train Epoch: 101 [200/6658 (3%)]\tLoss: 1.181119\n",
      "Train Epoch: 101 [300/6658 (5%)]\tLoss: 0.049679\n",
      "Train Epoch: 101 [400/6658 (6%)]\tLoss: 0.065620\n",
      "Train Epoch: 101 [500/6658 (8%)]\tLoss: 0.102671\n",
      "Train Epoch: 101 [600/6658 (9%)]\tLoss: 0.040689\n",
      "Train Epoch: 101 [700/6658 (11%)]\tLoss: 0.790398\n",
      "Train Epoch: 101 [800/6658 (12%)]\tLoss: 0.009739\n",
      "Train Epoch: 101 [900/6658 (14%)]\tLoss: 0.146632\n",
      "Train Epoch: 101 [1000/6658 (15%)]\tLoss: 0.073023\n",
      "Train Epoch: 101 [1100/6658 (17%)]\tLoss: 0.044979\n",
      "Train Epoch: 101 [1200/6658 (18%)]\tLoss: 0.024308\n",
      "Train Epoch: 101 [1300/6658 (20%)]\tLoss: 0.000096\n",
      "Train Epoch: 101 [1400/6658 (21%)]\tLoss: 0.001079\n",
      "Train Epoch: 101 [1500/6658 (23%)]\tLoss: 0.089385\n",
      "Train Epoch: 101 [1600/6658 (24%)]\tLoss: 0.469924\n",
      "Train Epoch: 101 [1700/6658 (26%)]\tLoss: 0.354047\n",
      "Train Epoch: 101 [1800/6658 (27%)]\tLoss: 0.118625\n",
      "Train Epoch: 101 [1900/6658 (29%)]\tLoss: 0.291546\n",
      "Train Epoch: 101 [2000/6658 (30%)]\tLoss: 0.670942\n",
      "Train Epoch: 101 [2100/6658 (32%)]\tLoss: 0.187301\n",
      "Train Epoch: 101 [2200/6658 (33%)]\tLoss: 0.036515\n",
      "Train Epoch: 101 [2300/6658 (35%)]\tLoss: 0.817569\n",
      "Train Epoch: 101 [2400/6658 (36%)]\tLoss: 0.001323\n",
      "Train Epoch: 101 [2500/6658 (38%)]\tLoss: 0.232045\n",
      "Train Epoch: 101 [2600/6658 (39%)]\tLoss: 0.698824\n",
      "Train Epoch: 101 [2700/6658 (41%)]\tLoss: 0.271425\n",
      "Train Epoch: 101 [2800/6658 (42%)]\tLoss: 0.019225\n",
      "Train Epoch: 101 [2900/6658 (44%)]\tLoss: 0.106754\n",
      "Train Epoch: 101 [3000/6658 (45%)]\tLoss: 0.050551\n",
      "Train Epoch: 101 [3100/6658 (47%)]\tLoss: 0.000313\n",
      "Train Epoch: 101 [3200/6658 (48%)]\tLoss: 0.115674\n",
      "Train Epoch: 101 [3300/6658 (50%)]\tLoss: 0.222839\n",
      "Train Epoch: 101 [3400/6658 (51%)]\tLoss: 0.640394\n",
      "Train Epoch: 101 [3500/6658 (53%)]\tLoss: 0.050269\n",
      "Train Epoch: 101 [3600/6658 (54%)]\tLoss: 0.194516\n",
      "Train Epoch: 101 [3700/6658 (56%)]\tLoss: 0.395047\n",
      "Train Epoch: 101 [3800/6658 (57%)]\tLoss: 0.596286\n",
      "Train Epoch: 101 [3900/6658 (59%)]\tLoss: 0.954355\n",
      "Train Epoch: 101 [4000/6658 (60%)]\tLoss: 0.367872\n",
      "Train Epoch: 101 [4100/6658 (62%)]\tLoss: 7.725697\n",
      "Train Epoch: 101 [4200/6658 (63%)]\tLoss: 0.578161\n",
      "Train Epoch: 101 [4300/6658 (65%)]\tLoss: 2.425291\n",
      "Train Epoch: 101 [4400/6658 (66%)]\tLoss: 0.694640\n",
      "Train Epoch: 101 [4500/6658 (68%)]\tLoss: 0.113306\n",
      "Train Epoch: 101 [4600/6658 (69%)]\tLoss: 1.039917\n",
      "Train Epoch: 101 [4700/6658 (71%)]\tLoss: 1.125169\n",
      "Train Epoch: 101 [4800/6658 (72%)]\tLoss: 1.607434\n",
      "Train Epoch: 101 [4900/6658 (74%)]\tLoss: 0.013540\n",
      "Train Epoch: 101 [5000/6658 (75%)]\tLoss: 0.027563\n",
      "Train Epoch: 101 [5100/6658 (77%)]\tLoss: 0.892137\n",
      "Train Epoch: 101 [5200/6658 (78%)]\tLoss: 0.007284\n",
      "Train Epoch: 101 [5300/6658 (80%)]\tLoss: 0.027904\n",
      "Train Epoch: 101 [5400/6658 (81%)]\tLoss: 0.014711\n",
      "Train Epoch: 101 [5500/6658 (83%)]\tLoss: 4.103950\n",
      "Train Epoch: 101 [5600/6658 (84%)]\tLoss: 0.255274\n",
      "Train Epoch: 101 [5700/6658 (86%)]\tLoss: 0.042202\n",
      "Train Epoch: 101 [5800/6658 (87%)]\tLoss: 0.110965\n",
      "Train Epoch: 101 [5900/6658 (89%)]\tLoss: 0.100826\n",
      "Train Epoch: 101 [6000/6658 (90%)]\tLoss: 0.006220\n",
      "Train Epoch: 101 [6100/6658 (92%)]\tLoss: 0.091480\n",
      "Train Epoch: 101 [6200/6658 (93%)]\tLoss: 1.762498\n",
      "Train Epoch: 101 [6300/6658 (95%)]\tLoss: 0.273386\n",
      "Train Epoch: 101 [6400/6658 (96%)]\tLoss: 0.554240\n",
      "Train Epoch: 101 [6500/6658 (98%)]\tLoss: 1.735737\n",
      "Train Epoch: 101 [6600/6658 (99%)]\tLoss: 4.956371\n",
      "train loss average =  0.7199213474573875\n",
      "\n",
      "Test set: Average loss: 0.7005\n",
      "\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2205, 6.0656, 5.8814, 5.8861, 6.1452, 6.0470], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 102 [0/6658 (0%)]\tLoss: 0.198395\n",
      "Train Epoch: 102 [100/6658 (2%)]\tLoss: 0.605259\n",
      "Train Epoch: 102 [200/6658 (3%)]\tLoss: 0.016677\n",
      "Train Epoch: 102 [300/6658 (5%)]\tLoss: 0.152777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 102 [400/6658 (6%)]\tLoss: 0.159762\n",
      "Train Epoch: 102 [500/6658 (8%)]\tLoss: 0.554792\n",
      "Train Epoch: 102 [600/6658 (9%)]\tLoss: 0.064679\n",
      "Train Epoch: 102 [700/6658 (11%)]\tLoss: 0.071167\n",
      "Train Epoch: 102 [800/6658 (12%)]\tLoss: 3.912619\n",
      "Train Epoch: 102 [900/6658 (14%)]\tLoss: 0.307297\n",
      "Train Epoch: 102 [1000/6658 (15%)]\tLoss: 0.740925\n",
      "Train Epoch: 102 [1100/6658 (17%)]\tLoss: 0.952116\n",
      "Train Epoch: 102 [1200/6658 (18%)]\tLoss: 3.479445\n",
      "Train Epoch: 102 [1300/6658 (20%)]\tLoss: 0.207082\n",
      "Train Epoch: 102 [1400/6658 (21%)]\tLoss: 0.928903\n",
      "Train Epoch: 102 [1500/6658 (23%)]\tLoss: 0.287181\n",
      "Train Epoch: 102 [1600/6658 (24%)]\tLoss: 0.323099\n",
      "Train Epoch: 102 [1700/6658 (26%)]\tLoss: 0.281307\n",
      "Train Epoch: 102 [1800/6658 (27%)]\tLoss: 0.081225\n",
      "Train Epoch: 102 [1900/6658 (29%)]\tLoss: 0.633843\n",
      "Train Epoch: 102 [2000/6658 (30%)]\tLoss: 0.263125\n",
      "Train Epoch: 102 [2100/6658 (32%)]\tLoss: 0.503168\n",
      "Train Epoch: 102 [2200/6658 (33%)]\tLoss: 2.498177\n",
      "Train Epoch: 102 [2300/6658 (35%)]\tLoss: 0.231538\n",
      "Train Epoch: 102 [2400/6658 (36%)]\tLoss: 0.912768\n",
      "Train Epoch: 102 [2500/6658 (38%)]\tLoss: 0.284074\n",
      "Train Epoch: 102 [2600/6658 (39%)]\tLoss: 1.168852\n",
      "Train Epoch: 102 [2700/6658 (41%)]\tLoss: 0.000148\n",
      "Train Epoch: 102 [2800/6658 (42%)]\tLoss: 0.654817\n",
      "Train Epoch: 102 [2900/6658 (44%)]\tLoss: 0.002057\n",
      "Train Epoch: 102 [3000/6658 (45%)]\tLoss: 0.344720\n",
      "Train Epoch: 102 [3100/6658 (47%)]\tLoss: 1.615005\n",
      "Train Epoch: 102 [3200/6658 (48%)]\tLoss: 0.204575\n",
      "Train Epoch: 102 [3300/6658 (50%)]\tLoss: 0.011240\n",
      "Train Epoch: 102 [3400/6658 (51%)]\tLoss: 0.046586\n",
      "Train Epoch: 102 [3500/6658 (53%)]\tLoss: 0.777613\n",
      "Train Epoch: 102 [3600/6658 (54%)]\tLoss: 0.026962\n",
      "Train Epoch: 102 [3700/6658 (56%)]\tLoss: 0.017780\n",
      "Train Epoch: 102 [3800/6658 (57%)]\tLoss: 0.300466\n",
      "Train Epoch: 102 [3900/6658 (59%)]\tLoss: 0.127558\n",
      "Train Epoch: 102 [4000/6658 (60%)]\tLoss: 0.000728\n",
      "Train Epoch: 102 [4100/6658 (62%)]\tLoss: 0.009657\n",
      "Train Epoch: 102 [4200/6658 (63%)]\tLoss: 3.502507\n",
      "Train Epoch: 102 [4300/6658 (65%)]\tLoss: 0.205504\n",
      "Train Epoch: 102 [4400/6658 (66%)]\tLoss: 0.229662\n",
      "Train Epoch: 102 [4500/6658 (68%)]\tLoss: 1.152955\n",
      "Train Epoch: 102 [4600/6658 (69%)]\tLoss: 1.020167\n",
      "Train Epoch: 102 [4700/6658 (71%)]\tLoss: 3.055102\n",
      "Train Epoch: 102 [4800/6658 (72%)]\tLoss: 0.414592\n",
      "Train Epoch: 102 [4900/6658 (74%)]\tLoss: 0.363718\n",
      "Train Epoch: 102 [5000/6658 (75%)]\tLoss: 0.267125\n",
      "Train Epoch: 102 [5100/6658 (77%)]\tLoss: 1.224737\n",
      "Train Epoch: 102 [5200/6658 (78%)]\tLoss: 0.121812\n",
      "Train Epoch: 102 [5300/6658 (80%)]\tLoss: 0.255030\n",
      "Train Epoch: 102 [5400/6658 (81%)]\tLoss: 0.090347\n",
      "Train Epoch: 102 [5500/6658 (83%)]\tLoss: 0.051253\n",
      "Train Epoch: 102 [5600/6658 (84%)]\tLoss: 0.047515\n",
      "Train Epoch: 102 [5700/6658 (86%)]\tLoss: 0.166749\n",
      "Train Epoch: 102 [5800/6658 (87%)]\tLoss: 0.017452\n",
      "Train Epoch: 102 [5900/6658 (89%)]\tLoss: 0.028756\n",
      "Train Epoch: 102 [6000/6658 (90%)]\tLoss: 1.360300\n",
      "Train Epoch: 102 [6100/6658 (92%)]\tLoss: 0.109791\n",
      "Train Epoch: 102 [6200/6658 (93%)]\tLoss: 0.390636\n",
      "Train Epoch: 102 [6300/6658 (95%)]\tLoss: 1.133840\n",
      "Train Epoch: 102 [6400/6658 (96%)]\tLoss: 0.607323\n",
      "Train Epoch: 102 [6500/6658 (98%)]\tLoss: 0.012312\n",
      "Train Epoch: 102 [6600/6658 (99%)]\tLoss: 0.653950\n",
      "train loss average =  0.7219623064490946\n",
      "\n",
      "Test set: Average loss: 0.7025\n",
      "\n",
      "EarlyStopping counter: 9 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2227, 6.0665, 5.8806, 5.8855, 6.1479, 6.0474], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 103 [0/6658 (0%)]\tLoss: 0.000029\n",
      "Train Epoch: 103 [100/6658 (2%)]\tLoss: 7.403654\n",
      "Train Epoch: 103 [200/6658 (3%)]\tLoss: 0.482178\n",
      "Train Epoch: 103 [300/6658 (5%)]\tLoss: 0.050323\n",
      "Train Epoch: 103 [400/6658 (6%)]\tLoss: 0.249884\n",
      "Train Epoch: 103 [500/6658 (8%)]\tLoss: 0.458152\n",
      "Train Epoch: 103 [600/6658 (9%)]\tLoss: 0.403246\n",
      "Train Epoch: 103 [700/6658 (11%)]\tLoss: 0.087816\n",
      "Train Epoch: 103 [800/6658 (12%)]\tLoss: 0.006773\n",
      "Train Epoch: 103 [900/6658 (14%)]\tLoss: 0.000757\n",
      "Train Epoch: 103 [1000/6658 (15%)]\tLoss: 0.060701\n",
      "Train Epoch: 103 [1100/6658 (17%)]\tLoss: 0.738906\n",
      "Train Epoch: 103 [1200/6658 (18%)]\tLoss: 0.402250\n",
      "Train Epoch: 103 [1300/6658 (20%)]\tLoss: 0.404358\n",
      "Train Epoch: 103 [1400/6658 (21%)]\tLoss: 0.004412\n",
      "Train Epoch: 103 [1500/6658 (23%)]\tLoss: 0.076234\n",
      "Train Epoch: 103 [1600/6658 (24%)]\tLoss: 0.000229\n",
      "Train Epoch: 103 [1700/6658 (26%)]\tLoss: 0.123367\n",
      "Train Epoch: 103 [1800/6658 (27%)]\tLoss: 0.029398\n",
      "Train Epoch: 103 [1900/6658 (29%)]\tLoss: 0.001248\n",
      "Train Epoch: 103 [2000/6658 (30%)]\tLoss: 0.000385\n",
      "Train Epoch: 103 [2100/6658 (32%)]\tLoss: 0.008031\n",
      "Train Epoch: 103 [2200/6658 (33%)]\tLoss: 0.039859\n",
      "Train Epoch: 103 [2300/6658 (35%)]\tLoss: 0.082479\n",
      "Train Epoch: 103 [2400/6658 (36%)]\tLoss: 1.401813\n",
      "Train Epoch: 103 [2500/6658 (38%)]\tLoss: 0.030203\n",
      "Train Epoch: 103 [2600/6658 (39%)]\tLoss: 0.409473\n",
      "Train Epoch: 103 [2700/6658 (41%)]\tLoss: 22.059309\n",
      "Train Epoch: 103 [2800/6658 (42%)]\tLoss: 1.248743\n",
      "Train Epoch: 103 [2900/6658 (44%)]\tLoss: 0.346915\n",
      "Train Epoch: 103 [3000/6658 (45%)]\tLoss: 0.036133\n",
      "Train Epoch: 103 [3100/6658 (47%)]\tLoss: 0.669054\n",
      "Train Epoch: 103 [3200/6658 (48%)]\tLoss: 0.000170\n",
      "Train Epoch: 103 [3300/6658 (50%)]\tLoss: 0.306703\n",
      "Train Epoch: 103 [3400/6658 (51%)]\tLoss: 0.067471\n",
      "Train Epoch: 103 [3500/6658 (53%)]\tLoss: 0.001524\n",
      "Train Epoch: 103 [3600/6658 (54%)]\tLoss: 0.323828\n",
      "Train Epoch: 103 [3700/6658 (56%)]\tLoss: 0.883980\n",
      "Train Epoch: 103 [3800/6658 (57%)]\tLoss: 0.448688\n",
      "Train Epoch: 103 [3900/6658 (59%)]\tLoss: 0.123113\n",
      "Train Epoch: 103 [4000/6658 (60%)]\tLoss: 0.182564\n",
      "Train Epoch: 103 [4100/6658 (62%)]\tLoss: 0.371745\n",
      "Train Epoch: 103 [4200/6658 (63%)]\tLoss: 0.006801\n",
      "Train Epoch: 103 [4300/6658 (65%)]\tLoss: 0.071953\n",
      "Train Epoch: 103 [4400/6658 (66%)]\tLoss: 0.950307\n",
      "Train Epoch: 103 [4500/6658 (68%)]\tLoss: 0.958229\n",
      "Train Epoch: 103 [4600/6658 (69%)]\tLoss: 0.187033\n",
      "Train Epoch: 103 [4700/6658 (71%)]\tLoss: 1.510316\n",
      "Train Epoch: 103 [4800/6658 (72%)]\tLoss: 0.091370\n",
      "Train Epoch: 103 [4900/6658 (74%)]\tLoss: 1.595436\n",
      "Train Epoch: 103 [5000/6658 (75%)]\tLoss: 0.004229\n",
      "Train Epoch: 103 [5100/6658 (77%)]\tLoss: 0.113615\n",
      "Train Epoch: 103 [5200/6658 (78%)]\tLoss: 0.125872\n",
      "Train Epoch: 103 [5300/6658 (80%)]\tLoss: 0.002784\n",
      "Train Epoch: 103 [5400/6658 (81%)]\tLoss: 0.199078\n",
      "Train Epoch: 103 [5500/6658 (83%)]\tLoss: 1.168235\n",
      "Train Epoch: 103 [5600/6658 (84%)]\tLoss: 0.228753\n",
      "Train Epoch: 103 [5700/6658 (86%)]\tLoss: 0.252319\n",
      "Train Epoch: 103 [5800/6658 (87%)]\tLoss: 0.556334\n",
      "Train Epoch: 103 [5900/6658 (89%)]\tLoss: 0.023423\n",
      "Train Epoch: 103 [6000/6658 (90%)]\tLoss: 0.236089\n",
      "Train Epoch: 103 [6100/6658 (92%)]\tLoss: 0.064179\n",
      "Train Epoch: 103 [6200/6658 (93%)]\tLoss: 0.006417\n",
      "Train Epoch: 103 [6300/6658 (95%)]\tLoss: 0.021556\n",
      "Train Epoch: 103 [6400/6658 (96%)]\tLoss: 0.051234\n",
      "Train Epoch: 103 [6500/6658 (98%)]\tLoss: 0.148703\n",
      "Train Epoch: 103 [6600/6658 (99%)]\tLoss: 0.003757\n",
      "train loss average =  0.7174535168697118\n",
      "\n",
      "Test set: Average loss: 0.6972\n",
      "\n",
      "EarlyStopping counter: 10 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2243, 6.0671, 5.8806, 5.8845, 6.1493, 6.0473], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 104 [0/6658 (0%)]\tLoss: 0.161245\n",
      "Train Epoch: 104 [100/6658 (2%)]\tLoss: 1.010887\n",
      "Train Epoch: 104 [200/6658 (3%)]\tLoss: 0.421295\n",
      "Train Epoch: 104 [300/6658 (5%)]\tLoss: 0.005799\n",
      "Train Epoch: 104 [400/6658 (6%)]\tLoss: 0.218676\n",
      "Train Epoch: 104 [500/6658 (8%)]\tLoss: 1.541665\n",
      "Train Epoch: 104 [600/6658 (9%)]\tLoss: 0.019997\n",
      "Train Epoch: 104 [700/6658 (11%)]\tLoss: 0.001761\n",
      "Train Epoch: 104 [800/6658 (12%)]\tLoss: 0.680304\n",
      "Train Epoch: 104 [900/6658 (14%)]\tLoss: 0.015306\n",
      "Train Epoch: 104 [1000/6658 (15%)]\tLoss: 2.858547\n",
      "Train Epoch: 104 [1100/6658 (17%)]\tLoss: 0.435935\n",
      "Train Epoch: 104 [1200/6658 (18%)]\tLoss: 7.588434\n",
      "Train Epoch: 104 [1300/6658 (20%)]\tLoss: 0.082570\n",
      "Train Epoch: 104 [1400/6658 (21%)]\tLoss: 0.632648\n",
      "Train Epoch: 104 [1500/6658 (23%)]\tLoss: 0.973653\n",
      "Train Epoch: 104 [1600/6658 (24%)]\tLoss: 0.049658\n",
      "Train Epoch: 104 [1700/6658 (26%)]\tLoss: 0.108025\n",
      "Train Epoch: 104 [1800/6658 (27%)]\tLoss: 7.994779\n",
      "Train Epoch: 104 [1900/6658 (29%)]\tLoss: 0.585325\n",
      "Train Epoch: 104 [2000/6658 (30%)]\tLoss: 0.037959\n",
      "Train Epoch: 104 [2100/6658 (32%)]\tLoss: 0.046192\n",
      "Train Epoch: 104 [2200/6658 (33%)]\tLoss: 0.865720\n",
      "Train Epoch: 104 [2300/6658 (35%)]\tLoss: 0.109504\n",
      "Train Epoch: 104 [2400/6658 (36%)]\tLoss: 1.732893\n",
      "Train Epoch: 104 [2500/6658 (38%)]\tLoss: 0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 104 [2600/6658 (39%)]\tLoss: 0.881384\n",
      "Train Epoch: 104 [2700/6658 (41%)]\tLoss: 0.266321\n",
      "Train Epoch: 104 [2800/6658 (42%)]\tLoss: 0.054513\n",
      "Train Epoch: 104 [2900/6658 (44%)]\tLoss: 0.983759\n",
      "Train Epoch: 104 [3000/6658 (45%)]\tLoss: 0.384623\n",
      "Train Epoch: 104 [3100/6658 (47%)]\tLoss: 0.797826\n",
      "Train Epoch: 104 [3200/6658 (48%)]\tLoss: 0.533566\n",
      "Train Epoch: 104 [3300/6658 (50%)]\tLoss: 0.154827\n",
      "Train Epoch: 104 [3400/6658 (51%)]\tLoss: 0.436880\n",
      "Train Epoch: 104 [3500/6658 (53%)]\tLoss: 1.551145\n",
      "Train Epoch: 104 [3600/6658 (54%)]\tLoss: 1.743700\n",
      "Train Epoch: 104 [3700/6658 (56%)]\tLoss: 1.770879\n",
      "Train Epoch: 104 [3800/6658 (57%)]\tLoss: 0.129419\n",
      "Train Epoch: 104 [3900/6658 (59%)]\tLoss: 0.194262\n",
      "Train Epoch: 104 [4000/6658 (60%)]\tLoss: 1.116783\n",
      "Train Epoch: 104 [4100/6658 (62%)]\tLoss: 0.001070\n",
      "Train Epoch: 104 [4200/6658 (63%)]\tLoss: 0.001720\n",
      "Train Epoch: 104 [4300/6658 (65%)]\tLoss: 0.112725\n",
      "Train Epoch: 104 [4400/6658 (66%)]\tLoss: 0.187308\n",
      "Train Epoch: 104 [4500/6658 (68%)]\tLoss: 0.000355\n",
      "Train Epoch: 104 [4600/6658 (69%)]\tLoss: 0.830340\n",
      "Train Epoch: 104 [4700/6658 (71%)]\tLoss: 0.000138\n",
      "Train Epoch: 104 [4800/6658 (72%)]\tLoss: 0.061954\n",
      "Train Epoch: 104 [4900/6658 (74%)]\tLoss: 0.322783\n",
      "Train Epoch: 104 [5000/6658 (75%)]\tLoss: 1.307689\n",
      "Train Epoch: 104 [5100/6658 (77%)]\tLoss: 0.532587\n",
      "Train Epoch: 104 [5200/6658 (78%)]\tLoss: 0.092719\n",
      "Train Epoch: 104 [5300/6658 (80%)]\tLoss: 0.212060\n",
      "Train Epoch: 104 [5400/6658 (81%)]\tLoss: 0.050340\n",
      "Train Epoch: 104 [5500/6658 (83%)]\tLoss: 0.350452\n",
      "Train Epoch: 104 [5600/6658 (84%)]\tLoss: 0.198684\n",
      "Train Epoch: 104 [5700/6658 (86%)]\tLoss: 1.420344\n",
      "Train Epoch: 104 [5800/6658 (87%)]\tLoss: 0.000002\n",
      "Train Epoch: 104 [5900/6658 (89%)]\tLoss: 0.000032\n",
      "Train Epoch: 104 [6000/6658 (90%)]\tLoss: 0.837719\n",
      "Train Epoch: 104 [6100/6658 (92%)]\tLoss: 0.935040\n",
      "Train Epoch: 104 [6200/6658 (93%)]\tLoss: 0.502634\n",
      "Train Epoch: 104 [6300/6658 (95%)]\tLoss: 1.153212\n",
      "Train Epoch: 104 [6400/6658 (96%)]\tLoss: 0.001931\n",
      "Train Epoch: 104 [6500/6658 (98%)]\tLoss: 0.180107\n",
      "Train Epoch: 104 [6600/6658 (99%)]\tLoss: 1.057003\n",
      "train loss average =  0.7182515921278684\n",
      "\n",
      "Test set: Average loss: 0.7344\n",
      "\n",
      "EarlyStopping counter: 11 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2277, 6.0682, 5.8801, 5.8834, 6.1502, 6.0469], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 105 [0/6658 (0%)]\tLoss: 0.103467\n",
      "Train Epoch: 105 [100/6658 (2%)]\tLoss: 0.026035\n",
      "Train Epoch: 105 [200/6658 (3%)]\tLoss: 0.716721\n",
      "Train Epoch: 105 [300/6658 (5%)]\tLoss: 0.458213\n",
      "Train Epoch: 105 [400/6658 (6%)]\tLoss: 0.048565\n",
      "Train Epoch: 105 [500/6658 (8%)]\tLoss: 0.057044\n",
      "Train Epoch: 105 [600/6658 (9%)]\tLoss: 0.181248\n",
      "Train Epoch: 105 [700/6658 (11%)]\tLoss: 1.061679\n",
      "Train Epoch: 105 [800/6658 (12%)]\tLoss: 0.107462\n",
      "Train Epoch: 105 [900/6658 (14%)]\tLoss: 0.061314\n",
      "Train Epoch: 105 [1000/6658 (15%)]\tLoss: 0.480761\n",
      "Train Epoch: 105 [1100/6658 (17%)]\tLoss: 0.112259\n",
      "Train Epoch: 105 [1200/6658 (18%)]\tLoss: 0.276803\n",
      "Train Epoch: 105 [1300/6658 (20%)]\tLoss: 2.600223\n",
      "Train Epoch: 105 [1400/6658 (21%)]\tLoss: 2.118477\n",
      "Train Epoch: 105 [1500/6658 (23%)]\tLoss: 1.141334\n",
      "Train Epoch: 105 [1600/6658 (24%)]\tLoss: 0.391242\n",
      "Train Epoch: 105 [1700/6658 (26%)]\tLoss: 0.011803\n",
      "Train Epoch: 105 [1800/6658 (27%)]\tLoss: 0.723324\n",
      "Train Epoch: 105 [1900/6658 (29%)]\tLoss: 0.625805\n",
      "Train Epoch: 105 [2000/6658 (30%)]\tLoss: 0.001984\n",
      "Train Epoch: 105 [2100/6658 (32%)]\tLoss: 0.323203\n",
      "Train Epoch: 105 [2200/6658 (33%)]\tLoss: 0.761264\n",
      "Train Epoch: 105 [2300/6658 (35%)]\tLoss: 0.000693\n",
      "Train Epoch: 105 [2400/6658 (36%)]\tLoss: 1.017772\n",
      "Train Epoch: 105 [2500/6658 (38%)]\tLoss: 0.220804\n",
      "Train Epoch: 105 [2600/6658 (39%)]\tLoss: 0.001006\n",
      "Train Epoch: 105 [2700/6658 (41%)]\tLoss: 0.332939\n",
      "Train Epoch: 105 [2800/6658 (42%)]\tLoss: 0.301826\n",
      "Train Epoch: 105 [2900/6658 (44%)]\tLoss: 0.325040\n",
      "Train Epoch: 105 [3000/6658 (45%)]\tLoss: 0.091359\n",
      "Train Epoch: 105 [3100/6658 (47%)]\tLoss: 0.045623\n",
      "Train Epoch: 105 [3200/6658 (48%)]\tLoss: 0.880120\n",
      "Train Epoch: 105 [3300/6658 (50%)]\tLoss: 0.234229\n",
      "Train Epoch: 105 [3400/6658 (51%)]\tLoss: 0.201647\n",
      "Train Epoch: 105 [3500/6658 (53%)]\tLoss: 0.109836\n",
      "Train Epoch: 105 [3600/6658 (54%)]\tLoss: 0.136667\n",
      "Train Epoch: 105 [3700/6658 (56%)]\tLoss: 0.000105\n",
      "Train Epoch: 105 [3800/6658 (57%)]\tLoss: 1.586877\n",
      "Train Epoch: 105 [3900/6658 (59%)]\tLoss: 0.016699\n",
      "Train Epoch: 105 [4000/6658 (60%)]\tLoss: 0.960532\n",
      "Train Epoch: 105 [4100/6658 (62%)]\tLoss: 0.142665\n",
      "Train Epoch: 105 [4200/6658 (63%)]\tLoss: 0.116642\n",
      "Train Epoch: 105 [4300/6658 (65%)]\tLoss: 0.000002\n",
      "Train Epoch: 105 [4400/6658 (66%)]\tLoss: 0.507314\n",
      "Train Epoch: 105 [4500/6658 (68%)]\tLoss: 0.507955\n",
      "Train Epoch: 105 [4600/6658 (69%)]\tLoss: 0.112623\n",
      "Train Epoch: 105 [4700/6658 (71%)]\tLoss: 0.072734\n",
      "Train Epoch: 105 [4800/6658 (72%)]\tLoss: 2.065242\n",
      "Train Epoch: 105 [4900/6658 (74%)]\tLoss: 0.000001\n",
      "Train Epoch: 105 [5000/6658 (75%)]\tLoss: 0.242182\n",
      "Train Epoch: 105 [5100/6658 (77%)]\tLoss: 0.122394\n",
      "Train Epoch: 105 [5200/6658 (78%)]\tLoss: 0.001765\n",
      "Train Epoch: 105 [5300/6658 (80%)]\tLoss: 0.079304\n",
      "Train Epoch: 105 [5400/6658 (81%)]\tLoss: 0.128832\n",
      "Train Epoch: 105 [5500/6658 (83%)]\tLoss: 0.011922\n",
      "Train Epoch: 105 [5600/6658 (84%)]\tLoss: 0.040199\n",
      "Train Epoch: 105 [5700/6658 (86%)]\tLoss: 0.039986\n",
      "Train Epoch: 105 [5800/6658 (87%)]\tLoss: 0.022678\n",
      "Train Epoch: 105 [5900/6658 (89%)]\tLoss: 0.017791\n",
      "Train Epoch: 105 [6000/6658 (90%)]\tLoss: 0.255743\n",
      "Train Epoch: 105 [6100/6658 (92%)]\tLoss: 0.348551\n",
      "Train Epoch: 105 [6200/6658 (93%)]\tLoss: 3.075698\n",
      "Train Epoch: 105 [6300/6658 (95%)]\tLoss: 0.483167\n",
      "Train Epoch: 105 [6400/6658 (96%)]\tLoss: 0.988671\n",
      "Train Epoch: 105 [6500/6658 (98%)]\tLoss: 1.291013\n",
      "Train Epoch: 105 [6600/6658 (99%)]\tLoss: 0.792819\n",
      "train loss average =  0.7236088915314924\n",
      "\n",
      "Test set: Average loss: 0.7045\n",
      "\n",
      "EarlyStopping counter: 12 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2298, 6.0699, 5.8795, 5.8823, 6.1517, 6.0469], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 106 [0/6658 (0%)]\tLoss: 0.265321\n",
      "Train Epoch: 106 [100/6658 (2%)]\tLoss: 0.085722\n",
      "Train Epoch: 106 [200/6658 (3%)]\tLoss: 0.410211\n",
      "Train Epoch: 106 [300/6658 (5%)]\tLoss: 0.349903\n",
      "Train Epoch: 106 [400/6658 (6%)]\tLoss: 0.107221\n",
      "Train Epoch: 106 [500/6658 (8%)]\tLoss: 1.070689\n",
      "Train Epoch: 106 [600/6658 (9%)]\tLoss: 0.073357\n",
      "Train Epoch: 106 [700/6658 (11%)]\tLoss: 0.021677\n",
      "Train Epoch: 106 [800/6658 (12%)]\tLoss: 0.000683\n",
      "Train Epoch: 106 [900/6658 (14%)]\tLoss: 2.490158\n",
      "Train Epoch: 106 [1000/6658 (15%)]\tLoss: 1.417008\n",
      "Train Epoch: 106 [1100/6658 (17%)]\tLoss: 0.051120\n",
      "Train Epoch: 106 [1200/6658 (18%)]\tLoss: 0.000324\n",
      "Train Epoch: 106 [1300/6658 (20%)]\tLoss: 0.922830\n",
      "Train Epoch: 106 [1400/6658 (21%)]\tLoss: 1.745569\n",
      "Train Epoch: 106 [1500/6658 (23%)]\tLoss: 0.260898\n",
      "Train Epoch: 106 [1600/6658 (24%)]\tLoss: 1.982177\n",
      "Train Epoch: 106 [1700/6658 (26%)]\tLoss: 0.103605\n",
      "Train Epoch: 106 [1800/6658 (27%)]\tLoss: 0.019263\n",
      "Train Epoch: 106 [1900/6658 (29%)]\tLoss: 0.027367\n",
      "Train Epoch: 106 [2000/6658 (30%)]\tLoss: 3.197460\n",
      "Train Epoch: 106 [2100/6658 (32%)]\tLoss: 0.034419\n",
      "Train Epoch: 106 [2200/6658 (33%)]\tLoss: 0.012508\n",
      "Train Epoch: 106 [2300/6658 (35%)]\tLoss: 1.034675\n",
      "Train Epoch: 106 [2400/6658 (36%)]\tLoss: 2.740412\n",
      "Train Epoch: 106 [2500/6658 (38%)]\tLoss: 4.097365\n",
      "Train Epoch: 106 [2600/6658 (39%)]\tLoss: 0.041665\n",
      "Train Epoch: 106 [2700/6658 (41%)]\tLoss: 0.795316\n",
      "Train Epoch: 106 [2800/6658 (42%)]\tLoss: 0.023819\n",
      "Train Epoch: 106 [2900/6658 (44%)]\tLoss: 1.819726\n",
      "Train Epoch: 106 [3000/6658 (45%)]\tLoss: 0.428921\n",
      "Train Epoch: 106 [3100/6658 (47%)]\tLoss: 0.939011\n",
      "Train Epoch: 106 [3200/6658 (48%)]\tLoss: 0.040729\n",
      "Train Epoch: 106 [3300/6658 (50%)]\tLoss: 0.345593\n",
      "Train Epoch: 106 [3400/6658 (51%)]\tLoss: 3.192830\n",
      "Train Epoch: 106 [3500/6658 (53%)]\tLoss: 0.043415\n",
      "Train Epoch: 106 [3600/6658 (54%)]\tLoss: 0.030942\n",
      "Train Epoch: 106 [3700/6658 (56%)]\tLoss: 10.980409\n",
      "Train Epoch: 106 [3800/6658 (57%)]\tLoss: 0.013619\n",
      "Train Epoch: 106 [3900/6658 (59%)]\tLoss: 0.058934\n",
      "Train Epoch: 106 [4000/6658 (60%)]\tLoss: 0.208679\n",
      "Train Epoch: 106 [4100/6658 (62%)]\tLoss: 0.353012\n",
      "Train Epoch: 106 [4200/6658 (63%)]\tLoss: 0.000669\n",
      "Train Epoch: 106 [4300/6658 (65%)]\tLoss: 0.796920\n",
      "Train Epoch: 106 [4400/6658 (66%)]\tLoss: 0.799624\n",
      "Train Epoch: 106 [4500/6658 (68%)]\tLoss: 0.164901\n",
      "Train Epoch: 106 [4600/6658 (69%)]\tLoss: 0.383648\n",
      "Train Epoch: 106 [4700/6658 (71%)]\tLoss: 0.025484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 106 [4800/6658 (72%)]\tLoss: 0.804968\n",
      "Train Epoch: 106 [4900/6658 (74%)]\tLoss: 0.019361\n",
      "Train Epoch: 106 [5000/6658 (75%)]\tLoss: 0.186102\n",
      "Train Epoch: 106 [5100/6658 (77%)]\tLoss: 0.338804\n",
      "Train Epoch: 106 [5200/6658 (78%)]\tLoss: 0.292831\n",
      "Train Epoch: 106 [5300/6658 (80%)]\tLoss: 0.388414\n",
      "Train Epoch: 106 [5400/6658 (81%)]\tLoss: 0.005457\n",
      "Train Epoch: 106 [5500/6658 (83%)]\tLoss: 0.321967\n",
      "Train Epoch: 106 [5600/6658 (84%)]\tLoss: 4.023903\n",
      "Train Epoch: 106 [5700/6658 (86%)]\tLoss: 0.016624\n",
      "Train Epoch: 106 [5800/6658 (87%)]\tLoss: 0.242198\n",
      "Train Epoch: 106 [5900/6658 (89%)]\tLoss: 0.542792\n",
      "Train Epoch: 106 [6000/6658 (90%)]\tLoss: 0.109994\n",
      "Train Epoch: 106 [6100/6658 (92%)]\tLoss: 0.007794\n",
      "Train Epoch: 106 [6200/6658 (93%)]\tLoss: 0.043319\n",
      "Train Epoch: 106 [6300/6658 (95%)]\tLoss: 1.062548\n",
      "Train Epoch: 106 [6400/6658 (96%)]\tLoss: 0.062460\n",
      "Train Epoch: 106 [6500/6658 (98%)]\tLoss: 0.161095\n",
      "Train Epoch: 106 [6600/6658 (99%)]\tLoss: 0.220249\n",
      "train loss average =  0.7210113786388779\n",
      "\n",
      "Test set: Average loss: 0.7053\n",
      "\n",
      "EarlyStopping counter: 13 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2318, 6.0700, 5.8785, 5.8806, 6.1532, 6.0479], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 107 [0/6658 (0%)]\tLoss: 0.206802\n",
      "Train Epoch: 107 [100/6658 (2%)]\tLoss: 0.126834\n",
      "Train Epoch: 107 [200/6658 (3%)]\tLoss: 0.019902\n",
      "Train Epoch: 107 [300/6658 (5%)]\tLoss: 0.004658\n",
      "Train Epoch: 107 [400/6658 (6%)]\tLoss: 0.092609\n",
      "Train Epoch: 107 [500/6658 (8%)]\tLoss: 0.343995\n",
      "Train Epoch: 107 [600/6658 (9%)]\tLoss: 0.403485\n",
      "Train Epoch: 107 [700/6658 (11%)]\tLoss: 0.024087\n",
      "Train Epoch: 107 [800/6658 (12%)]\tLoss: 0.048604\n",
      "Train Epoch: 107 [900/6658 (14%)]\tLoss: 0.404250\n",
      "Train Epoch: 107 [1000/6658 (15%)]\tLoss: 6.839478\n",
      "Train Epoch: 107 [1100/6658 (17%)]\tLoss: 0.014135\n",
      "Train Epoch: 107 [1200/6658 (18%)]\tLoss: 0.352347\n",
      "Train Epoch: 107 [1300/6658 (20%)]\tLoss: 0.646031\n",
      "Train Epoch: 107 [1400/6658 (21%)]\tLoss: 0.504756\n",
      "Train Epoch: 107 [1500/6658 (23%)]\tLoss: 0.054572\n",
      "Train Epoch: 107 [1600/6658 (24%)]\tLoss: 1.140830\n",
      "Train Epoch: 107 [1700/6658 (26%)]\tLoss: 0.044820\n",
      "Train Epoch: 107 [1800/6658 (27%)]\tLoss: 0.781831\n",
      "Train Epoch: 107 [1900/6658 (29%)]\tLoss: 0.708081\n",
      "Train Epoch: 107 [2000/6658 (30%)]\tLoss: 0.274079\n",
      "Train Epoch: 107 [2100/6658 (32%)]\tLoss: 0.034328\n",
      "Train Epoch: 107 [2200/6658 (33%)]\tLoss: 0.008396\n",
      "Train Epoch: 107 [2300/6658 (35%)]\tLoss: 0.127370\n",
      "Train Epoch: 107 [2400/6658 (36%)]\tLoss: 0.003467\n",
      "Train Epoch: 107 [2500/6658 (38%)]\tLoss: 0.054117\n",
      "Train Epoch: 107 [2600/6658 (39%)]\tLoss: 0.048076\n",
      "Train Epoch: 107 [2700/6658 (41%)]\tLoss: 0.564894\n",
      "Train Epoch: 107 [2800/6658 (42%)]\tLoss: 0.417906\n",
      "Train Epoch: 107 [2900/6658 (44%)]\tLoss: 0.338047\n",
      "Train Epoch: 107 [3000/6658 (45%)]\tLoss: 1.505629\n",
      "Train Epoch: 107 [3100/6658 (47%)]\tLoss: 0.496394\n",
      "Train Epoch: 107 [3200/6658 (48%)]\tLoss: 0.034829\n",
      "Train Epoch: 107 [3300/6658 (50%)]\tLoss: 0.478725\n",
      "Train Epoch: 107 [3400/6658 (51%)]\tLoss: 0.987573\n",
      "Train Epoch: 107 [3500/6658 (53%)]\tLoss: 0.055098\n",
      "Train Epoch: 107 [3600/6658 (54%)]\tLoss: 0.570521\n",
      "Train Epoch: 107 [3700/6658 (56%)]\tLoss: 1.691906\n",
      "Train Epoch: 107 [3800/6658 (57%)]\tLoss: 0.001201\n",
      "Train Epoch: 107 [3900/6658 (59%)]\tLoss: 0.294843\n",
      "Train Epoch: 107 [4000/6658 (60%)]\tLoss: 0.008619\n",
      "Train Epoch: 107 [4100/6658 (62%)]\tLoss: 0.406114\n",
      "Train Epoch: 107 [4200/6658 (63%)]\tLoss: 0.004951\n",
      "Train Epoch: 107 [4300/6658 (65%)]\tLoss: 0.048006\n",
      "Train Epoch: 107 [4400/6658 (66%)]\tLoss: 0.008142\n",
      "Train Epoch: 107 [4500/6658 (68%)]\tLoss: 0.002600\n",
      "Train Epoch: 107 [4600/6658 (69%)]\tLoss: 0.083921\n",
      "Train Epoch: 107 [4700/6658 (71%)]\tLoss: 0.006954\n",
      "Train Epoch: 107 [4800/6658 (72%)]\tLoss: 0.181100\n",
      "Train Epoch: 107 [4900/6658 (74%)]\tLoss: 0.517899\n",
      "Train Epoch: 107 [5000/6658 (75%)]\tLoss: 0.102257\n",
      "Train Epoch: 107 [5100/6658 (77%)]\tLoss: 0.593049\n",
      "Train Epoch: 107 [5200/6658 (78%)]\tLoss: 0.073209\n",
      "Train Epoch: 107 [5300/6658 (80%)]\tLoss: 4.735983\n",
      "Train Epoch: 107 [5400/6658 (81%)]\tLoss: 0.000109\n",
      "Train Epoch: 107 [5500/6658 (83%)]\tLoss: 0.022180\n",
      "Train Epoch: 107 [5600/6658 (84%)]\tLoss: 0.071754\n",
      "Train Epoch: 107 [5700/6658 (86%)]\tLoss: 0.004526\n",
      "Train Epoch: 107 [5800/6658 (87%)]\tLoss: 0.007775\n",
      "Train Epoch: 107 [5900/6658 (89%)]\tLoss: 0.023447\n",
      "Train Epoch: 107 [6000/6658 (90%)]\tLoss: 0.091948\n",
      "Train Epoch: 107 [6100/6658 (92%)]\tLoss: 0.412864\n",
      "Train Epoch: 107 [6200/6658 (93%)]\tLoss: 0.856551\n",
      "Train Epoch: 107 [6300/6658 (95%)]\tLoss: 3.866658\n",
      "Train Epoch: 107 [6400/6658 (96%)]\tLoss: 0.508023\n",
      "Train Epoch: 107 [6500/6658 (98%)]\tLoss: 0.040120\n",
      "Train Epoch: 107 [6600/6658 (99%)]\tLoss: 0.054990\n",
      "train loss average =  0.7159379654049808\n",
      "\n",
      "Test set: Average loss: 0.7062\n",
      "\n",
      "EarlyStopping counter: 14 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2334, 6.0704, 5.8771, 5.8801, 6.1547, 6.0478], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 108 [0/6658 (0%)]\tLoss: 0.296208\n",
      "Train Epoch: 108 [100/6658 (2%)]\tLoss: 1.392331\n",
      "Train Epoch: 108 [200/6658 (3%)]\tLoss: 1.122277\n",
      "Train Epoch: 108 [300/6658 (5%)]\tLoss: 0.520584\n",
      "Train Epoch: 108 [400/6658 (6%)]\tLoss: 0.258054\n",
      "Train Epoch: 108 [500/6658 (8%)]\tLoss: 0.787524\n",
      "Train Epoch: 108 [600/6658 (9%)]\tLoss: 6.498041\n",
      "Train Epoch: 108 [700/6658 (11%)]\tLoss: 0.000899\n",
      "Train Epoch: 108 [800/6658 (12%)]\tLoss: 0.843330\n",
      "Train Epoch: 108 [900/6658 (14%)]\tLoss: 1.921593\n",
      "Train Epoch: 108 [1000/6658 (15%)]\tLoss: 0.159379\n",
      "Train Epoch: 108 [1100/6658 (17%)]\tLoss: 0.147185\n",
      "Train Epoch: 108 [1200/6658 (18%)]\tLoss: 0.453929\n",
      "Train Epoch: 108 [1300/6658 (20%)]\tLoss: 0.572845\n",
      "Train Epoch: 108 [1400/6658 (21%)]\tLoss: 1.533740\n",
      "Train Epoch: 108 [1500/6658 (23%)]\tLoss: 1.415819\n",
      "Train Epoch: 108 [1600/6658 (24%)]\tLoss: 0.169294\n",
      "Train Epoch: 108 [1700/6658 (26%)]\tLoss: 1.211946\n",
      "Train Epoch: 108 [1800/6658 (27%)]\tLoss: 0.613860\n",
      "Train Epoch: 108 [1900/6658 (29%)]\tLoss: 2.595407\n",
      "Train Epoch: 108 [2000/6658 (30%)]\tLoss: 0.216430\n",
      "Train Epoch: 108 [2100/6658 (32%)]\tLoss: 0.662889\n",
      "Train Epoch: 108 [2200/6658 (33%)]\tLoss: 0.196274\n",
      "Train Epoch: 108 [2300/6658 (35%)]\tLoss: 0.083691\n",
      "Train Epoch: 108 [2400/6658 (36%)]\tLoss: 0.923613\n",
      "Train Epoch: 108 [2500/6658 (38%)]\tLoss: 0.320238\n",
      "Train Epoch: 108 [2600/6658 (39%)]\tLoss: 0.145489\n",
      "Train Epoch: 108 [2700/6658 (41%)]\tLoss: 0.001565\n",
      "Train Epoch: 108 [2800/6658 (42%)]\tLoss: 0.151135\n",
      "Train Epoch: 108 [2900/6658 (44%)]\tLoss: 0.012814\n",
      "Train Epoch: 108 [3000/6658 (45%)]\tLoss: 1.065812\n",
      "Train Epoch: 108 [3100/6658 (47%)]\tLoss: 0.013302\n",
      "Train Epoch: 108 [3200/6658 (48%)]\tLoss: 0.327979\n",
      "Train Epoch: 108 [3300/6658 (50%)]\tLoss: 0.525557\n",
      "Train Epoch: 108 [3400/6658 (51%)]\tLoss: 0.059644\n",
      "Train Epoch: 108 [3500/6658 (53%)]\tLoss: 0.521212\n",
      "Train Epoch: 108 [3600/6658 (54%)]\tLoss: 0.307830\n",
      "Train Epoch: 108 [3700/6658 (56%)]\tLoss: 0.011118\n",
      "Train Epoch: 108 [3800/6658 (57%)]\tLoss: 2.442195\n",
      "Train Epoch: 108 [3900/6658 (59%)]\tLoss: 0.060071\n",
      "Train Epoch: 108 [4000/6658 (60%)]\tLoss: 0.003102\n",
      "Train Epoch: 108 [4100/6658 (62%)]\tLoss: 2.004829\n",
      "Train Epoch: 108 [4200/6658 (63%)]\tLoss: 0.330806\n",
      "Train Epoch: 108 [4300/6658 (65%)]\tLoss: 0.067212\n",
      "Train Epoch: 108 [4400/6658 (66%)]\tLoss: 0.001053\n",
      "Train Epoch: 108 [4500/6658 (68%)]\tLoss: 0.186983\n",
      "Train Epoch: 108 [4600/6658 (69%)]\tLoss: 0.009464\n",
      "Train Epoch: 108 [4700/6658 (71%)]\tLoss: 0.003974\n",
      "Train Epoch: 108 [4800/6658 (72%)]\tLoss: 0.440243\n",
      "Train Epoch: 108 [4900/6658 (74%)]\tLoss: 0.254940\n",
      "Train Epoch: 108 [5000/6658 (75%)]\tLoss: 0.043517\n",
      "Train Epoch: 108 [5100/6658 (77%)]\tLoss: 0.186603\n",
      "Train Epoch: 108 [5200/6658 (78%)]\tLoss: 0.188537\n",
      "Train Epoch: 108 [5300/6658 (80%)]\tLoss: 0.149522\n",
      "Train Epoch: 108 [5400/6658 (81%)]\tLoss: 0.234148\n",
      "Train Epoch: 108 [5500/6658 (83%)]\tLoss: 0.118450\n",
      "Train Epoch: 108 [5600/6658 (84%)]\tLoss: 0.285745\n",
      "Train Epoch: 108 [5700/6658 (86%)]\tLoss: 0.339531\n",
      "Train Epoch: 108 [5800/6658 (87%)]\tLoss: 0.433762\n",
      "Train Epoch: 108 [5900/6658 (89%)]\tLoss: 0.000624\n",
      "Train Epoch: 108 [6000/6658 (90%)]\tLoss: 0.625595\n",
      "Train Epoch: 108 [6100/6658 (92%)]\tLoss: 0.049904\n",
      "Train Epoch: 108 [6200/6658 (93%)]\tLoss: 2.718300\n",
      "Train Epoch: 108 [6300/6658 (95%)]\tLoss: 0.336133\n",
      "Train Epoch: 108 [6400/6658 (96%)]\tLoss: 4.438700\n",
      "Train Epoch: 108 [6500/6658 (98%)]\tLoss: 0.016234\n",
      "Train Epoch: 108 [6600/6658 (99%)]\tLoss: 3.833577\n",
      "train loss average =  0.7206373802864163\n",
      "\n",
      "Test set: Average loss: 0.7041\n",
      "\n",
      "EarlyStopping counter: 15 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2354, 6.0717, 5.8752, 5.8788, 6.1567, 6.0480], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 109 [0/6658 (0%)]\tLoss: 0.113981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 109 [100/6658 (2%)]\tLoss: 0.302616\n",
      "Train Epoch: 109 [200/6658 (3%)]\tLoss: 0.000458\n",
      "Train Epoch: 109 [300/6658 (5%)]\tLoss: 0.883649\n",
      "Train Epoch: 109 [400/6658 (6%)]\tLoss: 2.734483\n",
      "Train Epoch: 109 [500/6658 (8%)]\tLoss: 0.026812\n",
      "Train Epoch: 109 [600/6658 (9%)]\tLoss: 0.270826\n",
      "Train Epoch: 109 [700/6658 (11%)]\tLoss: 0.011236\n",
      "Train Epoch: 109 [800/6658 (12%)]\tLoss: 0.426564\n",
      "Train Epoch: 109 [900/6658 (14%)]\tLoss: 0.289309\n",
      "Train Epoch: 109 [1000/6658 (15%)]\tLoss: 0.675914\n",
      "Train Epoch: 109 [1100/6658 (17%)]\tLoss: 0.486349\n",
      "Train Epoch: 109 [1200/6658 (18%)]\tLoss: 1.100772\n",
      "Train Epoch: 109 [1300/6658 (20%)]\tLoss: 0.000713\n",
      "Train Epoch: 109 [1400/6658 (21%)]\tLoss: 0.301643\n",
      "Train Epoch: 109 [1500/6658 (23%)]\tLoss: 0.449021\n",
      "Train Epoch: 109 [1600/6658 (24%)]\tLoss: 1.076348\n",
      "Train Epoch: 109 [1700/6658 (26%)]\tLoss: 0.365793\n",
      "Train Epoch: 109 [1800/6658 (27%)]\tLoss: 0.028762\n",
      "Train Epoch: 109 [1900/6658 (29%)]\tLoss: 0.002756\n",
      "Train Epoch: 109 [2000/6658 (30%)]\tLoss: 1.113653\n",
      "Train Epoch: 109 [2100/6658 (32%)]\tLoss: 0.444731\n",
      "Train Epoch: 109 [2200/6658 (33%)]\tLoss: 0.001090\n",
      "Train Epoch: 109 [2300/6658 (35%)]\tLoss: 0.000570\n",
      "Train Epoch: 109 [2400/6658 (36%)]\tLoss: 0.214864\n",
      "Train Epoch: 109 [2500/6658 (38%)]\tLoss: 0.181697\n",
      "Train Epoch: 109 [2600/6658 (39%)]\tLoss: 1.030695\n",
      "Train Epoch: 109 [2700/6658 (41%)]\tLoss: 0.442970\n",
      "Train Epoch: 109 [2800/6658 (42%)]\tLoss: 0.023112\n",
      "Train Epoch: 109 [2900/6658 (44%)]\tLoss: 0.164222\n",
      "Train Epoch: 109 [3000/6658 (45%)]\tLoss: 0.035231\n",
      "Train Epoch: 109 [3100/6658 (47%)]\tLoss: 0.661532\n",
      "Train Epoch: 109 [3200/6658 (48%)]\tLoss: 0.409648\n",
      "Train Epoch: 109 [3300/6658 (50%)]\tLoss: 0.571744\n",
      "Train Epoch: 109 [3400/6658 (51%)]\tLoss: 0.893810\n",
      "Train Epoch: 109 [3500/6658 (53%)]\tLoss: 0.695099\n",
      "Train Epoch: 109 [3600/6658 (54%)]\tLoss: 0.141708\n",
      "Train Epoch: 109 [3700/6658 (56%)]\tLoss: 0.023001\n",
      "Train Epoch: 109 [3800/6658 (57%)]\tLoss: 0.241213\n",
      "Train Epoch: 109 [3900/6658 (59%)]\tLoss: 0.004551\n",
      "Train Epoch: 109 [4000/6658 (60%)]\tLoss: 0.308956\n",
      "Train Epoch: 109 [4100/6658 (62%)]\tLoss: 0.080221\n",
      "Train Epoch: 109 [4200/6658 (63%)]\tLoss: 1.575662\n",
      "Train Epoch: 109 [4300/6658 (65%)]\tLoss: 0.207766\n",
      "Train Epoch: 109 [4400/6658 (66%)]\tLoss: 0.244588\n",
      "Train Epoch: 109 [4500/6658 (68%)]\tLoss: 0.495193\n",
      "Train Epoch: 109 [4600/6658 (69%)]\tLoss: 8.101247\n",
      "Train Epoch: 109 [4700/6658 (71%)]\tLoss: 0.427992\n",
      "Train Epoch: 109 [4800/6658 (72%)]\tLoss: 1.046347\n",
      "Train Epoch: 109 [4900/6658 (74%)]\tLoss: 0.720185\n",
      "Train Epoch: 109 [5000/6658 (75%)]\tLoss: 0.426782\n",
      "Train Epoch: 109 [5100/6658 (77%)]\tLoss: 0.069663\n",
      "Train Epoch: 109 [5200/6658 (78%)]\tLoss: 2.724509\n",
      "Train Epoch: 109 [5300/6658 (80%)]\tLoss: 0.005610\n",
      "Train Epoch: 109 [5400/6658 (81%)]\tLoss: 0.211825\n",
      "Train Epoch: 109 [5500/6658 (83%)]\tLoss: 0.018180\n",
      "Train Epoch: 109 [5600/6658 (84%)]\tLoss: 0.356869\n",
      "Train Epoch: 109 [5700/6658 (86%)]\tLoss: 0.212645\n",
      "Train Epoch: 109 [5800/6658 (87%)]\tLoss: 0.248289\n",
      "Train Epoch: 109 [5900/6658 (89%)]\tLoss: 0.136566\n",
      "Train Epoch: 109 [6000/6658 (90%)]\tLoss: 0.712239\n",
      "Train Epoch: 109 [6100/6658 (92%)]\tLoss: 0.820939\n",
      "Train Epoch: 109 [6200/6658 (93%)]\tLoss: 0.468035\n",
      "Train Epoch: 109 [6300/6658 (95%)]\tLoss: 3.418628\n",
      "Train Epoch: 109 [6400/6658 (96%)]\tLoss: 1.891721\n",
      "Train Epoch: 109 [6500/6658 (98%)]\tLoss: 0.072141\n",
      "Train Epoch: 109 [6600/6658 (99%)]\tLoss: 0.034113\n",
      "train loss average =  0.7151677260352436\n",
      "\n",
      "Test set: Average loss: 0.7096\n",
      "\n",
      "EarlyStopping counter: 16 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2367, 6.0717, 5.8741, 5.8779, 6.1583, 6.0479], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 110 [0/6658 (0%)]\tLoss: 0.000549\n",
      "Train Epoch: 110 [100/6658 (2%)]\tLoss: 2.241847\n",
      "Train Epoch: 110 [200/6658 (3%)]\tLoss: 0.333553\n",
      "Train Epoch: 110 [300/6658 (5%)]\tLoss: 0.501773\n",
      "Train Epoch: 110 [400/6658 (6%)]\tLoss: 3.328780\n",
      "Train Epoch: 110 [500/6658 (8%)]\tLoss: 0.847100\n",
      "Train Epoch: 110 [600/6658 (9%)]\tLoss: 0.050923\n",
      "Train Epoch: 110 [700/6658 (11%)]\tLoss: 0.997075\n",
      "Train Epoch: 110 [800/6658 (12%)]\tLoss: 0.000039\n",
      "Train Epoch: 110 [900/6658 (14%)]\tLoss: 4.796235\n",
      "Train Epoch: 110 [1000/6658 (15%)]\tLoss: 0.395825\n",
      "Train Epoch: 110 [1100/6658 (17%)]\tLoss: 0.019611\n",
      "Train Epoch: 110 [1200/6658 (18%)]\tLoss: 0.000355\n",
      "Train Epoch: 110 [1300/6658 (20%)]\tLoss: 0.003847\n",
      "Train Epoch: 110 [1400/6658 (21%)]\tLoss: 0.992879\n",
      "Train Epoch: 110 [1500/6658 (23%)]\tLoss: 0.864437\n",
      "Train Epoch: 110 [1600/6658 (24%)]\tLoss: 0.642206\n",
      "Train Epoch: 110 [1700/6658 (26%)]\tLoss: 0.301955\n",
      "Train Epoch: 110 [1800/6658 (27%)]\tLoss: 0.025423\n",
      "Train Epoch: 110 [1900/6658 (29%)]\tLoss: 0.091761\n",
      "Train Epoch: 110 [2000/6658 (30%)]\tLoss: 0.212692\n",
      "Train Epoch: 110 [2100/6658 (32%)]\tLoss: 0.286635\n",
      "Train Epoch: 110 [2200/6658 (33%)]\tLoss: 1.488117\n",
      "Train Epoch: 110 [2300/6658 (35%)]\tLoss: 0.523648\n",
      "Train Epoch: 110 [2400/6658 (36%)]\tLoss: 0.522309\n",
      "Train Epoch: 110 [2500/6658 (38%)]\tLoss: 0.903658\n",
      "Train Epoch: 110 [2600/6658 (39%)]\tLoss: 0.781344\n",
      "Train Epoch: 110 [2700/6658 (41%)]\tLoss: 0.061389\n",
      "Train Epoch: 110 [2800/6658 (42%)]\tLoss: 0.329053\n",
      "Train Epoch: 110 [2900/6658 (44%)]\tLoss: 0.000050\n",
      "Train Epoch: 110 [3000/6658 (45%)]\tLoss: 0.000020\n",
      "Train Epoch: 110 [3100/6658 (47%)]\tLoss: 0.540584\n",
      "Train Epoch: 110 [3200/6658 (48%)]\tLoss: 0.041384\n",
      "Train Epoch: 110 [3300/6658 (50%)]\tLoss: 1.831959\n",
      "Train Epoch: 110 [3400/6658 (51%)]\tLoss: 0.413756\n",
      "Train Epoch: 110 [3500/6658 (53%)]\tLoss: 0.387539\n",
      "Train Epoch: 110 [3600/6658 (54%)]\tLoss: 2.336696\n",
      "Train Epoch: 110 [3700/6658 (56%)]\tLoss: 1.129885\n",
      "Train Epoch: 110 [3800/6658 (57%)]\tLoss: 0.731048\n",
      "Train Epoch: 110 [3900/6658 (59%)]\tLoss: 0.356817\n",
      "Train Epoch: 110 [4000/6658 (60%)]\tLoss: 0.000335\n",
      "Train Epoch: 110 [4100/6658 (62%)]\tLoss: 0.087748\n",
      "Train Epoch: 110 [4200/6658 (63%)]\tLoss: 0.004175\n",
      "Train Epoch: 110 [4300/6658 (65%)]\tLoss: 0.056917\n",
      "Train Epoch: 110 [4400/6658 (66%)]\tLoss: 0.000057\n",
      "Train Epoch: 110 [4500/6658 (68%)]\tLoss: 0.027339\n",
      "Train Epoch: 110 [4600/6658 (69%)]\tLoss: 0.108338\n",
      "Train Epoch: 110 [4700/6658 (71%)]\tLoss: 0.103777\n",
      "Train Epoch: 110 [4800/6658 (72%)]\tLoss: 0.327620\n",
      "Train Epoch: 110 [4900/6658 (74%)]\tLoss: 0.332852\n",
      "Train Epoch: 110 [5000/6658 (75%)]\tLoss: 1.333103\n",
      "Train Epoch: 110 [5100/6658 (77%)]\tLoss: 0.844500\n",
      "Train Epoch: 110 [5200/6658 (78%)]\tLoss: 0.119542\n",
      "Train Epoch: 110 [5300/6658 (80%)]\tLoss: 2.739291\n",
      "Train Epoch: 110 [5400/6658 (81%)]\tLoss: 0.072081\n",
      "Train Epoch: 110 [5500/6658 (83%)]\tLoss: 0.026327\n",
      "Train Epoch: 110 [5600/6658 (84%)]\tLoss: 0.430220\n",
      "Train Epoch: 110 [5700/6658 (86%)]\tLoss: 0.000066\n",
      "Train Epoch: 110 [5800/6658 (87%)]\tLoss: 0.279317\n",
      "Train Epoch: 110 [5900/6658 (89%)]\tLoss: 0.317877\n",
      "Train Epoch: 110 [6000/6658 (90%)]\tLoss: 1.077094\n",
      "Train Epoch: 110 [6100/6658 (92%)]\tLoss: 0.867507\n",
      "Train Epoch: 110 [6200/6658 (93%)]\tLoss: 0.007612\n",
      "Train Epoch: 110 [6300/6658 (95%)]\tLoss: 0.145932\n",
      "Train Epoch: 110 [6400/6658 (96%)]\tLoss: 1.961376\n",
      "Train Epoch: 110 [6500/6658 (98%)]\tLoss: 0.947279\n",
      "Train Epoch: 110 [6600/6658 (99%)]\tLoss: 0.002585\n",
      "train loss average =  0.7193403717260802\n",
      "\n",
      "Test set: Average loss: 0.7127\n",
      "\n",
      "EarlyStopping counter: 17 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2389, 6.0725, 5.8736, 5.8772, 6.1605, 6.0480], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 111 [0/6658 (0%)]\tLoss: 0.131791\n",
      "Train Epoch: 111 [100/6658 (2%)]\tLoss: 2.038248\n",
      "Train Epoch: 111 [200/6658 (3%)]\tLoss: 0.641696\n",
      "Train Epoch: 111 [300/6658 (5%)]\tLoss: 0.248242\n",
      "Train Epoch: 111 [400/6658 (6%)]\tLoss: 0.237514\n",
      "Train Epoch: 111 [500/6658 (8%)]\tLoss: 1.147352\n",
      "Train Epoch: 111 [600/6658 (9%)]\tLoss: 1.636490\n",
      "Train Epoch: 111 [700/6658 (11%)]\tLoss: 1.991631\n",
      "Train Epoch: 111 [800/6658 (12%)]\tLoss: 3.110390\n",
      "Train Epoch: 111 [900/6658 (14%)]\tLoss: 0.161735\n",
      "Train Epoch: 111 [1000/6658 (15%)]\tLoss: 0.349709\n",
      "Train Epoch: 111 [1100/6658 (17%)]\tLoss: 0.510284\n",
      "Train Epoch: 111 [1200/6658 (18%)]\tLoss: 0.209492\n",
      "Train Epoch: 111 [1300/6658 (20%)]\tLoss: 0.017143\n",
      "Train Epoch: 111 [1400/6658 (21%)]\tLoss: 0.759484\n",
      "Train Epoch: 111 [1500/6658 (23%)]\tLoss: 0.427251\n",
      "Train Epoch: 111 [1600/6658 (24%)]\tLoss: 0.425151\n",
      "Train Epoch: 111 [1700/6658 (26%)]\tLoss: 0.504048\n",
      "Train Epoch: 111 [1800/6658 (27%)]\tLoss: 0.126252\n",
      "Train Epoch: 111 [1900/6658 (29%)]\tLoss: 2.348991\n",
      "Train Epoch: 111 [2000/6658 (30%)]\tLoss: 0.242748\n",
      "Train Epoch: 111 [2100/6658 (32%)]\tLoss: 0.212571\n",
      "Train Epoch: 111 [2200/6658 (33%)]\tLoss: 0.001945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 111 [2300/6658 (35%)]\tLoss: 2.102622\n",
      "Train Epoch: 111 [2400/6658 (36%)]\tLoss: 0.029442\n",
      "Train Epoch: 111 [2500/6658 (38%)]\tLoss: 0.405393\n",
      "Train Epoch: 111 [2600/6658 (39%)]\tLoss: 2.518512\n",
      "Train Epoch: 111 [2700/6658 (41%)]\tLoss: 0.149080\n",
      "Train Epoch: 111 [2800/6658 (42%)]\tLoss: 1.059623\n",
      "Train Epoch: 111 [2900/6658 (44%)]\tLoss: 0.016862\n",
      "Train Epoch: 111 [3000/6658 (45%)]\tLoss: 0.120570\n",
      "Train Epoch: 111 [3100/6658 (47%)]\tLoss: 0.993263\n",
      "Train Epoch: 111 [3200/6658 (48%)]\tLoss: 0.392137\n",
      "Train Epoch: 111 [3300/6658 (50%)]\tLoss: 5.834650\n",
      "Train Epoch: 111 [3400/6658 (51%)]\tLoss: 1.016602\n",
      "Train Epoch: 111 [3500/6658 (53%)]\tLoss: 3.574862\n",
      "Train Epoch: 111 [3600/6658 (54%)]\tLoss: 2.343864\n",
      "Train Epoch: 111 [3700/6658 (56%)]\tLoss: 0.295521\n",
      "Train Epoch: 111 [3800/6658 (57%)]\tLoss: 1.813426\n",
      "Train Epoch: 111 [3900/6658 (59%)]\tLoss: 0.678732\n",
      "Train Epoch: 111 [4000/6658 (60%)]\tLoss: 0.296033\n",
      "Train Epoch: 111 [4100/6658 (62%)]\tLoss: 0.024685\n",
      "Train Epoch: 111 [4200/6658 (63%)]\tLoss: 0.108759\n",
      "Train Epoch: 111 [4300/6658 (65%)]\tLoss: 2.656336\n",
      "Train Epoch: 111 [4400/6658 (66%)]\tLoss: 0.148778\n",
      "Train Epoch: 111 [4500/6658 (68%)]\tLoss: 6.274302\n",
      "Train Epoch: 111 [4600/6658 (69%)]\tLoss: 0.005645\n",
      "Train Epoch: 111 [4700/6658 (71%)]\tLoss: 0.007112\n",
      "Train Epoch: 111 [4800/6658 (72%)]\tLoss: 0.358547\n",
      "Train Epoch: 111 [4900/6658 (74%)]\tLoss: 0.544701\n",
      "Train Epoch: 111 [5000/6658 (75%)]\tLoss: 0.520112\n",
      "Train Epoch: 111 [5100/6658 (77%)]\tLoss: 0.051450\n",
      "Train Epoch: 111 [5200/6658 (78%)]\tLoss: 0.311223\n",
      "Train Epoch: 111 [5300/6658 (80%)]\tLoss: 0.631811\n",
      "Train Epoch: 111 [5400/6658 (81%)]\tLoss: 0.103740\n",
      "Train Epoch: 111 [5500/6658 (83%)]\tLoss: 0.789268\n",
      "Train Epoch: 111 [5600/6658 (84%)]\tLoss: 0.000096\n",
      "Train Epoch: 111 [5700/6658 (86%)]\tLoss: 0.098704\n",
      "Train Epoch: 111 [5800/6658 (87%)]\tLoss: 1.841242\n",
      "Train Epoch: 111 [5900/6658 (89%)]\tLoss: 0.042105\n",
      "Train Epoch: 111 [6000/6658 (90%)]\tLoss: 0.481335\n",
      "Train Epoch: 111 [6100/6658 (92%)]\tLoss: 0.008121\n",
      "Train Epoch: 111 [6200/6658 (93%)]\tLoss: 0.001902\n",
      "Train Epoch: 111 [6300/6658 (95%)]\tLoss: 0.057788\n",
      "Train Epoch: 111 [6400/6658 (96%)]\tLoss: 0.012132\n",
      "Train Epoch: 111 [6500/6658 (98%)]\tLoss: 0.008234\n",
      "Train Epoch: 111 [6600/6658 (99%)]\tLoss: 0.242598\n",
      "train loss average =  0.719503385176049\n",
      "\n",
      "Test set: Average loss: 0.7015\n",
      "\n",
      "EarlyStopping counter: 18 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2407, 6.0734, 5.8732, 5.8769, 6.1622, 6.0484], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 112 [0/6658 (0%)]\tLoss: 0.018270\n",
      "Train Epoch: 112 [100/6658 (2%)]\tLoss: 0.173907\n",
      "Train Epoch: 112 [200/6658 (3%)]\tLoss: 1.922982\n",
      "Train Epoch: 112 [300/6658 (5%)]\tLoss: 1.711976\n",
      "Train Epoch: 112 [400/6658 (6%)]\tLoss: 1.155056\n",
      "Train Epoch: 112 [500/6658 (8%)]\tLoss: 0.157041\n",
      "Train Epoch: 112 [600/6658 (9%)]\tLoss: 0.108734\n",
      "Train Epoch: 112 [700/6658 (11%)]\tLoss: 0.050258\n",
      "Train Epoch: 112 [800/6658 (12%)]\tLoss: 0.034103\n",
      "Train Epoch: 112 [900/6658 (14%)]\tLoss: 0.025794\n",
      "Train Epoch: 112 [1000/6658 (15%)]\tLoss: 0.007720\n",
      "Train Epoch: 112 [1100/6658 (17%)]\tLoss: 0.019450\n",
      "Train Epoch: 112 [1200/6658 (18%)]\tLoss: 0.718164\n",
      "Train Epoch: 112 [1300/6658 (20%)]\tLoss: 3.072657\n",
      "Train Epoch: 112 [1400/6658 (21%)]\tLoss: 1.237111\n",
      "Train Epoch: 112 [1500/6658 (23%)]\tLoss: 1.297452\n",
      "Train Epoch: 112 [1600/6658 (24%)]\tLoss: 0.555697\n",
      "Train Epoch: 112 [1700/6658 (26%)]\tLoss: 0.118825\n",
      "Train Epoch: 112 [1800/6658 (27%)]\tLoss: 0.023860\n",
      "Train Epoch: 112 [1900/6658 (29%)]\tLoss: 0.314119\n",
      "Train Epoch: 112 [2000/6658 (30%)]\tLoss: 0.082675\n",
      "Train Epoch: 112 [2100/6658 (32%)]\tLoss: 0.106825\n",
      "Train Epoch: 112 [2200/6658 (33%)]\tLoss: 1.245287\n",
      "Train Epoch: 112 [2300/6658 (35%)]\tLoss: 0.852594\n",
      "Train Epoch: 112 [2400/6658 (36%)]\tLoss: 0.037757\n",
      "Train Epoch: 112 [2500/6658 (38%)]\tLoss: 0.189666\n",
      "Train Epoch: 112 [2600/6658 (39%)]\tLoss: 0.001625\n",
      "Train Epoch: 112 [2700/6658 (41%)]\tLoss: 0.038161\n",
      "Train Epoch: 112 [2800/6658 (42%)]\tLoss: 0.025963\n",
      "Train Epoch: 112 [2900/6658 (44%)]\tLoss: 2.680992\n",
      "Train Epoch: 112 [3000/6658 (45%)]\tLoss: 0.554060\n",
      "Train Epoch: 112 [3100/6658 (47%)]\tLoss: 2.343990\n",
      "Train Epoch: 112 [3200/6658 (48%)]\tLoss: 0.255375\n",
      "Train Epoch: 112 [3300/6658 (50%)]\tLoss: 0.860499\n",
      "Train Epoch: 112 [3400/6658 (51%)]\tLoss: 0.947770\n",
      "Train Epoch: 112 [3500/6658 (53%)]\tLoss: 0.239387\n",
      "Train Epoch: 112 [3600/6658 (54%)]\tLoss: 0.472447\n",
      "Train Epoch: 112 [3700/6658 (56%)]\tLoss: 0.439026\n",
      "Train Epoch: 112 [3800/6658 (57%)]\tLoss: 0.099764\n",
      "Train Epoch: 112 [3900/6658 (59%)]\tLoss: 0.002901\n",
      "Train Epoch: 112 [4000/6658 (60%)]\tLoss: 0.001538\n",
      "Train Epoch: 112 [4100/6658 (62%)]\tLoss: 0.030561\n",
      "Train Epoch: 112 [4200/6658 (63%)]\tLoss: 0.085485\n",
      "Train Epoch: 112 [4300/6658 (65%)]\tLoss: 0.292075\n",
      "Train Epoch: 112 [4400/6658 (66%)]\tLoss: 0.730417\n",
      "Train Epoch: 112 [4500/6658 (68%)]\tLoss: 0.603800\n",
      "Train Epoch: 112 [4600/6658 (69%)]\tLoss: 0.746353\n",
      "Train Epoch: 112 [4700/6658 (71%)]\tLoss: 0.003351\n",
      "Train Epoch: 112 [4800/6658 (72%)]\tLoss: 3.123083\n",
      "Train Epoch: 112 [4900/6658 (74%)]\tLoss: 0.040973\n",
      "Train Epoch: 112 [5000/6658 (75%)]\tLoss: 0.123195\n",
      "Train Epoch: 112 [5100/6658 (77%)]\tLoss: 1.847474\n",
      "Train Epoch: 112 [5200/6658 (78%)]\tLoss: 0.988108\n",
      "Train Epoch: 112 [5300/6658 (80%)]\tLoss: 0.350540\n",
      "Train Epoch: 112 [5400/6658 (81%)]\tLoss: 0.026996\n",
      "Train Epoch: 112 [5500/6658 (83%)]\tLoss: 0.275099\n",
      "Train Epoch: 112 [5600/6658 (84%)]\tLoss: 0.014838\n",
      "Train Epoch: 112 [5700/6658 (86%)]\tLoss: 0.438068\n",
      "Train Epoch: 112 [5800/6658 (87%)]\tLoss: 0.148510\n",
      "Train Epoch: 112 [5900/6658 (89%)]\tLoss: 0.159189\n",
      "Train Epoch: 112 [6000/6658 (90%)]\tLoss: 0.000005\n",
      "Train Epoch: 112 [6100/6658 (92%)]\tLoss: 0.030270\n",
      "Train Epoch: 112 [6200/6658 (93%)]\tLoss: 0.122817\n",
      "Train Epoch: 112 [6300/6658 (95%)]\tLoss: 0.002420\n",
      "Train Epoch: 112 [6400/6658 (96%)]\tLoss: 1.194340\n",
      "Train Epoch: 112 [6500/6658 (98%)]\tLoss: 0.312916\n",
      "Train Epoch: 112 [6600/6658 (99%)]\tLoss: 0.514370\n",
      "train loss average =  0.7243997250716947\n",
      "\n",
      "Test set: Average loss: 0.6944\n",
      "\n",
      "EarlyStopping counter: 19 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2426, 6.0747, 5.8717, 5.8769, 6.1633, 6.0481], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 113 [0/6658 (0%)]\tLoss: 0.703547\n",
      "Train Epoch: 113 [100/6658 (2%)]\tLoss: 0.034495\n",
      "Train Epoch: 113 [200/6658 (3%)]\tLoss: 4.452356\n",
      "Train Epoch: 113 [300/6658 (5%)]\tLoss: 0.318161\n",
      "Train Epoch: 113 [400/6658 (6%)]\tLoss: 0.000049\n",
      "Train Epoch: 113 [500/6658 (8%)]\tLoss: 0.091221\n",
      "Train Epoch: 113 [600/6658 (9%)]\tLoss: 0.022341\n",
      "Train Epoch: 113 [700/6658 (11%)]\tLoss: 1.468830\n",
      "Train Epoch: 113 [800/6658 (12%)]\tLoss: 0.007209\n",
      "Train Epoch: 113 [900/6658 (14%)]\tLoss: 1.092375\n",
      "Train Epoch: 113 [1000/6658 (15%)]\tLoss: 0.307213\n",
      "Train Epoch: 113 [1100/6658 (17%)]\tLoss: 0.342089\n",
      "Train Epoch: 113 [1200/6658 (18%)]\tLoss: 0.113512\n",
      "Train Epoch: 113 [1300/6658 (20%)]\tLoss: 0.624819\n",
      "Train Epoch: 113 [1400/6658 (21%)]\tLoss: 0.317942\n",
      "Train Epoch: 113 [1500/6658 (23%)]\tLoss: 0.007760\n",
      "Train Epoch: 113 [1600/6658 (24%)]\tLoss: 0.989343\n",
      "Train Epoch: 113 [1700/6658 (26%)]\tLoss: 0.205084\n",
      "Train Epoch: 113 [1800/6658 (27%)]\tLoss: 0.198106\n",
      "Train Epoch: 113 [1900/6658 (29%)]\tLoss: 0.433700\n",
      "Train Epoch: 113 [2000/6658 (30%)]\tLoss: 0.301304\n",
      "Train Epoch: 113 [2100/6658 (32%)]\tLoss: 0.015216\n",
      "Train Epoch: 113 [2200/6658 (33%)]\tLoss: 0.222074\n",
      "Train Epoch: 113 [2300/6658 (35%)]\tLoss: 0.285042\n",
      "Train Epoch: 113 [2400/6658 (36%)]\tLoss: 0.076413\n",
      "Train Epoch: 113 [2500/6658 (38%)]\tLoss: 0.694992\n",
      "Train Epoch: 113 [2600/6658 (39%)]\tLoss: 0.141462\n",
      "Train Epoch: 113 [2700/6658 (41%)]\tLoss: 0.527269\n",
      "Train Epoch: 113 [2800/6658 (42%)]\tLoss: 0.894514\n",
      "Train Epoch: 113 [2900/6658 (44%)]\tLoss: 0.218856\n",
      "Train Epoch: 113 [3000/6658 (45%)]\tLoss: 0.563408\n",
      "Train Epoch: 113 [3100/6658 (47%)]\tLoss: 0.003057\n",
      "Train Epoch: 113 [3200/6658 (48%)]\tLoss: 0.137544\n",
      "Train Epoch: 113 [3300/6658 (50%)]\tLoss: 0.803309\n",
      "Train Epoch: 113 [3400/6658 (51%)]\tLoss: 9.011571\n",
      "Train Epoch: 113 [3500/6658 (53%)]\tLoss: 0.215759\n",
      "Train Epoch: 113 [3600/6658 (54%)]\tLoss: 1.394387\n",
      "Train Epoch: 113 [3700/6658 (56%)]\tLoss: 0.100501\n",
      "Train Epoch: 113 [3800/6658 (57%)]\tLoss: 1.279319\n",
      "Train Epoch: 113 [3900/6658 (59%)]\tLoss: 0.038146\n",
      "Train Epoch: 113 [4000/6658 (60%)]\tLoss: 0.150914\n",
      "Train Epoch: 113 [4100/6658 (62%)]\tLoss: 7.120031\n",
      "Train Epoch: 113 [4200/6658 (63%)]\tLoss: 0.973266\n",
      "Train Epoch: 113 [4300/6658 (65%)]\tLoss: 0.935081\n",
      "Train Epoch: 113 [4400/6658 (66%)]\tLoss: 1.041701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 113 [4500/6658 (68%)]\tLoss: 0.415546\n",
      "Train Epoch: 113 [4600/6658 (69%)]\tLoss: 1.096434\n",
      "Train Epoch: 113 [4700/6658 (71%)]\tLoss: 0.114322\n",
      "Train Epoch: 113 [4800/6658 (72%)]\tLoss: 1.897553\n",
      "Train Epoch: 113 [4900/6658 (74%)]\tLoss: 0.334036\n",
      "Train Epoch: 113 [5000/6658 (75%)]\tLoss: 0.458444\n",
      "Train Epoch: 113 [5100/6658 (77%)]\tLoss: 7.522258\n",
      "Train Epoch: 113 [5200/6658 (78%)]\tLoss: 0.391170\n",
      "Train Epoch: 113 [5300/6658 (80%)]\tLoss: 0.004784\n",
      "Train Epoch: 113 [5400/6658 (81%)]\tLoss: 0.214953\n",
      "Train Epoch: 113 [5500/6658 (83%)]\tLoss: 0.369506\n",
      "Train Epoch: 113 [5600/6658 (84%)]\tLoss: 0.098284\n",
      "Train Epoch: 113 [5700/6658 (86%)]\tLoss: 0.005546\n",
      "Train Epoch: 113 [5800/6658 (87%)]\tLoss: 0.498900\n",
      "Train Epoch: 113 [5900/6658 (89%)]\tLoss: 2.299187\n",
      "Train Epoch: 113 [6000/6658 (90%)]\tLoss: 0.451200\n",
      "Train Epoch: 113 [6100/6658 (92%)]\tLoss: 2.543011\n",
      "Train Epoch: 113 [6200/6658 (93%)]\tLoss: 8.920391\n",
      "Train Epoch: 113 [6300/6658 (95%)]\tLoss: 0.272744\n",
      "Train Epoch: 113 [6400/6658 (96%)]\tLoss: 0.406351\n",
      "Train Epoch: 113 [6500/6658 (98%)]\tLoss: 0.351214\n",
      "Train Epoch: 113 [6600/6658 (99%)]\tLoss: 0.000076\n",
      "train loss average =  0.7229492056395599\n",
      "\n",
      "Test set: Average loss: 0.6974\n",
      "\n",
      "EarlyStopping counter: 20 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2440, 6.0757, 5.8715, 5.8763, 6.1649, 6.0484], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 114 [0/6658 (0%)]\tLoss: 0.607594\n",
      "Train Epoch: 114 [100/6658 (2%)]\tLoss: 0.517183\n",
      "Train Epoch: 114 [200/6658 (3%)]\tLoss: 2.915891\n",
      "Train Epoch: 114 [300/6658 (5%)]\tLoss: 1.163727\n",
      "Train Epoch: 114 [400/6658 (6%)]\tLoss: 5.175501\n",
      "Train Epoch: 114 [500/6658 (8%)]\tLoss: 1.905861\n",
      "Train Epoch: 114 [600/6658 (9%)]\tLoss: 0.058936\n",
      "Train Epoch: 114 [700/6658 (11%)]\tLoss: 0.675794\n",
      "Train Epoch: 114 [800/6658 (12%)]\tLoss: 0.046954\n",
      "Train Epoch: 114 [900/6658 (14%)]\tLoss: 0.156280\n",
      "Train Epoch: 114 [1000/6658 (15%)]\tLoss: 0.040935\n",
      "Train Epoch: 114 [1100/6658 (17%)]\tLoss: 0.096345\n",
      "Train Epoch: 114 [1200/6658 (18%)]\tLoss: 0.721174\n",
      "Train Epoch: 114 [1300/6658 (20%)]\tLoss: 0.001085\n",
      "Train Epoch: 114 [1400/6658 (21%)]\tLoss: 0.231109\n",
      "Train Epoch: 114 [1500/6658 (23%)]\tLoss: 0.199581\n",
      "Train Epoch: 114 [1600/6658 (24%)]\tLoss: 0.000030\n",
      "Train Epoch: 114 [1700/6658 (26%)]\tLoss: 0.844117\n",
      "Train Epoch: 114 [1800/6658 (27%)]\tLoss: 0.208203\n",
      "Train Epoch: 114 [1900/6658 (29%)]\tLoss: 0.151767\n",
      "Train Epoch: 114 [2000/6658 (30%)]\tLoss: 0.000039\n",
      "Train Epoch: 114 [2100/6658 (32%)]\tLoss: 0.032340\n",
      "Train Epoch: 114 [2200/6658 (33%)]\tLoss: 0.700061\n",
      "Train Epoch: 114 [2300/6658 (35%)]\tLoss: 0.117162\n",
      "Train Epoch: 114 [2400/6658 (36%)]\tLoss: 2.537109\n",
      "Train Epoch: 114 [2500/6658 (38%)]\tLoss: 0.582626\n",
      "Train Epoch: 114 [2600/6658 (39%)]\tLoss: 0.172019\n",
      "Train Epoch: 114 [2700/6658 (41%)]\tLoss: 1.198744\n",
      "Train Epoch: 114 [2800/6658 (42%)]\tLoss: 0.505316\n",
      "Train Epoch: 114 [2900/6658 (44%)]\tLoss: 1.473317\n",
      "Train Epoch: 114 [3000/6658 (45%)]\tLoss: 0.094824\n",
      "Train Epoch: 114 [3100/6658 (47%)]\tLoss: 0.229590\n",
      "Train Epoch: 114 [3200/6658 (48%)]\tLoss: 0.331372\n",
      "Train Epoch: 114 [3300/6658 (50%)]\tLoss: 0.007086\n",
      "Train Epoch: 114 [3400/6658 (51%)]\tLoss: 0.241951\n",
      "Train Epoch: 114 [3500/6658 (53%)]\tLoss: 1.672241\n",
      "Train Epoch: 114 [3600/6658 (54%)]\tLoss: 0.009169\n",
      "Train Epoch: 114 [3700/6658 (56%)]\tLoss: 7.739368\n",
      "Train Epoch: 114 [3800/6658 (57%)]\tLoss: 0.606548\n",
      "Train Epoch: 114 [3900/6658 (59%)]\tLoss: 1.215735\n",
      "Train Epoch: 114 [4000/6658 (60%)]\tLoss: 0.228525\n",
      "Train Epoch: 114 [4100/6658 (62%)]\tLoss: 0.012310\n",
      "Train Epoch: 114 [4200/6658 (63%)]\tLoss: 0.190559\n",
      "Train Epoch: 114 [4300/6658 (65%)]\tLoss: 0.051846\n",
      "Train Epoch: 114 [4400/6658 (66%)]\tLoss: 0.263284\n",
      "Train Epoch: 114 [4500/6658 (68%)]\tLoss: 0.029807\n",
      "Train Epoch: 114 [4600/6658 (69%)]\tLoss: 0.773885\n",
      "Train Epoch: 114 [4700/6658 (71%)]\tLoss: 2.032608\n",
      "Train Epoch: 114 [4800/6658 (72%)]\tLoss: 0.358304\n",
      "Train Epoch: 114 [4900/6658 (74%)]\tLoss: 0.071342\n",
      "Train Epoch: 114 [5000/6658 (75%)]\tLoss: 2.182281\n",
      "Train Epoch: 114 [5100/6658 (77%)]\tLoss: 0.073838\n",
      "Train Epoch: 114 [5200/6658 (78%)]\tLoss: 0.579785\n",
      "Train Epoch: 114 [5300/6658 (80%)]\tLoss: 0.104157\n",
      "Train Epoch: 114 [5400/6658 (81%)]\tLoss: 0.064121\n",
      "Train Epoch: 114 [5500/6658 (83%)]\tLoss: 1.010988\n",
      "Train Epoch: 114 [5600/6658 (84%)]\tLoss: 0.190849\n",
      "Train Epoch: 114 [5700/6658 (86%)]\tLoss: 2.033535\n",
      "Train Epoch: 114 [5800/6658 (87%)]\tLoss: 1.063035\n",
      "Train Epoch: 114 [5900/6658 (89%)]\tLoss: 0.134268\n",
      "Train Epoch: 114 [6000/6658 (90%)]\tLoss: 0.009469\n",
      "Train Epoch: 114 [6100/6658 (92%)]\tLoss: 0.906505\n",
      "Train Epoch: 114 [6200/6658 (93%)]\tLoss: 0.154165\n",
      "Train Epoch: 114 [6300/6658 (95%)]\tLoss: 0.290534\n",
      "Train Epoch: 114 [6400/6658 (96%)]\tLoss: 0.360454\n",
      "Train Epoch: 114 [6500/6658 (98%)]\tLoss: 1.091611\n",
      "Train Epoch: 114 [6600/6658 (99%)]\tLoss: 0.027209\n",
      "train loss average =  0.7138466116021813\n",
      "\n",
      "Test set: Average loss: 0.7090\n",
      "\n",
      "EarlyStopping counter: 21 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2461, 6.0760, 5.8719, 5.8756, 6.1663, 6.0484], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 115 [0/6658 (0%)]\tLoss: 0.772849\n",
      "Train Epoch: 115 [100/6658 (2%)]\tLoss: 0.592448\n",
      "Train Epoch: 115 [200/6658 (3%)]\tLoss: 1.311782\n",
      "Train Epoch: 115 [300/6658 (5%)]\tLoss: 0.057578\n",
      "Train Epoch: 115 [400/6658 (6%)]\tLoss: 0.005915\n",
      "Train Epoch: 115 [500/6658 (8%)]\tLoss: 0.010169\n",
      "Train Epoch: 115 [600/6658 (9%)]\tLoss: 0.333972\n",
      "Train Epoch: 115 [700/6658 (11%)]\tLoss: 0.085733\n",
      "Train Epoch: 115 [800/6658 (12%)]\tLoss: 0.274039\n",
      "Train Epoch: 115 [900/6658 (14%)]\tLoss: 0.001310\n",
      "Train Epoch: 115 [1000/6658 (15%)]\tLoss: 0.073712\n",
      "Train Epoch: 115 [1100/6658 (17%)]\tLoss: 0.273581\n",
      "Train Epoch: 115 [1200/6658 (18%)]\tLoss: 1.106174\n",
      "Train Epoch: 115 [1300/6658 (20%)]\tLoss: 0.426139\n",
      "Train Epoch: 115 [1400/6658 (21%)]\tLoss: 3.204404\n",
      "Train Epoch: 115 [1500/6658 (23%)]\tLoss: 0.245669\n",
      "Train Epoch: 115 [1600/6658 (24%)]\tLoss: 0.012401\n",
      "Train Epoch: 115 [1700/6658 (26%)]\tLoss: 0.868112\n",
      "Train Epoch: 115 [1800/6658 (27%)]\tLoss: 0.749025\n",
      "Train Epoch: 115 [1900/6658 (29%)]\tLoss: 0.383359\n",
      "Train Epoch: 115 [2000/6658 (30%)]\tLoss: 0.116358\n",
      "Train Epoch: 115 [2100/6658 (32%)]\tLoss: 0.083665\n",
      "Train Epoch: 115 [2200/6658 (33%)]\tLoss: 0.003231\n",
      "Train Epoch: 115 [2300/6658 (35%)]\tLoss: 0.029581\n",
      "Train Epoch: 115 [2400/6658 (36%)]\tLoss: 0.116260\n",
      "Train Epoch: 115 [2500/6658 (38%)]\tLoss: 0.032370\n",
      "Train Epoch: 115 [2600/6658 (39%)]\tLoss: 0.326357\n",
      "Train Epoch: 115 [2700/6658 (41%)]\tLoss: 0.606054\n",
      "Train Epoch: 115 [2800/6658 (42%)]\tLoss: 1.036785\n",
      "Train Epoch: 115 [2900/6658 (44%)]\tLoss: 0.192576\n",
      "Train Epoch: 115 [3000/6658 (45%)]\tLoss: 7.417623\n",
      "Train Epoch: 115 [3100/6658 (47%)]\tLoss: 0.197085\n",
      "Train Epoch: 115 [3200/6658 (48%)]\tLoss: 0.890611\n",
      "Train Epoch: 115 [3300/6658 (50%)]\tLoss: 0.020742\n",
      "Train Epoch: 115 [3400/6658 (51%)]\tLoss: 2.916883\n",
      "Train Epoch: 115 [3500/6658 (53%)]\tLoss: 0.589691\n",
      "Train Epoch: 115 [3600/6658 (54%)]\tLoss: 1.234813\n",
      "Train Epoch: 115 [3700/6658 (56%)]\tLoss: 0.757997\n",
      "Train Epoch: 115 [3800/6658 (57%)]\tLoss: 3.300906\n",
      "Train Epoch: 115 [3900/6658 (59%)]\tLoss: 2.119493\n",
      "Train Epoch: 115 [4000/6658 (60%)]\tLoss: 0.002309\n",
      "Train Epoch: 115 [4100/6658 (62%)]\tLoss: 1.822761\n",
      "Train Epoch: 115 [4200/6658 (63%)]\tLoss: 2.148129\n",
      "Train Epoch: 115 [4300/6658 (65%)]\tLoss: 0.149275\n",
      "Train Epoch: 115 [4400/6658 (66%)]\tLoss: 0.812056\n",
      "Train Epoch: 115 [4500/6658 (68%)]\tLoss: 0.709692\n",
      "Train Epoch: 115 [4600/6658 (69%)]\tLoss: 7.092523\n",
      "Train Epoch: 115 [4700/6658 (71%)]\tLoss: 0.021888\n",
      "Train Epoch: 115 [4800/6658 (72%)]\tLoss: 0.255060\n",
      "Train Epoch: 115 [4900/6658 (74%)]\tLoss: 1.393618\n",
      "Train Epoch: 115 [5000/6658 (75%)]\tLoss: 0.009602\n",
      "Train Epoch: 115 [5100/6658 (77%)]\tLoss: 0.071656\n",
      "Train Epoch: 115 [5200/6658 (78%)]\tLoss: 0.597271\n",
      "Train Epoch: 115 [5300/6658 (80%)]\tLoss: 0.000554\n",
      "Train Epoch: 115 [5400/6658 (81%)]\tLoss: 5.519080\n",
      "Train Epoch: 115 [5500/6658 (83%)]\tLoss: 1.496225\n",
      "Train Epoch: 115 [5600/6658 (84%)]\tLoss: 0.078156\n",
      "Train Epoch: 115 [5700/6658 (86%)]\tLoss: 0.004113\n",
      "Train Epoch: 115 [5800/6658 (87%)]\tLoss: 0.686161\n",
      "Train Epoch: 115 [5900/6658 (89%)]\tLoss: 1.566369\n",
      "Train Epoch: 115 [6000/6658 (90%)]\tLoss: 0.596783\n",
      "Train Epoch: 115 [6100/6658 (92%)]\tLoss: 0.767556\n",
      "Train Epoch: 115 [6200/6658 (93%)]\tLoss: 0.283040\n",
      "Train Epoch: 115 [6300/6658 (95%)]\tLoss: 3.662806\n",
      "Train Epoch: 115 [6400/6658 (96%)]\tLoss: 0.287909\n",
      "Train Epoch: 115 [6500/6658 (98%)]\tLoss: 0.531962\n",
      "Train Epoch: 115 [6600/6658 (99%)]\tLoss: 1.143814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss average =  0.7137937478863536\n",
      "\n",
      "Test set: Average loss: 0.6957\n",
      "\n",
      "EarlyStopping counter: 22 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2481, 6.0764, 5.8717, 5.8755, 6.1673, 6.0483], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 116 [0/6658 (0%)]\tLoss: 1.166316\n",
      "Train Epoch: 116 [100/6658 (2%)]\tLoss: 5.357756\n",
      "Train Epoch: 116 [200/6658 (3%)]\tLoss: 0.368237\n",
      "Train Epoch: 116 [300/6658 (5%)]\tLoss: 0.564019\n",
      "Train Epoch: 116 [400/6658 (6%)]\tLoss: 1.229488\n",
      "Train Epoch: 116 [500/6658 (8%)]\tLoss: 0.040231\n",
      "Train Epoch: 116 [600/6658 (9%)]\tLoss: 0.284688\n",
      "Train Epoch: 116 [700/6658 (11%)]\tLoss: 1.984969\n",
      "Train Epoch: 116 [800/6658 (12%)]\tLoss: 0.202865\n",
      "Train Epoch: 116 [900/6658 (14%)]\tLoss: 1.524251\n",
      "Train Epoch: 116 [1000/6658 (15%)]\tLoss: 0.325618\n",
      "Train Epoch: 116 [1100/6658 (17%)]\tLoss: 0.363452\n",
      "Train Epoch: 116 [1200/6658 (18%)]\tLoss: 0.254534\n",
      "Train Epoch: 116 [1300/6658 (20%)]\tLoss: 0.063949\n",
      "Train Epoch: 116 [1400/6658 (21%)]\tLoss: 0.393674\n",
      "Train Epoch: 116 [1500/6658 (23%)]\tLoss: 0.001926\n",
      "Train Epoch: 116 [1600/6658 (24%)]\tLoss: 0.156678\n",
      "Train Epoch: 116 [1700/6658 (26%)]\tLoss: 0.431612\n",
      "Train Epoch: 116 [1800/6658 (27%)]\tLoss: 0.082902\n",
      "Train Epoch: 116 [1900/6658 (29%)]\tLoss: 4.427735\n",
      "Train Epoch: 116 [2000/6658 (30%)]\tLoss: 0.006950\n",
      "Train Epoch: 116 [2100/6658 (32%)]\tLoss: 0.451563\n",
      "Train Epoch: 116 [2200/6658 (33%)]\tLoss: 0.083970\n",
      "Train Epoch: 116 [2300/6658 (35%)]\tLoss: 0.026108\n",
      "Train Epoch: 116 [2400/6658 (36%)]\tLoss: 0.000156\n",
      "Train Epoch: 116 [2500/6658 (38%)]\tLoss: 0.733426\n",
      "Train Epoch: 116 [2600/6658 (39%)]\tLoss: 0.289491\n",
      "Train Epoch: 116 [2700/6658 (41%)]\tLoss: 0.000037\n",
      "Train Epoch: 116 [2800/6658 (42%)]\tLoss: 0.212559\n",
      "Train Epoch: 116 [2900/6658 (44%)]\tLoss: 1.019897\n",
      "Train Epoch: 116 [3000/6658 (45%)]\tLoss: 0.091503\n",
      "Train Epoch: 116 [3100/6658 (47%)]\tLoss: 0.110098\n",
      "Train Epoch: 116 [3200/6658 (48%)]\tLoss: 0.987972\n",
      "Train Epoch: 116 [3300/6658 (50%)]\tLoss: 0.552179\n",
      "Train Epoch: 116 [3400/6658 (51%)]\tLoss: 0.000876\n",
      "Train Epoch: 116 [3500/6658 (53%)]\tLoss: 2.773621\n",
      "Train Epoch: 116 [3600/6658 (54%)]\tLoss: 0.001622\n",
      "Train Epoch: 116 [3700/6658 (56%)]\tLoss: 3.459008\n",
      "Train Epoch: 116 [3800/6658 (57%)]\tLoss: 0.056320\n",
      "Train Epoch: 116 [3900/6658 (59%)]\tLoss: 0.242677\n",
      "Train Epoch: 116 [4000/6658 (60%)]\tLoss: 0.134943\n",
      "Train Epoch: 116 [4100/6658 (62%)]\tLoss: 0.425819\n",
      "Train Epoch: 116 [4200/6658 (63%)]\tLoss: 0.522328\n",
      "Train Epoch: 116 [4300/6658 (65%)]\tLoss: 2.556271\n",
      "Train Epoch: 116 [4400/6658 (66%)]\tLoss: 0.505493\n",
      "Train Epoch: 116 [4500/6658 (68%)]\tLoss: 0.704978\n",
      "Train Epoch: 116 [4600/6658 (69%)]\tLoss: 0.433987\n",
      "Train Epoch: 116 [4700/6658 (71%)]\tLoss: 0.172263\n",
      "Train Epoch: 116 [4800/6658 (72%)]\tLoss: 0.458538\n",
      "Train Epoch: 116 [4900/6658 (74%)]\tLoss: 0.021665\n",
      "Train Epoch: 116 [5000/6658 (75%)]\tLoss: 0.022704\n",
      "Train Epoch: 116 [5100/6658 (77%)]\tLoss: 0.813290\n",
      "Train Epoch: 116 [5200/6658 (78%)]\tLoss: 0.841524\n",
      "Train Epoch: 116 [5300/6658 (80%)]\tLoss: 0.121071\n",
      "Train Epoch: 116 [5400/6658 (81%)]\tLoss: 2.210183\n",
      "Train Epoch: 116 [5500/6658 (83%)]\tLoss: 0.908653\n",
      "Train Epoch: 116 [5600/6658 (84%)]\tLoss: 0.101257\n",
      "Train Epoch: 116 [5700/6658 (86%)]\tLoss: 0.062414\n",
      "Train Epoch: 116 [5800/6658 (87%)]\tLoss: 0.466864\n",
      "Train Epoch: 116 [5900/6658 (89%)]\tLoss: 0.271738\n",
      "Train Epoch: 116 [6000/6658 (90%)]\tLoss: 0.029405\n",
      "Train Epoch: 116 [6100/6658 (92%)]\tLoss: 0.619609\n",
      "Train Epoch: 116 [6200/6658 (93%)]\tLoss: 0.319002\n",
      "Train Epoch: 116 [6300/6658 (95%)]\tLoss: 0.142759\n",
      "Train Epoch: 116 [6400/6658 (96%)]\tLoss: 1.650243\n",
      "Train Epoch: 116 [6500/6658 (98%)]\tLoss: 0.239778\n",
      "Train Epoch: 116 [6600/6658 (99%)]\tLoss: 0.325355\n",
      "train loss average =  0.7228371870308553\n",
      "\n",
      "Test set: Average loss: 0.7089\n",
      "\n",
      "EarlyStopping counter: 23 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2504, 6.0777, 5.8702, 5.8742, 6.1694, 6.0489], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 117 [0/6658 (0%)]\tLoss: 0.271990\n",
      "Train Epoch: 117 [100/6658 (2%)]\tLoss: 0.292513\n",
      "Train Epoch: 117 [200/6658 (3%)]\tLoss: 1.117112\n",
      "Train Epoch: 117 [300/6658 (5%)]\tLoss: 0.423693\n",
      "Train Epoch: 117 [400/6658 (6%)]\tLoss: 0.219330\n",
      "Train Epoch: 117 [500/6658 (8%)]\tLoss: 0.000996\n",
      "Train Epoch: 117 [600/6658 (9%)]\tLoss: 0.647626\n",
      "Train Epoch: 117 [700/6658 (11%)]\tLoss: 1.186410\n",
      "Train Epoch: 117 [800/6658 (12%)]\tLoss: 1.111071\n",
      "Train Epoch: 117 [900/6658 (14%)]\tLoss: 0.126549\n",
      "Train Epoch: 117 [1000/6658 (15%)]\tLoss: 0.455896\n",
      "Train Epoch: 117 [1100/6658 (17%)]\tLoss: 0.004099\n",
      "Train Epoch: 117 [1200/6658 (18%)]\tLoss: 0.004077\n",
      "Train Epoch: 117 [1300/6658 (20%)]\tLoss: 0.159999\n",
      "Train Epoch: 117 [1400/6658 (21%)]\tLoss: 0.164067\n",
      "Train Epoch: 117 [1500/6658 (23%)]\tLoss: 2.056989\n",
      "Train Epoch: 117 [1600/6658 (24%)]\tLoss: 1.093484\n",
      "Train Epoch: 117 [1700/6658 (26%)]\tLoss: 0.032126\n",
      "Train Epoch: 117 [1800/6658 (27%)]\tLoss: 0.563390\n",
      "Train Epoch: 117 [1900/6658 (29%)]\tLoss: 0.000019\n",
      "Train Epoch: 117 [2000/6658 (30%)]\tLoss: 0.390875\n",
      "Train Epoch: 117 [2100/6658 (32%)]\tLoss: 0.003281\n",
      "Train Epoch: 117 [2200/6658 (33%)]\tLoss: 0.468862\n",
      "Train Epoch: 117 [2300/6658 (35%)]\tLoss: 0.022352\n",
      "Train Epoch: 117 [2400/6658 (36%)]\tLoss: 0.654285\n",
      "Train Epoch: 117 [2500/6658 (38%)]\tLoss: 0.948791\n",
      "Train Epoch: 117 [2600/6658 (39%)]\tLoss: 0.016889\n",
      "Train Epoch: 117 [2700/6658 (41%)]\tLoss: 0.026911\n",
      "Train Epoch: 117 [2800/6658 (42%)]\tLoss: 0.739352\n",
      "Train Epoch: 117 [2900/6658 (44%)]\tLoss: 2.928267\n",
      "Train Epoch: 117 [3000/6658 (45%)]\tLoss: 0.107819\n",
      "Train Epoch: 117 [3100/6658 (47%)]\tLoss: 0.043849\n",
      "Train Epoch: 117 [3200/6658 (48%)]\tLoss: 0.070853\n",
      "Train Epoch: 117 [3300/6658 (50%)]\tLoss: 0.062369\n",
      "Train Epoch: 117 [3400/6658 (51%)]\tLoss: 0.649144\n",
      "Train Epoch: 117 [3500/6658 (53%)]\tLoss: 4.497235\n",
      "Train Epoch: 117 [3600/6658 (54%)]\tLoss: 0.078817\n",
      "Train Epoch: 117 [3700/6658 (56%)]\tLoss: 2.464390\n",
      "Train Epoch: 117 [3800/6658 (57%)]\tLoss: 1.502935\n",
      "Train Epoch: 117 [3900/6658 (59%)]\tLoss: 0.228216\n",
      "Train Epoch: 117 [4000/6658 (60%)]\tLoss: 0.584713\n",
      "Train Epoch: 117 [4100/6658 (62%)]\tLoss: 1.540145\n",
      "Train Epoch: 117 [4200/6658 (63%)]\tLoss: 0.963479\n",
      "Train Epoch: 117 [4300/6658 (65%)]\tLoss: 0.006211\n",
      "Train Epoch: 117 [4400/6658 (66%)]\tLoss: 0.847529\n",
      "Train Epoch: 117 [4500/6658 (68%)]\tLoss: 0.097630\n",
      "Train Epoch: 117 [4600/6658 (69%)]\tLoss: 0.591387\n",
      "Train Epoch: 117 [4700/6658 (71%)]\tLoss: 0.000001\n",
      "Train Epoch: 117 [4800/6658 (72%)]\tLoss: 0.048097\n",
      "Train Epoch: 117 [4900/6658 (74%)]\tLoss: 0.283512\n",
      "Train Epoch: 117 [5000/6658 (75%)]\tLoss: 0.004565\n",
      "Train Epoch: 117 [5100/6658 (77%)]\tLoss: 0.003195\n",
      "Train Epoch: 117 [5200/6658 (78%)]\tLoss: 1.022476\n",
      "Train Epoch: 117 [5300/6658 (80%)]\tLoss: 0.240267\n",
      "Train Epoch: 117 [5400/6658 (81%)]\tLoss: 0.269748\n",
      "Train Epoch: 117 [5500/6658 (83%)]\tLoss: 4.807960\n",
      "Train Epoch: 117 [5600/6658 (84%)]\tLoss: 0.009072\n",
      "Train Epoch: 117 [5700/6658 (86%)]\tLoss: 0.693736\n",
      "Train Epoch: 117 [5800/6658 (87%)]\tLoss: 0.287969\n",
      "Train Epoch: 117 [5900/6658 (89%)]\tLoss: 0.002531\n",
      "Train Epoch: 117 [6000/6658 (90%)]\tLoss: 0.225057\n",
      "Train Epoch: 117 [6100/6658 (92%)]\tLoss: 0.546500\n",
      "Train Epoch: 117 [6200/6658 (93%)]\tLoss: 0.647830\n",
      "Train Epoch: 117 [6300/6658 (95%)]\tLoss: 0.847955\n",
      "Train Epoch: 117 [6400/6658 (96%)]\tLoss: 2.045853\n",
      "Train Epoch: 117 [6500/6658 (98%)]\tLoss: 0.002059\n",
      "Train Epoch: 117 [6600/6658 (99%)]\tLoss: 0.135622\n",
      "train loss average =  0.716657538894472\n",
      "\n",
      "Test set: Average loss: 0.6914\n",
      "\n",
      "Validation loss decreased (0.692768 --> 0.691429).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.2528, 6.0777, 5.8700, 5.8725, 6.1711, 6.0488], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 118 [0/6658 (0%)]\tLoss: 0.365026\n",
      "Train Epoch: 118 [100/6658 (2%)]\tLoss: 0.151475\n",
      "Train Epoch: 118 [200/6658 (3%)]\tLoss: 0.560556\n",
      "Train Epoch: 118 [300/6658 (5%)]\tLoss: 0.053658\n",
      "Train Epoch: 118 [400/6658 (6%)]\tLoss: 4.274836\n",
      "Train Epoch: 118 [500/6658 (8%)]\tLoss: 0.010195\n",
      "Train Epoch: 118 [600/6658 (9%)]\tLoss: 0.112099\n",
      "Train Epoch: 118 [700/6658 (11%)]\tLoss: 0.011372\n",
      "Train Epoch: 118 [800/6658 (12%)]\tLoss: 0.000121\n",
      "Train Epoch: 118 [900/6658 (14%)]\tLoss: 0.006592\n",
      "Train Epoch: 118 [1000/6658 (15%)]\tLoss: 0.100882\n",
      "Train Epoch: 118 [1100/6658 (17%)]\tLoss: 0.137029\n",
      "Train Epoch: 118 [1200/6658 (18%)]\tLoss: 0.301822\n",
      "Train Epoch: 118 [1300/6658 (20%)]\tLoss: 0.072147\n",
      "Train Epoch: 118 [1400/6658 (21%)]\tLoss: 5.335826\n",
      "Train Epoch: 118 [1500/6658 (23%)]\tLoss: 0.052100\n",
      "Train Epoch: 118 [1600/6658 (24%)]\tLoss: 0.210665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 118 [1700/6658 (26%)]\tLoss: 1.126628\n",
      "Train Epoch: 118 [1800/6658 (27%)]\tLoss: 0.488277\n",
      "Train Epoch: 118 [1900/6658 (29%)]\tLoss: 1.383450\n",
      "Train Epoch: 118 [2000/6658 (30%)]\tLoss: 0.473167\n",
      "Train Epoch: 118 [2100/6658 (32%)]\tLoss: 0.134394\n",
      "Train Epoch: 118 [2200/6658 (33%)]\tLoss: 0.676459\n",
      "Train Epoch: 118 [2300/6658 (35%)]\tLoss: 0.467156\n",
      "Train Epoch: 118 [2400/6658 (36%)]\tLoss: 0.546179\n",
      "Train Epoch: 118 [2500/6658 (38%)]\tLoss: 0.030339\n",
      "Train Epoch: 118 [2600/6658 (39%)]\tLoss: 0.160567\n",
      "Train Epoch: 118 [2700/6658 (41%)]\tLoss: 0.075711\n",
      "Train Epoch: 118 [2800/6658 (42%)]\tLoss: 1.072062\n",
      "Train Epoch: 118 [2900/6658 (44%)]\tLoss: 0.069114\n",
      "Train Epoch: 118 [3000/6658 (45%)]\tLoss: 0.024656\n",
      "Train Epoch: 118 [3100/6658 (47%)]\tLoss: 0.250123\n",
      "Train Epoch: 118 [3200/6658 (48%)]\tLoss: 0.008603\n",
      "Train Epoch: 118 [3300/6658 (50%)]\tLoss: 0.172903\n",
      "Train Epoch: 118 [3400/6658 (51%)]\tLoss: 0.231461\n",
      "Train Epoch: 118 [3500/6658 (53%)]\tLoss: 1.335612\n",
      "Train Epoch: 118 [3600/6658 (54%)]\tLoss: 1.904869\n",
      "Train Epoch: 118 [3700/6658 (56%)]\tLoss: 2.463110\n",
      "Train Epoch: 118 [3800/6658 (57%)]\tLoss: 1.624899\n",
      "Train Epoch: 118 [3900/6658 (59%)]\tLoss: 0.058149\n",
      "Train Epoch: 118 [4000/6658 (60%)]\tLoss: 0.132784\n",
      "Train Epoch: 118 [4100/6658 (62%)]\tLoss: 0.156468\n",
      "Train Epoch: 118 [4200/6658 (63%)]\tLoss: 0.379874\n",
      "Train Epoch: 118 [4300/6658 (65%)]\tLoss: 0.492176\n",
      "Train Epoch: 118 [4400/6658 (66%)]\tLoss: 0.114496\n",
      "Train Epoch: 118 [4500/6658 (68%)]\tLoss: 0.098651\n",
      "Train Epoch: 118 [4600/6658 (69%)]\tLoss: 0.324786\n",
      "Train Epoch: 118 [4700/6658 (71%)]\tLoss: 0.191645\n",
      "Train Epoch: 118 [4800/6658 (72%)]\tLoss: 0.809560\n",
      "Train Epoch: 118 [4900/6658 (74%)]\tLoss: 0.595569\n",
      "Train Epoch: 118 [5000/6658 (75%)]\tLoss: 0.064257\n",
      "Train Epoch: 118 [5100/6658 (77%)]\tLoss: 0.027183\n",
      "Train Epoch: 118 [5200/6658 (78%)]\tLoss: 0.035030\n",
      "Train Epoch: 118 [5300/6658 (80%)]\tLoss: 0.721034\n",
      "Train Epoch: 118 [5400/6658 (81%)]\tLoss: 0.155708\n",
      "Train Epoch: 118 [5500/6658 (83%)]\tLoss: 1.690258\n",
      "Train Epoch: 118 [5600/6658 (84%)]\tLoss: 0.146747\n",
      "Train Epoch: 118 [5700/6658 (86%)]\tLoss: 0.052063\n",
      "Train Epoch: 118 [5800/6658 (87%)]\tLoss: 0.056862\n",
      "Train Epoch: 118 [5900/6658 (89%)]\tLoss: 0.129140\n",
      "Train Epoch: 118 [6000/6658 (90%)]\tLoss: 0.441191\n",
      "Train Epoch: 118 [6100/6658 (92%)]\tLoss: 0.128454\n",
      "Train Epoch: 118 [6200/6658 (93%)]\tLoss: 1.800810\n",
      "Train Epoch: 118 [6300/6658 (95%)]\tLoss: 0.117587\n",
      "Train Epoch: 118 [6400/6658 (96%)]\tLoss: 0.241133\n",
      "Train Epoch: 118 [6500/6658 (98%)]\tLoss: 0.011848\n",
      "Train Epoch: 118 [6600/6658 (99%)]\tLoss: 0.267930\n",
      "train loss average =  0.7167670523176116\n",
      "\n",
      "Test set: Average loss: 0.6984\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2550, 6.0768, 5.8692, 5.8724, 6.1729, 6.0490], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 119 [0/6658 (0%)]\tLoss: 0.431623\n",
      "Train Epoch: 119 [100/6658 (2%)]\tLoss: 3.167422\n",
      "Train Epoch: 119 [200/6658 (3%)]\tLoss: 2.961650\n",
      "Train Epoch: 119 [300/6658 (5%)]\tLoss: 0.000046\n",
      "Train Epoch: 119 [400/6658 (6%)]\tLoss: 1.057089\n",
      "Train Epoch: 119 [500/6658 (8%)]\tLoss: 0.198434\n",
      "Train Epoch: 119 [600/6658 (9%)]\tLoss: 0.096303\n",
      "Train Epoch: 119 [700/6658 (11%)]\tLoss: 0.011529\n",
      "Train Epoch: 119 [800/6658 (12%)]\tLoss: 0.071422\n",
      "Train Epoch: 119 [900/6658 (14%)]\tLoss: 0.569675\n",
      "Train Epoch: 119 [1000/6658 (15%)]\tLoss: 1.079764\n",
      "Train Epoch: 119 [1100/6658 (17%)]\tLoss: 0.112514\n",
      "Train Epoch: 119 [1200/6658 (18%)]\tLoss: 2.330985\n",
      "Train Epoch: 119 [1300/6658 (20%)]\tLoss: 0.100823\n",
      "Train Epoch: 119 [1400/6658 (21%)]\tLoss: 0.067680\n",
      "Train Epoch: 119 [1500/6658 (23%)]\tLoss: 0.000095\n",
      "Train Epoch: 119 [1600/6658 (24%)]\tLoss: 0.030230\n",
      "Train Epoch: 119 [1700/6658 (26%)]\tLoss: 0.568457\n",
      "Train Epoch: 119 [1800/6658 (27%)]\tLoss: 0.353104\n",
      "Train Epoch: 119 [1900/6658 (29%)]\tLoss: 0.000152\n",
      "Train Epoch: 119 [2000/6658 (30%)]\tLoss: 0.228130\n",
      "Train Epoch: 119 [2100/6658 (32%)]\tLoss: 0.352815\n",
      "Train Epoch: 119 [2200/6658 (33%)]\tLoss: 5.548590\n",
      "Train Epoch: 119 [2300/6658 (35%)]\tLoss: 0.206416\n",
      "Train Epoch: 119 [2400/6658 (36%)]\tLoss: 0.170575\n",
      "Train Epoch: 119 [2500/6658 (38%)]\tLoss: 0.257603\n",
      "Train Epoch: 119 [2600/6658 (39%)]\tLoss: 0.408661\n",
      "Train Epoch: 119 [2700/6658 (41%)]\tLoss: 0.524476\n",
      "Train Epoch: 119 [2800/6658 (42%)]\tLoss: 0.076408\n",
      "Train Epoch: 119 [2900/6658 (44%)]\tLoss: 0.731450\n",
      "Train Epoch: 119 [3000/6658 (45%)]\tLoss: 0.303585\n",
      "Train Epoch: 119 [3100/6658 (47%)]\tLoss: 0.585514\n",
      "Train Epoch: 119 [3200/6658 (48%)]\tLoss: 0.000124\n",
      "Train Epoch: 119 [3300/6658 (50%)]\tLoss: 0.041235\n",
      "Train Epoch: 119 [3400/6658 (51%)]\tLoss: 0.246970\n",
      "Train Epoch: 119 [3500/6658 (53%)]\tLoss: 1.099199\n",
      "Train Epoch: 119 [3600/6658 (54%)]\tLoss: 0.349323\n",
      "Train Epoch: 119 [3700/6658 (56%)]\tLoss: 0.616016\n",
      "Train Epoch: 119 [3800/6658 (57%)]\tLoss: 0.207700\n",
      "Train Epoch: 119 [3900/6658 (59%)]\tLoss: 0.434086\n",
      "Train Epoch: 119 [4000/6658 (60%)]\tLoss: 0.382456\n",
      "Train Epoch: 119 [4100/6658 (62%)]\tLoss: 0.075529\n",
      "Train Epoch: 119 [4200/6658 (63%)]\tLoss: 1.212740\n",
      "Train Epoch: 119 [4300/6658 (65%)]\tLoss: 1.936201\n",
      "Train Epoch: 119 [4400/6658 (66%)]\tLoss: 0.036036\n",
      "Train Epoch: 119 [4500/6658 (68%)]\tLoss: 3.003522\n",
      "Train Epoch: 119 [4600/6658 (69%)]\tLoss: 0.004493\n",
      "Train Epoch: 119 [4700/6658 (71%)]\tLoss: 0.059426\n",
      "Train Epoch: 119 [4800/6658 (72%)]\tLoss: 0.003371\n",
      "Train Epoch: 119 [4900/6658 (74%)]\tLoss: 0.611411\n",
      "Train Epoch: 119 [5000/6658 (75%)]\tLoss: 0.290182\n",
      "Train Epoch: 119 [5100/6658 (77%)]\tLoss: 0.581455\n",
      "Train Epoch: 119 [5200/6658 (78%)]\tLoss: 0.356333\n",
      "Train Epoch: 119 [5300/6658 (80%)]\tLoss: 0.147454\n",
      "Train Epoch: 119 [5400/6658 (81%)]\tLoss: 0.727413\n",
      "Train Epoch: 119 [5500/6658 (83%)]\tLoss: 0.200975\n",
      "Train Epoch: 119 [5600/6658 (84%)]\tLoss: 0.756204\n",
      "Train Epoch: 119 [5700/6658 (86%)]\tLoss: 0.637613\n",
      "Train Epoch: 119 [5800/6658 (87%)]\tLoss: 0.479505\n",
      "Train Epoch: 119 [5900/6658 (89%)]\tLoss: 0.715485\n",
      "Train Epoch: 119 [6000/6658 (90%)]\tLoss: 0.162254\n",
      "Train Epoch: 119 [6100/6658 (92%)]\tLoss: 0.085884\n",
      "Train Epoch: 119 [6200/6658 (93%)]\tLoss: 0.023342\n",
      "Train Epoch: 119 [6300/6658 (95%)]\tLoss: 0.971274\n",
      "Train Epoch: 119 [6400/6658 (96%)]\tLoss: 0.517899\n",
      "Train Epoch: 119 [6500/6658 (98%)]\tLoss: 0.194124\n",
      "Train Epoch: 119 [6600/6658 (99%)]\tLoss: 0.109597\n",
      "train loss average =  0.7169781789097007\n",
      "\n",
      "Test set: Average loss: 0.6988\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2572, 6.0773, 5.8693, 5.8714, 6.1742, 6.0490], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 120 [0/6658 (0%)]\tLoss: 0.146223\n",
      "Train Epoch: 120 [100/6658 (2%)]\tLoss: 1.197445\n",
      "Train Epoch: 120 [200/6658 (3%)]\tLoss: 0.021760\n",
      "Train Epoch: 120 [300/6658 (5%)]\tLoss: 0.283718\n",
      "Train Epoch: 120 [400/6658 (6%)]\tLoss: 0.085135\n",
      "Train Epoch: 120 [500/6658 (8%)]\tLoss: 0.538734\n",
      "Train Epoch: 120 [600/6658 (9%)]\tLoss: 1.554861\n",
      "Train Epoch: 120 [700/6658 (11%)]\tLoss: 1.000530\n",
      "Train Epoch: 120 [800/6658 (12%)]\tLoss: 1.706484\n",
      "Train Epoch: 120 [900/6658 (14%)]\tLoss: 0.693376\n",
      "Train Epoch: 120 [1000/6658 (15%)]\tLoss: 0.011948\n",
      "Train Epoch: 120 [1100/6658 (17%)]\tLoss: 0.767051\n",
      "Train Epoch: 120 [1200/6658 (18%)]\tLoss: 0.158665\n",
      "Train Epoch: 120 [1300/6658 (20%)]\tLoss: 0.350050\n",
      "Train Epoch: 120 [1400/6658 (21%)]\tLoss: 0.766343\n",
      "Train Epoch: 120 [1500/6658 (23%)]\tLoss: 0.154189\n",
      "Train Epoch: 120 [1600/6658 (24%)]\tLoss: 0.003013\n",
      "Train Epoch: 120 [1700/6658 (26%)]\tLoss: 0.339253\n",
      "Train Epoch: 120 [1800/6658 (27%)]\tLoss: 0.230308\n",
      "Train Epoch: 120 [1900/6658 (29%)]\tLoss: 0.260938\n",
      "Train Epoch: 120 [2000/6658 (30%)]\tLoss: 1.061627\n",
      "Train Epoch: 120 [2100/6658 (32%)]\tLoss: 1.360808\n",
      "Train Epoch: 120 [2200/6658 (33%)]\tLoss: 0.059980\n",
      "Train Epoch: 120 [2300/6658 (35%)]\tLoss: 2.310612\n",
      "Train Epoch: 120 [2400/6658 (36%)]\tLoss: 0.595852\n",
      "Train Epoch: 120 [2500/6658 (38%)]\tLoss: 0.415754\n",
      "Train Epoch: 120 [2600/6658 (39%)]\tLoss: 0.160173\n",
      "Train Epoch: 120 [2700/6658 (41%)]\tLoss: 0.720452\n",
      "Train Epoch: 120 [2800/6658 (42%)]\tLoss: 14.809112\n",
      "Train Epoch: 120 [2900/6658 (44%)]\tLoss: 0.952216\n",
      "Train Epoch: 120 [3000/6658 (45%)]\tLoss: 0.381064\n",
      "Train Epoch: 120 [3100/6658 (47%)]\tLoss: 0.019006\n",
      "Train Epoch: 120 [3200/6658 (48%)]\tLoss: 0.142122\n",
      "Train Epoch: 120 [3300/6658 (50%)]\tLoss: 2.214549\n",
      "Train Epoch: 120 [3400/6658 (51%)]\tLoss: 0.602490\n",
      "Train Epoch: 120 [3500/6658 (53%)]\tLoss: 0.007091\n",
      "Train Epoch: 120 [3600/6658 (54%)]\tLoss: 0.171616\n",
      "Train Epoch: 120 [3700/6658 (56%)]\tLoss: 0.308464\n",
      "Train Epoch: 120 [3800/6658 (57%)]\tLoss: 0.152940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 120 [3900/6658 (59%)]\tLoss: 0.785214\n",
      "Train Epoch: 120 [4000/6658 (60%)]\tLoss: 0.087777\n",
      "Train Epoch: 120 [4100/6658 (62%)]\tLoss: 0.037609\n",
      "Train Epoch: 120 [4200/6658 (63%)]\tLoss: 0.924260\n",
      "Train Epoch: 120 [4300/6658 (65%)]\tLoss: 0.630243\n",
      "Train Epoch: 120 [4400/6658 (66%)]\tLoss: 0.012511\n",
      "Train Epoch: 120 [4500/6658 (68%)]\tLoss: 0.023043\n",
      "Train Epoch: 120 [4600/6658 (69%)]\tLoss: 0.205981\n",
      "Train Epoch: 120 [4700/6658 (71%)]\tLoss: 0.076603\n",
      "Train Epoch: 120 [4800/6658 (72%)]\tLoss: 0.388087\n",
      "Train Epoch: 120 [4900/6658 (74%)]\tLoss: 0.070182\n",
      "Train Epoch: 120 [5000/6658 (75%)]\tLoss: 0.010162\n",
      "Train Epoch: 120 [5100/6658 (77%)]\tLoss: 0.491647\n",
      "Train Epoch: 120 [5200/6658 (78%)]\tLoss: 0.057870\n",
      "Train Epoch: 120 [5300/6658 (80%)]\tLoss: 2.839195\n",
      "Train Epoch: 120 [5400/6658 (81%)]\tLoss: 1.603271\n",
      "Train Epoch: 120 [5500/6658 (83%)]\tLoss: 0.009077\n",
      "Train Epoch: 120 [5600/6658 (84%)]\tLoss: 0.075731\n",
      "Train Epoch: 120 [5700/6658 (86%)]\tLoss: 0.388233\n",
      "Train Epoch: 120 [5800/6658 (87%)]\tLoss: 0.046877\n",
      "Train Epoch: 120 [5900/6658 (89%)]\tLoss: 0.177888\n",
      "Train Epoch: 120 [6000/6658 (90%)]\tLoss: 0.498886\n",
      "Train Epoch: 120 [6100/6658 (92%)]\tLoss: 0.118906\n",
      "Train Epoch: 120 [6200/6658 (93%)]\tLoss: 0.142190\n",
      "Train Epoch: 120 [6300/6658 (95%)]\tLoss: 0.002835\n",
      "Train Epoch: 120 [6400/6658 (96%)]\tLoss: 0.210136\n",
      "Train Epoch: 120 [6500/6658 (98%)]\tLoss: 0.364050\n",
      "Train Epoch: 120 [6600/6658 (99%)]\tLoss: 0.009153\n",
      "train loss average =  0.7136792133572238\n",
      "\n",
      "Test set: Average loss: 0.7031\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2599, 6.0780, 5.8693, 5.8714, 6.1759, 6.0490], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 121 [0/6658 (0%)]\tLoss: 1.280278\n",
      "Train Epoch: 121 [100/6658 (2%)]\tLoss: 0.676636\n",
      "Train Epoch: 121 [200/6658 (3%)]\tLoss: 8.384684\n",
      "Train Epoch: 121 [300/6658 (5%)]\tLoss: 1.091218\n",
      "Train Epoch: 121 [400/6658 (6%)]\tLoss: 0.847427\n",
      "Train Epoch: 121 [500/6658 (8%)]\tLoss: 0.041703\n",
      "Train Epoch: 121 [600/6658 (9%)]\tLoss: 0.069159\n",
      "Train Epoch: 121 [700/6658 (11%)]\tLoss: 41.982002\n",
      "Train Epoch: 121 [800/6658 (12%)]\tLoss: 0.165655\n",
      "Train Epoch: 121 [900/6658 (14%)]\tLoss: 0.049046\n",
      "Train Epoch: 121 [1000/6658 (15%)]\tLoss: 0.008443\n",
      "Train Epoch: 121 [1100/6658 (17%)]\tLoss: 0.254188\n",
      "Train Epoch: 121 [1200/6658 (18%)]\tLoss: 0.008076\n",
      "Train Epoch: 121 [1300/6658 (20%)]\tLoss: 0.066772\n",
      "Train Epoch: 121 [1400/6658 (21%)]\tLoss: 0.002701\n",
      "Train Epoch: 121 [1500/6658 (23%)]\tLoss: 0.398306\n",
      "Train Epoch: 121 [1600/6658 (24%)]\tLoss: 1.198214\n",
      "Train Epoch: 121 [1700/6658 (26%)]\tLoss: 1.386863\n",
      "Train Epoch: 121 [1800/6658 (27%)]\tLoss: 0.484647\n",
      "Train Epoch: 121 [1900/6658 (29%)]\tLoss: 0.108458\n",
      "Train Epoch: 121 [2000/6658 (30%)]\tLoss: 0.067121\n",
      "Train Epoch: 121 [2100/6658 (32%)]\tLoss: 0.297376\n",
      "Train Epoch: 121 [2200/6658 (33%)]\tLoss: 0.815614\n",
      "Train Epoch: 121 [2300/6658 (35%)]\tLoss: 2.703004\n",
      "Train Epoch: 121 [2400/6658 (36%)]\tLoss: 0.559896\n",
      "Train Epoch: 121 [2500/6658 (38%)]\tLoss: 0.173147\n",
      "Train Epoch: 121 [2600/6658 (39%)]\tLoss: 0.113391\n",
      "Train Epoch: 121 [2700/6658 (41%)]\tLoss: 0.000008\n",
      "Train Epoch: 121 [2800/6658 (42%)]\tLoss: 0.505490\n",
      "Train Epoch: 121 [2900/6658 (44%)]\tLoss: 4.491885\n",
      "Train Epoch: 121 [3000/6658 (45%)]\tLoss: 0.578796\n",
      "Train Epoch: 121 [3100/6658 (47%)]\tLoss: 0.245245\n",
      "Train Epoch: 121 [3200/6658 (48%)]\tLoss: 0.201242\n",
      "Train Epoch: 121 [3300/6658 (50%)]\tLoss: 0.012914\n",
      "Train Epoch: 121 [3400/6658 (51%)]\tLoss: 0.046202\n",
      "Train Epoch: 121 [3500/6658 (53%)]\tLoss: 0.316730\n",
      "Train Epoch: 121 [3600/6658 (54%)]\tLoss: 0.444383\n",
      "Train Epoch: 121 [3700/6658 (56%)]\tLoss: 0.292669\n",
      "Train Epoch: 121 [3800/6658 (57%)]\tLoss: 0.000351\n",
      "Train Epoch: 121 [3900/6658 (59%)]\tLoss: 0.000015\n",
      "Train Epoch: 121 [4000/6658 (60%)]\tLoss: 0.774053\n",
      "Train Epoch: 121 [4100/6658 (62%)]\tLoss: 0.033158\n",
      "Train Epoch: 121 [4200/6658 (63%)]\tLoss: 0.756833\n",
      "Train Epoch: 121 [4300/6658 (65%)]\tLoss: 0.905739\n",
      "Train Epoch: 121 [4400/6658 (66%)]\tLoss: 0.319399\n",
      "Train Epoch: 121 [4500/6658 (68%)]\tLoss: 0.619926\n",
      "Train Epoch: 121 [4600/6658 (69%)]\tLoss: 9.181787\n",
      "Train Epoch: 121 [4700/6658 (71%)]\tLoss: 0.180518\n",
      "Train Epoch: 121 [4800/6658 (72%)]\tLoss: 0.051268\n",
      "Train Epoch: 121 [4900/6658 (74%)]\tLoss: 2.354356\n",
      "Train Epoch: 121 [5000/6658 (75%)]\tLoss: 0.409626\n",
      "Train Epoch: 121 [5100/6658 (77%)]\tLoss: 0.006511\n",
      "Train Epoch: 121 [5200/6658 (78%)]\tLoss: 0.006870\n",
      "Train Epoch: 121 [5300/6658 (80%)]\tLoss: 0.964197\n",
      "Train Epoch: 121 [5400/6658 (81%)]\tLoss: 0.086772\n",
      "Train Epoch: 121 [5500/6658 (83%)]\tLoss: 0.413697\n",
      "Train Epoch: 121 [5600/6658 (84%)]\tLoss: 0.314127\n",
      "Train Epoch: 121 [5700/6658 (86%)]\tLoss: 0.304126\n",
      "Train Epoch: 121 [5800/6658 (87%)]\tLoss: 0.013385\n",
      "Train Epoch: 121 [5900/6658 (89%)]\tLoss: 0.086274\n",
      "Train Epoch: 121 [6000/6658 (90%)]\tLoss: 0.046296\n",
      "Train Epoch: 121 [6100/6658 (92%)]\tLoss: 1.003537\n",
      "Train Epoch: 121 [6200/6658 (93%)]\tLoss: 0.713430\n",
      "Train Epoch: 121 [6300/6658 (95%)]\tLoss: 1.812058\n",
      "Train Epoch: 121 [6400/6658 (96%)]\tLoss: 0.919348\n",
      "Train Epoch: 121 [6500/6658 (98%)]\tLoss: 0.000001\n",
      "Train Epoch: 121 [6600/6658 (99%)]\tLoss: 1.465967\n",
      "train loss average =  0.7178262389256808\n",
      "\n",
      "Test set: Average loss: 0.6926\n",
      "\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2618, 6.0784, 5.8682, 5.8714, 6.1778, 6.0487], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 122 [0/6658 (0%)]\tLoss: 0.180454\n",
      "Train Epoch: 122 [100/6658 (2%)]\tLoss: 1.520298\n",
      "Train Epoch: 122 [200/6658 (3%)]\tLoss: 0.380072\n",
      "Train Epoch: 122 [300/6658 (5%)]\tLoss: 0.667847\n",
      "Train Epoch: 122 [400/6658 (6%)]\tLoss: 0.072258\n",
      "Train Epoch: 122 [500/6658 (8%)]\tLoss: 0.381857\n",
      "Train Epoch: 122 [600/6658 (9%)]\tLoss: 0.566360\n",
      "Train Epoch: 122 [700/6658 (11%)]\tLoss: 0.317567\n",
      "Train Epoch: 122 [800/6658 (12%)]\tLoss: 0.006656\n",
      "Train Epoch: 122 [900/6658 (14%)]\tLoss: 0.286548\n",
      "Train Epoch: 122 [1000/6658 (15%)]\tLoss: 0.774339\n",
      "Train Epoch: 122 [1100/6658 (17%)]\tLoss: 0.126495\n",
      "Train Epoch: 122 [1200/6658 (18%)]\tLoss: 0.201924\n",
      "Train Epoch: 122 [1300/6658 (20%)]\tLoss: 0.590879\n",
      "Train Epoch: 122 [1400/6658 (21%)]\tLoss: 0.787380\n",
      "Train Epoch: 122 [1500/6658 (23%)]\tLoss: 0.334891\n",
      "Train Epoch: 122 [1600/6658 (24%)]\tLoss: 0.025660\n",
      "Train Epoch: 122 [1700/6658 (26%)]\tLoss: 1.399405\n",
      "Train Epoch: 122 [1800/6658 (27%)]\tLoss: 0.803946\n",
      "Train Epoch: 122 [1900/6658 (29%)]\tLoss: 0.037999\n",
      "Train Epoch: 122 [2000/6658 (30%)]\tLoss: 0.144806\n",
      "Train Epoch: 122 [2100/6658 (32%)]\tLoss: 0.082211\n",
      "Train Epoch: 122 [2200/6658 (33%)]\tLoss: 0.124691\n",
      "Train Epoch: 122 [2300/6658 (35%)]\tLoss: 0.710001\n",
      "Train Epoch: 122 [2400/6658 (36%)]\tLoss: 0.945447\n",
      "Train Epoch: 122 [2500/6658 (38%)]\tLoss: 0.056927\n",
      "Train Epoch: 122 [2600/6658 (39%)]\tLoss: 0.029085\n",
      "Train Epoch: 122 [2700/6658 (41%)]\tLoss: 13.346539\n",
      "Train Epoch: 122 [2800/6658 (42%)]\tLoss: 0.629683\n",
      "Train Epoch: 122 [2900/6658 (44%)]\tLoss: 0.270302\n",
      "Train Epoch: 122 [3000/6658 (45%)]\tLoss: 0.492264\n",
      "Train Epoch: 122 [3100/6658 (47%)]\tLoss: 0.714912\n",
      "Train Epoch: 122 [3200/6658 (48%)]\tLoss: 0.022961\n",
      "Train Epoch: 122 [3300/6658 (50%)]\tLoss: 0.062831\n",
      "Train Epoch: 122 [3400/6658 (51%)]\tLoss: 0.008349\n",
      "Train Epoch: 122 [3500/6658 (53%)]\tLoss: 0.015191\n",
      "Train Epoch: 122 [3600/6658 (54%)]\tLoss: 0.282194\n",
      "Train Epoch: 122 [3700/6658 (56%)]\tLoss: 0.270879\n",
      "Train Epoch: 122 [3800/6658 (57%)]\tLoss: 0.675326\n",
      "Train Epoch: 122 [3900/6658 (59%)]\tLoss: 1.920142\n",
      "Train Epoch: 122 [4000/6658 (60%)]\tLoss: 0.002994\n",
      "Train Epoch: 122 [4100/6658 (62%)]\tLoss: 0.503643\n",
      "Train Epoch: 122 [4200/6658 (63%)]\tLoss: 0.443926\n",
      "Train Epoch: 122 [4300/6658 (65%)]\tLoss: 0.381477\n",
      "Train Epoch: 122 [4400/6658 (66%)]\tLoss: 1.018116\n",
      "Train Epoch: 122 [4500/6658 (68%)]\tLoss: 0.506427\n",
      "Train Epoch: 122 [4600/6658 (69%)]\tLoss: 0.839309\n",
      "Train Epoch: 122 [4700/6658 (71%)]\tLoss: 0.017130\n",
      "Train Epoch: 122 [4800/6658 (72%)]\tLoss: 0.259222\n",
      "Train Epoch: 122 [4900/6658 (74%)]\tLoss: 0.076444\n",
      "Train Epoch: 122 [5000/6658 (75%)]\tLoss: 0.000009\n",
      "Train Epoch: 122 [5100/6658 (77%)]\tLoss: 0.047514\n",
      "Train Epoch: 122 [5200/6658 (78%)]\tLoss: 0.008008\n",
      "Train Epoch: 122 [5300/6658 (80%)]\tLoss: 0.000703\n",
      "Train Epoch: 122 [5400/6658 (81%)]\tLoss: 0.438801\n",
      "Train Epoch: 122 [5500/6658 (83%)]\tLoss: 2.643639\n",
      "Train Epoch: 122 [5600/6658 (84%)]\tLoss: 0.500777\n",
      "Train Epoch: 122 [5700/6658 (86%)]\tLoss: 0.036218\n",
      "Train Epoch: 122 [5800/6658 (87%)]\tLoss: 0.713190\n",
      "Train Epoch: 122 [5900/6658 (89%)]\tLoss: 0.687834\n",
      "Train Epoch: 122 [6000/6658 (90%)]\tLoss: 0.286271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 122 [6100/6658 (92%)]\tLoss: 0.867434\n",
      "Train Epoch: 122 [6200/6658 (93%)]\tLoss: 0.882591\n",
      "Train Epoch: 122 [6300/6658 (95%)]\tLoss: 0.002980\n",
      "Train Epoch: 122 [6400/6658 (96%)]\tLoss: 0.112547\n",
      "Train Epoch: 122 [6500/6658 (98%)]\tLoss: 0.707050\n",
      "Train Epoch: 122 [6600/6658 (99%)]\tLoss: 1.035646\n",
      "train loss average =  0.7238324023961318\n",
      "\n",
      "Test set: Average loss: 0.7015\n",
      "\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2639, 6.0792, 5.8680, 5.8711, 6.1795, 6.0486], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 123 [0/6658 (0%)]\tLoss: 0.396730\n",
      "Train Epoch: 123 [100/6658 (2%)]\tLoss: 0.014403\n",
      "Train Epoch: 123 [200/6658 (3%)]\tLoss: 0.038329\n",
      "Train Epoch: 123 [300/6658 (5%)]\tLoss: 0.450057\n",
      "Train Epoch: 123 [400/6658 (6%)]\tLoss: 0.885117\n",
      "Train Epoch: 123 [500/6658 (8%)]\tLoss: 0.004014\n",
      "Train Epoch: 123 [600/6658 (9%)]\tLoss: 0.360092\n",
      "Train Epoch: 123 [700/6658 (11%)]\tLoss: 2.379434\n",
      "Train Epoch: 123 [800/6658 (12%)]\tLoss: 0.001744\n",
      "Train Epoch: 123 [900/6658 (14%)]\tLoss: 0.436153\n",
      "Train Epoch: 123 [1000/6658 (15%)]\tLoss: 1.138511\n",
      "Train Epoch: 123 [1100/6658 (17%)]\tLoss: 0.019043\n",
      "Train Epoch: 123 [1200/6658 (18%)]\tLoss: 0.006409\n",
      "Train Epoch: 123 [1300/6658 (20%)]\tLoss: 0.067766\n",
      "Train Epoch: 123 [1400/6658 (21%)]\tLoss: 0.027414\n",
      "Train Epoch: 123 [1500/6658 (23%)]\tLoss: 1.894015\n",
      "Train Epoch: 123 [1600/6658 (24%)]\tLoss: 0.000445\n",
      "Train Epoch: 123 [1700/6658 (26%)]\tLoss: 0.146542\n",
      "Train Epoch: 123 [1800/6658 (27%)]\tLoss: 0.014243\n",
      "Train Epoch: 123 [1900/6658 (29%)]\tLoss: 0.136257\n",
      "Train Epoch: 123 [2000/6658 (30%)]\tLoss: 5.954455\n",
      "Train Epoch: 123 [2100/6658 (32%)]\tLoss: 1.386004\n",
      "Train Epoch: 123 [2200/6658 (33%)]\tLoss: 0.000935\n",
      "Train Epoch: 123 [2300/6658 (35%)]\tLoss: 0.377871\n",
      "Train Epoch: 123 [2400/6658 (36%)]\tLoss: 0.475026\n",
      "Train Epoch: 123 [2500/6658 (38%)]\tLoss: 2.017457\n",
      "Train Epoch: 123 [2600/6658 (39%)]\tLoss: 0.023341\n",
      "Train Epoch: 123 [2700/6658 (41%)]\tLoss: 0.892352\n",
      "Train Epoch: 123 [2800/6658 (42%)]\tLoss: 0.025197\n",
      "Train Epoch: 123 [2900/6658 (44%)]\tLoss: 0.051882\n",
      "Train Epoch: 123 [3000/6658 (45%)]\tLoss: 1.808836\n",
      "Train Epoch: 123 [3100/6658 (47%)]\tLoss: 0.028726\n",
      "Train Epoch: 123 [3200/6658 (48%)]\tLoss: 0.009257\n",
      "Train Epoch: 123 [3300/6658 (50%)]\tLoss: 0.151360\n",
      "Train Epoch: 123 [3400/6658 (51%)]\tLoss: 2.940368\n",
      "Train Epoch: 123 [3500/6658 (53%)]\tLoss: 0.049895\n",
      "Train Epoch: 123 [3600/6658 (54%)]\tLoss: 0.598392\n",
      "Train Epoch: 123 [3700/6658 (56%)]\tLoss: 3.590152\n",
      "Train Epoch: 123 [3800/6658 (57%)]\tLoss: 1.014409\n",
      "Train Epoch: 123 [3900/6658 (59%)]\tLoss: 0.060902\n",
      "Train Epoch: 123 [4000/6658 (60%)]\tLoss: 2.732677\n",
      "Train Epoch: 123 [4100/6658 (62%)]\tLoss: 5.225503\n",
      "Train Epoch: 123 [4200/6658 (63%)]\tLoss: 0.524467\n",
      "Train Epoch: 123 [4300/6658 (65%)]\tLoss: 0.079016\n",
      "Train Epoch: 123 [4400/6658 (66%)]\tLoss: 0.704551\n",
      "Train Epoch: 123 [4500/6658 (68%)]\tLoss: 0.000770\n",
      "Train Epoch: 123 [4600/6658 (69%)]\tLoss: 0.187360\n",
      "Train Epoch: 123 [4700/6658 (71%)]\tLoss: 0.100557\n",
      "Train Epoch: 123 [4800/6658 (72%)]\tLoss: 0.253084\n",
      "Train Epoch: 123 [4900/6658 (74%)]\tLoss: 0.000179\n",
      "Train Epoch: 123 [5000/6658 (75%)]\tLoss: 0.391245\n",
      "Train Epoch: 123 [5100/6658 (77%)]\tLoss: 0.016855\n",
      "Train Epoch: 123 [5200/6658 (78%)]\tLoss: 0.011922\n",
      "Train Epoch: 123 [5300/6658 (80%)]\tLoss: 3.853831\n",
      "Train Epoch: 123 [5400/6658 (81%)]\tLoss: 0.401484\n",
      "Train Epoch: 123 [5500/6658 (83%)]\tLoss: 0.000054\n",
      "Train Epoch: 123 [5600/6658 (84%)]\tLoss: 0.041055\n",
      "Train Epoch: 123 [5700/6658 (86%)]\tLoss: 0.031270\n",
      "Train Epoch: 123 [5800/6658 (87%)]\tLoss: 0.971988\n",
      "Train Epoch: 123 [5900/6658 (89%)]\tLoss: 0.163411\n",
      "Train Epoch: 123 [6000/6658 (90%)]\tLoss: 0.006071\n",
      "Train Epoch: 123 [6100/6658 (92%)]\tLoss: 0.001470\n",
      "Train Epoch: 123 [6200/6658 (93%)]\tLoss: 0.464146\n",
      "Train Epoch: 123 [6300/6658 (95%)]\tLoss: 0.140301\n",
      "Train Epoch: 123 [6400/6658 (96%)]\tLoss: 0.438223\n",
      "Train Epoch: 123 [6500/6658 (98%)]\tLoss: 0.186239\n",
      "Train Epoch: 123 [6600/6658 (99%)]\tLoss: 0.078516\n",
      "train loss average =  0.7184003074331378\n",
      "\n",
      "Test set: Average loss: 0.6806\n",
      "\n",
      "Validation loss decreased (0.691429 --> 0.680577).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.2652, 6.0798, 5.8669, 5.8704, 6.1807, 6.0487], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 124 [0/6658 (0%)]\tLoss: 0.457731\n",
      "Train Epoch: 124 [100/6658 (2%)]\tLoss: 0.110930\n",
      "Train Epoch: 124 [200/6658 (3%)]\tLoss: 0.335976\n",
      "Train Epoch: 124 [300/6658 (5%)]\tLoss: 0.099280\n",
      "Train Epoch: 124 [400/6658 (6%)]\tLoss: 0.050273\n",
      "Train Epoch: 124 [500/6658 (8%)]\tLoss: 0.149864\n",
      "Train Epoch: 124 [600/6658 (9%)]\tLoss: 0.020163\n",
      "Train Epoch: 124 [700/6658 (11%)]\tLoss: 0.003530\n",
      "Train Epoch: 124 [800/6658 (12%)]\tLoss: 0.272724\n",
      "Train Epoch: 124 [900/6658 (14%)]\tLoss: 1.039713\n",
      "Train Epoch: 124 [1000/6658 (15%)]\tLoss: 0.559322\n",
      "Train Epoch: 124 [1100/6658 (17%)]\tLoss: 0.270203\n",
      "Train Epoch: 124 [1200/6658 (18%)]\tLoss: 0.190184\n",
      "Train Epoch: 124 [1300/6658 (20%)]\tLoss: 0.068387\n",
      "Train Epoch: 124 [1400/6658 (21%)]\tLoss: 0.293483\n",
      "Train Epoch: 124 [1500/6658 (23%)]\tLoss: 0.180571\n",
      "Train Epoch: 124 [1600/6658 (24%)]\tLoss: 0.414856\n",
      "Train Epoch: 124 [1700/6658 (26%)]\tLoss: 0.400063\n",
      "Train Epoch: 124 [1800/6658 (27%)]\tLoss: 1.175430\n",
      "Train Epoch: 124 [1900/6658 (29%)]\tLoss: 1.000516\n",
      "Train Epoch: 124 [2000/6658 (30%)]\tLoss: 0.000144\n",
      "Train Epoch: 124 [2100/6658 (32%)]\tLoss: 0.094661\n",
      "Train Epoch: 124 [2200/6658 (33%)]\tLoss: 0.109813\n",
      "Train Epoch: 124 [2300/6658 (35%)]\tLoss: 0.899118\n",
      "Train Epoch: 124 [2400/6658 (36%)]\tLoss: 0.266745\n",
      "Train Epoch: 124 [2500/6658 (38%)]\tLoss: 0.410194\n",
      "Train Epoch: 124 [2600/6658 (39%)]\tLoss: 0.233758\n",
      "Train Epoch: 124 [2700/6658 (41%)]\tLoss: 1.426873\n",
      "Train Epoch: 124 [2800/6658 (42%)]\tLoss: 1.483347\n",
      "Train Epoch: 124 [2900/6658 (44%)]\tLoss: 0.064833\n",
      "Train Epoch: 124 [3000/6658 (45%)]\tLoss: 0.398851\n",
      "Train Epoch: 124 [3100/6658 (47%)]\tLoss: 3.714713\n",
      "Train Epoch: 124 [3200/6658 (48%)]\tLoss: 0.183560\n",
      "Train Epoch: 124 [3300/6658 (50%)]\tLoss: 1.012246\n",
      "Train Epoch: 124 [3400/6658 (51%)]\tLoss: 0.057519\n",
      "Train Epoch: 124 [3500/6658 (53%)]\tLoss: 0.304996\n",
      "Train Epoch: 124 [3600/6658 (54%)]\tLoss: 0.031049\n",
      "Train Epoch: 124 [3700/6658 (56%)]\tLoss: 0.806293\n",
      "Train Epoch: 124 [3800/6658 (57%)]\tLoss: 3.756638\n",
      "Train Epoch: 124 [3900/6658 (59%)]\tLoss: 0.336259\n",
      "Train Epoch: 124 [4000/6658 (60%)]\tLoss: 0.238107\n",
      "Train Epoch: 124 [4100/6658 (62%)]\tLoss: 0.049052\n",
      "Train Epoch: 124 [4200/6658 (63%)]\tLoss: 0.207329\n",
      "Train Epoch: 124 [4300/6658 (65%)]\tLoss: 0.003824\n",
      "Train Epoch: 124 [4400/6658 (66%)]\tLoss: 0.611609\n",
      "Train Epoch: 124 [4500/6658 (68%)]\tLoss: 0.735608\n",
      "Train Epoch: 124 [4600/6658 (69%)]\tLoss: 0.032176\n",
      "Train Epoch: 124 [4700/6658 (71%)]\tLoss: 0.575606\n",
      "Train Epoch: 124 [4800/6658 (72%)]\tLoss: 0.084666\n",
      "Train Epoch: 124 [4900/6658 (74%)]\tLoss: 0.032014\n",
      "Train Epoch: 124 [5000/6658 (75%)]\tLoss: 0.474224\n",
      "Train Epoch: 124 [5100/6658 (77%)]\tLoss: 0.004052\n",
      "Train Epoch: 124 [5200/6658 (78%)]\tLoss: 0.008603\n",
      "Train Epoch: 124 [5300/6658 (80%)]\tLoss: 0.109642\n",
      "Train Epoch: 124 [5400/6658 (81%)]\tLoss: 0.018939\n",
      "Train Epoch: 124 [5500/6658 (83%)]\tLoss: 0.374336\n",
      "Train Epoch: 124 [5600/6658 (84%)]\tLoss: 0.131643\n",
      "Train Epoch: 124 [5700/6658 (86%)]\tLoss: 0.290386\n",
      "Train Epoch: 124 [5800/6658 (87%)]\tLoss: 0.344464\n",
      "Train Epoch: 124 [5900/6658 (89%)]\tLoss: 0.002867\n",
      "Train Epoch: 124 [6000/6658 (90%)]\tLoss: 0.588775\n",
      "Train Epoch: 124 [6100/6658 (92%)]\tLoss: 1.273380\n",
      "Train Epoch: 124 [6200/6658 (93%)]\tLoss: 0.727057\n",
      "Train Epoch: 124 [6300/6658 (95%)]\tLoss: 2.650312\n",
      "Train Epoch: 124 [6400/6658 (96%)]\tLoss: 1.333505\n",
      "Train Epoch: 124 [6500/6658 (98%)]\tLoss: 0.041169\n",
      "Train Epoch: 124 [6600/6658 (99%)]\tLoss: 0.002991\n",
      "train loss average =  0.7158759020701514\n",
      "\n",
      "Test set: Average loss: 0.6834\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2670, 6.0804, 5.8656, 5.8711, 6.1830, 6.0487], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 125 [0/6658 (0%)]\tLoss: 0.297720\n",
      "Train Epoch: 125 [100/6658 (2%)]\tLoss: 0.002343\n",
      "Train Epoch: 125 [200/6658 (3%)]\tLoss: 0.424429\n",
      "Train Epoch: 125 [300/6658 (5%)]\tLoss: 0.095096\n",
      "Train Epoch: 125 [400/6658 (6%)]\tLoss: 0.039382\n",
      "Train Epoch: 125 [500/6658 (8%)]\tLoss: 0.060282\n",
      "Train Epoch: 125 [600/6658 (9%)]\tLoss: 0.268656\n",
      "Train Epoch: 125 [700/6658 (11%)]\tLoss: 0.296812\n",
      "Train Epoch: 125 [800/6658 (12%)]\tLoss: 0.742668\n",
      "Train Epoch: 125 [900/6658 (14%)]\tLoss: 0.108104\n",
      "Train Epoch: 125 [1000/6658 (15%)]\tLoss: 1.490379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 125 [1100/6658 (17%)]\tLoss: 0.122205\n",
      "Train Epoch: 125 [1200/6658 (18%)]\tLoss: 0.051914\n",
      "Train Epoch: 125 [1300/6658 (20%)]\tLoss: 0.805602\n",
      "Train Epoch: 125 [1400/6658 (21%)]\tLoss: 1.266171\n",
      "Train Epoch: 125 [1500/6658 (23%)]\tLoss: 0.146816\n",
      "Train Epoch: 125 [1600/6658 (24%)]\tLoss: 0.262752\n",
      "Train Epoch: 125 [1700/6658 (26%)]\tLoss: 0.325573\n",
      "Train Epoch: 125 [1800/6658 (27%)]\tLoss: 3.451020\n",
      "Train Epoch: 125 [1900/6658 (29%)]\tLoss: 0.233676\n",
      "Train Epoch: 125 [2000/6658 (30%)]\tLoss: 0.000591\n",
      "Train Epoch: 125 [2100/6658 (32%)]\tLoss: 0.203718\n",
      "Train Epoch: 125 [2200/6658 (33%)]\tLoss: 0.407615\n",
      "Train Epoch: 125 [2300/6658 (35%)]\tLoss: 0.747947\n",
      "Train Epoch: 125 [2400/6658 (36%)]\tLoss: 0.544101\n",
      "Train Epoch: 125 [2500/6658 (38%)]\tLoss: 0.377080\n",
      "Train Epoch: 125 [2600/6658 (39%)]\tLoss: 0.151733\n",
      "Train Epoch: 125 [2700/6658 (41%)]\tLoss: 0.011757\n",
      "Train Epoch: 125 [2800/6658 (42%)]\tLoss: 0.240235\n",
      "Train Epoch: 125 [2900/6658 (44%)]\tLoss: 0.090287\n",
      "Train Epoch: 125 [3000/6658 (45%)]\tLoss: 0.284563\n",
      "Train Epoch: 125 [3100/6658 (47%)]\tLoss: 0.001690\n",
      "Train Epoch: 125 [3200/6658 (48%)]\tLoss: 0.680404\n",
      "Train Epoch: 125 [3300/6658 (50%)]\tLoss: 0.451886\n",
      "Train Epoch: 125 [3400/6658 (51%)]\tLoss: 0.393686\n",
      "Train Epoch: 125 [3500/6658 (53%)]\tLoss: 0.186274\n",
      "Train Epoch: 125 [3600/6658 (54%)]\tLoss: 0.132256\n",
      "Train Epoch: 125 [3700/6658 (56%)]\tLoss: 0.444066\n",
      "Train Epoch: 125 [3800/6658 (57%)]\tLoss: 1.762641\n",
      "Train Epoch: 125 [3900/6658 (59%)]\tLoss: 0.084608\n",
      "Train Epoch: 125 [4000/6658 (60%)]\tLoss: 0.016213\n",
      "Train Epoch: 125 [4100/6658 (62%)]\tLoss: 0.195351\n",
      "Train Epoch: 125 [4200/6658 (63%)]\tLoss: 0.233990\n",
      "Train Epoch: 125 [4300/6658 (65%)]\tLoss: 0.615549\n",
      "Train Epoch: 125 [4400/6658 (66%)]\tLoss: 0.129274\n",
      "Train Epoch: 125 [4500/6658 (68%)]\tLoss: 3.400076\n",
      "Train Epoch: 125 [4600/6658 (69%)]\tLoss: 0.015980\n",
      "Train Epoch: 125 [4700/6658 (71%)]\tLoss: 0.565419\n",
      "Train Epoch: 125 [4800/6658 (72%)]\tLoss: 0.237325\n",
      "Train Epoch: 125 [4900/6658 (74%)]\tLoss: 0.615518\n",
      "Train Epoch: 125 [5000/6658 (75%)]\tLoss: 0.002507\n",
      "Train Epoch: 125 [5100/6658 (77%)]\tLoss: 0.864838\n",
      "Train Epoch: 125 [5200/6658 (78%)]\tLoss: 0.609785\n",
      "Train Epoch: 125 [5300/6658 (80%)]\tLoss: 0.613638\n",
      "Train Epoch: 125 [5400/6658 (81%)]\tLoss: 0.000308\n",
      "Train Epoch: 125 [5500/6658 (83%)]\tLoss: 0.002170\n",
      "Train Epoch: 125 [5600/6658 (84%)]\tLoss: 0.940136\n",
      "Train Epoch: 125 [5700/6658 (86%)]\tLoss: 0.332099\n",
      "Train Epoch: 125 [5800/6658 (87%)]\tLoss: 0.082145\n",
      "Train Epoch: 125 [5900/6658 (89%)]\tLoss: 0.586378\n",
      "Train Epoch: 125 [6000/6658 (90%)]\tLoss: 0.242796\n",
      "Train Epoch: 125 [6100/6658 (92%)]\tLoss: 0.002647\n",
      "Train Epoch: 125 [6200/6658 (93%)]\tLoss: 1.497050\n",
      "Train Epoch: 125 [6300/6658 (95%)]\tLoss: 0.010892\n",
      "Train Epoch: 125 [6400/6658 (96%)]\tLoss: 0.726006\n",
      "Train Epoch: 125 [6500/6658 (98%)]\tLoss: 1.187501\n",
      "Train Epoch: 125 [6600/6658 (99%)]\tLoss: 0.769162\n",
      "train loss average =  0.7136670791075912\n",
      "\n",
      "Test set: Average loss: 0.7026\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2684, 6.0818, 5.8656, 5.8711, 6.1843, 6.0484], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 126 [0/6658 (0%)]\tLoss: 0.014576\n",
      "Train Epoch: 126 [100/6658 (2%)]\tLoss: 0.937005\n",
      "Train Epoch: 126 [200/6658 (3%)]\tLoss: 0.091173\n",
      "Train Epoch: 126 [300/6658 (5%)]\tLoss: 0.098871\n",
      "Train Epoch: 126 [400/6658 (6%)]\tLoss: 0.074476\n",
      "Train Epoch: 126 [500/6658 (8%)]\tLoss: 0.001059\n",
      "Train Epoch: 126 [600/6658 (9%)]\tLoss: 3.596342\n",
      "Train Epoch: 126 [700/6658 (11%)]\tLoss: 0.601726\n",
      "Train Epoch: 126 [800/6658 (12%)]\tLoss: 0.071471\n",
      "Train Epoch: 126 [900/6658 (14%)]\tLoss: 0.685914\n",
      "Train Epoch: 126 [1000/6658 (15%)]\tLoss: 2.838957\n",
      "Train Epoch: 126 [1100/6658 (17%)]\tLoss: 0.276493\n",
      "Train Epoch: 126 [1200/6658 (18%)]\tLoss: 2.328304\n",
      "Train Epoch: 126 [1300/6658 (20%)]\tLoss: 0.257971\n",
      "Train Epoch: 126 [1400/6658 (21%)]\tLoss: 1.030360\n",
      "Train Epoch: 126 [1500/6658 (23%)]\tLoss: 1.610011\n",
      "Train Epoch: 126 [1600/6658 (24%)]\tLoss: 0.297319\n",
      "Train Epoch: 126 [1700/6658 (26%)]\tLoss: 0.249903\n",
      "Train Epoch: 126 [1800/6658 (27%)]\tLoss: 1.378123\n",
      "Train Epoch: 126 [1900/6658 (29%)]\tLoss: 0.139756\n",
      "Train Epoch: 126 [2000/6658 (30%)]\tLoss: 0.000001\n",
      "Train Epoch: 126 [2100/6658 (32%)]\tLoss: 0.506173\n",
      "Train Epoch: 126 [2200/6658 (33%)]\tLoss: 0.005708\n",
      "Train Epoch: 126 [2300/6658 (35%)]\tLoss: 0.461595\n",
      "Train Epoch: 126 [2400/6658 (36%)]\tLoss: 0.010294\n",
      "Train Epoch: 126 [2500/6658 (38%)]\tLoss: 0.250171\n",
      "Train Epoch: 126 [2600/6658 (39%)]\tLoss: 0.635491\n",
      "Train Epoch: 126 [2700/6658 (41%)]\tLoss: 0.000157\n",
      "Train Epoch: 126 [2800/6658 (42%)]\tLoss: 0.526181\n",
      "Train Epoch: 126 [2900/6658 (44%)]\tLoss: 0.029567\n",
      "Train Epoch: 126 [3000/6658 (45%)]\tLoss: 0.014672\n",
      "Train Epoch: 126 [3100/6658 (47%)]\tLoss: 0.011701\n",
      "Train Epoch: 126 [3200/6658 (48%)]\tLoss: 0.044726\n",
      "Train Epoch: 126 [3300/6658 (50%)]\tLoss: 0.566705\n",
      "Train Epoch: 126 [3400/6658 (51%)]\tLoss: 0.034171\n",
      "Train Epoch: 126 [3500/6658 (53%)]\tLoss: 0.047222\n",
      "Train Epoch: 126 [3600/6658 (54%)]\tLoss: 1.349910\n",
      "Train Epoch: 126 [3700/6658 (56%)]\tLoss: 0.186472\n",
      "Train Epoch: 126 [3800/6658 (57%)]\tLoss: 0.009052\n",
      "Train Epoch: 126 [3900/6658 (59%)]\tLoss: 0.004400\n",
      "Train Epoch: 126 [4000/6658 (60%)]\tLoss: 0.486073\n",
      "Train Epoch: 126 [4100/6658 (62%)]\tLoss: 1.377979\n",
      "Train Epoch: 126 [4200/6658 (63%)]\tLoss: 0.142015\n",
      "Train Epoch: 126 [4300/6658 (65%)]\tLoss: 0.016694\n",
      "Train Epoch: 126 [4400/6658 (66%)]\tLoss: 0.329847\n",
      "Train Epoch: 126 [4500/6658 (68%)]\tLoss: 0.417200\n",
      "Train Epoch: 126 [4600/6658 (69%)]\tLoss: 2.672668\n",
      "Train Epoch: 126 [4700/6658 (71%)]\tLoss: 0.001257\n",
      "Train Epoch: 126 [4800/6658 (72%)]\tLoss: 0.090339\n",
      "Train Epoch: 126 [4900/6658 (74%)]\tLoss: 2.227524\n",
      "Train Epoch: 126 [5000/6658 (75%)]\tLoss: 0.179596\n",
      "Train Epoch: 126 [5100/6658 (77%)]\tLoss: 0.136479\n",
      "Train Epoch: 126 [5200/6658 (78%)]\tLoss: 0.046562\n",
      "Train Epoch: 126 [5300/6658 (80%)]\tLoss: 1.303774\n",
      "Train Epoch: 126 [5400/6658 (81%)]\tLoss: 0.696230\n",
      "Train Epoch: 126 [5500/6658 (83%)]\tLoss: 0.656775\n",
      "Train Epoch: 126 [5600/6658 (84%)]\tLoss: 0.137274\n",
      "Train Epoch: 126 [5700/6658 (86%)]\tLoss: 0.113915\n",
      "Train Epoch: 126 [5800/6658 (87%)]\tLoss: 0.213824\n",
      "Train Epoch: 126 [5900/6658 (89%)]\tLoss: 0.776263\n",
      "Train Epoch: 126 [6000/6658 (90%)]\tLoss: 0.011345\n",
      "Train Epoch: 126 [6100/6658 (92%)]\tLoss: 0.027499\n",
      "Train Epoch: 126 [6200/6658 (93%)]\tLoss: 0.015124\n",
      "Train Epoch: 126 [6300/6658 (95%)]\tLoss: 0.031347\n",
      "Train Epoch: 126 [6400/6658 (96%)]\tLoss: 0.726854\n",
      "Train Epoch: 126 [6500/6658 (98%)]\tLoss: 3.078956\n",
      "Train Epoch: 126 [6600/6658 (99%)]\tLoss: 0.633525\n",
      "train loss average =  0.7234267833902972\n",
      "\n",
      "Test set: Average loss: 0.7088\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2714, 6.0822, 5.8648, 5.8707, 6.1865, 6.0484], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 127 [0/6658 (0%)]\tLoss: 0.005945\n",
      "Train Epoch: 127 [100/6658 (2%)]\tLoss: 0.038295\n",
      "Train Epoch: 127 [200/6658 (3%)]\tLoss: 0.002617\n",
      "Train Epoch: 127 [300/6658 (5%)]\tLoss: 0.249650\n",
      "Train Epoch: 127 [400/6658 (6%)]\tLoss: 1.654341\n",
      "Train Epoch: 127 [500/6658 (8%)]\tLoss: 0.009519\n",
      "Train Epoch: 127 [600/6658 (9%)]\tLoss: 0.712866\n",
      "Train Epoch: 127 [700/6658 (11%)]\tLoss: 0.016412\n",
      "Train Epoch: 127 [800/6658 (12%)]\tLoss: 0.013520\n",
      "Train Epoch: 127 [900/6658 (14%)]\tLoss: 0.238971\n",
      "Train Epoch: 127 [1000/6658 (15%)]\tLoss: 0.224256\n",
      "Train Epoch: 127 [1100/6658 (17%)]\tLoss: 0.108775\n",
      "Train Epoch: 127 [1200/6658 (18%)]\tLoss: 0.084950\n",
      "Train Epoch: 127 [1300/6658 (20%)]\tLoss: 0.018012\n",
      "Train Epoch: 127 [1400/6658 (21%)]\tLoss: 0.816822\n",
      "Train Epoch: 127 [1500/6658 (23%)]\tLoss: 0.001689\n",
      "Train Epoch: 127 [1600/6658 (24%)]\tLoss: 0.000378\n",
      "Train Epoch: 127 [1700/6658 (26%)]\tLoss: 0.362055\n",
      "Train Epoch: 127 [1800/6658 (27%)]\tLoss: 0.012101\n",
      "Train Epoch: 127 [1900/6658 (29%)]\tLoss: 0.056115\n",
      "Train Epoch: 127 [2000/6658 (30%)]\tLoss: 0.143507\n",
      "Train Epoch: 127 [2100/6658 (32%)]\tLoss: 0.265891\n",
      "Train Epoch: 127 [2200/6658 (33%)]\tLoss: 0.234279\n",
      "Train Epoch: 127 [2300/6658 (35%)]\tLoss: 0.002906\n",
      "Train Epoch: 127 [2400/6658 (36%)]\tLoss: 0.111639\n",
      "Train Epoch: 127 [2500/6658 (38%)]\tLoss: 0.128575\n",
      "Train Epoch: 127 [2600/6658 (39%)]\tLoss: 0.299194\n",
      "Train Epoch: 127 [2700/6658 (41%)]\tLoss: 0.182486\n",
      "Train Epoch: 127 [2800/6658 (42%)]\tLoss: 0.574673\n",
      "Train Epoch: 127 [2900/6658 (44%)]\tLoss: 3.261996\n",
      "Train Epoch: 127 [3000/6658 (45%)]\tLoss: 0.026800\n",
      "Train Epoch: 127 [3100/6658 (47%)]\tLoss: 0.045833\n",
      "Train Epoch: 127 [3200/6658 (48%)]\tLoss: 0.867345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 127 [3300/6658 (50%)]\tLoss: 0.374498\n",
      "Train Epoch: 127 [3400/6658 (51%)]\tLoss: 0.156464\n",
      "Train Epoch: 127 [3500/6658 (53%)]\tLoss: 0.482508\n",
      "Train Epoch: 127 [3600/6658 (54%)]\tLoss: 0.330689\n",
      "Train Epoch: 127 [3700/6658 (56%)]\tLoss: 2.917871\n",
      "Train Epoch: 127 [3800/6658 (57%)]\tLoss: 0.004359\n",
      "Train Epoch: 127 [3900/6658 (59%)]\tLoss: 0.002766\n",
      "Train Epoch: 127 [4000/6658 (60%)]\tLoss: 2.432419\n",
      "Train Epoch: 127 [4100/6658 (62%)]\tLoss: 0.515564\n",
      "Train Epoch: 127 [4200/6658 (63%)]\tLoss: 1.107030\n",
      "Train Epoch: 127 [4300/6658 (65%)]\tLoss: 3.843211\n",
      "Train Epoch: 127 [4400/6658 (66%)]\tLoss: 3.850039\n",
      "Train Epoch: 127 [4500/6658 (68%)]\tLoss: 0.147286\n",
      "Train Epoch: 127 [4600/6658 (69%)]\tLoss: 0.072357\n",
      "Train Epoch: 127 [4700/6658 (71%)]\tLoss: 0.230325\n",
      "Train Epoch: 127 [4800/6658 (72%)]\tLoss: 1.143746\n",
      "Train Epoch: 127 [4900/6658 (74%)]\tLoss: 1.132827\n",
      "Train Epoch: 127 [5000/6658 (75%)]\tLoss: 0.090867\n",
      "Train Epoch: 127 [5100/6658 (77%)]\tLoss: 0.092435\n",
      "Train Epoch: 127 [5200/6658 (78%)]\tLoss: 0.086849\n",
      "Train Epoch: 127 [5300/6658 (80%)]\tLoss: 0.000062\n",
      "Train Epoch: 127 [5400/6658 (81%)]\tLoss: 0.014458\n",
      "Train Epoch: 127 [5500/6658 (83%)]\tLoss: 0.150713\n",
      "Train Epoch: 127 [5600/6658 (84%)]\tLoss: 9.560678\n",
      "Train Epoch: 127 [5700/6658 (86%)]\tLoss: 0.008113\n",
      "Train Epoch: 127 [5800/6658 (87%)]\tLoss: 0.263079\n",
      "Train Epoch: 127 [5900/6658 (89%)]\tLoss: 0.244938\n",
      "Train Epoch: 127 [6000/6658 (90%)]\tLoss: 1.840858\n",
      "Train Epoch: 127 [6100/6658 (92%)]\tLoss: 0.206115\n",
      "Train Epoch: 127 [6200/6658 (93%)]\tLoss: 0.813723\n",
      "Train Epoch: 127 [6300/6658 (95%)]\tLoss: 0.584724\n",
      "Train Epoch: 127 [6400/6658 (96%)]\tLoss: 2.071896\n",
      "Train Epoch: 127 [6500/6658 (98%)]\tLoss: 0.381208\n",
      "Train Epoch: 127 [6600/6658 (99%)]\tLoss: 0.580186\n",
      "train loss average =  0.7220254178823564\n",
      "\n",
      "Test set: Average loss: 0.7041\n",
      "\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2724, 6.0830, 5.8647, 5.8712, 6.1876, 6.0482], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 128 [0/6658 (0%)]\tLoss: 1.670905\n",
      "Train Epoch: 128 [100/6658 (2%)]\tLoss: 0.410701\n",
      "Train Epoch: 128 [200/6658 (3%)]\tLoss: 0.044819\n",
      "Train Epoch: 128 [300/6658 (5%)]\tLoss: 0.064661\n",
      "Train Epoch: 128 [400/6658 (6%)]\tLoss: 0.693564\n",
      "Train Epoch: 128 [500/6658 (8%)]\tLoss: 21.126085\n",
      "Train Epoch: 128 [600/6658 (9%)]\tLoss: 0.341792\n",
      "Train Epoch: 128 [700/6658 (11%)]\tLoss: 0.792325\n",
      "Train Epoch: 128 [800/6658 (12%)]\tLoss: 0.077604\n",
      "Train Epoch: 128 [900/6658 (14%)]\tLoss: 0.073683\n",
      "Train Epoch: 128 [1000/6658 (15%)]\tLoss: 0.000546\n",
      "Train Epoch: 128 [1100/6658 (17%)]\tLoss: 0.130849\n",
      "Train Epoch: 128 [1200/6658 (18%)]\tLoss: 0.534360\n",
      "Train Epoch: 128 [1300/6658 (20%)]\tLoss: 1.649547\n",
      "Train Epoch: 128 [1400/6658 (21%)]\tLoss: 0.349706\n",
      "Train Epoch: 128 [1500/6658 (23%)]\tLoss: 0.068539\n",
      "Train Epoch: 128 [1600/6658 (24%)]\tLoss: 0.113242\n",
      "Train Epoch: 128 [1700/6658 (26%)]\tLoss: 0.125780\n",
      "Train Epoch: 128 [1800/6658 (27%)]\tLoss: 0.006369\n",
      "Train Epoch: 128 [1900/6658 (29%)]\tLoss: 0.143587\n",
      "Train Epoch: 128 [2000/6658 (30%)]\tLoss: 0.624476\n",
      "Train Epoch: 128 [2100/6658 (32%)]\tLoss: 0.234233\n",
      "Train Epoch: 128 [2200/6658 (33%)]\tLoss: 1.872563\n",
      "Train Epoch: 128 [2300/6658 (35%)]\tLoss: 0.048871\n",
      "Train Epoch: 128 [2400/6658 (36%)]\tLoss: 0.232083\n",
      "Train Epoch: 128 [2500/6658 (38%)]\tLoss: 2.511200\n",
      "Train Epoch: 128 [2600/6658 (39%)]\tLoss: 0.381789\n",
      "Train Epoch: 128 [2700/6658 (41%)]\tLoss: 0.044301\n",
      "Train Epoch: 128 [2800/6658 (42%)]\tLoss: 0.186776\n",
      "Train Epoch: 128 [2900/6658 (44%)]\tLoss: 0.054581\n",
      "Train Epoch: 128 [3000/6658 (45%)]\tLoss: 0.492127\n",
      "Train Epoch: 128 [3100/6658 (47%)]\tLoss: 0.635615\n",
      "Train Epoch: 128 [3200/6658 (48%)]\tLoss: 0.182386\n",
      "Train Epoch: 128 [3300/6658 (50%)]\tLoss: 0.022026\n",
      "Train Epoch: 128 [3400/6658 (51%)]\tLoss: 0.210212\n",
      "Train Epoch: 128 [3500/6658 (53%)]\tLoss: 4.027807\n",
      "Train Epoch: 128 [3600/6658 (54%)]\tLoss: 0.154330\n",
      "Train Epoch: 128 [3700/6658 (56%)]\tLoss: 0.032427\n",
      "Train Epoch: 128 [3800/6658 (57%)]\tLoss: 0.020649\n",
      "Train Epoch: 128 [3900/6658 (59%)]\tLoss: 0.180421\n",
      "Train Epoch: 128 [4000/6658 (60%)]\tLoss: 0.009268\n",
      "Train Epoch: 128 [4100/6658 (62%)]\tLoss: 0.283759\n",
      "Train Epoch: 128 [4200/6658 (63%)]\tLoss: 1.071805\n",
      "Train Epoch: 128 [4300/6658 (65%)]\tLoss: 4.511878\n",
      "Train Epoch: 128 [4400/6658 (66%)]\tLoss: 0.007645\n",
      "Train Epoch: 128 [4500/6658 (68%)]\tLoss: 0.100016\n",
      "Train Epoch: 128 [4600/6658 (69%)]\tLoss: 0.008807\n",
      "Train Epoch: 128 [4700/6658 (71%)]\tLoss: 0.914617\n",
      "Train Epoch: 128 [4800/6658 (72%)]\tLoss: 0.115434\n",
      "Train Epoch: 128 [4900/6658 (74%)]\tLoss: 0.053537\n",
      "Train Epoch: 128 [5000/6658 (75%)]\tLoss: 0.872428\n",
      "Train Epoch: 128 [5100/6658 (77%)]\tLoss: 0.188359\n",
      "Train Epoch: 128 [5200/6658 (78%)]\tLoss: 0.392648\n",
      "Train Epoch: 128 [5300/6658 (80%)]\tLoss: 2.585612\n",
      "Train Epoch: 128 [5400/6658 (81%)]\tLoss: 1.109265\n",
      "Train Epoch: 128 [5500/6658 (83%)]\tLoss: 0.707138\n",
      "Train Epoch: 128 [5600/6658 (84%)]\tLoss: 0.025499\n",
      "Train Epoch: 128 [5700/6658 (86%)]\tLoss: 0.571665\n",
      "Train Epoch: 128 [5800/6658 (87%)]\tLoss: 0.242307\n",
      "Train Epoch: 128 [5900/6658 (89%)]\tLoss: 0.351329\n",
      "Train Epoch: 128 [6000/6658 (90%)]\tLoss: 0.000147\n",
      "Train Epoch: 128 [6100/6658 (92%)]\tLoss: 0.506690\n",
      "Train Epoch: 128 [6200/6658 (93%)]\tLoss: 0.003129\n",
      "Train Epoch: 128 [6300/6658 (95%)]\tLoss: 2.464948\n",
      "Train Epoch: 128 [6400/6658 (96%)]\tLoss: 0.061348\n",
      "Train Epoch: 128 [6500/6658 (98%)]\tLoss: 0.144258\n",
      "Train Epoch: 128 [6600/6658 (99%)]\tLoss: 0.066022\n",
      "train loss average =  0.7147518152723807\n",
      "\n",
      "Test set: Average loss: 0.6996\n",
      "\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2741, 6.0840, 5.8629, 5.8710, 6.1897, 6.0483], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 129 [0/6658 (0%)]\tLoss: 0.166259\n",
      "Train Epoch: 129 [100/6658 (2%)]\tLoss: 0.141982\n",
      "Train Epoch: 129 [200/6658 (3%)]\tLoss: 0.909523\n",
      "Train Epoch: 129 [300/6658 (5%)]\tLoss: 0.042504\n",
      "Train Epoch: 129 [400/6658 (6%)]\tLoss: 0.069429\n",
      "Train Epoch: 129 [500/6658 (8%)]\tLoss: 0.669318\n",
      "Train Epoch: 129 [600/6658 (9%)]\tLoss: 0.262861\n",
      "Train Epoch: 129 [700/6658 (11%)]\tLoss: 0.081128\n",
      "Train Epoch: 129 [800/6658 (12%)]\tLoss: 4.925348\n",
      "Train Epoch: 129 [900/6658 (14%)]\tLoss: 0.576646\n",
      "Train Epoch: 129 [1000/6658 (15%)]\tLoss: 0.042827\n",
      "Train Epoch: 129 [1100/6658 (17%)]\tLoss: 0.754212\n",
      "Train Epoch: 129 [1200/6658 (18%)]\tLoss: 0.013292\n",
      "Train Epoch: 129 [1300/6658 (20%)]\tLoss: 0.544283\n",
      "Train Epoch: 129 [1400/6658 (21%)]\tLoss: 1.220568\n",
      "Train Epoch: 129 [1500/6658 (23%)]\tLoss: 1.299232\n",
      "Train Epoch: 129 [1600/6658 (24%)]\tLoss: 0.201712\n",
      "Train Epoch: 129 [1700/6658 (26%)]\tLoss: 0.008830\n",
      "Train Epoch: 129 [1800/6658 (27%)]\tLoss: 0.015089\n",
      "Train Epoch: 129 [1900/6658 (29%)]\tLoss: 0.415430\n",
      "Train Epoch: 129 [2000/6658 (30%)]\tLoss: 0.104944\n",
      "Train Epoch: 129 [2100/6658 (32%)]\tLoss: 0.002505\n",
      "Train Epoch: 129 [2200/6658 (33%)]\tLoss: 0.212945\n",
      "Train Epoch: 129 [2300/6658 (35%)]\tLoss: 0.008834\n",
      "Train Epoch: 129 [2400/6658 (36%)]\tLoss: 1.872461\n",
      "Train Epoch: 129 [2500/6658 (38%)]\tLoss: 0.033219\n",
      "Train Epoch: 129 [2600/6658 (39%)]\tLoss: 0.062842\n",
      "Train Epoch: 129 [2700/6658 (41%)]\tLoss: 0.547401\n",
      "Train Epoch: 129 [2800/6658 (42%)]\tLoss: 0.968897\n",
      "Train Epoch: 129 [2900/6658 (44%)]\tLoss: 0.670896\n",
      "Train Epoch: 129 [3000/6658 (45%)]\tLoss: 0.008331\n",
      "Train Epoch: 129 [3100/6658 (47%)]\tLoss: 0.530454\n",
      "Train Epoch: 129 [3200/6658 (48%)]\tLoss: 0.662817\n",
      "Train Epoch: 129 [3300/6658 (50%)]\tLoss: 0.007662\n",
      "Train Epoch: 129 [3400/6658 (51%)]\tLoss: 0.943527\n",
      "Train Epoch: 129 [3500/6658 (53%)]\tLoss: 4.853426\n",
      "Train Epoch: 129 [3600/6658 (54%)]\tLoss: 0.152378\n",
      "Train Epoch: 129 [3700/6658 (56%)]\tLoss: 1.333186\n",
      "Train Epoch: 129 [3800/6658 (57%)]\tLoss: 0.864594\n",
      "Train Epoch: 129 [3900/6658 (59%)]\tLoss: 0.784219\n",
      "Train Epoch: 129 [4000/6658 (60%)]\tLoss: 0.002206\n",
      "Train Epoch: 129 [4100/6658 (62%)]\tLoss: 0.182966\n",
      "Train Epoch: 129 [4200/6658 (63%)]\tLoss: 0.669395\n",
      "Train Epoch: 129 [4300/6658 (65%)]\tLoss: 0.524815\n",
      "Train Epoch: 129 [4400/6658 (66%)]\tLoss: 0.043386\n",
      "Train Epoch: 129 [4500/6658 (68%)]\tLoss: 2.244332\n",
      "Train Epoch: 129 [4600/6658 (69%)]\tLoss: 0.702925\n",
      "Train Epoch: 129 [4700/6658 (71%)]\tLoss: 0.008413\n",
      "Train Epoch: 129 [4800/6658 (72%)]\tLoss: 0.288115\n",
      "Train Epoch: 129 [4900/6658 (74%)]\tLoss: 0.003793\n",
      "Train Epoch: 129 [5000/6658 (75%)]\tLoss: 0.298965\n",
      "Train Epoch: 129 [5100/6658 (77%)]\tLoss: 0.440975\n",
      "Train Epoch: 129 [5200/6658 (78%)]\tLoss: 0.006033\n",
      "Train Epoch: 129 [5300/6658 (80%)]\tLoss: 0.010968\n",
      "Train Epoch: 129 [5400/6658 (81%)]\tLoss: 2.457564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 129 [5500/6658 (83%)]\tLoss: 0.374928\n",
      "Train Epoch: 129 [5600/6658 (84%)]\tLoss: 0.061457\n",
      "Train Epoch: 129 [5700/6658 (86%)]\tLoss: 0.000706\n",
      "Train Epoch: 129 [5800/6658 (87%)]\tLoss: 0.758650\n",
      "Train Epoch: 129 [5900/6658 (89%)]\tLoss: 0.876656\n",
      "Train Epoch: 129 [6000/6658 (90%)]\tLoss: 1.101319\n",
      "Train Epoch: 129 [6100/6658 (92%)]\tLoss: 0.452653\n",
      "Train Epoch: 129 [6200/6658 (93%)]\tLoss: 0.618433\n",
      "Train Epoch: 129 [6300/6658 (95%)]\tLoss: 0.679459\n",
      "Train Epoch: 129 [6400/6658 (96%)]\tLoss: 3.013656\n",
      "Train Epoch: 129 [6500/6658 (98%)]\tLoss: 0.357725\n",
      "Train Epoch: 129 [6600/6658 (99%)]\tLoss: 0.009688\n",
      "train loss average =  0.70968277610221\n",
      "\n",
      "Test set: Average loss: 0.7012\n",
      "\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2759, 6.0851, 5.8633, 5.8702, 6.1913, 6.0484], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 130 [0/6658 (0%)]\tLoss: 0.029933\n",
      "Train Epoch: 130 [100/6658 (2%)]\tLoss: 0.811964\n",
      "Train Epoch: 130 [200/6658 (3%)]\tLoss: 0.319304\n",
      "Train Epoch: 130 [300/6658 (5%)]\tLoss: 0.597242\n",
      "Train Epoch: 130 [400/6658 (6%)]\tLoss: 0.009227\n",
      "Train Epoch: 130 [500/6658 (8%)]\tLoss: 0.239766\n",
      "Train Epoch: 130 [600/6658 (9%)]\tLoss: 0.070235\n",
      "Train Epoch: 130 [700/6658 (11%)]\tLoss: 0.155078\n",
      "Train Epoch: 130 [800/6658 (12%)]\tLoss: 0.076057\n",
      "Train Epoch: 130 [900/6658 (14%)]\tLoss: 0.152304\n",
      "Train Epoch: 130 [1000/6658 (15%)]\tLoss: 0.076472\n",
      "Train Epoch: 130 [1100/6658 (17%)]\tLoss: 1.079069\n",
      "Train Epoch: 130 [1200/6658 (18%)]\tLoss: 0.638901\n",
      "Train Epoch: 130 [1300/6658 (20%)]\tLoss: 0.335584\n",
      "Train Epoch: 130 [1400/6658 (21%)]\tLoss: 0.121282\n",
      "Train Epoch: 130 [1500/6658 (23%)]\tLoss: 12.058908\n",
      "Train Epoch: 130 [1600/6658 (24%)]\tLoss: 0.003305\n",
      "Train Epoch: 130 [1700/6658 (26%)]\tLoss: 0.620407\n",
      "Train Epoch: 130 [1800/6658 (27%)]\tLoss: 0.021280\n",
      "Train Epoch: 130 [1900/6658 (29%)]\tLoss: 0.085164\n",
      "Train Epoch: 130 [2000/6658 (30%)]\tLoss: 0.627275\n",
      "Train Epoch: 130 [2100/6658 (32%)]\tLoss: 0.000623\n",
      "Train Epoch: 130 [2200/6658 (33%)]\tLoss: 0.160532\n",
      "Train Epoch: 130 [2300/6658 (35%)]\tLoss: 0.812930\n",
      "Train Epoch: 130 [2400/6658 (36%)]\tLoss: 0.024195\n",
      "Train Epoch: 130 [2500/6658 (38%)]\tLoss: 0.123214\n",
      "Train Epoch: 130 [2600/6658 (39%)]\tLoss: 0.121070\n",
      "Train Epoch: 130 [2700/6658 (41%)]\tLoss: 0.052239\n",
      "Train Epoch: 130 [2800/6658 (42%)]\tLoss: 0.341448\n",
      "Train Epoch: 130 [2900/6658 (44%)]\tLoss: 0.079221\n",
      "Train Epoch: 130 [3000/6658 (45%)]\tLoss: 0.043691\n",
      "Train Epoch: 130 [3100/6658 (47%)]\tLoss: 0.592344\n",
      "Train Epoch: 130 [3200/6658 (48%)]\tLoss: 0.008386\n",
      "Train Epoch: 130 [3300/6658 (50%)]\tLoss: 0.043602\n",
      "Train Epoch: 130 [3400/6658 (51%)]\tLoss: 6.603282\n",
      "Train Epoch: 130 [3500/6658 (53%)]\tLoss: 0.110100\n",
      "Train Epoch: 130 [3600/6658 (54%)]\tLoss: 0.627707\n",
      "Train Epoch: 130 [3700/6658 (56%)]\tLoss: 0.027668\n",
      "Train Epoch: 130 [3800/6658 (57%)]\tLoss: 0.179360\n",
      "Train Epoch: 130 [3900/6658 (59%)]\tLoss: 0.791013\n",
      "Train Epoch: 130 [4000/6658 (60%)]\tLoss: 0.856705\n",
      "Train Epoch: 130 [4100/6658 (62%)]\tLoss: 0.383082\n",
      "Train Epoch: 130 [4200/6658 (63%)]\tLoss: 2.161899\n",
      "Train Epoch: 130 [4300/6658 (65%)]\tLoss: 0.743848\n",
      "Train Epoch: 130 [4400/6658 (66%)]\tLoss: 1.520170\n",
      "Train Epoch: 130 [4500/6658 (68%)]\tLoss: 0.011357\n",
      "Train Epoch: 130 [4600/6658 (69%)]\tLoss: 0.487169\n",
      "Train Epoch: 130 [4700/6658 (71%)]\tLoss: 0.189597\n",
      "Train Epoch: 130 [4800/6658 (72%)]\tLoss: 0.094978\n",
      "Train Epoch: 130 [4900/6658 (74%)]\tLoss: 0.165317\n",
      "Train Epoch: 130 [5000/6658 (75%)]\tLoss: 0.420637\n",
      "Train Epoch: 130 [5100/6658 (77%)]\tLoss: 0.418501\n",
      "Train Epoch: 130 [5200/6658 (78%)]\tLoss: 0.530324\n",
      "Train Epoch: 130 [5300/6658 (80%)]\tLoss: 0.010893\n",
      "Train Epoch: 130 [5400/6658 (81%)]\tLoss: 2.010320\n",
      "Train Epoch: 130 [5500/6658 (83%)]\tLoss: 0.326311\n",
      "Train Epoch: 130 [5600/6658 (84%)]\tLoss: 0.008515\n",
      "Train Epoch: 130 [5700/6658 (86%)]\tLoss: 0.748075\n",
      "Train Epoch: 130 [5800/6658 (87%)]\tLoss: 0.095137\n",
      "Train Epoch: 130 [5900/6658 (89%)]\tLoss: 0.002179\n",
      "Train Epoch: 130 [6000/6658 (90%)]\tLoss: 0.139787\n",
      "Train Epoch: 130 [6100/6658 (92%)]\tLoss: 0.115290\n",
      "Train Epoch: 130 [6200/6658 (93%)]\tLoss: 0.267517\n",
      "Train Epoch: 130 [6300/6658 (95%)]\tLoss: 0.003860\n",
      "Train Epoch: 130 [6400/6658 (96%)]\tLoss: 0.501753\n",
      "Train Epoch: 130 [6500/6658 (98%)]\tLoss: 0.005840\n",
      "Train Epoch: 130 [6600/6658 (99%)]\tLoss: 0.918575\n",
      "train loss average =  0.7155849801090914\n",
      "\n",
      "Test set: Average loss: 0.6945\n",
      "\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2785, 6.0861, 5.8635, 5.8708, 6.1926, 6.0484], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 131 [0/6658 (0%)]\tLoss: 0.199015\n",
      "Train Epoch: 131 [100/6658 (2%)]\tLoss: 1.068882\n",
      "Train Epoch: 131 [200/6658 (3%)]\tLoss: 0.000080\n",
      "Train Epoch: 131 [300/6658 (5%)]\tLoss: 0.190834\n",
      "Train Epoch: 131 [400/6658 (6%)]\tLoss: 0.123673\n",
      "Train Epoch: 131 [500/6658 (8%)]\tLoss: 0.026117\n",
      "Train Epoch: 131 [600/6658 (9%)]\tLoss: 0.006211\n",
      "Train Epoch: 131 [700/6658 (11%)]\tLoss: 1.589294\n",
      "Train Epoch: 131 [800/6658 (12%)]\tLoss: 0.386370\n",
      "Train Epoch: 131 [900/6658 (14%)]\tLoss: 0.419248\n",
      "Train Epoch: 131 [1000/6658 (15%)]\tLoss: 1.971902\n",
      "Train Epoch: 131 [1100/6658 (17%)]\tLoss: 1.430531\n",
      "Train Epoch: 131 [1200/6658 (18%)]\tLoss: 0.443870\n",
      "Train Epoch: 131 [1300/6658 (20%)]\tLoss: 0.810330\n",
      "Train Epoch: 131 [1400/6658 (21%)]\tLoss: 0.248561\n",
      "Train Epoch: 131 [1500/6658 (23%)]\tLoss: 0.019700\n",
      "Train Epoch: 131 [1600/6658 (24%)]\tLoss: 0.134573\n",
      "Train Epoch: 131 [1700/6658 (26%)]\tLoss: 0.163156\n",
      "Train Epoch: 131 [1800/6658 (27%)]\tLoss: 0.759977\n",
      "Train Epoch: 131 [1900/6658 (29%)]\tLoss: 0.097440\n",
      "Train Epoch: 131 [2000/6658 (30%)]\tLoss: 0.115952\n",
      "Train Epoch: 131 [2100/6658 (32%)]\tLoss: 0.069485\n",
      "Train Epoch: 131 [2200/6658 (33%)]\tLoss: 0.092580\n",
      "Train Epoch: 131 [2300/6658 (35%)]\tLoss: 1.605031\n",
      "Train Epoch: 131 [2400/6658 (36%)]\tLoss: 0.004229\n",
      "Train Epoch: 131 [2500/6658 (38%)]\tLoss: 0.323622\n",
      "Train Epoch: 131 [2600/6658 (39%)]\tLoss: 0.194240\n",
      "Train Epoch: 131 [2700/6658 (41%)]\tLoss: 1.037161\n",
      "Train Epoch: 131 [2800/6658 (42%)]\tLoss: 0.268405\n",
      "Train Epoch: 131 [2900/6658 (44%)]\tLoss: 1.848934\n",
      "Train Epoch: 131 [3000/6658 (45%)]\tLoss: 0.796693\n",
      "Train Epoch: 131 [3100/6658 (47%)]\tLoss: 0.399996\n",
      "Train Epoch: 131 [3200/6658 (48%)]\tLoss: 0.000379\n",
      "Train Epoch: 131 [3300/6658 (50%)]\tLoss: 0.656200\n",
      "Train Epoch: 131 [3400/6658 (51%)]\tLoss: 0.006274\n",
      "Train Epoch: 131 [3500/6658 (53%)]\tLoss: 0.029404\n",
      "Train Epoch: 131 [3600/6658 (54%)]\tLoss: 0.043997\n",
      "Train Epoch: 131 [3700/6658 (56%)]\tLoss: 0.048349\n",
      "Train Epoch: 131 [3800/6658 (57%)]\tLoss: 0.385069\n",
      "Train Epoch: 131 [3900/6658 (59%)]\tLoss: 3.707501\n",
      "Train Epoch: 131 [4000/6658 (60%)]\tLoss: 5.715571\n",
      "Train Epoch: 131 [4100/6658 (62%)]\tLoss: 0.002721\n",
      "Train Epoch: 131 [4200/6658 (63%)]\tLoss: 0.027393\n",
      "Train Epoch: 131 [4300/6658 (65%)]\tLoss: 1.154076\n",
      "Train Epoch: 131 [4400/6658 (66%)]\tLoss: 1.385507\n",
      "Train Epoch: 131 [4500/6658 (68%)]\tLoss: 0.526196\n",
      "Train Epoch: 131 [4600/6658 (69%)]\tLoss: 1.048191\n",
      "Train Epoch: 131 [4700/6658 (71%)]\tLoss: 0.544372\n",
      "Train Epoch: 131 [4800/6658 (72%)]\tLoss: 0.005891\n",
      "Train Epoch: 131 [4900/6658 (74%)]\tLoss: 0.000019\n",
      "Train Epoch: 131 [5000/6658 (75%)]\tLoss: 0.273247\n",
      "Train Epoch: 131 [5100/6658 (77%)]\tLoss: 0.616303\n",
      "Train Epoch: 131 [5200/6658 (78%)]\tLoss: 0.920766\n",
      "Train Epoch: 131 [5300/6658 (80%)]\tLoss: 4.842200\n",
      "Train Epoch: 131 [5400/6658 (81%)]\tLoss: 0.159118\n",
      "Train Epoch: 131 [5500/6658 (83%)]\tLoss: 0.158973\n",
      "Train Epoch: 131 [5600/6658 (84%)]\tLoss: 0.063324\n",
      "Train Epoch: 131 [5700/6658 (86%)]\tLoss: 0.053075\n",
      "Train Epoch: 131 [5800/6658 (87%)]\tLoss: 0.005172\n",
      "Train Epoch: 131 [5900/6658 (89%)]\tLoss: 0.027064\n",
      "Train Epoch: 131 [6000/6658 (90%)]\tLoss: 3.232825\n",
      "Train Epoch: 131 [6100/6658 (92%)]\tLoss: 1.627766\n",
      "Train Epoch: 131 [6200/6658 (93%)]\tLoss: 0.573679\n",
      "Train Epoch: 131 [6300/6658 (95%)]\tLoss: 0.285644\n",
      "Train Epoch: 131 [6400/6658 (96%)]\tLoss: 6.228532\n",
      "Train Epoch: 131 [6500/6658 (98%)]\tLoss: 0.010585\n",
      "Train Epoch: 131 [6600/6658 (99%)]\tLoss: 0.023869\n",
      "train loss average =  0.7153874834740667\n",
      "\n",
      "Test set: Average loss: 0.7036\n",
      "\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2801, 6.0855, 5.8624, 5.8698, 6.1936, 6.0484], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 132 [0/6658 (0%)]\tLoss: 0.037998\n",
      "Train Epoch: 132 [100/6658 (2%)]\tLoss: 0.361889\n",
      "Train Epoch: 132 [200/6658 (3%)]\tLoss: 0.169290\n",
      "Train Epoch: 132 [300/6658 (5%)]\tLoss: 2.440094\n",
      "Train Epoch: 132 [400/6658 (6%)]\tLoss: 0.495241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 132 [500/6658 (8%)]\tLoss: 0.079879\n",
      "Train Epoch: 132 [600/6658 (9%)]\tLoss: 0.068799\n",
      "Train Epoch: 132 [700/6658 (11%)]\tLoss: 0.241080\n",
      "Train Epoch: 132 [800/6658 (12%)]\tLoss: 1.579643\n",
      "Train Epoch: 132 [900/6658 (14%)]\tLoss: 0.047902\n",
      "Train Epoch: 132 [1000/6658 (15%)]\tLoss: 16.351971\n",
      "Train Epoch: 132 [1100/6658 (17%)]\tLoss: 0.552816\n",
      "Train Epoch: 132 [1200/6658 (18%)]\tLoss: 0.013212\n",
      "Train Epoch: 132 [1300/6658 (20%)]\tLoss: 0.001698\n",
      "Train Epoch: 132 [1400/6658 (21%)]\tLoss: 0.007786\n",
      "Train Epoch: 132 [1500/6658 (23%)]\tLoss: 0.357404\n",
      "Train Epoch: 132 [1600/6658 (24%)]\tLoss: 0.630833\n",
      "Train Epoch: 132 [1700/6658 (26%)]\tLoss: 0.037179\n",
      "Train Epoch: 132 [1800/6658 (27%)]\tLoss: 2.250308\n",
      "Train Epoch: 132 [1900/6658 (29%)]\tLoss: 0.273745\n",
      "Train Epoch: 132 [2000/6658 (30%)]\tLoss: 0.697484\n",
      "Train Epoch: 132 [2100/6658 (32%)]\tLoss: 0.144008\n",
      "Train Epoch: 132 [2200/6658 (33%)]\tLoss: 1.825981\n",
      "Train Epoch: 132 [2300/6658 (35%)]\tLoss: 0.037933\n",
      "Train Epoch: 132 [2400/6658 (36%)]\tLoss: 0.127163\n",
      "Train Epoch: 132 [2500/6658 (38%)]\tLoss: 0.118973\n",
      "Train Epoch: 132 [2600/6658 (39%)]\tLoss: 1.918622\n",
      "Train Epoch: 132 [2700/6658 (41%)]\tLoss: 0.813246\n",
      "Train Epoch: 132 [2800/6658 (42%)]\tLoss: 0.074036\n",
      "Train Epoch: 132 [2900/6658 (44%)]\tLoss: 0.568571\n",
      "Train Epoch: 132 [3000/6658 (45%)]\tLoss: 0.054298\n",
      "Train Epoch: 132 [3100/6658 (47%)]\tLoss: 0.251737\n",
      "Train Epoch: 132 [3200/6658 (48%)]\tLoss: 0.023032\n",
      "Train Epoch: 132 [3300/6658 (50%)]\tLoss: 0.815532\n",
      "Train Epoch: 132 [3400/6658 (51%)]\tLoss: 0.551186\n",
      "Train Epoch: 132 [3500/6658 (53%)]\tLoss: 2.355665\n",
      "Train Epoch: 132 [3600/6658 (54%)]\tLoss: 2.240282\n",
      "Train Epoch: 132 [3700/6658 (56%)]\tLoss: 0.058620\n",
      "Train Epoch: 132 [3800/6658 (57%)]\tLoss: 1.003869\n",
      "Train Epoch: 132 [3900/6658 (59%)]\tLoss: 0.089422\n",
      "Train Epoch: 132 [4000/6658 (60%)]\tLoss: 0.018228\n",
      "Train Epoch: 132 [4100/6658 (62%)]\tLoss: 0.895879\n",
      "Train Epoch: 132 [4200/6658 (63%)]\tLoss: 0.792568\n",
      "Train Epoch: 132 [4300/6658 (65%)]\tLoss: 4.758013\n",
      "Train Epoch: 132 [4400/6658 (66%)]\tLoss: 0.255585\n",
      "Train Epoch: 132 [4500/6658 (68%)]\tLoss: 0.406638\n",
      "Train Epoch: 132 [4600/6658 (69%)]\tLoss: 0.628049\n",
      "Train Epoch: 132 [4700/6658 (71%)]\tLoss: 0.035898\n",
      "Train Epoch: 132 [4800/6658 (72%)]\tLoss: 0.256100\n",
      "Train Epoch: 132 [4900/6658 (74%)]\tLoss: 0.208380\n",
      "Train Epoch: 132 [5000/6658 (75%)]\tLoss: 0.250397\n",
      "Train Epoch: 132 [5100/6658 (77%)]\tLoss: 0.238144\n",
      "Train Epoch: 132 [5200/6658 (78%)]\tLoss: 0.000953\n",
      "Train Epoch: 132 [5300/6658 (80%)]\tLoss: 0.087951\n",
      "Train Epoch: 132 [5400/6658 (81%)]\tLoss: 0.075977\n",
      "Train Epoch: 132 [5500/6658 (83%)]\tLoss: 0.079994\n",
      "Train Epoch: 132 [5600/6658 (84%)]\tLoss: 0.079159\n",
      "Train Epoch: 132 [5700/6658 (86%)]\tLoss: 0.014669\n",
      "Train Epoch: 132 [5800/6658 (87%)]\tLoss: 0.307569\n",
      "Train Epoch: 132 [5900/6658 (89%)]\tLoss: 0.279691\n",
      "Train Epoch: 132 [6000/6658 (90%)]\tLoss: 0.033665\n",
      "Train Epoch: 132 [6100/6658 (92%)]\tLoss: 0.786015\n",
      "Train Epoch: 132 [6200/6658 (93%)]\tLoss: 0.226728\n",
      "Train Epoch: 132 [6300/6658 (95%)]\tLoss: 0.004666\n",
      "Train Epoch: 132 [6400/6658 (96%)]\tLoss: 0.333281\n",
      "Train Epoch: 132 [6500/6658 (98%)]\tLoss: 0.248739\n",
      "Train Epoch: 132 [6600/6658 (99%)]\tLoss: 0.491613\n",
      "train loss average =  0.7128565059230274\n",
      "\n",
      "Test set: Average loss: 0.6946\n",
      "\n",
      "EarlyStopping counter: 9 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2813, 6.0869, 5.8611, 5.8699, 6.1955, 6.0483], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 133 [0/6658 (0%)]\tLoss: 0.209463\n",
      "Train Epoch: 133 [100/6658 (2%)]\tLoss: 0.530239\n",
      "Train Epoch: 133 [200/6658 (3%)]\tLoss: 2.737468\n",
      "Train Epoch: 133 [300/6658 (5%)]\tLoss: 0.273200\n",
      "Train Epoch: 133 [400/6658 (6%)]\tLoss: 0.000480\n",
      "Train Epoch: 133 [500/6658 (8%)]\tLoss: 0.162962\n",
      "Train Epoch: 133 [600/6658 (9%)]\tLoss: 0.552661\n",
      "Train Epoch: 133 [700/6658 (11%)]\tLoss: 1.065093\n",
      "Train Epoch: 133 [800/6658 (12%)]\tLoss: 0.725391\n",
      "Train Epoch: 133 [900/6658 (14%)]\tLoss: 0.075609\n",
      "Train Epoch: 133 [1000/6658 (15%)]\tLoss: 0.896620\n",
      "Train Epoch: 133 [1100/6658 (17%)]\tLoss: 0.504747\n",
      "Train Epoch: 133 [1200/6658 (18%)]\tLoss: 0.020053\n",
      "Train Epoch: 133 [1300/6658 (20%)]\tLoss: 0.627761\n",
      "Train Epoch: 133 [1400/6658 (21%)]\tLoss: 0.435109\n",
      "Train Epoch: 133 [1500/6658 (23%)]\tLoss: 0.000196\n",
      "Train Epoch: 133 [1600/6658 (24%)]\tLoss: 0.569629\n",
      "Train Epoch: 133 [1700/6658 (26%)]\tLoss: 0.943108\n",
      "Train Epoch: 133 [1800/6658 (27%)]\tLoss: 0.530058\n",
      "Train Epoch: 133 [1900/6658 (29%)]\tLoss: 0.001487\n",
      "Train Epoch: 133 [2000/6658 (30%)]\tLoss: 0.041367\n",
      "Train Epoch: 133 [2100/6658 (32%)]\tLoss: 0.371164\n",
      "Train Epoch: 133 [2200/6658 (33%)]\tLoss: 0.375582\n",
      "Train Epoch: 133 [2300/6658 (35%)]\tLoss: 0.127177\n",
      "Train Epoch: 133 [2400/6658 (36%)]\tLoss: 0.526975\n",
      "Train Epoch: 133 [2500/6658 (38%)]\tLoss: 0.134282\n",
      "Train Epoch: 133 [2600/6658 (39%)]\tLoss: 0.240505\n",
      "Train Epoch: 133 [2700/6658 (41%)]\tLoss: 2.093734\n",
      "Train Epoch: 133 [2800/6658 (42%)]\tLoss: 0.428906\n",
      "Train Epoch: 133 [2900/6658 (44%)]\tLoss: 0.017345\n",
      "Train Epoch: 133 [3000/6658 (45%)]\tLoss: 0.940772\n",
      "Train Epoch: 133 [3100/6658 (47%)]\tLoss: 0.352656\n",
      "Train Epoch: 133 [3200/6658 (48%)]\tLoss: 0.114965\n",
      "Train Epoch: 133 [3300/6658 (50%)]\tLoss: 0.280072\n",
      "Train Epoch: 133 [3400/6658 (51%)]\tLoss: 0.316210\n",
      "Train Epoch: 133 [3500/6658 (53%)]\tLoss: 0.324297\n",
      "Train Epoch: 133 [3600/6658 (54%)]\tLoss: 0.140278\n",
      "Train Epoch: 133 [3700/6658 (56%)]\tLoss: 0.612063\n",
      "Train Epoch: 133 [3800/6658 (57%)]\tLoss: 0.841323\n",
      "Train Epoch: 133 [3900/6658 (59%)]\tLoss: 1.067456\n",
      "Train Epoch: 133 [4000/6658 (60%)]\tLoss: 0.043330\n",
      "Train Epoch: 133 [4100/6658 (62%)]\tLoss: 0.118388\n",
      "Train Epoch: 133 [4200/6658 (63%)]\tLoss: 1.261436\n",
      "Train Epoch: 133 [4300/6658 (65%)]\tLoss: 0.132418\n",
      "Train Epoch: 133 [4400/6658 (66%)]\tLoss: 0.011555\n",
      "Train Epoch: 133 [4500/6658 (68%)]\tLoss: 0.289319\n",
      "Train Epoch: 133 [4600/6658 (69%)]\tLoss: 0.052006\n",
      "Train Epoch: 133 [4700/6658 (71%)]\tLoss: 0.823489\n",
      "Train Epoch: 133 [4800/6658 (72%)]\tLoss: 0.055239\n",
      "Train Epoch: 133 [4900/6658 (74%)]\tLoss: 2.704122\n",
      "Train Epoch: 133 [5000/6658 (75%)]\tLoss: 0.927908\n",
      "Train Epoch: 133 [5100/6658 (77%)]\tLoss: 1.005602\n",
      "Train Epoch: 133 [5200/6658 (78%)]\tLoss: 0.461577\n",
      "Train Epoch: 133 [5300/6658 (80%)]\tLoss: 1.255355\n",
      "Train Epoch: 133 [5400/6658 (81%)]\tLoss: 0.477027\n",
      "Train Epoch: 133 [5500/6658 (83%)]\tLoss: 0.005229\n",
      "Train Epoch: 133 [5600/6658 (84%)]\tLoss: 0.518755\n",
      "Train Epoch: 133 [5700/6658 (86%)]\tLoss: 0.768455\n",
      "Train Epoch: 133 [5800/6658 (87%)]\tLoss: 0.054717\n",
      "Train Epoch: 133 [5900/6658 (89%)]\tLoss: 0.752359\n",
      "Train Epoch: 133 [6000/6658 (90%)]\tLoss: 0.244765\n",
      "Train Epoch: 133 [6100/6658 (92%)]\tLoss: 0.146872\n",
      "Train Epoch: 133 [6200/6658 (93%)]\tLoss: 0.219662\n",
      "Train Epoch: 133 [6300/6658 (95%)]\tLoss: 0.292161\n",
      "Train Epoch: 133 [6400/6658 (96%)]\tLoss: 0.169299\n",
      "Train Epoch: 133 [6500/6658 (98%)]\tLoss: 0.237132\n",
      "Train Epoch: 133 [6600/6658 (99%)]\tLoss: 2.021323\n",
      "train loss average =  0.72126931158206\n",
      "\n",
      "Test set: Average loss: 0.7008\n",
      "\n",
      "EarlyStopping counter: 10 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2833, 6.0879, 5.8611, 5.8691, 6.1972, 6.0475], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 134 [0/6658 (0%)]\tLoss: 0.751530\n",
      "Train Epoch: 134 [100/6658 (2%)]\tLoss: 0.570673\n",
      "Train Epoch: 134 [200/6658 (3%)]\tLoss: 0.553232\n",
      "Train Epoch: 134 [300/6658 (5%)]\tLoss: 0.621202\n",
      "Train Epoch: 134 [400/6658 (6%)]\tLoss: 0.394083\n",
      "Train Epoch: 134 [500/6658 (8%)]\tLoss: 0.175617\n",
      "Train Epoch: 134 [600/6658 (9%)]\tLoss: 0.003185\n",
      "Train Epoch: 134 [700/6658 (11%)]\tLoss: 0.695181\n",
      "Train Epoch: 134 [800/6658 (12%)]\tLoss: 0.328121\n",
      "Train Epoch: 134 [900/6658 (14%)]\tLoss: 5.498042\n",
      "Train Epoch: 134 [1000/6658 (15%)]\tLoss: 0.080760\n",
      "Train Epoch: 134 [1100/6658 (17%)]\tLoss: 5.523298\n",
      "Train Epoch: 134 [1200/6658 (18%)]\tLoss: 0.403998\n",
      "Train Epoch: 134 [1300/6658 (20%)]\tLoss: 0.394185\n",
      "Train Epoch: 134 [1400/6658 (21%)]\tLoss: 0.658146\n",
      "Train Epoch: 134 [1500/6658 (23%)]\tLoss: 1.378330\n",
      "Train Epoch: 134 [1600/6658 (24%)]\tLoss: 7.735140\n",
      "Train Epoch: 134 [1700/6658 (26%)]\tLoss: 0.913712\n",
      "Train Epoch: 134 [1800/6658 (27%)]\tLoss: 0.005248\n",
      "Train Epoch: 134 [1900/6658 (29%)]\tLoss: 2.428904\n",
      "Train Epoch: 134 [2000/6658 (30%)]\tLoss: 2.064309\n",
      "Train Epoch: 134 [2100/6658 (32%)]\tLoss: 2.685187\n",
      "Train Epoch: 134 [2200/6658 (33%)]\tLoss: 0.325355\n",
      "Train Epoch: 134 [2300/6658 (35%)]\tLoss: 0.062066\n",
      "Train Epoch: 134 [2400/6658 (36%)]\tLoss: 0.076861\n",
      "Train Epoch: 134 [2500/6658 (38%)]\tLoss: 0.010534\n",
      "Train Epoch: 134 [2600/6658 (39%)]\tLoss: 0.273035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 134 [2700/6658 (41%)]\tLoss: 0.089789\n",
      "Train Epoch: 134 [2800/6658 (42%)]\tLoss: 0.835486\n",
      "Train Epoch: 134 [2900/6658 (44%)]\tLoss: 0.723097\n",
      "Train Epoch: 134 [3000/6658 (45%)]\tLoss: 0.053861\n",
      "Train Epoch: 134 [3100/6658 (47%)]\tLoss: 0.000441\n",
      "Train Epoch: 134 [3200/6658 (48%)]\tLoss: 1.686520\n",
      "Train Epoch: 134 [3300/6658 (50%)]\tLoss: 0.256303\n",
      "Train Epoch: 134 [3400/6658 (51%)]\tLoss: 0.734667\n",
      "Train Epoch: 134 [3500/6658 (53%)]\tLoss: 0.020263\n",
      "Train Epoch: 134 [3600/6658 (54%)]\tLoss: 0.004946\n",
      "Train Epoch: 134 [3700/6658 (56%)]\tLoss: 0.013773\n",
      "Train Epoch: 134 [3800/6658 (57%)]\tLoss: 0.226809\n",
      "Train Epoch: 134 [3900/6658 (59%)]\tLoss: 0.208109\n",
      "Train Epoch: 134 [4000/6658 (60%)]\tLoss: 2.621765\n",
      "Train Epoch: 134 [4100/6658 (62%)]\tLoss: 18.799366\n",
      "Train Epoch: 134 [4200/6658 (63%)]\tLoss: 0.544870\n",
      "Train Epoch: 134 [4300/6658 (65%)]\tLoss: 0.318744\n",
      "Train Epoch: 134 [4400/6658 (66%)]\tLoss: 0.402067\n",
      "Train Epoch: 134 [4500/6658 (68%)]\tLoss: 1.285367\n",
      "Train Epoch: 134 [4600/6658 (69%)]\tLoss: 0.608080\n",
      "Train Epoch: 134 [4700/6658 (71%)]\tLoss: 1.378778\n",
      "Train Epoch: 134 [4800/6658 (72%)]\tLoss: 0.945294\n",
      "Train Epoch: 134 [4900/6658 (74%)]\tLoss: 0.320578\n",
      "Train Epoch: 134 [5000/6658 (75%)]\tLoss: 0.650195\n",
      "Train Epoch: 134 [5100/6658 (77%)]\tLoss: 0.087944\n",
      "Train Epoch: 134 [5200/6658 (78%)]\tLoss: 0.921544\n",
      "Train Epoch: 134 [5300/6658 (80%)]\tLoss: 0.582047\n",
      "Train Epoch: 134 [5400/6658 (81%)]\tLoss: 0.022711\n",
      "Train Epoch: 134 [5500/6658 (83%)]\tLoss: 7.216668\n",
      "Train Epoch: 134 [5600/6658 (84%)]\tLoss: 0.000453\n",
      "Train Epoch: 134 [5700/6658 (86%)]\tLoss: 0.007360\n",
      "Train Epoch: 134 [5800/6658 (87%)]\tLoss: 0.488482\n",
      "Train Epoch: 134 [5900/6658 (89%)]\tLoss: 0.396397\n",
      "Train Epoch: 134 [6000/6658 (90%)]\tLoss: 0.085157\n",
      "Train Epoch: 134 [6100/6658 (92%)]\tLoss: 0.395342\n",
      "Train Epoch: 134 [6200/6658 (93%)]\tLoss: 0.004042\n",
      "Train Epoch: 134 [6300/6658 (95%)]\tLoss: 0.528535\n",
      "Train Epoch: 134 [6400/6658 (96%)]\tLoss: 0.028277\n",
      "Train Epoch: 134 [6500/6658 (98%)]\tLoss: 0.123613\n",
      "Train Epoch: 134 [6600/6658 (99%)]\tLoss: 0.005668\n",
      "train loss average =  0.7160996937573957\n",
      "\n",
      "Test set: Average loss: 0.7056\n",
      "\n",
      "EarlyStopping counter: 11 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2851, 6.0883, 5.8611, 5.8693, 6.1991, 6.0476], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 135 [0/6658 (0%)]\tLoss: 0.289180\n",
      "Train Epoch: 135 [100/6658 (2%)]\tLoss: 0.015828\n",
      "Train Epoch: 135 [200/6658 (3%)]\tLoss: 0.086376\n",
      "Train Epoch: 135 [300/6658 (5%)]\tLoss: 0.014201\n",
      "Train Epoch: 135 [400/6658 (6%)]\tLoss: 0.136919\n",
      "Train Epoch: 135 [500/6658 (8%)]\tLoss: 0.913463\n",
      "Train Epoch: 135 [600/6658 (9%)]\tLoss: 0.492423\n",
      "Train Epoch: 135 [700/6658 (11%)]\tLoss: 4.270535\n",
      "Train Epoch: 135 [800/6658 (12%)]\tLoss: 0.082544\n",
      "Train Epoch: 135 [900/6658 (14%)]\tLoss: 0.237047\n",
      "Train Epoch: 135 [1000/6658 (15%)]\tLoss: 0.587615\n",
      "Train Epoch: 135 [1100/6658 (17%)]\tLoss: 2.181733\n",
      "Train Epoch: 135 [1200/6658 (18%)]\tLoss: 0.027579\n",
      "Train Epoch: 135 [1300/6658 (20%)]\tLoss: 0.343517\n",
      "Train Epoch: 135 [1400/6658 (21%)]\tLoss: 1.581557\n",
      "Train Epoch: 135 [1500/6658 (23%)]\tLoss: 0.298463\n",
      "Train Epoch: 135 [1600/6658 (24%)]\tLoss: 1.141427\n",
      "Train Epoch: 135 [1700/6658 (26%)]\tLoss: 0.792377\n",
      "Train Epoch: 135 [1800/6658 (27%)]\tLoss: 0.164175\n",
      "Train Epoch: 135 [1900/6658 (29%)]\tLoss: 0.125602\n",
      "Train Epoch: 135 [2000/6658 (30%)]\tLoss: 0.042622\n",
      "Train Epoch: 135 [2100/6658 (32%)]\tLoss: 0.846472\n",
      "Train Epoch: 135 [2200/6658 (33%)]\tLoss: 0.013568\n",
      "Train Epoch: 135 [2300/6658 (35%)]\tLoss: 0.751936\n",
      "Train Epoch: 135 [2400/6658 (36%)]\tLoss: 0.055580\n",
      "Train Epoch: 135 [2500/6658 (38%)]\tLoss: 0.159852\n",
      "Train Epoch: 135 [2600/6658 (39%)]\tLoss: 0.433091\n",
      "Train Epoch: 135 [2700/6658 (41%)]\tLoss: 1.206565\n",
      "Train Epoch: 135 [2800/6658 (42%)]\tLoss: 1.904255\n",
      "Train Epoch: 135 [2900/6658 (44%)]\tLoss: 1.657540\n",
      "Train Epoch: 135 [3000/6658 (45%)]\tLoss: 0.950269\n",
      "Train Epoch: 135 [3100/6658 (47%)]\tLoss: 0.628615\n",
      "Train Epoch: 135 [3200/6658 (48%)]\tLoss: 0.245134\n",
      "Train Epoch: 135 [3300/6658 (50%)]\tLoss: 0.283989\n",
      "Train Epoch: 135 [3400/6658 (51%)]\tLoss: 0.473413\n",
      "Train Epoch: 135 [3500/6658 (53%)]\tLoss: 0.609542\n",
      "Train Epoch: 135 [3600/6658 (54%)]\tLoss: 1.899638\n",
      "Train Epoch: 135 [3700/6658 (56%)]\tLoss: 0.213911\n",
      "Train Epoch: 135 [3800/6658 (57%)]\tLoss: 0.330138\n",
      "Train Epoch: 135 [3900/6658 (59%)]\tLoss: 1.486001\n",
      "Train Epoch: 135 [4000/6658 (60%)]\tLoss: 0.017176\n",
      "Train Epoch: 135 [4100/6658 (62%)]\tLoss: 0.696987\n",
      "Train Epoch: 135 [4200/6658 (63%)]\tLoss: 0.063006\n",
      "Train Epoch: 135 [4300/6658 (65%)]\tLoss: 3.518098\n",
      "Train Epoch: 135 [4400/6658 (66%)]\tLoss: 0.049466\n",
      "Train Epoch: 135 [4500/6658 (68%)]\tLoss: 0.031386\n",
      "Train Epoch: 135 [4600/6658 (69%)]\tLoss: 0.069094\n",
      "Train Epoch: 135 [4700/6658 (71%)]\tLoss: 0.622788\n",
      "Train Epoch: 135 [4800/6658 (72%)]\tLoss: 0.043012\n",
      "Train Epoch: 135 [4900/6658 (74%)]\tLoss: 1.551420\n",
      "Train Epoch: 135 [5000/6658 (75%)]\tLoss: 0.010192\n",
      "Train Epoch: 135 [5100/6658 (77%)]\tLoss: 0.259307\n",
      "Train Epoch: 135 [5200/6658 (78%)]\tLoss: 0.000476\n",
      "Train Epoch: 135 [5300/6658 (80%)]\tLoss: 0.052191\n",
      "Train Epoch: 135 [5400/6658 (81%)]\tLoss: 0.144808\n",
      "Train Epoch: 135 [5500/6658 (83%)]\tLoss: 0.262075\n",
      "Train Epoch: 135 [5600/6658 (84%)]\tLoss: 0.553118\n",
      "Train Epoch: 135 [5700/6658 (86%)]\tLoss: 0.310869\n",
      "Train Epoch: 135 [5800/6658 (87%)]\tLoss: 0.159876\n",
      "Train Epoch: 135 [5900/6658 (89%)]\tLoss: 0.000076\n",
      "Train Epoch: 135 [6000/6658 (90%)]\tLoss: 0.121453\n",
      "Train Epoch: 135 [6100/6658 (92%)]\tLoss: 0.078760\n",
      "Train Epoch: 135 [6200/6658 (93%)]\tLoss: 0.097359\n",
      "Train Epoch: 135 [6300/6658 (95%)]\tLoss: 0.952666\n",
      "Train Epoch: 135 [6400/6658 (96%)]\tLoss: 0.956041\n",
      "Train Epoch: 135 [6500/6658 (98%)]\tLoss: 0.272645\n",
      "Train Epoch: 135 [6600/6658 (99%)]\tLoss: 0.632139\n",
      "train loss average =  0.7092722053919582\n",
      "\n",
      "Test set: Average loss: 0.7038\n",
      "\n",
      "EarlyStopping counter: 12 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2878, 6.0899, 5.8610, 5.8682, 6.2005, 6.0475], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 136 [0/6658 (0%)]\tLoss: 0.060735\n",
      "Train Epoch: 136 [100/6658 (2%)]\tLoss: 0.572198\n",
      "Train Epoch: 136 [200/6658 (3%)]\tLoss: 2.274080\n",
      "Train Epoch: 136 [300/6658 (5%)]\tLoss: 0.192891\n",
      "Train Epoch: 136 [400/6658 (6%)]\tLoss: 0.095748\n",
      "Train Epoch: 136 [500/6658 (8%)]\tLoss: 0.734273\n",
      "Train Epoch: 136 [600/6658 (9%)]\tLoss: 0.035307\n",
      "Train Epoch: 136 [700/6658 (11%)]\tLoss: 0.301426\n",
      "Train Epoch: 136 [800/6658 (12%)]\tLoss: 0.616367\n",
      "Train Epoch: 136 [900/6658 (14%)]\tLoss: 0.053040\n",
      "Train Epoch: 136 [1000/6658 (15%)]\tLoss: 0.001065\n",
      "Train Epoch: 136 [1100/6658 (17%)]\tLoss: 0.276209\n",
      "Train Epoch: 136 [1200/6658 (18%)]\tLoss: 10.366283\n",
      "Train Epoch: 136 [1300/6658 (20%)]\tLoss: 1.639459\n",
      "Train Epoch: 136 [1400/6658 (21%)]\tLoss: 0.002822\n",
      "Train Epoch: 136 [1500/6658 (23%)]\tLoss: 0.674421\n",
      "Train Epoch: 136 [1600/6658 (24%)]\tLoss: 2.334427\n",
      "Train Epoch: 136 [1700/6658 (26%)]\tLoss: 0.297975\n",
      "Train Epoch: 136 [1800/6658 (27%)]\tLoss: 0.703899\n",
      "Train Epoch: 136 [1900/6658 (29%)]\tLoss: 0.005056\n",
      "Train Epoch: 136 [2000/6658 (30%)]\tLoss: 0.431942\n",
      "Train Epoch: 136 [2100/6658 (32%)]\tLoss: 0.312973\n",
      "Train Epoch: 136 [2200/6658 (33%)]\tLoss: 0.077156\n",
      "Train Epoch: 136 [2300/6658 (35%)]\tLoss: 0.023253\n",
      "Train Epoch: 136 [2400/6658 (36%)]\tLoss: 0.004527\n",
      "Train Epoch: 136 [2500/6658 (38%)]\tLoss: 0.761875\n",
      "Train Epoch: 136 [2600/6658 (39%)]\tLoss: 1.445029\n",
      "Train Epoch: 136 [2700/6658 (41%)]\tLoss: 0.133033\n",
      "Train Epoch: 136 [2800/6658 (42%)]\tLoss: 0.586283\n",
      "Train Epoch: 136 [2900/6658 (44%)]\tLoss: 0.150022\n",
      "Train Epoch: 136 [3000/6658 (45%)]\tLoss: 0.155799\n",
      "Train Epoch: 136 [3100/6658 (47%)]\tLoss: 3.145530\n",
      "Train Epoch: 136 [3200/6658 (48%)]\tLoss: 0.096354\n",
      "Train Epoch: 136 [3300/6658 (50%)]\tLoss: 0.715323\n",
      "Train Epoch: 136 [3400/6658 (51%)]\tLoss: 1.026366\n",
      "Train Epoch: 136 [3500/6658 (53%)]\tLoss: 0.306358\n",
      "Train Epoch: 136 [3600/6658 (54%)]\tLoss: 1.122573\n",
      "Train Epoch: 136 [3700/6658 (56%)]\tLoss: 2.475613\n",
      "Train Epoch: 136 [3800/6658 (57%)]\tLoss: 0.109471\n",
      "Train Epoch: 136 [3900/6658 (59%)]\tLoss: 0.037417\n",
      "Train Epoch: 136 [4000/6658 (60%)]\tLoss: 0.127486\n",
      "Train Epoch: 136 [4100/6658 (62%)]\tLoss: 0.280716\n",
      "Train Epoch: 136 [4200/6658 (63%)]\tLoss: 0.001710\n",
      "Train Epoch: 136 [4300/6658 (65%)]\tLoss: 0.689051\n",
      "Train Epoch: 136 [4400/6658 (66%)]\tLoss: 1.505652\n",
      "Train Epoch: 136 [4500/6658 (68%)]\tLoss: 0.726946\n",
      "Train Epoch: 136 [4600/6658 (69%)]\tLoss: 0.450586\n",
      "Train Epoch: 136 [4700/6658 (71%)]\tLoss: 0.478288\n",
      "Train Epoch: 136 [4800/6658 (72%)]\tLoss: 3.220610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 136 [4900/6658 (74%)]\tLoss: 0.038771\n",
      "Train Epoch: 136 [5000/6658 (75%)]\tLoss: 0.331121\n",
      "Train Epoch: 136 [5100/6658 (77%)]\tLoss: 0.530644\n",
      "Train Epoch: 136 [5200/6658 (78%)]\tLoss: 0.313720\n",
      "Train Epoch: 136 [5300/6658 (80%)]\tLoss: 0.187815\n",
      "Train Epoch: 136 [5400/6658 (81%)]\tLoss: 0.857450\n",
      "Train Epoch: 136 [5500/6658 (83%)]\tLoss: 3.847273\n",
      "Train Epoch: 136 [5600/6658 (84%)]\tLoss: 0.025616\n",
      "Train Epoch: 136 [5700/6658 (86%)]\tLoss: 0.052818\n",
      "Train Epoch: 136 [5800/6658 (87%)]\tLoss: 2.147145\n",
      "Train Epoch: 136 [5900/6658 (89%)]\tLoss: 0.255014\n",
      "Train Epoch: 136 [6000/6658 (90%)]\tLoss: 0.006461\n",
      "Train Epoch: 136 [6100/6658 (92%)]\tLoss: 0.204413\n",
      "Train Epoch: 136 [6200/6658 (93%)]\tLoss: 0.839519\n",
      "Train Epoch: 136 [6300/6658 (95%)]\tLoss: 0.636882\n",
      "Train Epoch: 136 [6400/6658 (96%)]\tLoss: 1.565051\n",
      "Train Epoch: 136 [6500/6658 (98%)]\tLoss: 1.067613\n",
      "Train Epoch: 136 [6600/6658 (99%)]\tLoss: 0.182314\n",
      "train loss average =  0.7130279689449629\n",
      "\n",
      "Test set: Average loss: 0.6992\n",
      "\n",
      "EarlyStopping counter: 13 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2897, 6.0904, 5.8602, 5.8673, 6.2022, 6.0482], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 137 [0/6658 (0%)]\tLoss: 0.097808\n",
      "Train Epoch: 137 [100/6658 (2%)]\tLoss: 0.093422\n",
      "Train Epoch: 137 [200/6658 (3%)]\tLoss: 1.065841\n",
      "Train Epoch: 137 [300/6658 (5%)]\tLoss: 0.349300\n",
      "Train Epoch: 137 [400/6658 (6%)]\tLoss: 0.299319\n",
      "Train Epoch: 137 [500/6658 (8%)]\tLoss: 0.084735\n",
      "Train Epoch: 137 [600/6658 (9%)]\tLoss: 1.269818\n",
      "Train Epoch: 137 [700/6658 (11%)]\tLoss: 0.144039\n",
      "Train Epoch: 137 [800/6658 (12%)]\tLoss: 0.249265\n",
      "Train Epoch: 137 [900/6658 (14%)]\tLoss: 0.165453\n",
      "Train Epoch: 137 [1000/6658 (15%)]\tLoss: 0.005259\n",
      "Train Epoch: 137 [1100/6658 (17%)]\tLoss: 0.245808\n",
      "Train Epoch: 137 [1200/6658 (18%)]\tLoss: 1.299499\n",
      "Train Epoch: 137 [1300/6658 (20%)]\tLoss: 0.029561\n",
      "Train Epoch: 137 [1400/6658 (21%)]\tLoss: 0.317207\n",
      "Train Epoch: 137 [1500/6658 (23%)]\tLoss: 0.919996\n",
      "Train Epoch: 137 [1600/6658 (24%)]\tLoss: 0.018776\n",
      "Train Epoch: 137 [1700/6658 (26%)]\tLoss: 0.392923\n",
      "Train Epoch: 137 [1800/6658 (27%)]\tLoss: 0.256404\n",
      "Train Epoch: 137 [1900/6658 (29%)]\tLoss: 0.443711\n",
      "Train Epoch: 137 [2000/6658 (30%)]\tLoss: 0.168822\n",
      "Train Epoch: 137 [2100/6658 (32%)]\tLoss: 0.132189\n",
      "Train Epoch: 137 [2200/6658 (33%)]\tLoss: 0.060648\n",
      "Train Epoch: 137 [2300/6658 (35%)]\tLoss: 0.121857\n",
      "Train Epoch: 137 [2400/6658 (36%)]\tLoss: 0.246340\n",
      "Train Epoch: 137 [2500/6658 (38%)]\tLoss: 1.367957\n",
      "Train Epoch: 137 [2600/6658 (39%)]\tLoss: 0.411504\n",
      "Train Epoch: 137 [2700/6658 (41%)]\tLoss: 0.037235\n",
      "Train Epoch: 137 [2800/6658 (42%)]\tLoss: 0.240742\n",
      "Train Epoch: 137 [2900/6658 (44%)]\tLoss: 1.914752\n",
      "Train Epoch: 137 [3000/6658 (45%)]\tLoss: 0.074727\n",
      "Train Epoch: 137 [3100/6658 (47%)]\tLoss: 0.226436\n",
      "Train Epoch: 137 [3200/6658 (48%)]\tLoss: 0.181077\n",
      "Train Epoch: 137 [3300/6658 (50%)]\tLoss: 1.639614\n",
      "Train Epoch: 137 [3400/6658 (51%)]\tLoss: 0.559851\n",
      "Train Epoch: 137 [3500/6658 (53%)]\tLoss: 4.810309\n",
      "Train Epoch: 137 [3600/6658 (54%)]\tLoss: 0.000576\n",
      "Train Epoch: 137 [3700/6658 (56%)]\tLoss: 0.358045\n",
      "Train Epoch: 137 [3800/6658 (57%)]\tLoss: 0.363190\n",
      "Train Epoch: 137 [3900/6658 (59%)]\tLoss: 0.155036\n",
      "Train Epoch: 137 [4000/6658 (60%)]\tLoss: 0.052883\n",
      "Train Epoch: 137 [4100/6658 (62%)]\tLoss: 0.039381\n",
      "Train Epoch: 137 [4200/6658 (63%)]\tLoss: 0.407573\n",
      "Train Epoch: 137 [4300/6658 (65%)]\tLoss: 0.398968\n",
      "Train Epoch: 137 [4400/6658 (66%)]\tLoss: 0.320824\n",
      "Train Epoch: 137 [4500/6658 (68%)]\tLoss: 0.159690\n",
      "Train Epoch: 137 [4600/6658 (69%)]\tLoss: 0.122060\n",
      "Train Epoch: 137 [4700/6658 (71%)]\tLoss: 0.208675\n",
      "Train Epoch: 137 [4800/6658 (72%)]\tLoss: 0.025236\n",
      "Train Epoch: 137 [4900/6658 (74%)]\tLoss: 0.251390\n",
      "Train Epoch: 137 [5000/6658 (75%)]\tLoss: 2.015586\n",
      "Train Epoch: 137 [5100/6658 (77%)]\tLoss: 0.365576\n",
      "Train Epoch: 137 [5200/6658 (78%)]\tLoss: 0.553329\n",
      "Train Epoch: 137 [5300/6658 (80%)]\tLoss: 0.017469\n",
      "Train Epoch: 137 [5400/6658 (81%)]\tLoss: 0.009384\n",
      "Train Epoch: 137 [5500/6658 (83%)]\tLoss: 0.034868\n",
      "Train Epoch: 137 [5600/6658 (84%)]\tLoss: 1.748926\n",
      "Train Epoch: 137 [5700/6658 (86%)]\tLoss: 0.106347\n",
      "Train Epoch: 137 [5800/6658 (87%)]\tLoss: 0.420253\n",
      "Train Epoch: 137 [5900/6658 (89%)]\tLoss: 1.122917\n",
      "Train Epoch: 137 [6000/6658 (90%)]\tLoss: 0.398638\n",
      "Train Epoch: 137 [6100/6658 (92%)]\tLoss: 0.292623\n",
      "Train Epoch: 137 [6200/6658 (93%)]\tLoss: 0.038905\n",
      "Train Epoch: 137 [6300/6658 (95%)]\tLoss: 1.604162\n",
      "Train Epoch: 137 [6400/6658 (96%)]\tLoss: 0.197375\n",
      "Train Epoch: 137 [6500/6658 (98%)]\tLoss: 0.034555\n",
      "Train Epoch: 137 [6600/6658 (99%)]\tLoss: 5.870079\n",
      "train loss average =  0.7175217461456994\n",
      "\n",
      "Test set: Average loss: 0.6971\n",
      "\n",
      "EarlyStopping counter: 14 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2918, 6.0906, 5.8598, 5.8667, 6.2032, 6.0477], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 138 [0/6658 (0%)]\tLoss: 0.342502\n",
      "Train Epoch: 138 [100/6658 (2%)]\tLoss: 0.000101\n",
      "Train Epoch: 138 [200/6658 (3%)]\tLoss: 4.960964\n",
      "Train Epoch: 138 [300/6658 (5%)]\tLoss: 0.672961\n",
      "Train Epoch: 138 [400/6658 (6%)]\tLoss: 0.087372\n",
      "Train Epoch: 138 [500/6658 (8%)]\tLoss: 0.249156\n",
      "Train Epoch: 138 [600/6658 (9%)]\tLoss: 1.014602\n",
      "Train Epoch: 138 [700/6658 (11%)]\tLoss: 0.251849\n",
      "Train Epoch: 138 [800/6658 (12%)]\tLoss: 2.593533\n",
      "Train Epoch: 138 [900/6658 (14%)]\tLoss: 0.956164\n",
      "Train Epoch: 138 [1000/6658 (15%)]\tLoss: 3.279357\n",
      "Train Epoch: 138 [1100/6658 (17%)]\tLoss: 5.669697\n",
      "Train Epoch: 138 [1200/6658 (18%)]\tLoss: 0.093740\n",
      "Train Epoch: 138 [1300/6658 (20%)]\tLoss: 0.155408\n",
      "Train Epoch: 138 [1400/6658 (21%)]\tLoss: 0.613495\n",
      "Train Epoch: 138 [1500/6658 (23%)]\tLoss: 0.002432\n",
      "Train Epoch: 138 [1600/6658 (24%)]\tLoss: 0.065764\n",
      "Train Epoch: 138 [1700/6658 (26%)]\tLoss: 0.200860\n",
      "Train Epoch: 138 [1800/6658 (27%)]\tLoss: 2.928378\n",
      "Train Epoch: 138 [1900/6658 (29%)]\tLoss: 1.737581\n",
      "Train Epoch: 138 [2000/6658 (30%)]\tLoss: 0.050785\n",
      "Train Epoch: 138 [2100/6658 (32%)]\tLoss: 0.249403\n",
      "Train Epoch: 138 [2200/6658 (33%)]\tLoss: 0.170710\n",
      "Train Epoch: 138 [2300/6658 (35%)]\tLoss: 0.237863\n",
      "Train Epoch: 138 [2400/6658 (36%)]\tLoss: 0.049870\n",
      "Train Epoch: 138 [2500/6658 (38%)]\tLoss: 0.333054\n",
      "Train Epoch: 138 [2600/6658 (39%)]\tLoss: 0.635628\n",
      "Train Epoch: 138 [2700/6658 (41%)]\tLoss: 0.241380\n",
      "Train Epoch: 138 [2800/6658 (42%)]\tLoss: 0.006065\n",
      "Train Epoch: 138 [2900/6658 (44%)]\tLoss: 0.098971\n",
      "Train Epoch: 138 [3000/6658 (45%)]\tLoss: 0.000086\n",
      "Train Epoch: 138 [3100/6658 (47%)]\tLoss: 0.015486\n",
      "Train Epoch: 138 [3200/6658 (48%)]\tLoss: 0.173547\n",
      "Train Epoch: 138 [3300/6658 (50%)]\tLoss: 0.730231\n",
      "Train Epoch: 138 [3400/6658 (51%)]\tLoss: 0.113517\n",
      "Train Epoch: 138 [3500/6658 (53%)]\tLoss: 1.223158\n",
      "Train Epoch: 138 [3600/6658 (54%)]\tLoss: 3.399491\n",
      "Train Epoch: 138 [3700/6658 (56%)]\tLoss: 0.805535\n",
      "Train Epoch: 138 [3800/6658 (57%)]\tLoss: 0.961767\n",
      "Train Epoch: 138 [3900/6658 (59%)]\tLoss: 0.037040\n",
      "Train Epoch: 138 [4000/6658 (60%)]\tLoss: 0.010677\n",
      "Train Epoch: 138 [4100/6658 (62%)]\tLoss: 11.251751\n",
      "Train Epoch: 138 [4200/6658 (63%)]\tLoss: 0.355601\n",
      "Train Epoch: 138 [4300/6658 (65%)]\tLoss: 0.448036\n",
      "Train Epoch: 138 [4400/6658 (66%)]\tLoss: 1.128657\n",
      "Train Epoch: 138 [4500/6658 (68%)]\tLoss: 0.440943\n",
      "Train Epoch: 138 [4600/6658 (69%)]\tLoss: 1.007482\n",
      "Train Epoch: 138 [4700/6658 (71%)]\tLoss: 0.004714\n",
      "Train Epoch: 138 [4800/6658 (72%)]\tLoss: 0.557521\n",
      "Train Epoch: 138 [4900/6658 (74%)]\tLoss: 1.257266\n",
      "Train Epoch: 138 [5000/6658 (75%)]\tLoss: 1.719815\n",
      "Train Epoch: 138 [5100/6658 (77%)]\tLoss: 0.000002\n",
      "Train Epoch: 138 [5200/6658 (78%)]\tLoss: 1.357067\n",
      "Train Epoch: 138 [5300/6658 (80%)]\tLoss: 4.305950\n",
      "Train Epoch: 138 [5400/6658 (81%)]\tLoss: 1.859888\n",
      "Train Epoch: 138 [5500/6658 (83%)]\tLoss: 0.059555\n",
      "Train Epoch: 138 [5600/6658 (84%)]\tLoss: 0.061895\n",
      "Train Epoch: 138 [5700/6658 (86%)]\tLoss: 0.488950\n",
      "Train Epoch: 138 [5800/6658 (87%)]\tLoss: 0.040960\n",
      "Train Epoch: 138 [5900/6658 (89%)]\tLoss: 0.203606\n",
      "Train Epoch: 138 [6000/6658 (90%)]\tLoss: 0.012976\n",
      "Train Epoch: 138 [6100/6658 (92%)]\tLoss: 0.687015\n",
      "Train Epoch: 138 [6200/6658 (93%)]\tLoss: 1.773404\n",
      "Train Epoch: 138 [6300/6658 (95%)]\tLoss: 0.002484\n",
      "Train Epoch: 138 [6400/6658 (96%)]\tLoss: 0.000133\n",
      "Train Epoch: 138 [6500/6658 (98%)]\tLoss: 0.007276\n",
      "Train Epoch: 138 [6600/6658 (99%)]\tLoss: 0.381285\n",
      "train loss average =  0.7134282819738546\n",
      "\n",
      "Test set: Average loss: 0.7085\n",
      "\n",
      "EarlyStopping counter: 15 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2940, 6.0921, 5.8592, 5.8663, 6.2051, 6.0477], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 139 [0/6658 (0%)]\tLoss: 0.180680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 139 [100/6658 (2%)]\tLoss: 0.123411\n",
      "Train Epoch: 139 [200/6658 (3%)]\tLoss: 0.003439\n",
      "Train Epoch: 139 [300/6658 (5%)]\tLoss: 2.431713\n",
      "Train Epoch: 139 [400/6658 (6%)]\tLoss: 0.073490\n",
      "Train Epoch: 139 [500/6658 (8%)]\tLoss: 1.480790\n",
      "Train Epoch: 139 [600/6658 (9%)]\tLoss: 0.111071\n",
      "Train Epoch: 139 [700/6658 (11%)]\tLoss: 0.021699\n",
      "Train Epoch: 139 [800/6658 (12%)]\tLoss: 0.493087\n",
      "Train Epoch: 139 [900/6658 (14%)]\tLoss: 0.165763\n",
      "Train Epoch: 139 [1000/6658 (15%)]\tLoss: 23.747185\n",
      "Train Epoch: 139 [1100/6658 (17%)]\tLoss: 0.007658\n",
      "Train Epoch: 139 [1200/6658 (18%)]\tLoss: 0.491980\n",
      "Train Epoch: 139 [1300/6658 (20%)]\tLoss: 0.008887\n",
      "Train Epoch: 139 [1400/6658 (21%)]\tLoss: 0.118458\n",
      "Train Epoch: 139 [1500/6658 (23%)]\tLoss: 0.055211\n",
      "Train Epoch: 139 [1600/6658 (24%)]\tLoss: 0.015221\n",
      "Train Epoch: 139 [1700/6658 (26%)]\tLoss: 0.404840\n",
      "Train Epoch: 139 [1800/6658 (27%)]\tLoss: 0.154699\n",
      "Train Epoch: 139 [1900/6658 (29%)]\tLoss: 0.207419\n",
      "Train Epoch: 139 [2000/6658 (30%)]\tLoss: 0.354593\n",
      "Train Epoch: 139 [2100/6658 (32%)]\tLoss: 0.829164\n",
      "Train Epoch: 139 [2200/6658 (33%)]\tLoss: 0.077644\n",
      "Train Epoch: 139 [2300/6658 (35%)]\tLoss: 0.913936\n",
      "Train Epoch: 139 [2400/6658 (36%)]\tLoss: 0.141807\n",
      "Train Epoch: 139 [2500/6658 (38%)]\tLoss: 0.000617\n",
      "Train Epoch: 139 [2600/6658 (39%)]\tLoss: 0.000355\n",
      "Train Epoch: 139 [2700/6658 (41%)]\tLoss: 0.015733\n",
      "Train Epoch: 139 [2800/6658 (42%)]\tLoss: 0.274689\n",
      "Train Epoch: 139 [2900/6658 (44%)]\tLoss: 0.210091\n",
      "Train Epoch: 139 [3000/6658 (45%)]\tLoss: 0.242803\n",
      "Train Epoch: 139 [3100/6658 (47%)]\tLoss: 0.124434\n",
      "Train Epoch: 139 [3200/6658 (48%)]\tLoss: 2.234582\n",
      "Train Epoch: 139 [3300/6658 (50%)]\tLoss: 0.000001\n",
      "Train Epoch: 139 [3400/6658 (51%)]\tLoss: 2.317144\n",
      "Train Epoch: 139 [3500/6658 (53%)]\tLoss: 0.098063\n",
      "Train Epoch: 139 [3600/6658 (54%)]\tLoss: 2.720745\n",
      "Train Epoch: 139 [3700/6658 (56%)]\tLoss: 0.347121\n",
      "Train Epoch: 139 [3800/6658 (57%)]\tLoss: 0.407476\n",
      "Train Epoch: 139 [3900/6658 (59%)]\tLoss: 0.225612\n",
      "Train Epoch: 139 [4000/6658 (60%)]\tLoss: 0.132184\n",
      "Train Epoch: 139 [4100/6658 (62%)]\tLoss: 0.471533\n",
      "Train Epoch: 139 [4200/6658 (63%)]\tLoss: 0.024794\n",
      "Train Epoch: 139 [4300/6658 (65%)]\tLoss: 0.387732\n",
      "Train Epoch: 139 [4400/6658 (66%)]\tLoss: 0.115851\n",
      "Train Epoch: 139 [4500/6658 (68%)]\tLoss: 0.002281\n",
      "Train Epoch: 139 [4600/6658 (69%)]\tLoss: 0.914217\n",
      "Train Epoch: 139 [4700/6658 (71%)]\tLoss: 0.248036\n",
      "Train Epoch: 139 [4800/6658 (72%)]\tLoss: 0.085093\n",
      "Train Epoch: 139 [4900/6658 (74%)]\tLoss: 0.027058\n",
      "Train Epoch: 139 [5000/6658 (75%)]\tLoss: 0.113829\n",
      "Train Epoch: 139 [5100/6658 (77%)]\tLoss: 0.002249\n",
      "Train Epoch: 139 [5200/6658 (78%)]\tLoss: 0.161377\n",
      "Train Epoch: 139 [5300/6658 (80%)]\tLoss: 0.054857\n",
      "Train Epoch: 139 [5400/6658 (81%)]\tLoss: 3.488668\n",
      "Train Epoch: 139 [5500/6658 (83%)]\tLoss: 0.009691\n",
      "Train Epoch: 139 [5600/6658 (84%)]\tLoss: 0.240469\n",
      "Train Epoch: 139 [5700/6658 (86%)]\tLoss: 0.150885\n",
      "Train Epoch: 139 [5800/6658 (87%)]\tLoss: 0.082129\n",
      "Train Epoch: 139 [5900/6658 (89%)]\tLoss: 0.015988\n",
      "Train Epoch: 139 [6000/6658 (90%)]\tLoss: 18.312517\n",
      "Train Epoch: 139 [6100/6658 (92%)]\tLoss: 0.851654\n",
      "Train Epoch: 139 [6200/6658 (93%)]\tLoss: 0.357212\n",
      "Train Epoch: 139 [6300/6658 (95%)]\tLoss: 0.232668\n",
      "Train Epoch: 139 [6400/6658 (96%)]\tLoss: 0.167767\n",
      "Train Epoch: 139 [6500/6658 (98%)]\tLoss: 1.778503\n",
      "Train Epoch: 139 [6600/6658 (99%)]\tLoss: 1.197568\n",
      "train loss average =  0.7190609299847661\n",
      "\n",
      "Test set: Average loss: 0.7393\n",
      "\n",
      "EarlyStopping counter: 16 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2962, 6.0924, 5.8588, 5.8658, 6.2073, 6.0476], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 140 [0/6658 (0%)]\tLoss: 0.527981\n",
      "Train Epoch: 140 [100/6658 (2%)]\tLoss: 0.105286\n",
      "Train Epoch: 140 [200/6658 (3%)]\tLoss: 0.008133\n",
      "Train Epoch: 140 [300/6658 (5%)]\tLoss: 0.208827\n",
      "Train Epoch: 140 [400/6658 (6%)]\tLoss: 0.218467\n",
      "Train Epoch: 140 [500/6658 (8%)]\tLoss: 2.875796\n",
      "Train Epoch: 140 [600/6658 (9%)]\tLoss: 0.030584\n",
      "Train Epoch: 140 [700/6658 (11%)]\tLoss: 0.103665\n",
      "Train Epoch: 140 [800/6658 (12%)]\tLoss: 0.293280\n",
      "Train Epoch: 140 [900/6658 (14%)]\tLoss: 1.322140\n",
      "Train Epoch: 140 [1000/6658 (15%)]\tLoss: 0.115288\n",
      "Train Epoch: 140 [1100/6658 (17%)]\tLoss: 0.068702\n",
      "Train Epoch: 140 [1200/6658 (18%)]\tLoss: 0.105282\n",
      "Train Epoch: 140 [1300/6658 (20%)]\tLoss: 0.008264\n",
      "Train Epoch: 140 [1400/6658 (21%)]\tLoss: 0.190839\n",
      "Train Epoch: 140 [1500/6658 (23%)]\tLoss: 0.665062\n",
      "Train Epoch: 140 [1600/6658 (24%)]\tLoss: 0.013986\n",
      "Train Epoch: 140 [1700/6658 (26%)]\tLoss: 0.116046\n",
      "Train Epoch: 140 [1800/6658 (27%)]\tLoss: 1.724913\n",
      "Train Epoch: 140 [1900/6658 (29%)]\tLoss: 1.105716\n",
      "Train Epoch: 140 [2000/6658 (30%)]\tLoss: 0.151127\n",
      "Train Epoch: 140 [2100/6658 (32%)]\tLoss: 0.647971\n",
      "Train Epoch: 140 [2200/6658 (33%)]\tLoss: 0.336399\n",
      "Train Epoch: 140 [2300/6658 (35%)]\tLoss: 0.055730\n",
      "Train Epoch: 140 [2400/6658 (36%)]\tLoss: 0.041528\n",
      "Train Epoch: 140 [2500/6658 (38%)]\tLoss: 0.089183\n",
      "Train Epoch: 140 [2600/6658 (39%)]\tLoss: 0.933116\n",
      "Train Epoch: 140 [2700/6658 (41%)]\tLoss: 0.524864\n",
      "Train Epoch: 140 [2800/6658 (42%)]\tLoss: 0.925642\n",
      "Train Epoch: 140 [2900/6658 (44%)]\tLoss: 1.067215\n",
      "Train Epoch: 140 [3000/6658 (45%)]\tLoss: 0.759117\n",
      "Train Epoch: 140 [3100/6658 (47%)]\tLoss: 0.486449\n",
      "Train Epoch: 140 [3200/6658 (48%)]\tLoss: 0.286818\n",
      "Train Epoch: 140 [3300/6658 (50%)]\tLoss: 0.932792\n",
      "Train Epoch: 140 [3400/6658 (51%)]\tLoss: 0.039347\n",
      "Train Epoch: 140 [3500/6658 (53%)]\tLoss: 0.064244\n",
      "Train Epoch: 140 [3600/6658 (54%)]\tLoss: 0.285523\n",
      "Train Epoch: 140 [3700/6658 (56%)]\tLoss: 0.167105\n",
      "Train Epoch: 140 [3800/6658 (57%)]\tLoss: 0.405978\n",
      "Train Epoch: 140 [3900/6658 (59%)]\tLoss: 0.033084\n",
      "Train Epoch: 140 [4000/6658 (60%)]\tLoss: 0.116348\n",
      "Train Epoch: 140 [4100/6658 (62%)]\tLoss: 2.504688\n",
      "Train Epoch: 140 [4200/6658 (63%)]\tLoss: 2.414665\n",
      "Train Epoch: 140 [4300/6658 (65%)]\tLoss: 1.924275\n",
      "Train Epoch: 140 [4400/6658 (66%)]\tLoss: 0.018945\n",
      "Train Epoch: 140 [4500/6658 (68%)]\tLoss: 0.597730\n",
      "Train Epoch: 140 [4600/6658 (69%)]\tLoss: 0.370982\n",
      "Train Epoch: 140 [4700/6658 (71%)]\tLoss: 2.830471\n",
      "Train Epoch: 140 [4800/6658 (72%)]\tLoss: 0.804724\n",
      "Train Epoch: 140 [4900/6658 (74%)]\tLoss: 0.039192\n",
      "Train Epoch: 140 [5000/6658 (75%)]\tLoss: 0.098116\n",
      "Train Epoch: 140 [5100/6658 (77%)]\tLoss: 0.771180\n",
      "Train Epoch: 140 [5200/6658 (78%)]\tLoss: 0.571614\n",
      "Train Epoch: 140 [5300/6658 (80%)]\tLoss: 0.049350\n",
      "Train Epoch: 140 [5400/6658 (81%)]\tLoss: 0.443607\n",
      "Train Epoch: 140 [5500/6658 (83%)]\tLoss: 0.088433\n",
      "Train Epoch: 140 [5600/6658 (84%)]\tLoss: 11.618191\n",
      "Train Epoch: 140 [5700/6658 (86%)]\tLoss: 1.421385\n",
      "Train Epoch: 140 [5800/6658 (87%)]\tLoss: 0.584665\n",
      "Train Epoch: 140 [5900/6658 (89%)]\tLoss: 0.622182\n",
      "Train Epoch: 140 [6000/6658 (90%)]\tLoss: 0.302742\n",
      "Train Epoch: 140 [6100/6658 (92%)]\tLoss: 1.003844\n",
      "Train Epoch: 140 [6200/6658 (93%)]\tLoss: 1.257890\n",
      "Train Epoch: 140 [6300/6658 (95%)]\tLoss: 0.012831\n",
      "Train Epoch: 140 [6400/6658 (96%)]\tLoss: 0.107383\n",
      "Train Epoch: 140 [6500/6658 (98%)]\tLoss: 0.000018\n",
      "Train Epoch: 140 [6600/6658 (99%)]\tLoss: 1.093980\n",
      "train loss average =  0.7162125657410179\n",
      "\n",
      "Test set: Average loss: 0.6922\n",
      "\n",
      "EarlyStopping counter: 17 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2973, 6.0931, 5.8573, 5.8649, 6.2091, 6.0474], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 141 [0/6658 (0%)]\tLoss: 0.433784\n",
      "Train Epoch: 141 [100/6658 (2%)]\tLoss: 0.010749\n",
      "Train Epoch: 141 [200/6658 (3%)]\tLoss: 0.146583\n",
      "Train Epoch: 141 [300/6658 (5%)]\tLoss: 0.159576\n",
      "Train Epoch: 141 [400/6658 (6%)]\tLoss: 0.002189\n",
      "Train Epoch: 141 [500/6658 (8%)]\tLoss: 0.421742\n",
      "Train Epoch: 141 [600/6658 (9%)]\tLoss: 1.432895\n",
      "Train Epoch: 141 [700/6658 (11%)]\tLoss: 3.990115\n",
      "Train Epoch: 141 [800/6658 (12%)]\tLoss: 0.306206\n",
      "Train Epoch: 141 [900/6658 (14%)]\tLoss: 0.554543\n",
      "Train Epoch: 141 [1000/6658 (15%)]\tLoss: 0.132115\n",
      "Train Epoch: 141 [1100/6658 (17%)]\tLoss: 0.223057\n",
      "Train Epoch: 141 [1200/6658 (18%)]\tLoss: 1.195628\n",
      "Train Epoch: 141 [1300/6658 (20%)]\tLoss: 0.178957\n",
      "Train Epoch: 141 [1400/6658 (21%)]\tLoss: 4.186067\n",
      "Train Epoch: 141 [1500/6658 (23%)]\tLoss: 0.195187\n",
      "Train Epoch: 141 [1600/6658 (24%)]\tLoss: 0.209972\n",
      "Train Epoch: 141 [1700/6658 (26%)]\tLoss: 0.381768\n",
      "Train Epoch: 141 [1800/6658 (27%)]\tLoss: 0.099440\n",
      "Train Epoch: 141 [1900/6658 (29%)]\tLoss: 0.324337\n",
      "Train Epoch: 141 [2000/6658 (30%)]\tLoss: 7.708877\n",
      "Train Epoch: 141 [2100/6658 (32%)]\tLoss: 0.389936\n",
      "Train Epoch: 141 [2200/6658 (33%)]\tLoss: 0.023718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 141 [2300/6658 (35%)]\tLoss: 0.000388\n",
      "Train Epoch: 141 [2400/6658 (36%)]\tLoss: 0.514010\n",
      "Train Epoch: 141 [2500/6658 (38%)]\tLoss: 0.082639\n",
      "Train Epoch: 141 [2600/6658 (39%)]\tLoss: 0.171072\n",
      "Train Epoch: 141 [2700/6658 (41%)]\tLoss: 1.573631\n",
      "Train Epoch: 141 [2800/6658 (42%)]\tLoss: 0.492342\n",
      "Train Epoch: 141 [2900/6658 (44%)]\tLoss: 2.697240\n",
      "Train Epoch: 141 [3000/6658 (45%)]\tLoss: 3.187993\n",
      "Train Epoch: 141 [3100/6658 (47%)]\tLoss: 0.446589\n",
      "Train Epoch: 141 [3200/6658 (48%)]\tLoss: 2.550427\n",
      "Train Epoch: 141 [3300/6658 (50%)]\tLoss: 0.653397\n",
      "Train Epoch: 141 [3400/6658 (51%)]\tLoss: 0.001935\n",
      "Train Epoch: 141 [3500/6658 (53%)]\tLoss: 1.508858\n",
      "Train Epoch: 141 [3600/6658 (54%)]\tLoss: 0.383501\n",
      "Train Epoch: 141 [3700/6658 (56%)]\tLoss: 0.779746\n",
      "Train Epoch: 141 [3800/6658 (57%)]\tLoss: 2.800129\n",
      "Train Epoch: 141 [3900/6658 (59%)]\tLoss: 2.654394\n",
      "Train Epoch: 141 [4000/6658 (60%)]\tLoss: 0.005475\n",
      "Train Epoch: 141 [4100/6658 (62%)]\tLoss: 2.305733\n",
      "Train Epoch: 141 [4200/6658 (63%)]\tLoss: 0.065832\n",
      "Train Epoch: 141 [4300/6658 (65%)]\tLoss: 0.004447\n",
      "Train Epoch: 141 [4400/6658 (66%)]\tLoss: 0.068253\n",
      "Train Epoch: 141 [4500/6658 (68%)]\tLoss: 0.000898\n",
      "Train Epoch: 141 [4600/6658 (69%)]\tLoss: 0.009871\n",
      "Train Epoch: 141 [4700/6658 (71%)]\tLoss: 0.041172\n",
      "Train Epoch: 141 [4800/6658 (72%)]\tLoss: 0.016696\n",
      "Train Epoch: 141 [4900/6658 (74%)]\tLoss: 0.917886\n",
      "Train Epoch: 141 [5000/6658 (75%)]\tLoss: 0.357582\n",
      "Train Epoch: 141 [5100/6658 (77%)]\tLoss: 0.009641\n",
      "Train Epoch: 141 [5200/6658 (78%)]\tLoss: 0.805816\n",
      "Train Epoch: 141 [5300/6658 (80%)]\tLoss: 4.893095\n",
      "Train Epoch: 141 [5400/6658 (81%)]\tLoss: 0.358051\n",
      "Train Epoch: 141 [5500/6658 (83%)]\tLoss: 0.302672\n",
      "Train Epoch: 141 [5600/6658 (84%)]\tLoss: 0.442250\n",
      "Train Epoch: 141 [5700/6658 (86%)]\tLoss: 0.025683\n",
      "Train Epoch: 141 [5800/6658 (87%)]\tLoss: 0.227645\n",
      "Train Epoch: 141 [5900/6658 (89%)]\tLoss: 0.236475\n",
      "Train Epoch: 141 [6000/6658 (90%)]\tLoss: 0.001540\n",
      "Train Epoch: 141 [6100/6658 (92%)]\tLoss: 0.964147\n",
      "Train Epoch: 141 [6200/6658 (93%)]\tLoss: 0.251556\n",
      "Train Epoch: 141 [6300/6658 (95%)]\tLoss: 0.270786\n",
      "Train Epoch: 141 [6400/6658 (96%)]\tLoss: 14.784552\n",
      "Train Epoch: 141 [6500/6658 (98%)]\tLoss: 0.017687\n",
      "Train Epoch: 141 [6600/6658 (99%)]\tLoss: 0.700406\n",
      "train loss average =  0.7159912091007793\n",
      "\n",
      "Test set: Average loss: 0.6900\n",
      "\n",
      "EarlyStopping counter: 18 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.2995, 6.0936, 5.8575, 5.8651, 6.2113, 6.0473], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 142 [0/6658 (0%)]\tLoss: 0.899424\n",
      "Train Epoch: 142 [100/6658 (2%)]\tLoss: 0.001119\n",
      "Train Epoch: 142 [200/6658 (3%)]\tLoss: 0.439612\n",
      "Train Epoch: 142 [300/6658 (5%)]\tLoss: 0.017592\n",
      "Train Epoch: 142 [400/6658 (6%)]\tLoss: 4.108781\n",
      "Train Epoch: 142 [500/6658 (8%)]\tLoss: 0.349973\n",
      "Train Epoch: 142 [600/6658 (9%)]\tLoss: 0.105789\n",
      "Train Epoch: 142 [700/6658 (11%)]\tLoss: 1.458563\n",
      "Train Epoch: 142 [800/6658 (12%)]\tLoss: 0.020984\n",
      "Train Epoch: 142 [900/6658 (14%)]\tLoss: 0.887885\n",
      "Train Epoch: 142 [1000/6658 (15%)]\tLoss: 0.205263\n",
      "Train Epoch: 142 [1100/6658 (17%)]\tLoss: 0.000408\n",
      "Train Epoch: 142 [1200/6658 (18%)]\tLoss: 0.021596\n",
      "Train Epoch: 142 [1300/6658 (20%)]\tLoss: 0.086032\n",
      "Train Epoch: 142 [1400/6658 (21%)]\tLoss: 0.609125\n",
      "Train Epoch: 142 [1500/6658 (23%)]\tLoss: 0.226615\n",
      "Train Epoch: 142 [1600/6658 (24%)]\tLoss: 0.055143\n",
      "Train Epoch: 142 [1700/6658 (26%)]\tLoss: 1.474752\n",
      "Train Epoch: 142 [1800/6658 (27%)]\tLoss: 0.030968\n",
      "Train Epoch: 142 [1900/6658 (29%)]\tLoss: 1.309823\n",
      "Train Epoch: 142 [2000/6658 (30%)]\tLoss: 0.623159\n",
      "Train Epoch: 142 [2100/6658 (32%)]\tLoss: 0.004518\n",
      "Train Epoch: 142 [2200/6658 (33%)]\tLoss: 0.373039\n",
      "Train Epoch: 142 [2300/6658 (35%)]\tLoss: 0.141008\n",
      "Train Epoch: 142 [2400/6658 (36%)]\tLoss: 0.042222\n",
      "Train Epoch: 142 [2500/6658 (38%)]\tLoss: 0.108214\n",
      "Train Epoch: 142 [2600/6658 (39%)]\tLoss: 0.000572\n",
      "Train Epoch: 142 [2700/6658 (41%)]\tLoss: 0.076785\n",
      "Train Epoch: 142 [2800/6658 (42%)]\tLoss: 0.964737\n",
      "Train Epoch: 142 [2900/6658 (44%)]\tLoss: 0.000465\n",
      "Train Epoch: 142 [3000/6658 (45%)]\tLoss: 0.277422\n",
      "Train Epoch: 142 [3100/6658 (47%)]\tLoss: 0.139459\n",
      "Train Epoch: 142 [3200/6658 (48%)]\tLoss: 0.189359\n",
      "Train Epoch: 142 [3300/6658 (50%)]\tLoss: 1.077676\n",
      "Train Epoch: 142 [3400/6658 (51%)]\tLoss: 0.004208\n",
      "Train Epoch: 142 [3500/6658 (53%)]\tLoss: 0.113623\n",
      "Train Epoch: 142 [3600/6658 (54%)]\tLoss: 0.434172\n",
      "Train Epoch: 142 [3700/6658 (56%)]\tLoss: 0.924887\n",
      "Train Epoch: 142 [3800/6658 (57%)]\tLoss: 0.250533\n",
      "Train Epoch: 142 [3900/6658 (59%)]\tLoss: 0.105025\n",
      "Train Epoch: 142 [4000/6658 (60%)]\tLoss: 0.298050\n",
      "Train Epoch: 142 [4100/6658 (62%)]\tLoss: 0.433620\n",
      "Train Epoch: 142 [4200/6658 (63%)]\tLoss: 2.576393\n",
      "Train Epoch: 142 [4300/6658 (65%)]\tLoss: 0.939739\n",
      "Train Epoch: 142 [4400/6658 (66%)]\tLoss: 0.060784\n",
      "Train Epoch: 142 [4500/6658 (68%)]\tLoss: 0.877969\n",
      "Train Epoch: 142 [4600/6658 (69%)]\tLoss: 0.000779\n",
      "Train Epoch: 142 [4700/6658 (71%)]\tLoss: 0.067792\n",
      "Train Epoch: 142 [4800/6658 (72%)]\tLoss: 0.196681\n",
      "Train Epoch: 142 [4900/6658 (74%)]\tLoss: 0.435581\n",
      "Train Epoch: 142 [5000/6658 (75%)]\tLoss: 0.010981\n",
      "Train Epoch: 142 [5100/6658 (77%)]\tLoss: 2.226665\n",
      "Train Epoch: 142 [5200/6658 (78%)]\tLoss: 0.814096\n",
      "Train Epoch: 142 [5300/6658 (80%)]\tLoss: 0.310759\n",
      "Train Epoch: 142 [5400/6658 (81%)]\tLoss: 0.002702\n",
      "Train Epoch: 142 [5500/6658 (83%)]\tLoss: 0.103815\n",
      "Train Epoch: 142 [5600/6658 (84%)]\tLoss: 0.090080\n",
      "Train Epoch: 142 [5700/6658 (86%)]\tLoss: 0.028145\n",
      "Train Epoch: 142 [5800/6658 (87%)]\tLoss: 0.478480\n",
      "Train Epoch: 142 [5900/6658 (89%)]\tLoss: 0.001027\n",
      "Train Epoch: 142 [6000/6658 (90%)]\tLoss: 0.165439\n",
      "Train Epoch: 142 [6100/6658 (92%)]\tLoss: 0.019512\n",
      "Train Epoch: 142 [6200/6658 (93%)]\tLoss: 0.108071\n",
      "Train Epoch: 142 [6300/6658 (95%)]\tLoss: 0.736365\n",
      "Train Epoch: 142 [6400/6658 (96%)]\tLoss: 0.440577\n",
      "Train Epoch: 142 [6500/6658 (98%)]\tLoss: 0.194289\n",
      "Train Epoch: 142 [6600/6658 (99%)]\tLoss: 1.642875\n",
      "train loss average =  0.7135864471062531\n",
      "\n",
      "Test set: Average loss: 0.6961\n",
      "\n",
      "EarlyStopping counter: 19 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3017, 6.0940, 5.8578, 5.8649, 6.2130, 6.0470], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 143 [0/6658 (0%)]\tLoss: 0.000151\n",
      "Train Epoch: 143 [100/6658 (2%)]\tLoss: 0.055227\n",
      "Train Epoch: 143 [200/6658 (3%)]\tLoss: 0.256751\n",
      "Train Epoch: 143 [300/6658 (5%)]\tLoss: 0.229542\n",
      "Train Epoch: 143 [400/6658 (6%)]\tLoss: 0.047289\n",
      "Train Epoch: 143 [500/6658 (8%)]\tLoss: 0.096047\n",
      "Train Epoch: 143 [600/6658 (9%)]\tLoss: 12.074193\n",
      "Train Epoch: 143 [700/6658 (11%)]\tLoss: 0.205307\n",
      "Train Epoch: 143 [800/6658 (12%)]\tLoss: 0.757017\n",
      "Train Epoch: 143 [900/6658 (14%)]\tLoss: 0.145421\n",
      "Train Epoch: 143 [1000/6658 (15%)]\tLoss: 0.218052\n",
      "Train Epoch: 143 [1100/6658 (17%)]\tLoss: 0.955698\n",
      "Train Epoch: 143 [1200/6658 (18%)]\tLoss: 0.000184\n",
      "Train Epoch: 143 [1300/6658 (20%)]\tLoss: 0.583135\n",
      "Train Epoch: 143 [1400/6658 (21%)]\tLoss: 0.321111\n",
      "Train Epoch: 143 [1500/6658 (23%)]\tLoss: 0.266935\n",
      "Train Epoch: 143 [1600/6658 (24%)]\tLoss: 0.145764\n",
      "Train Epoch: 143 [1700/6658 (26%)]\tLoss: 0.833960\n",
      "Train Epoch: 143 [1800/6658 (27%)]\tLoss: 0.149961\n",
      "Train Epoch: 143 [1900/6658 (29%)]\tLoss: 0.076845\n",
      "Train Epoch: 143 [2000/6658 (30%)]\tLoss: 0.008622\n",
      "Train Epoch: 143 [2100/6658 (32%)]\tLoss: 0.334615\n",
      "Train Epoch: 143 [2200/6658 (33%)]\tLoss: 5.148676\n",
      "Train Epoch: 143 [2300/6658 (35%)]\tLoss: 0.019230\n",
      "Train Epoch: 143 [2400/6658 (36%)]\tLoss: 0.006122\n",
      "Train Epoch: 143 [2500/6658 (38%)]\tLoss: 0.273114\n",
      "Train Epoch: 143 [2600/6658 (39%)]\tLoss: 0.156801\n",
      "Train Epoch: 143 [2700/6658 (41%)]\tLoss: 1.602373\n",
      "Train Epoch: 143 [2800/6658 (42%)]\tLoss: 0.104942\n",
      "Train Epoch: 143 [2900/6658 (44%)]\tLoss: 2.234613\n",
      "Train Epoch: 143 [3000/6658 (45%)]\tLoss: 0.308647\n",
      "Train Epoch: 143 [3100/6658 (47%)]\tLoss: 0.334371\n",
      "Train Epoch: 143 [3200/6658 (48%)]\tLoss: 0.206512\n",
      "Train Epoch: 143 [3300/6658 (50%)]\tLoss: 0.494669\n",
      "Train Epoch: 143 [3400/6658 (51%)]\tLoss: 0.538697\n",
      "Train Epoch: 143 [3500/6658 (53%)]\tLoss: 0.120295\n",
      "Train Epoch: 143 [3600/6658 (54%)]\tLoss: 0.918612\n",
      "Train Epoch: 143 [3700/6658 (56%)]\tLoss: 6.871235\n",
      "Train Epoch: 143 [3800/6658 (57%)]\tLoss: 2.554845\n",
      "Train Epoch: 143 [3900/6658 (59%)]\tLoss: 0.234137\n",
      "Train Epoch: 143 [4000/6658 (60%)]\tLoss: 0.050638\n",
      "Train Epoch: 143 [4100/6658 (62%)]\tLoss: 9.016671\n",
      "Train Epoch: 143 [4200/6658 (63%)]\tLoss: 0.017010\n",
      "Train Epoch: 143 [4300/6658 (65%)]\tLoss: 0.224990\n",
      "Train Epoch: 143 [4400/6658 (66%)]\tLoss: 0.470042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 143 [4500/6658 (68%)]\tLoss: 0.200467\n",
      "Train Epoch: 143 [4600/6658 (69%)]\tLoss: 0.108372\n",
      "Train Epoch: 143 [4700/6658 (71%)]\tLoss: 0.376354\n",
      "Train Epoch: 143 [4800/6658 (72%)]\tLoss: 0.302207\n",
      "Train Epoch: 143 [4900/6658 (74%)]\tLoss: 1.602875\n",
      "Train Epoch: 143 [5000/6658 (75%)]\tLoss: 0.469870\n",
      "Train Epoch: 143 [5100/6658 (77%)]\tLoss: 0.123448\n",
      "Train Epoch: 143 [5200/6658 (78%)]\tLoss: 0.009100\n",
      "Train Epoch: 143 [5300/6658 (80%)]\tLoss: 0.000110\n",
      "Train Epoch: 143 [5400/6658 (81%)]\tLoss: 0.916834\n",
      "Train Epoch: 143 [5500/6658 (83%)]\tLoss: 0.010146\n",
      "Train Epoch: 143 [5600/6658 (84%)]\tLoss: 0.000004\n",
      "Train Epoch: 143 [5700/6658 (86%)]\tLoss: 0.052045\n",
      "Train Epoch: 143 [5800/6658 (87%)]\tLoss: 0.432448\n",
      "Train Epoch: 143 [5900/6658 (89%)]\tLoss: 2.964029\n",
      "Train Epoch: 143 [6000/6658 (90%)]\tLoss: 0.231403\n",
      "Train Epoch: 143 [6100/6658 (92%)]\tLoss: 0.244022\n",
      "Train Epoch: 143 [6200/6658 (93%)]\tLoss: 1.419309\n",
      "Train Epoch: 143 [6300/6658 (95%)]\tLoss: 2.755133\n",
      "Train Epoch: 143 [6400/6658 (96%)]\tLoss: 0.026209\n",
      "Train Epoch: 143 [6500/6658 (98%)]\tLoss: 0.915499\n",
      "Train Epoch: 143 [6600/6658 (99%)]\tLoss: 0.635838\n",
      "train loss average =  0.7111746917955905\n",
      "\n",
      "Test set: Average loss: 0.6969\n",
      "\n",
      "EarlyStopping counter: 20 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3036, 6.0950, 5.8575, 5.8645, 6.2150, 6.0470], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 144 [0/6658 (0%)]\tLoss: 4.145036\n",
      "Train Epoch: 144 [100/6658 (2%)]\tLoss: 0.696977\n",
      "Train Epoch: 144 [200/6658 (3%)]\tLoss: 2.739207\n",
      "Train Epoch: 144 [300/6658 (5%)]\tLoss: 0.022984\n",
      "Train Epoch: 144 [400/6658 (6%)]\tLoss: 0.043470\n",
      "Train Epoch: 144 [500/6658 (8%)]\tLoss: 0.494958\n",
      "Train Epoch: 144 [600/6658 (9%)]\tLoss: 0.035149\n",
      "Train Epoch: 144 [700/6658 (11%)]\tLoss: 0.739017\n",
      "Train Epoch: 144 [800/6658 (12%)]\tLoss: 1.017621\n",
      "Train Epoch: 144 [900/6658 (14%)]\tLoss: 0.029515\n",
      "Train Epoch: 144 [1000/6658 (15%)]\tLoss: 0.149623\n",
      "Train Epoch: 144 [1100/6658 (17%)]\tLoss: 0.101707\n",
      "Train Epoch: 144 [1200/6658 (18%)]\tLoss: 0.000843\n",
      "Train Epoch: 144 [1300/6658 (20%)]\tLoss: 0.993858\n",
      "Train Epoch: 144 [1400/6658 (21%)]\tLoss: 1.099338\n",
      "Train Epoch: 144 [1500/6658 (23%)]\tLoss: 0.275829\n",
      "Train Epoch: 144 [1600/6658 (24%)]\tLoss: 0.152955\n",
      "Train Epoch: 144 [1700/6658 (26%)]\tLoss: 0.047650\n",
      "Train Epoch: 144 [1800/6658 (27%)]\tLoss: 1.368467\n",
      "Train Epoch: 144 [1900/6658 (29%)]\tLoss: 0.239674\n",
      "Train Epoch: 144 [2000/6658 (30%)]\tLoss: 0.003613\n",
      "Train Epoch: 144 [2100/6658 (32%)]\tLoss: 0.261857\n",
      "Train Epoch: 144 [2200/6658 (33%)]\tLoss: 0.000210\n",
      "Train Epoch: 144 [2300/6658 (35%)]\tLoss: 2.555573\n",
      "Train Epoch: 144 [2400/6658 (36%)]\tLoss: 0.053576\n",
      "Train Epoch: 144 [2500/6658 (38%)]\tLoss: 0.011177\n",
      "Train Epoch: 144 [2600/6658 (39%)]\tLoss: 0.024910\n",
      "Train Epoch: 144 [2700/6658 (41%)]\tLoss: 0.243635\n",
      "Train Epoch: 144 [2800/6658 (42%)]\tLoss: 0.000481\n",
      "Train Epoch: 144 [2900/6658 (44%)]\tLoss: 0.390085\n",
      "Train Epoch: 144 [3000/6658 (45%)]\tLoss: 0.122584\n",
      "Train Epoch: 144 [3100/6658 (47%)]\tLoss: 0.000730\n",
      "Train Epoch: 144 [3200/6658 (48%)]\tLoss: 0.315057\n",
      "Train Epoch: 144 [3300/6658 (50%)]\tLoss: 0.531935\n",
      "Train Epoch: 144 [3400/6658 (51%)]\tLoss: 0.548660\n",
      "Train Epoch: 144 [3500/6658 (53%)]\tLoss: 0.389671\n",
      "Train Epoch: 144 [3600/6658 (54%)]\tLoss: 0.174849\n",
      "Train Epoch: 144 [3700/6658 (56%)]\tLoss: 0.069693\n",
      "Train Epoch: 144 [3800/6658 (57%)]\tLoss: 0.003397\n",
      "Train Epoch: 144 [3900/6658 (59%)]\tLoss: 0.130529\n",
      "Train Epoch: 144 [4000/6658 (60%)]\tLoss: 2.289366\n",
      "Train Epoch: 144 [4100/6658 (62%)]\tLoss: 0.248284\n",
      "Train Epoch: 144 [4200/6658 (63%)]\tLoss: 0.247888\n",
      "Train Epoch: 144 [4300/6658 (65%)]\tLoss: 0.066658\n",
      "Train Epoch: 144 [4400/6658 (66%)]\tLoss: 1.236940\n",
      "Train Epoch: 144 [4500/6658 (68%)]\tLoss: 1.208664\n",
      "Train Epoch: 144 [4600/6658 (69%)]\tLoss: 0.579899\n",
      "Train Epoch: 144 [4700/6658 (71%)]\tLoss: 0.060613\n",
      "Train Epoch: 144 [4800/6658 (72%)]\tLoss: 0.002972\n",
      "Train Epoch: 144 [4900/6658 (74%)]\tLoss: 2.204353\n",
      "Train Epoch: 144 [5000/6658 (75%)]\tLoss: 0.342020\n",
      "Train Epoch: 144 [5100/6658 (77%)]\tLoss: 0.508981\n",
      "Train Epoch: 144 [5200/6658 (78%)]\tLoss: 0.722295\n",
      "Train Epoch: 144 [5300/6658 (80%)]\tLoss: 0.149844\n",
      "Train Epoch: 144 [5400/6658 (81%)]\tLoss: 0.863499\n",
      "Train Epoch: 144 [5500/6658 (83%)]\tLoss: 0.690634\n",
      "Train Epoch: 144 [5600/6658 (84%)]\tLoss: 0.721247\n",
      "Train Epoch: 144 [5700/6658 (86%)]\tLoss: 0.693064\n",
      "Train Epoch: 144 [5800/6658 (87%)]\tLoss: 0.133338\n",
      "Train Epoch: 144 [5900/6658 (89%)]\tLoss: 0.126043\n",
      "Train Epoch: 144 [6000/6658 (90%)]\tLoss: 0.209201\n",
      "Train Epoch: 144 [6100/6658 (92%)]\tLoss: 0.044949\n",
      "Train Epoch: 144 [6200/6658 (93%)]\tLoss: 0.015078\n",
      "Train Epoch: 144 [6300/6658 (95%)]\tLoss: 0.321229\n",
      "Train Epoch: 144 [6400/6658 (96%)]\tLoss: 14.879612\n",
      "Train Epoch: 144 [6500/6658 (98%)]\tLoss: 0.660005\n",
      "Train Epoch: 144 [6600/6658 (99%)]\tLoss: 0.042970\n",
      "train loss average =  0.7118847814133651\n",
      "\n",
      "Test set: Average loss: 0.7212\n",
      "\n",
      "EarlyStopping counter: 21 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3057, 6.0960, 5.8574, 5.8657, 6.2168, 6.0467], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 145 [0/6658 (0%)]\tLoss: 0.356282\n",
      "Train Epoch: 145 [100/6658 (2%)]\tLoss: 1.448838\n",
      "Train Epoch: 145 [200/6658 (3%)]\tLoss: 0.099196\n",
      "Train Epoch: 145 [300/6658 (5%)]\tLoss: 0.003589\n",
      "Train Epoch: 145 [400/6658 (6%)]\tLoss: 0.064257\n",
      "Train Epoch: 145 [500/6658 (8%)]\tLoss: 0.048288\n",
      "Train Epoch: 145 [600/6658 (9%)]\tLoss: 0.365944\n",
      "Train Epoch: 145 [700/6658 (11%)]\tLoss: 0.193725\n",
      "Train Epoch: 145 [800/6658 (12%)]\tLoss: 0.866210\n",
      "Train Epoch: 145 [900/6658 (14%)]\tLoss: 0.153750\n",
      "Train Epoch: 145 [1000/6658 (15%)]\tLoss: 0.629816\n",
      "Train Epoch: 145 [1100/6658 (17%)]\tLoss: 0.789932\n",
      "Train Epoch: 145 [1200/6658 (18%)]\tLoss: 0.006361\n",
      "Train Epoch: 145 [1300/6658 (20%)]\tLoss: 0.012694\n",
      "Train Epoch: 145 [1400/6658 (21%)]\tLoss: 0.636153\n",
      "Train Epoch: 145 [1500/6658 (23%)]\tLoss: 0.061584\n",
      "Train Epoch: 145 [1600/6658 (24%)]\tLoss: 0.239330\n",
      "Train Epoch: 145 [1700/6658 (26%)]\tLoss: 0.677851\n",
      "Train Epoch: 145 [1800/6658 (27%)]\tLoss: 0.392055\n",
      "Train Epoch: 145 [1900/6658 (29%)]\tLoss: 0.300071\n",
      "Train Epoch: 145 [2000/6658 (30%)]\tLoss: 0.192239\n",
      "Train Epoch: 145 [2100/6658 (32%)]\tLoss: 0.240044\n",
      "Train Epoch: 145 [2200/6658 (33%)]\tLoss: 0.085209\n",
      "Train Epoch: 145 [2300/6658 (35%)]\tLoss: 0.029107\n",
      "Train Epoch: 145 [2400/6658 (36%)]\tLoss: 0.002632\n",
      "Train Epoch: 145 [2500/6658 (38%)]\tLoss: 0.012277\n",
      "Train Epoch: 145 [2600/6658 (39%)]\tLoss: 0.760323\n",
      "Train Epoch: 145 [2700/6658 (41%)]\tLoss: 3.372816\n",
      "Train Epoch: 145 [2800/6658 (42%)]\tLoss: 1.503413\n",
      "Train Epoch: 145 [2900/6658 (44%)]\tLoss: 1.928356\n",
      "Train Epoch: 145 [3000/6658 (45%)]\tLoss: 0.169193\n",
      "Train Epoch: 145 [3100/6658 (47%)]\tLoss: 0.112856\n",
      "Train Epoch: 145 [3200/6658 (48%)]\tLoss: 0.520642\n",
      "Train Epoch: 145 [3300/6658 (50%)]\tLoss: 0.103422\n",
      "Train Epoch: 145 [3400/6658 (51%)]\tLoss: 0.189912\n",
      "Train Epoch: 145 [3500/6658 (53%)]\tLoss: 0.278028\n",
      "Train Epoch: 145 [3600/6658 (54%)]\tLoss: 0.808943\n",
      "Train Epoch: 145 [3700/6658 (56%)]\tLoss: 0.620236\n",
      "Train Epoch: 145 [3800/6658 (57%)]\tLoss: 0.896982\n",
      "Train Epoch: 145 [3900/6658 (59%)]\tLoss: 0.073320\n",
      "Train Epoch: 145 [4000/6658 (60%)]\tLoss: 0.193397\n",
      "Train Epoch: 145 [4100/6658 (62%)]\tLoss: 0.256303\n",
      "Train Epoch: 145 [4200/6658 (63%)]\tLoss: 0.079174\n",
      "Train Epoch: 145 [4300/6658 (65%)]\tLoss: 0.393484\n",
      "Train Epoch: 145 [4400/6658 (66%)]\tLoss: 0.236514\n",
      "Train Epoch: 145 [4500/6658 (68%)]\tLoss: 0.295401\n",
      "Train Epoch: 145 [4600/6658 (69%)]\tLoss: 0.074498\n",
      "Train Epoch: 145 [4700/6658 (71%)]\tLoss: 0.040162\n",
      "Train Epoch: 145 [4800/6658 (72%)]\tLoss: 0.057138\n",
      "Train Epoch: 145 [4900/6658 (74%)]\tLoss: 0.266317\n",
      "Train Epoch: 145 [5000/6658 (75%)]\tLoss: 2.897529\n",
      "Train Epoch: 145 [5100/6658 (77%)]\tLoss: 0.016815\n",
      "Train Epoch: 145 [5200/6658 (78%)]\tLoss: 0.059798\n",
      "Train Epoch: 145 [5300/6658 (80%)]\tLoss: 0.373802\n",
      "Train Epoch: 145 [5400/6658 (81%)]\tLoss: 0.231790\n",
      "Train Epoch: 145 [5500/6658 (83%)]\tLoss: 0.412889\n",
      "Train Epoch: 145 [5600/6658 (84%)]\tLoss: 0.106840\n",
      "Train Epoch: 145 [5700/6658 (86%)]\tLoss: 0.195153\n",
      "Train Epoch: 145 [5800/6658 (87%)]\tLoss: 0.162419\n",
      "Train Epoch: 145 [5900/6658 (89%)]\tLoss: 1.167644\n",
      "Train Epoch: 145 [6000/6658 (90%)]\tLoss: 0.000147\n",
      "Train Epoch: 145 [6100/6658 (92%)]\tLoss: 0.184552\n",
      "Train Epoch: 145 [6200/6658 (93%)]\tLoss: 0.001965\n",
      "Train Epoch: 145 [6300/6658 (95%)]\tLoss: 1.298430\n",
      "Train Epoch: 145 [6400/6658 (96%)]\tLoss: 0.800753\n",
      "Train Epoch: 145 [6500/6658 (98%)]\tLoss: 0.015209\n",
      "Train Epoch: 145 [6600/6658 (99%)]\tLoss: 0.531476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss average =  0.7098454562365712\n",
      "\n",
      "Test set: Average loss: 0.6868\n",
      "\n",
      "EarlyStopping counter: 22 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3071, 6.0961, 5.8565, 5.8654, 6.2186, 6.0470], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 146 [0/6658 (0%)]\tLoss: 1.282556\n",
      "Train Epoch: 146 [100/6658 (2%)]\tLoss: 0.679427\n",
      "Train Epoch: 146 [200/6658 (3%)]\tLoss: 0.044789\n",
      "Train Epoch: 146 [300/6658 (5%)]\tLoss: 1.436080\n",
      "Train Epoch: 146 [400/6658 (6%)]\tLoss: 0.002972\n",
      "Train Epoch: 146 [500/6658 (8%)]\tLoss: 6.590546\n",
      "Train Epoch: 146 [600/6658 (9%)]\tLoss: 0.654935\n",
      "Train Epoch: 146 [700/6658 (11%)]\tLoss: 0.838389\n",
      "Train Epoch: 146 [800/6658 (12%)]\tLoss: 0.967209\n",
      "Train Epoch: 146 [900/6658 (14%)]\tLoss: 1.162666\n",
      "Train Epoch: 146 [1000/6658 (15%)]\tLoss: 0.218789\n",
      "Train Epoch: 146 [1100/6658 (17%)]\tLoss: 0.058918\n",
      "Train Epoch: 146 [1200/6658 (18%)]\tLoss: 0.797414\n",
      "Train Epoch: 146 [1300/6658 (20%)]\tLoss: 0.003443\n",
      "Train Epoch: 146 [1400/6658 (21%)]\tLoss: 0.028047\n",
      "Train Epoch: 146 [1500/6658 (23%)]\tLoss: 0.028996\n",
      "Train Epoch: 146 [1600/6658 (24%)]\tLoss: 0.141288\n",
      "Train Epoch: 146 [1700/6658 (26%)]\tLoss: 0.134180\n",
      "Train Epoch: 146 [1800/6658 (27%)]\tLoss: 0.037963\n",
      "Train Epoch: 146 [1900/6658 (29%)]\tLoss: 3.789562\n",
      "Train Epoch: 146 [2000/6658 (30%)]\tLoss: 0.054873\n",
      "Train Epoch: 146 [2100/6658 (32%)]\tLoss: 0.008800\n",
      "Train Epoch: 146 [2200/6658 (33%)]\tLoss: 0.778853\n",
      "Train Epoch: 146 [2300/6658 (35%)]\tLoss: 0.049898\n",
      "Train Epoch: 146 [2400/6658 (36%)]\tLoss: 0.062621\n",
      "Train Epoch: 146 [2500/6658 (38%)]\tLoss: 0.381496\n",
      "Train Epoch: 146 [2600/6658 (39%)]\tLoss: 2.318540\n",
      "Train Epoch: 146 [2700/6658 (41%)]\tLoss: 0.096498\n",
      "Train Epoch: 146 [2800/6658 (42%)]\tLoss: 0.000955\n",
      "Train Epoch: 146 [2900/6658 (44%)]\tLoss: 0.982331\n",
      "Train Epoch: 146 [3000/6658 (45%)]\tLoss: 1.342961\n",
      "Train Epoch: 146 [3100/6658 (47%)]\tLoss: 1.330586\n",
      "Train Epoch: 146 [3200/6658 (48%)]\tLoss: 0.009943\n",
      "Train Epoch: 146 [3300/6658 (50%)]\tLoss: 0.045666\n",
      "Train Epoch: 146 [3400/6658 (51%)]\tLoss: 0.380428\n",
      "Train Epoch: 146 [3500/6658 (53%)]\tLoss: 0.238233\n",
      "Train Epoch: 146 [3600/6658 (54%)]\tLoss: 0.171990\n",
      "Train Epoch: 146 [3700/6658 (56%)]\tLoss: 0.008078\n",
      "Train Epoch: 146 [3800/6658 (57%)]\tLoss: 0.028320\n",
      "Train Epoch: 146 [3900/6658 (59%)]\tLoss: 0.047185\n",
      "Train Epoch: 146 [4000/6658 (60%)]\tLoss: 0.005582\n",
      "Train Epoch: 146 [4100/6658 (62%)]\tLoss: 0.022807\n",
      "Train Epoch: 146 [4200/6658 (63%)]\tLoss: 0.352978\n",
      "Train Epoch: 146 [4300/6658 (65%)]\tLoss: 0.368698\n",
      "Train Epoch: 146 [4400/6658 (66%)]\tLoss: 0.001666\n",
      "Train Epoch: 146 [4500/6658 (68%)]\tLoss: 0.038643\n",
      "Train Epoch: 146 [4600/6658 (69%)]\tLoss: 0.225645\n",
      "Train Epoch: 146 [4700/6658 (71%)]\tLoss: 0.332885\n",
      "Train Epoch: 146 [4800/6658 (72%)]\tLoss: 0.283423\n",
      "Train Epoch: 146 [4900/6658 (74%)]\tLoss: 0.856928\n",
      "Train Epoch: 146 [5000/6658 (75%)]\tLoss: 0.003572\n",
      "Train Epoch: 146 [5100/6658 (77%)]\tLoss: 0.776325\n",
      "Train Epoch: 146 [5200/6658 (78%)]\tLoss: 0.607265\n",
      "Train Epoch: 146 [5300/6658 (80%)]\tLoss: 0.398668\n",
      "Train Epoch: 146 [5400/6658 (81%)]\tLoss: 0.290178\n",
      "Train Epoch: 146 [5500/6658 (83%)]\tLoss: 8.296324\n",
      "Train Epoch: 146 [5600/6658 (84%)]\tLoss: 0.493602\n",
      "Train Epoch: 146 [5700/6658 (86%)]\tLoss: 0.002678\n",
      "Train Epoch: 146 [5800/6658 (87%)]\tLoss: 0.043865\n",
      "Train Epoch: 146 [5900/6658 (89%)]\tLoss: 0.627692\n",
      "Train Epoch: 146 [6000/6658 (90%)]\tLoss: 0.002346\n",
      "Train Epoch: 146 [6100/6658 (92%)]\tLoss: 0.003470\n",
      "Train Epoch: 146 [6200/6658 (93%)]\tLoss: 1.934691\n",
      "Train Epoch: 146 [6300/6658 (95%)]\tLoss: 0.006697\n",
      "Train Epoch: 146 [6400/6658 (96%)]\tLoss: 0.001339\n",
      "Train Epoch: 146 [6500/6658 (98%)]\tLoss: 0.230832\n",
      "Train Epoch: 146 [6600/6658 (99%)]\tLoss: 0.008897\n",
      "train loss average =  0.7131141807588375\n",
      "\n",
      "Test set: Average loss: 0.6952\n",
      "\n",
      "EarlyStopping counter: 23 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3087, 6.0976, 5.8558, 5.8635, 6.2205, 6.0469], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 147 [0/6658 (0%)]\tLoss: 0.004684\n",
      "Train Epoch: 147 [100/6658 (2%)]\tLoss: 0.291442\n",
      "Train Epoch: 147 [200/6658 (3%)]\tLoss: 0.065931\n",
      "Train Epoch: 147 [300/6658 (5%)]\tLoss: 0.326612\n",
      "Train Epoch: 147 [400/6658 (6%)]\tLoss: 0.910104\n",
      "Train Epoch: 147 [500/6658 (8%)]\tLoss: 0.279086\n",
      "Train Epoch: 147 [600/6658 (9%)]\tLoss: 1.830327\n",
      "Train Epoch: 147 [700/6658 (11%)]\tLoss: 0.189418\n",
      "Train Epoch: 147 [800/6658 (12%)]\tLoss: 1.290980\n",
      "Train Epoch: 147 [900/6658 (14%)]\tLoss: 1.889185\n",
      "Train Epoch: 147 [1000/6658 (15%)]\tLoss: 0.386238\n",
      "Train Epoch: 147 [1100/6658 (17%)]\tLoss: 0.475870\n",
      "Train Epoch: 147 [1200/6658 (18%)]\tLoss: 0.777691\n",
      "Train Epoch: 147 [1300/6658 (20%)]\tLoss: 0.386786\n",
      "Train Epoch: 147 [1400/6658 (21%)]\tLoss: 7.183645\n",
      "Train Epoch: 147 [1500/6658 (23%)]\tLoss: 0.069241\n",
      "Train Epoch: 147 [1600/6658 (24%)]\tLoss: 0.008246\n",
      "Train Epoch: 147 [1700/6658 (26%)]\tLoss: 0.402010\n",
      "Train Epoch: 147 [1800/6658 (27%)]\tLoss: 0.403818\n",
      "Train Epoch: 147 [1900/6658 (29%)]\tLoss: 0.319773\n",
      "Train Epoch: 147 [2000/6658 (30%)]\tLoss: 0.941651\n",
      "Train Epoch: 147 [2100/6658 (32%)]\tLoss: 1.357171\n",
      "Train Epoch: 147 [2200/6658 (33%)]\tLoss: 0.131277\n",
      "Train Epoch: 147 [2300/6658 (35%)]\tLoss: 0.005874\n",
      "Train Epoch: 147 [2400/6658 (36%)]\tLoss: 0.238026\n",
      "Train Epoch: 147 [2500/6658 (38%)]\tLoss: 0.007224\n",
      "Train Epoch: 147 [2600/6658 (39%)]\tLoss: 0.002923\n",
      "Train Epoch: 147 [2700/6658 (41%)]\tLoss: 0.284289\n",
      "Train Epoch: 147 [2800/6658 (42%)]\tLoss: 0.147888\n",
      "Train Epoch: 147 [2900/6658 (44%)]\tLoss: 1.408829\n",
      "Train Epoch: 147 [3000/6658 (45%)]\tLoss: 0.602283\n",
      "Train Epoch: 147 [3100/6658 (47%)]\tLoss: 0.137680\n",
      "Train Epoch: 147 [3200/6658 (48%)]\tLoss: 0.059143\n",
      "Train Epoch: 147 [3300/6658 (50%)]\tLoss: 0.409786\n",
      "Train Epoch: 147 [3400/6658 (51%)]\tLoss: 0.844609\n",
      "Train Epoch: 147 [3500/6658 (53%)]\tLoss: 0.047350\n",
      "Train Epoch: 147 [3600/6658 (54%)]\tLoss: 0.000003\n",
      "Train Epoch: 147 [3700/6658 (56%)]\tLoss: 0.170043\n",
      "Train Epoch: 147 [3800/6658 (57%)]\tLoss: 0.939339\n",
      "Train Epoch: 147 [3900/6658 (59%)]\tLoss: 0.095375\n",
      "Train Epoch: 147 [4000/6658 (60%)]\tLoss: 0.148334\n",
      "Train Epoch: 147 [4100/6658 (62%)]\tLoss: 0.379412\n",
      "Train Epoch: 147 [4200/6658 (63%)]\tLoss: 0.021742\n",
      "Train Epoch: 147 [4300/6658 (65%)]\tLoss: 0.047732\n",
      "Train Epoch: 147 [4400/6658 (66%)]\tLoss: 0.877806\n",
      "Train Epoch: 147 [4500/6658 (68%)]\tLoss: 4.921599\n",
      "Train Epoch: 147 [4600/6658 (69%)]\tLoss: 0.031407\n",
      "Train Epoch: 147 [4700/6658 (71%)]\tLoss: 0.005178\n",
      "Train Epoch: 147 [4800/6658 (72%)]\tLoss: 0.067595\n",
      "Train Epoch: 147 [4900/6658 (74%)]\tLoss: 0.276940\n",
      "Train Epoch: 147 [5000/6658 (75%)]\tLoss: 0.220834\n",
      "Train Epoch: 147 [5100/6658 (77%)]\tLoss: 0.000033\n",
      "Train Epoch: 147 [5200/6658 (78%)]\tLoss: 0.575547\n",
      "Train Epoch: 147 [5300/6658 (80%)]\tLoss: 0.620806\n",
      "Train Epoch: 147 [5400/6658 (81%)]\tLoss: 0.212645\n",
      "Train Epoch: 147 [5500/6658 (83%)]\tLoss: 0.201461\n",
      "Train Epoch: 147 [5600/6658 (84%)]\tLoss: 0.002645\n",
      "Train Epoch: 147 [5700/6658 (86%)]\tLoss: 6.507442\n",
      "Train Epoch: 147 [5800/6658 (87%)]\tLoss: 0.472688\n",
      "Train Epoch: 147 [5900/6658 (89%)]\tLoss: 0.035244\n",
      "Train Epoch: 147 [6000/6658 (90%)]\tLoss: 0.000739\n",
      "Train Epoch: 147 [6100/6658 (92%)]\tLoss: 0.978927\n",
      "Train Epoch: 147 [6200/6658 (93%)]\tLoss: 0.006075\n",
      "Train Epoch: 147 [6300/6658 (95%)]\tLoss: 12.307249\n",
      "Train Epoch: 147 [6400/6658 (96%)]\tLoss: 1.143214\n",
      "Train Epoch: 147 [6500/6658 (98%)]\tLoss: 0.921441\n",
      "Train Epoch: 147 [6600/6658 (99%)]\tLoss: 0.202385\n",
      "train loss average =  0.7161635351558978\n",
      "\n",
      "Test set: Average loss: 0.6769\n",
      "\n",
      "Validation loss decreased (0.680577 --> 0.676924).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.3107, 6.0986, 5.8557, 5.8629, 6.2220, 6.0467], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 148 [0/6658 (0%)]\tLoss: 0.059562\n",
      "Train Epoch: 148 [100/6658 (2%)]\tLoss: 0.024776\n",
      "Train Epoch: 148 [200/6658 (3%)]\tLoss: 0.031825\n",
      "Train Epoch: 148 [300/6658 (5%)]\tLoss: 0.083744\n",
      "Train Epoch: 148 [400/6658 (6%)]\tLoss: 1.917762\n",
      "Train Epoch: 148 [500/6658 (8%)]\tLoss: 0.208919\n",
      "Train Epoch: 148 [600/6658 (9%)]\tLoss: 0.010601\n",
      "Train Epoch: 148 [700/6658 (11%)]\tLoss: 0.010717\n",
      "Train Epoch: 148 [800/6658 (12%)]\tLoss: 0.432205\n",
      "Train Epoch: 148 [900/6658 (14%)]\tLoss: 0.002830\n",
      "Train Epoch: 148 [1000/6658 (15%)]\tLoss: 0.045122\n",
      "Train Epoch: 148 [1100/6658 (17%)]\tLoss: 0.411491\n",
      "Train Epoch: 148 [1200/6658 (18%)]\tLoss: 6.157866\n",
      "Train Epoch: 148 [1300/6658 (20%)]\tLoss: 0.004298\n",
      "Train Epoch: 148 [1400/6658 (21%)]\tLoss: 0.568401\n",
      "Train Epoch: 148 [1500/6658 (23%)]\tLoss: 0.586071\n",
      "Train Epoch: 148 [1600/6658 (24%)]\tLoss: 1.185485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 148 [1700/6658 (26%)]\tLoss: 0.141456\n",
      "Train Epoch: 148 [1800/6658 (27%)]\tLoss: 0.001487\n",
      "Train Epoch: 148 [1900/6658 (29%)]\tLoss: 0.187820\n",
      "Train Epoch: 148 [2000/6658 (30%)]\tLoss: 0.191946\n",
      "Train Epoch: 148 [2100/6658 (32%)]\tLoss: 0.093457\n",
      "Train Epoch: 148 [2200/6658 (33%)]\tLoss: 0.015336\n",
      "Train Epoch: 148 [2300/6658 (35%)]\tLoss: 0.000283\n",
      "Train Epoch: 148 [2400/6658 (36%)]\tLoss: 0.216545\n",
      "Train Epoch: 148 [2500/6658 (38%)]\tLoss: 0.687102\n",
      "Train Epoch: 148 [2600/6658 (39%)]\tLoss: 1.093004\n",
      "Train Epoch: 148 [2700/6658 (41%)]\tLoss: 0.129295\n",
      "Train Epoch: 148 [2800/6658 (42%)]\tLoss: 0.292254\n",
      "Train Epoch: 148 [2900/6658 (44%)]\tLoss: 0.084189\n",
      "Train Epoch: 148 [3000/6658 (45%)]\tLoss: 0.011455\n",
      "Train Epoch: 148 [3100/6658 (47%)]\tLoss: 0.025861\n",
      "Train Epoch: 148 [3200/6658 (48%)]\tLoss: 0.956965\n",
      "Train Epoch: 148 [3300/6658 (50%)]\tLoss: 0.293932\n",
      "Train Epoch: 148 [3400/6658 (51%)]\tLoss: 0.196962\n",
      "Train Epoch: 148 [3500/6658 (53%)]\tLoss: 1.640819\n",
      "Train Epoch: 148 [3600/6658 (54%)]\tLoss: 0.371033\n",
      "Train Epoch: 148 [3700/6658 (56%)]\tLoss: 10.168427\n",
      "Train Epoch: 148 [3800/6658 (57%)]\tLoss: 0.023932\n",
      "Train Epoch: 148 [3900/6658 (59%)]\tLoss: 1.657231\n",
      "Train Epoch: 148 [4000/6658 (60%)]\tLoss: 0.062220\n",
      "Train Epoch: 148 [4100/6658 (62%)]\tLoss: 0.000731\n",
      "Train Epoch: 148 [4200/6658 (63%)]\tLoss: 0.924527\n",
      "Train Epoch: 148 [4300/6658 (65%)]\tLoss: 0.069949\n",
      "Train Epoch: 148 [4400/6658 (66%)]\tLoss: 0.126144\n",
      "Train Epoch: 148 [4500/6658 (68%)]\tLoss: 0.034431\n",
      "Train Epoch: 148 [4600/6658 (69%)]\tLoss: 9.817514\n",
      "Train Epoch: 148 [4700/6658 (71%)]\tLoss: 0.000128\n",
      "Train Epoch: 148 [4800/6658 (72%)]\tLoss: 2.043488\n",
      "Train Epoch: 148 [4900/6658 (74%)]\tLoss: 0.577841\n",
      "Train Epoch: 148 [5000/6658 (75%)]\tLoss: 0.397810\n",
      "Train Epoch: 148 [5100/6658 (77%)]\tLoss: 0.097951\n",
      "Train Epoch: 148 [5200/6658 (78%)]\tLoss: 0.006340\n",
      "Train Epoch: 148 [5300/6658 (80%)]\tLoss: 1.292311\n",
      "Train Epoch: 148 [5400/6658 (81%)]\tLoss: 1.274338\n",
      "Train Epoch: 148 [5500/6658 (83%)]\tLoss: 0.046682\n",
      "Train Epoch: 148 [5600/6658 (84%)]\tLoss: 0.214323\n",
      "Train Epoch: 148 [5700/6658 (86%)]\tLoss: 0.166414\n",
      "Train Epoch: 148 [5800/6658 (87%)]\tLoss: 0.010911\n",
      "Train Epoch: 148 [5900/6658 (89%)]\tLoss: 0.035451\n",
      "Train Epoch: 148 [6000/6658 (90%)]\tLoss: 0.352488\n",
      "Train Epoch: 148 [6100/6658 (92%)]\tLoss: 0.453025\n",
      "Train Epoch: 148 [6200/6658 (93%)]\tLoss: 0.075025\n",
      "Train Epoch: 148 [6300/6658 (95%)]\tLoss: 0.719940\n",
      "Train Epoch: 148 [6400/6658 (96%)]\tLoss: 3.080300\n",
      "Train Epoch: 148 [6500/6658 (98%)]\tLoss: 0.056812\n",
      "Train Epoch: 148 [6600/6658 (99%)]\tLoss: 2.412157\n",
      "train loss average =  0.7047829396573312\n",
      "\n",
      "Test set: Average loss: 0.6934\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3125, 6.0997, 5.8553, 5.8625, 6.2227, 6.0466], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 149 [0/6658 (0%)]\tLoss: 0.006485\n",
      "Train Epoch: 149 [100/6658 (2%)]\tLoss: 0.100496\n",
      "Train Epoch: 149 [200/6658 (3%)]\tLoss: 0.344813\n",
      "Train Epoch: 149 [300/6658 (5%)]\tLoss: 0.001094\n",
      "Train Epoch: 149 [400/6658 (6%)]\tLoss: 0.055889\n",
      "Train Epoch: 149 [500/6658 (8%)]\tLoss: 2.710822\n",
      "Train Epoch: 149 [600/6658 (9%)]\tLoss: 0.070486\n",
      "Train Epoch: 149 [700/6658 (11%)]\tLoss: 0.004682\n",
      "Train Epoch: 149 [800/6658 (12%)]\tLoss: 2.120078\n",
      "Train Epoch: 149 [900/6658 (14%)]\tLoss: 0.504774\n",
      "Train Epoch: 149 [1000/6658 (15%)]\tLoss: 0.661326\n",
      "Train Epoch: 149 [1100/6658 (17%)]\tLoss: 1.399712\n",
      "Train Epoch: 149 [1200/6658 (18%)]\tLoss: 0.774237\n",
      "Train Epoch: 149 [1300/6658 (20%)]\tLoss: 0.092415\n",
      "Train Epoch: 149 [1400/6658 (21%)]\tLoss: 0.005327\n",
      "Train Epoch: 149 [1500/6658 (23%)]\tLoss: 0.295953\n",
      "Train Epoch: 149 [1600/6658 (24%)]\tLoss: 0.304804\n",
      "Train Epoch: 149 [1700/6658 (26%)]\tLoss: 0.090185\n",
      "Train Epoch: 149 [1800/6658 (27%)]\tLoss: 1.821275\n",
      "Train Epoch: 149 [1900/6658 (29%)]\tLoss: 2.208256\n",
      "Train Epoch: 149 [2000/6658 (30%)]\tLoss: 1.405385\n",
      "Train Epoch: 149 [2100/6658 (32%)]\tLoss: 0.543362\n",
      "Train Epoch: 149 [2200/6658 (33%)]\tLoss: 0.343574\n",
      "Train Epoch: 149 [2300/6658 (35%)]\tLoss: 0.084971\n",
      "Train Epoch: 149 [2400/6658 (36%)]\tLoss: 0.947548\n",
      "Train Epoch: 149 [2500/6658 (38%)]\tLoss: 0.447841\n",
      "Train Epoch: 149 [2600/6658 (39%)]\tLoss: 0.321094\n",
      "Train Epoch: 149 [2700/6658 (41%)]\tLoss: 0.054753\n",
      "Train Epoch: 149 [2800/6658 (42%)]\tLoss: 0.174511\n",
      "Train Epoch: 149 [2900/6658 (44%)]\tLoss: 0.222760\n",
      "Train Epoch: 149 [3000/6658 (45%)]\tLoss: 0.000545\n",
      "Train Epoch: 149 [3100/6658 (47%)]\tLoss: 0.108154\n",
      "Train Epoch: 149 [3200/6658 (48%)]\tLoss: 0.000192\n",
      "Train Epoch: 149 [3300/6658 (50%)]\tLoss: 0.022139\n",
      "Train Epoch: 149 [3400/6658 (51%)]\tLoss: 3.600479\n",
      "Train Epoch: 149 [3500/6658 (53%)]\tLoss: 0.325471\n",
      "Train Epoch: 149 [3600/6658 (54%)]\tLoss: 0.020291\n",
      "Train Epoch: 149 [3700/6658 (56%)]\tLoss: 0.726064\n",
      "Train Epoch: 149 [3800/6658 (57%)]\tLoss: 14.504232\n",
      "Train Epoch: 149 [3900/6658 (59%)]\tLoss: 0.078534\n",
      "Train Epoch: 149 [4000/6658 (60%)]\tLoss: 5.686054\n",
      "Train Epoch: 149 [4100/6658 (62%)]\tLoss: 6.345655\n",
      "Train Epoch: 149 [4200/6658 (63%)]\tLoss: 0.006212\n",
      "Train Epoch: 149 [4300/6658 (65%)]\tLoss: 2.239208\n",
      "Train Epoch: 149 [4400/6658 (66%)]\tLoss: 0.035165\n",
      "Train Epoch: 149 [4500/6658 (68%)]\tLoss: 0.341469\n",
      "Train Epoch: 149 [4600/6658 (69%)]\tLoss: 0.149842\n",
      "Train Epoch: 149 [4700/6658 (71%)]\tLoss: 0.086281\n",
      "Train Epoch: 149 [4800/6658 (72%)]\tLoss: 0.064520\n",
      "Train Epoch: 149 [4900/6658 (74%)]\tLoss: 0.030215\n",
      "Train Epoch: 149 [5000/6658 (75%)]\tLoss: 0.154156\n",
      "Train Epoch: 149 [5100/6658 (77%)]\tLoss: 0.503805\n",
      "Train Epoch: 149 [5200/6658 (78%)]\tLoss: 0.204096\n",
      "Train Epoch: 149 [5300/6658 (80%)]\tLoss: 0.702501\n",
      "Train Epoch: 149 [5400/6658 (81%)]\tLoss: 0.336027\n",
      "Train Epoch: 149 [5500/6658 (83%)]\tLoss: 0.038824\n",
      "Train Epoch: 149 [5600/6658 (84%)]\tLoss: 4.812958\n",
      "Train Epoch: 149 [5700/6658 (86%)]\tLoss: 0.102585\n",
      "Train Epoch: 149 [5800/6658 (87%)]\tLoss: 0.035748\n",
      "Train Epoch: 149 [5900/6658 (89%)]\tLoss: 0.123331\n",
      "Train Epoch: 149 [6000/6658 (90%)]\tLoss: 1.117291\n",
      "Train Epoch: 149 [6100/6658 (92%)]\tLoss: 0.298638\n",
      "Train Epoch: 149 [6200/6658 (93%)]\tLoss: 0.042313\n",
      "Train Epoch: 149 [6300/6658 (95%)]\tLoss: 0.203185\n",
      "Train Epoch: 149 [6400/6658 (96%)]\tLoss: 0.537649\n",
      "Train Epoch: 149 [6500/6658 (98%)]\tLoss: 0.049413\n",
      "Train Epoch: 149 [6600/6658 (99%)]\tLoss: 0.065543\n",
      "train loss average =  0.7162508413445705\n",
      "\n",
      "Test set: Average loss: 0.7125\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3148, 6.1014, 5.8549, 5.8617, 6.2246, 6.0463], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 150 [0/6658 (0%)]\tLoss: 0.172200\n",
      "Train Epoch: 150 [100/6658 (2%)]\tLoss: 0.009363\n",
      "Train Epoch: 150 [200/6658 (3%)]\tLoss: 0.560468\n",
      "Train Epoch: 150 [300/6658 (5%)]\tLoss: 0.026300\n",
      "Train Epoch: 150 [400/6658 (6%)]\tLoss: 1.080563\n",
      "Train Epoch: 150 [500/6658 (8%)]\tLoss: 2.356808\n",
      "Train Epoch: 150 [600/6658 (9%)]\tLoss: 0.239803\n",
      "Train Epoch: 150 [700/6658 (11%)]\tLoss: 0.593184\n",
      "Train Epoch: 150 [800/6658 (12%)]\tLoss: 0.298667\n",
      "Train Epoch: 150 [900/6658 (14%)]\tLoss: 0.777062\n",
      "Train Epoch: 150 [1000/6658 (15%)]\tLoss: 0.002163\n",
      "Train Epoch: 150 [1100/6658 (17%)]\tLoss: 0.815507\n",
      "Train Epoch: 150 [1200/6658 (18%)]\tLoss: 0.438517\n",
      "Train Epoch: 150 [1300/6658 (20%)]\tLoss: 0.275551\n",
      "Train Epoch: 150 [1400/6658 (21%)]\tLoss: 0.000080\n",
      "Train Epoch: 150 [1500/6658 (23%)]\tLoss: 0.114167\n",
      "Train Epoch: 150 [1600/6658 (24%)]\tLoss: 0.202661\n",
      "Train Epoch: 150 [1700/6658 (26%)]\tLoss: 0.857483\n",
      "Train Epoch: 150 [1800/6658 (27%)]\tLoss: 10.933158\n",
      "Train Epoch: 150 [1900/6658 (29%)]\tLoss: 0.190360\n",
      "Train Epoch: 150 [2000/6658 (30%)]\tLoss: 0.487251\n",
      "Train Epoch: 150 [2100/6658 (32%)]\tLoss: 0.070903\n",
      "Train Epoch: 150 [2200/6658 (33%)]\tLoss: 0.095764\n",
      "Train Epoch: 150 [2300/6658 (35%)]\tLoss: 0.204825\n",
      "Train Epoch: 150 [2400/6658 (36%)]\tLoss: 0.626363\n",
      "Train Epoch: 150 [2500/6658 (38%)]\tLoss: 0.391337\n",
      "Train Epoch: 150 [2600/6658 (39%)]\tLoss: 0.357057\n",
      "Train Epoch: 150 [2700/6658 (41%)]\tLoss: 0.266980\n",
      "Train Epoch: 150 [2800/6658 (42%)]\tLoss: 1.774093\n",
      "Train Epoch: 150 [2900/6658 (44%)]\tLoss: 0.067744\n",
      "Train Epoch: 150 [3000/6658 (45%)]\tLoss: 0.016391\n",
      "Train Epoch: 150 [3100/6658 (47%)]\tLoss: 0.012091\n",
      "Train Epoch: 150 [3200/6658 (48%)]\tLoss: 0.271313\n",
      "Train Epoch: 150 [3300/6658 (50%)]\tLoss: 0.129368\n",
      "Train Epoch: 150 [3400/6658 (51%)]\tLoss: 0.090879\n",
      "Train Epoch: 150 [3500/6658 (53%)]\tLoss: 0.071004\n",
      "Train Epoch: 150 [3600/6658 (54%)]\tLoss: 0.084455\n",
      "Train Epoch: 150 [3700/6658 (56%)]\tLoss: 0.528458\n",
      "Train Epoch: 150 [3800/6658 (57%)]\tLoss: 0.027582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 150 [3900/6658 (59%)]\tLoss: 0.287841\n",
      "Train Epoch: 150 [4000/6658 (60%)]\tLoss: 1.076788\n",
      "Train Epoch: 150 [4100/6658 (62%)]\tLoss: 0.529765\n",
      "Train Epoch: 150 [4200/6658 (63%)]\tLoss: 1.179940\n",
      "Train Epoch: 150 [4300/6658 (65%)]\tLoss: 0.093152\n",
      "Train Epoch: 150 [4400/6658 (66%)]\tLoss: 0.016184\n",
      "Train Epoch: 150 [4500/6658 (68%)]\tLoss: 0.018658\n",
      "Train Epoch: 150 [4600/6658 (69%)]\tLoss: 0.225046\n",
      "Train Epoch: 150 [4700/6658 (71%)]\tLoss: 1.281680\n",
      "Train Epoch: 150 [4800/6658 (72%)]\tLoss: 0.140128\n",
      "Train Epoch: 150 [4900/6658 (74%)]\tLoss: 0.012570\n",
      "Train Epoch: 150 [5000/6658 (75%)]\tLoss: 0.005480\n",
      "Train Epoch: 150 [5100/6658 (77%)]\tLoss: 0.465689\n",
      "Train Epoch: 150 [5200/6658 (78%)]\tLoss: 0.004470\n",
      "Train Epoch: 150 [5300/6658 (80%)]\tLoss: 0.929465\n",
      "Train Epoch: 150 [5400/6658 (81%)]\tLoss: 0.028804\n",
      "Train Epoch: 150 [5500/6658 (83%)]\tLoss: 0.817062\n",
      "Train Epoch: 150 [5600/6658 (84%)]\tLoss: 0.627292\n",
      "Train Epoch: 150 [5700/6658 (86%)]\tLoss: 0.000905\n",
      "Train Epoch: 150 [5800/6658 (87%)]\tLoss: 0.692255\n",
      "Train Epoch: 150 [5900/6658 (89%)]\tLoss: 0.088907\n",
      "Train Epoch: 150 [6000/6658 (90%)]\tLoss: 0.490434\n",
      "Train Epoch: 150 [6100/6658 (92%)]\tLoss: 0.051137\n",
      "Train Epoch: 150 [6200/6658 (93%)]\tLoss: 0.160345\n",
      "Train Epoch: 150 [6300/6658 (95%)]\tLoss: 0.835950\n",
      "Train Epoch: 150 [6400/6658 (96%)]\tLoss: 0.001858\n",
      "Train Epoch: 150 [6500/6658 (98%)]\tLoss: 0.070815\n",
      "Train Epoch: 150 [6600/6658 (99%)]\tLoss: 0.092103\n",
      "train loss average =  0.7030626131937744\n",
      "\n",
      "Test set: Average loss: 0.6956\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3166, 6.1023, 5.8544, 5.8615, 6.2258, 6.0464], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 151 [0/6658 (0%)]\tLoss: 0.460176\n",
      "Train Epoch: 151 [100/6658 (2%)]\tLoss: 0.232806\n",
      "Train Epoch: 151 [200/6658 (3%)]\tLoss: 0.508189\n",
      "Train Epoch: 151 [300/6658 (5%)]\tLoss: 0.009876\n",
      "Train Epoch: 151 [400/6658 (6%)]\tLoss: 1.044665\n",
      "Train Epoch: 151 [500/6658 (8%)]\tLoss: 3.140790\n",
      "Train Epoch: 151 [600/6658 (9%)]\tLoss: 0.123280\n",
      "Train Epoch: 151 [700/6658 (11%)]\tLoss: 0.180237\n",
      "Train Epoch: 151 [800/6658 (12%)]\tLoss: 0.592103\n",
      "Train Epoch: 151 [900/6658 (14%)]\tLoss: 0.003057\n",
      "Train Epoch: 151 [1000/6658 (15%)]\tLoss: 1.341894\n",
      "Train Epoch: 151 [1100/6658 (17%)]\tLoss: 0.089714\n",
      "Train Epoch: 151 [1200/6658 (18%)]\tLoss: 0.061781\n",
      "Train Epoch: 151 [1300/6658 (20%)]\tLoss: 0.477045\n",
      "Train Epoch: 151 [1400/6658 (21%)]\tLoss: 0.235098\n",
      "Train Epoch: 151 [1500/6658 (23%)]\tLoss: 0.144485\n",
      "Train Epoch: 151 [1600/6658 (24%)]\tLoss: 0.059513\n",
      "Train Epoch: 151 [1700/6658 (26%)]\tLoss: 0.282894\n",
      "Train Epoch: 151 [1800/6658 (27%)]\tLoss: 0.000456\n",
      "Train Epoch: 151 [1900/6658 (29%)]\tLoss: 0.002002\n",
      "Train Epoch: 151 [2000/6658 (30%)]\tLoss: 0.172526\n",
      "Train Epoch: 151 [2100/6658 (32%)]\tLoss: 0.751846\n",
      "Train Epoch: 151 [2200/6658 (33%)]\tLoss: 0.002092\n",
      "Train Epoch: 151 [2300/6658 (35%)]\tLoss: 0.052218\n",
      "Train Epoch: 151 [2400/6658 (36%)]\tLoss: 0.669008\n",
      "Train Epoch: 151 [2500/6658 (38%)]\tLoss: 9.574574\n",
      "Train Epoch: 151 [2600/6658 (39%)]\tLoss: 0.111727\n",
      "Train Epoch: 151 [2700/6658 (41%)]\tLoss: 0.349885\n",
      "Train Epoch: 151 [2800/6658 (42%)]\tLoss: 0.499220\n",
      "Train Epoch: 151 [2900/6658 (44%)]\tLoss: 0.471645\n",
      "Train Epoch: 151 [3000/6658 (45%)]\tLoss: 0.216723\n",
      "Train Epoch: 151 [3100/6658 (47%)]\tLoss: 1.996888\n",
      "Train Epoch: 151 [3200/6658 (48%)]\tLoss: 0.003082\n",
      "Train Epoch: 151 [3300/6658 (50%)]\tLoss: 1.665300\n",
      "Train Epoch: 151 [3400/6658 (51%)]\tLoss: 0.000179\n",
      "Train Epoch: 151 [3500/6658 (53%)]\tLoss: 0.063350\n",
      "Train Epoch: 151 [3600/6658 (54%)]\tLoss: 2.317280\n",
      "Train Epoch: 151 [3700/6658 (56%)]\tLoss: 0.728915\n",
      "Train Epoch: 151 [3800/6658 (57%)]\tLoss: 0.004285\n",
      "Train Epoch: 151 [3900/6658 (59%)]\tLoss: 0.898908\n",
      "Train Epoch: 151 [4000/6658 (60%)]\tLoss: 0.344906\n",
      "Train Epoch: 151 [4100/6658 (62%)]\tLoss: 0.209717\n",
      "Train Epoch: 151 [4200/6658 (63%)]\tLoss: 0.283846\n",
      "Train Epoch: 151 [4300/6658 (65%)]\tLoss: 0.314418\n",
      "Train Epoch: 151 [4400/6658 (66%)]\tLoss: 0.018648\n",
      "Train Epoch: 151 [4500/6658 (68%)]\tLoss: 0.000761\n",
      "Train Epoch: 151 [4600/6658 (69%)]\tLoss: 0.311576\n",
      "Train Epoch: 151 [4700/6658 (71%)]\tLoss: 0.765711\n",
      "Train Epoch: 151 [4800/6658 (72%)]\tLoss: 0.208785\n",
      "Train Epoch: 151 [4900/6658 (74%)]\tLoss: 0.968965\n",
      "Train Epoch: 151 [5000/6658 (75%)]\tLoss: 0.000356\n",
      "Train Epoch: 151 [5100/6658 (77%)]\tLoss: 2.604270\n",
      "Train Epoch: 151 [5200/6658 (78%)]\tLoss: 0.274127\n",
      "Train Epoch: 151 [5300/6658 (80%)]\tLoss: 0.077408\n",
      "Train Epoch: 151 [5400/6658 (81%)]\tLoss: 0.045670\n",
      "Train Epoch: 151 [5500/6658 (83%)]\tLoss: 0.308983\n",
      "Train Epoch: 151 [5600/6658 (84%)]\tLoss: 0.013221\n",
      "Train Epoch: 151 [5700/6658 (86%)]\tLoss: 0.008969\n",
      "Train Epoch: 151 [5800/6658 (87%)]\tLoss: 2.930196\n",
      "Train Epoch: 151 [5900/6658 (89%)]\tLoss: 0.442593\n",
      "Train Epoch: 151 [6000/6658 (90%)]\tLoss: 5.086779\n",
      "Train Epoch: 151 [6100/6658 (92%)]\tLoss: 0.440421\n",
      "Train Epoch: 151 [6200/6658 (93%)]\tLoss: 0.624901\n",
      "Train Epoch: 151 [6300/6658 (95%)]\tLoss: 0.472913\n",
      "Train Epoch: 151 [6400/6658 (96%)]\tLoss: 0.054319\n",
      "Train Epoch: 151 [6500/6658 (98%)]\tLoss: 0.284530\n",
      "Train Epoch: 151 [6600/6658 (99%)]\tLoss: 3.628418\n",
      "train loss average =  0.7140438660555662\n",
      "\n",
      "Test set: Average loss: 0.7023\n",
      "\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3186, 6.1035, 5.8543, 5.8616, 6.2275, 6.0461], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 152 [0/6658 (0%)]\tLoss: 0.000474\n",
      "Train Epoch: 152 [100/6658 (2%)]\tLoss: 0.039194\n",
      "Train Epoch: 152 [200/6658 (3%)]\tLoss: 0.128100\n",
      "Train Epoch: 152 [300/6658 (5%)]\tLoss: 0.040005\n",
      "Train Epoch: 152 [400/6658 (6%)]\tLoss: 0.001364\n",
      "Train Epoch: 152 [500/6658 (8%)]\tLoss: 0.077199\n",
      "Train Epoch: 152 [600/6658 (9%)]\tLoss: 0.001196\n",
      "Train Epoch: 152 [700/6658 (11%)]\tLoss: 0.004434\n",
      "Train Epoch: 152 [800/6658 (12%)]\tLoss: 0.748544\n",
      "Train Epoch: 152 [900/6658 (14%)]\tLoss: 0.568481\n",
      "Train Epoch: 152 [1000/6658 (15%)]\tLoss: 0.149131\n",
      "Train Epoch: 152 [1100/6658 (17%)]\tLoss: 0.000172\n",
      "Train Epoch: 152 [1200/6658 (18%)]\tLoss: 4.119463\n",
      "Train Epoch: 152 [1300/6658 (20%)]\tLoss: 0.223291\n",
      "Train Epoch: 152 [1400/6658 (21%)]\tLoss: 0.063562\n",
      "Train Epoch: 152 [1500/6658 (23%)]\tLoss: 0.344908\n",
      "Train Epoch: 152 [1600/6658 (24%)]\tLoss: 0.436751\n",
      "Train Epoch: 152 [1700/6658 (26%)]\tLoss: 0.433979\n",
      "Train Epoch: 152 [1800/6658 (27%)]\tLoss: 0.073957\n",
      "Train Epoch: 152 [1900/6658 (29%)]\tLoss: 3.477586\n",
      "Train Epoch: 152 [2000/6658 (30%)]\tLoss: 0.531500\n",
      "Train Epoch: 152 [2100/6658 (32%)]\tLoss: 0.087559\n",
      "Train Epoch: 152 [2200/6658 (33%)]\tLoss: 0.385090\n",
      "Train Epoch: 152 [2300/6658 (35%)]\tLoss: 0.186990\n",
      "Train Epoch: 152 [2400/6658 (36%)]\tLoss: 0.448136\n",
      "Train Epoch: 152 [2500/6658 (38%)]\tLoss: 0.359370\n",
      "Train Epoch: 152 [2600/6658 (39%)]\tLoss: 0.104080\n",
      "Train Epoch: 152 [2700/6658 (41%)]\tLoss: 0.538583\n",
      "Train Epoch: 152 [2800/6658 (42%)]\tLoss: 0.205211\n",
      "Train Epoch: 152 [2900/6658 (44%)]\tLoss: 0.313619\n",
      "Train Epoch: 152 [3000/6658 (45%)]\tLoss: 1.449595\n",
      "Train Epoch: 152 [3100/6658 (47%)]\tLoss: 0.382480\n",
      "Train Epoch: 152 [3200/6658 (48%)]\tLoss: 0.198506\n",
      "Train Epoch: 152 [3300/6658 (50%)]\tLoss: 0.336868\n",
      "Train Epoch: 152 [3400/6658 (51%)]\tLoss: 0.184146\n",
      "Train Epoch: 152 [3500/6658 (53%)]\tLoss: 0.019370\n",
      "Train Epoch: 152 [3600/6658 (54%)]\tLoss: 0.223948\n",
      "Train Epoch: 152 [3700/6658 (56%)]\tLoss: 0.340857\n",
      "Train Epoch: 152 [3800/6658 (57%)]\tLoss: 0.714865\n",
      "Train Epoch: 152 [3900/6658 (59%)]\tLoss: 0.250495\n",
      "Train Epoch: 152 [4000/6658 (60%)]\tLoss: 0.955394\n",
      "Train Epoch: 152 [4100/6658 (62%)]\tLoss: 0.241746\n",
      "Train Epoch: 152 [4200/6658 (63%)]\tLoss: 0.285702\n",
      "Train Epoch: 152 [4300/6658 (65%)]\tLoss: 0.169059\n",
      "Train Epoch: 152 [4400/6658 (66%)]\tLoss: 0.732092\n",
      "Train Epoch: 152 [4500/6658 (68%)]\tLoss: 0.065250\n",
      "Train Epoch: 152 [4600/6658 (69%)]\tLoss: 0.141796\n",
      "Train Epoch: 152 [4700/6658 (71%)]\tLoss: 0.214819\n",
      "Train Epoch: 152 [4800/6658 (72%)]\tLoss: 0.053060\n",
      "Train Epoch: 152 [4900/6658 (74%)]\tLoss: 0.020504\n",
      "Train Epoch: 152 [5000/6658 (75%)]\tLoss: 0.584723\n",
      "Train Epoch: 152 [5100/6658 (77%)]\tLoss: 0.040800\n",
      "Train Epoch: 152 [5200/6658 (78%)]\tLoss: 0.001041\n",
      "Train Epoch: 152 [5300/6658 (80%)]\tLoss: 0.013865\n",
      "Train Epoch: 152 [5400/6658 (81%)]\tLoss: 1.503756\n",
      "Train Epoch: 152 [5500/6658 (83%)]\tLoss: 0.071368\n",
      "Train Epoch: 152 [5600/6658 (84%)]\tLoss: 0.007848\n",
      "Train Epoch: 152 [5700/6658 (86%)]\tLoss: 5.075502\n",
      "Train Epoch: 152 [5800/6658 (87%)]\tLoss: 6.356186\n",
      "Train Epoch: 152 [5900/6658 (89%)]\tLoss: 0.618255\n",
      "Train Epoch: 152 [6000/6658 (90%)]\tLoss: 0.144820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 152 [6100/6658 (92%)]\tLoss: 0.057197\n",
      "Train Epoch: 152 [6200/6658 (93%)]\tLoss: 0.114035\n",
      "Train Epoch: 152 [6300/6658 (95%)]\tLoss: 0.115234\n",
      "Train Epoch: 152 [6400/6658 (96%)]\tLoss: 2.568081\n",
      "Train Epoch: 152 [6500/6658 (98%)]\tLoss: 1.406747\n",
      "Train Epoch: 152 [6600/6658 (99%)]\tLoss: 0.154499\n",
      "train loss average =  0.7133833880178558\n",
      "\n",
      "Test set: Average loss: 0.6863\n",
      "\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3196, 6.1039, 5.8547, 5.8622, 6.2294, 6.0458], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 153 [0/6658 (0%)]\tLoss: 0.394561\n",
      "Train Epoch: 153 [100/6658 (2%)]\tLoss: 0.145787\n",
      "Train Epoch: 153 [200/6658 (3%)]\tLoss: 0.100406\n",
      "Train Epoch: 153 [300/6658 (5%)]\tLoss: 2.702294\n",
      "Train Epoch: 153 [400/6658 (6%)]\tLoss: 0.115415\n",
      "Train Epoch: 153 [500/6658 (8%)]\tLoss: 1.343240\n",
      "Train Epoch: 153 [600/6658 (9%)]\tLoss: 0.002543\n",
      "Train Epoch: 153 [700/6658 (11%)]\tLoss: 3.056088\n",
      "Train Epoch: 153 [800/6658 (12%)]\tLoss: 0.042954\n",
      "Train Epoch: 153 [900/6658 (14%)]\tLoss: 0.566478\n",
      "Train Epoch: 153 [1000/6658 (15%)]\tLoss: 0.169311\n",
      "Train Epoch: 153 [1100/6658 (17%)]\tLoss: 1.411401\n",
      "Train Epoch: 153 [1200/6658 (18%)]\tLoss: 0.390696\n",
      "Train Epoch: 153 [1300/6658 (20%)]\tLoss: 0.002413\n",
      "Train Epoch: 153 [1400/6658 (21%)]\tLoss: 0.289571\n",
      "Train Epoch: 153 [1500/6658 (23%)]\tLoss: 0.415986\n",
      "Train Epoch: 153 [1600/6658 (24%)]\tLoss: 0.005666\n",
      "Train Epoch: 153 [1700/6658 (26%)]\tLoss: 0.079765\n",
      "Train Epoch: 153 [1800/6658 (27%)]\tLoss: 0.011310\n",
      "Train Epoch: 153 [1900/6658 (29%)]\tLoss: 1.791857\n",
      "Train Epoch: 153 [2000/6658 (30%)]\tLoss: 0.124205\n",
      "Train Epoch: 153 [2100/6658 (32%)]\tLoss: 0.168207\n",
      "Train Epoch: 153 [2200/6658 (33%)]\tLoss: 0.082632\n",
      "Train Epoch: 153 [2300/6658 (35%)]\tLoss: 1.028382\n",
      "Train Epoch: 153 [2400/6658 (36%)]\tLoss: 0.086148\n",
      "Train Epoch: 153 [2500/6658 (38%)]\tLoss: 0.696095\n",
      "Train Epoch: 153 [2600/6658 (39%)]\tLoss: 0.495156\n",
      "Train Epoch: 153 [2700/6658 (41%)]\tLoss: 0.639140\n",
      "Train Epoch: 153 [2800/6658 (42%)]\tLoss: 0.766155\n",
      "Train Epoch: 153 [2900/6658 (44%)]\tLoss: 0.071616\n",
      "Train Epoch: 153 [3000/6658 (45%)]\tLoss: 0.010896\n",
      "Train Epoch: 153 [3100/6658 (47%)]\tLoss: 0.454930\n",
      "Train Epoch: 153 [3200/6658 (48%)]\tLoss: 0.407448\n",
      "Train Epoch: 153 [3300/6658 (50%)]\tLoss: 0.753388\n",
      "Train Epoch: 153 [3400/6658 (51%)]\tLoss: 0.097001\n",
      "Train Epoch: 153 [3500/6658 (53%)]\tLoss: 0.297737\n",
      "Train Epoch: 153 [3600/6658 (54%)]\tLoss: 0.303011\n",
      "Train Epoch: 153 [3700/6658 (56%)]\tLoss: 1.303830\n",
      "Train Epoch: 153 [3800/6658 (57%)]\tLoss: 6.050372\n",
      "Train Epoch: 153 [3900/6658 (59%)]\tLoss: 0.006878\n",
      "Train Epoch: 153 [4000/6658 (60%)]\tLoss: 2.111312\n",
      "Train Epoch: 153 [4100/6658 (62%)]\tLoss: 0.209605\n",
      "Train Epoch: 153 [4200/6658 (63%)]\tLoss: 0.456500\n",
      "Train Epoch: 153 [4300/6658 (65%)]\tLoss: 0.120374\n",
      "Train Epoch: 153 [4400/6658 (66%)]\tLoss: 0.222633\n",
      "Train Epoch: 153 [4500/6658 (68%)]\tLoss: 0.753101\n",
      "Train Epoch: 153 [4600/6658 (69%)]\tLoss: 0.291968\n",
      "Train Epoch: 153 [4700/6658 (71%)]\tLoss: 0.073144\n",
      "Train Epoch: 153 [4800/6658 (72%)]\tLoss: 0.088902\n",
      "Train Epoch: 153 [4900/6658 (74%)]\tLoss: 1.043657\n",
      "Train Epoch: 153 [5000/6658 (75%)]\tLoss: 0.093389\n",
      "Train Epoch: 153 [5100/6658 (77%)]\tLoss: 0.356737\n",
      "Train Epoch: 153 [5200/6658 (78%)]\tLoss: 0.479110\n",
      "Train Epoch: 153 [5300/6658 (80%)]\tLoss: 0.165627\n",
      "Train Epoch: 153 [5400/6658 (81%)]\tLoss: 0.089505\n",
      "Train Epoch: 153 [5500/6658 (83%)]\tLoss: 1.756150\n",
      "Train Epoch: 153 [5600/6658 (84%)]\tLoss: 1.003168\n",
      "Train Epoch: 153 [5700/6658 (86%)]\tLoss: 0.452198\n",
      "Train Epoch: 153 [5800/6658 (87%)]\tLoss: 0.065649\n",
      "Train Epoch: 153 [5900/6658 (89%)]\tLoss: 1.091893\n",
      "Train Epoch: 153 [6000/6658 (90%)]\tLoss: 0.079743\n",
      "Train Epoch: 153 [6100/6658 (92%)]\tLoss: 1.654745\n",
      "Train Epoch: 153 [6200/6658 (93%)]\tLoss: 0.015239\n",
      "Train Epoch: 153 [6300/6658 (95%)]\tLoss: 0.003782\n",
      "Train Epoch: 153 [6400/6658 (96%)]\tLoss: 0.304462\n",
      "Train Epoch: 153 [6500/6658 (98%)]\tLoss: 0.324974\n",
      "Train Epoch: 153 [6600/6658 (99%)]\tLoss: 0.003615\n",
      "train loss average =  0.7044694536497669\n",
      "\n",
      "Test set: Average loss: 0.6871\n",
      "\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3217, 6.1045, 5.8545, 5.8620, 6.2309, 6.0454], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 154 [0/6658 (0%)]\tLoss: 0.158195\n",
      "Train Epoch: 154 [100/6658 (2%)]\tLoss: 0.949568\n",
      "Train Epoch: 154 [200/6658 (3%)]\tLoss: 0.029219\n",
      "Train Epoch: 154 [300/6658 (5%)]\tLoss: 0.026015\n",
      "Train Epoch: 154 [400/6658 (6%)]\tLoss: 0.735092\n",
      "Train Epoch: 154 [500/6658 (8%)]\tLoss: 0.012954\n",
      "Train Epoch: 154 [600/6658 (9%)]\tLoss: 0.109610\n",
      "Train Epoch: 154 [700/6658 (11%)]\tLoss: 0.059921\n",
      "Train Epoch: 154 [800/6658 (12%)]\tLoss: 1.310990\n",
      "Train Epoch: 154 [900/6658 (14%)]\tLoss: 0.575488\n",
      "Train Epoch: 154 [1000/6658 (15%)]\tLoss: 0.002991\n",
      "Train Epoch: 154 [1100/6658 (17%)]\tLoss: 0.073886\n",
      "Train Epoch: 154 [1200/6658 (18%)]\tLoss: 0.840110\n",
      "Train Epoch: 154 [1300/6658 (20%)]\tLoss: 0.283910\n",
      "Train Epoch: 154 [1400/6658 (21%)]\tLoss: 0.381618\n",
      "Train Epoch: 154 [1500/6658 (23%)]\tLoss: 0.699982\n",
      "Train Epoch: 154 [1600/6658 (24%)]\tLoss: 0.724315\n",
      "Train Epoch: 154 [1700/6658 (26%)]\tLoss: 0.012631\n",
      "Train Epoch: 154 [1800/6658 (27%)]\tLoss: 0.132554\n",
      "Train Epoch: 154 [1900/6658 (29%)]\tLoss: 1.505041\n",
      "Train Epoch: 154 [2000/6658 (30%)]\tLoss: 0.432043\n",
      "Train Epoch: 154 [2100/6658 (32%)]\tLoss: 0.221120\n",
      "Train Epoch: 154 [2200/6658 (33%)]\tLoss: 0.229156\n",
      "Train Epoch: 154 [2300/6658 (35%)]\tLoss: 0.070782\n",
      "Train Epoch: 154 [2400/6658 (36%)]\tLoss: 0.137966\n",
      "Train Epoch: 154 [2500/6658 (38%)]\tLoss: 0.006447\n",
      "Train Epoch: 154 [2600/6658 (39%)]\tLoss: 0.025460\n",
      "Train Epoch: 154 [2700/6658 (41%)]\tLoss: 1.735156\n",
      "Train Epoch: 154 [2800/6658 (42%)]\tLoss: 0.996341\n",
      "Train Epoch: 154 [2900/6658 (44%)]\tLoss: 0.154080\n",
      "Train Epoch: 154 [3000/6658 (45%)]\tLoss: 0.964653\n",
      "Train Epoch: 154 [3100/6658 (47%)]\tLoss: 0.000553\n",
      "Train Epoch: 154 [3200/6658 (48%)]\tLoss: 0.206113\n",
      "Train Epoch: 154 [3300/6658 (50%)]\tLoss: 0.971320\n",
      "Train Epoch: 154 [3400/6658 (51%)]\tLoss: 0.127759\n",
      "Train Epoch: 154 [3500/6658 (53%)]\tLoss: 0.012066\n",
      "Train Epoch: 154 [3600/6658 (54%)]\tLoss: 0.263450\n",
      "Train Epoch: 154 [3700/6658 (56%)]\tLoss: 0.001486\n",
      "Train Epoch: 154 [3800/6658 (57%)]\tLoss: 3.738872\n",
      "Train Epoch: 154 [3900/6658 (59%)]\tLoss: 0.134194\n",
      "Train Epoch: 154 [4000/6658 (60%)]\tLoss: 0.378239\n",
      "Train Epoch: 154 [4100/6658 (62%)]\tLoss: 2.781775\n",
      "Train Epoch: 154 [4200/6658 (63%)]\tLoss: 0.379393\n",
      "Train Epoch: 154 [4300/6658 (65%)]\tLoss: 0.007976\n",
      "Train Epoch: 154 [4400/6658 (66%)]\tLoss: 0.319512\n",
      "Train Epoch: 154 [4500/6658 (68%)]\tLoss: 0.624289\n",
      "Train Epoch: 154 [4600/6658 (69%)]\tLoss: 0.489581\n",
      "Train Epoch: 154 [4700/6658 (71%)]\tLoss: 0.677060\n",
      "Train Epoch: 154 [4800/6658 (72%)]\tLoss: 0.217433\n",
      "Train Epoch: 154 [4900/6658 (74%)]\tLoss: 0.072098\n",
      "Train Epoch: 154 [5000/6658 (75%)]\tLoss: 0.203417\n",
      "Train Epoch: 154 [5100/6658 (77%)]\tLoss: 0.392702\n",
      "Train Epoch: 154 [5200/6658 (78%)]\tLoss: 0.598272\n",
      "Train Epoch: 154 [5300/6658 (80%)]\tLoss: 0.299086\n",
      "Train Epoch: 154 [5400/6658 (81%)]\tLoss: 0.163116\n",
      "Train Epoch: 154 [5500/6658 (83%)]\tLoss: 0.121799\n",
      "Train Epoch: 154 [5600/6658 (84%)]\tLoss: 0.002743\n",
      "Train Epoch: 154 [5700/6658 (86%)]\tLoss: 2.619120\n",
      "Train Epoch: 154 [5800/6658 (87%)]\tLoss: 0.234581\n",
      "Train Epoch: 154 [5900/6658 (89%)]\tLoss: 0.376909\n",
      "Train Epoch: 154 [6000/6658 (90%)]\tLoss: 0.251546\n",
      "Train Epoch: 154 [6100/6658 (92%)]\tLoss: 0.181712\n",
      "Train Epoch: 154 [6200/6658 (93%)]\tLoss: 0.421690\n",
      "Train Epoch: 154 [6300/6658 (95%)]\tLoss: 0.331639\n",
      "Train Epoch: 154 [6400/6658 (96%)]\tLoss: 0.546903\n",
      "Train Epoch: 154 [6500/6658 (98%)]\tLoss: 0.477961\n",
      "Train Epoch: 154 [6600/6658 (99%)]\tLoss: 0.279865\n",
      "train loss average =  0.7096736430151241\n",
      "\n",
      "Test set: Average loss: 0.7010\n",
      "\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3235, 6.1052, 5.8540, 5.8611, 6.2326, 6.0455], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 155 [0/6658 (0%)]\tLoss: 0.165225\n",
      "Train Epoch: 155 [100/6658 (2%)]\tLoss: 0.166009\n",
      "Train Epoch: 155 [200/6658 (3%)]\tLoss: 0.525103\n",
      "Train Epoch: 155 [300/6658 (5%)]\tLoss: 0.250828\n",
      "Train Epoch: 155 [400/6658 (6%)]\tLoss: 0.009890\n",
      "Train Epoch: 155 [500/6658 (8%)]\tLoss: 1.557025\n",
      "Train Epoch: 155 [600/6658 (9%)]\tLoss: 0.976508\n",
      "Train Epoch: 155 [700/6658 (11%)]\tLoss: 0.235172\n",
      "Train Epoch: 155 [800/6658 (12%)]\tLoss: 0.099078\n",
      "Train Epoch: 155 [900/6658 (14%)]\tLoss: 0.741742\n",
      "Train Epoch: 155 [1000/6658 (15%)]\tLoss: 0.070948\n",
      "Train Epoch: 155 [1100/6658 (17%)]\tLoss: 2.342048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 155 [1200/6658 (18%)]\tLoss: 0.172210\n",
      "Train Epoch: 155 [1300/6658 (20%)]\tLoss: 0.532301\n",
      "Train Epoch: 155 [1400/6658 (21%)]\tLoss: 0.012758\n",
      "Train Epoch: 155 [1500/6658 (23%)]\tLoss: 2.401612\n",
      "Train Epoch: 155 [1600/6658 (24%)]\tLoss: 1.389983\n",
      "Train Epoch: 155 [1700/6658 (26%)]\tLoss: 0.373480\n",
      "Train Epoch: 155 [1800/6658 (27%)]\tLoss: 0.143908\n",
      "Train Epoch: 155 [1900/6658 (29%)]\tLoss: 0.508312\n",
      "Train Epoch: 155 [2000/6658 (30%)]\tLoss: 0.071579\n",
      "Train Epoch: 155 [2100/6658 (32%)]\tLoss: 0.123351\n",
      "Train Epoch: 155 [2200/6658 (33%)]\tLoss: 2.017543\n",
      "Train Epoch: 155 [2300/6658 (35%)]\tLoss: 0.089745\n",
      "Train Epoch: 155 [2400/6658 (36%)]\tLoss: 0.154289\n",
      "Train Epoch: 155 [2500/6658 (38%)]\tLoss: 0.037095\n",
      "Train Epoch: 155 [2600/6658 (39%)]\tLoss: 0.137249\n",
      "Train Epoch: 155 [2700/6658 (41%)]\tLoss: 0.246960\n",
      "Train Epoch: 155 [2800/6658 (42%)]\tLoss: 0.739902\n",
      "Train Epoch: 155 [2900/6658 (44%)]\tLoss: 0.287378\n",
      "Train Epoch: 155 [3000/6658 (45%)]\tLoss: 0.453908\n",
      "Train Epoch: 155 [3100/6658 (47%)]\tLoss: 1.090136\n",
      "Train Epoch: 155 [3200/6658 (48%)]\tLoss: 0.058511\n",
      "Train Epoch: 155 [3300/6658 (50%)]\tLoss: 0.137572\n",
      "Train Epoch: 155 [3400/6658 (51%)]\tLoss: 0.379182\n",
      "Train Epoch: 155 [3500/6658 (53%)]\tLoss: 0.089956\n",
      "Train Epoch: 155 [3600/6658 (54%)]\tLoss: 0.197567\n",
      "Train Epoch: 155 [3700/6658 (56%)]\tLoss: 0.947441\n",
      "Train Epoch: 155 [3800/6658 (57%)]\tLoss: 1.459222\n",
      "Train Epoch: 155 [3900/6658 (59%)]\tLoss: 0.000426\n",
      "Train Epoch: 155 [4000/6658 (60%)]\tLoss: 0.140361\n",
      "Train Epoch: 155 [4100/6658 (62%)]\tLoss: 0.746121\n",
      "Train Epoch: 155 [4200/6658 (63%)]\tLoss: 0.274435\n",
      "Train Epoch: 155 [4300/6658 (65%)]\tLoss: 0.264689\n",
      "Train Epoch: 155 [4400/6658 (66%)]\tLoss: 1.176755\n",
      "Train Epoch: 155 [4500/6658 (68%)]\tLoss: 0.171174\n",
      "Train Epoch: 155 [4600/6658 (69%)]\tLoss: 4.933309\n",
      "Train Epoch: 155 [4700/6658 (71%)]\tLoss: 0.228049\n",
      "Train Epoch: 155 [4800/6658 (72%)]\tLoss: 0.448129\n",
      "Train Epoch: 155 [4900/6658 (74%)]\tLoss: 0.004945\n",
      "Train Epoch: 155 [5000/6658 (75%)]\tLoss: 0.275330\n",
      "Train Epoch: 155 [5100/6658 (77%)]\tLoss: 1.244594\n",
      "Train Epoch: 155 [5200/6658 (78%)]\tLoss: 0.172449\n",
      "Train Epoch: 155 [5300/6658 (80%)]\tLoss: 0.075559\n",
      "Train Epoch: 155 [5400/6658 (81%)]\tLoss: 5.054762\n",
      "Train Epoch: 155 [5500/6658 (83%)]\tLoss: 1.704731\n",
      "Train Epoch: 155 [5600/6658 (84%)]\tLoss: 0.217638\n",
      "Train Epoch: 155 [5700/6658 (86%)]\tLoss: 1.388224\n",
      "Train Epoch: 155 [5800/6658 (87%)]\tLoss: 1.215930\n",
      "Train Epoch: 155 [5900/6658 (89%)]\tLoss: 0.626022\n",
      "Train Epoch: 155 [6000/6658 (90%)]\tLoss: 0.000345\n",
      "Train Epoch: 155 [6100/6658 (92%)]\tLoss: 0.277404\n",
      "Train Epoch: 155 [6200/6658 (93%)]\tLoss: 0.150438\n",
      "Train Epoch: 155 [6300/6658 (95%)]\tLoss: 1.095788\n",
      "Train Epoch: 155 [6400/6658 (96%)]\tLoss: 0.026401\n",
      "Train Epoch: 155 [6500/6658 (98%)]\tLoss: 0.044820\n",
      "Train Epoch: 155 [6600/6658 (99%)]\tLoss: 8.636023\n",
      "train loss average =  0.7075956022786384\n",
      "\n",
      "Test set: Average loss: 0.6923\n",
      "\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3255, 6.1062, 5.8532, 5.8606, 6.2343, 6.0454], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 156 [0/6658 (0%)]\tLoss: 0.164147\n",
      "Train Epoch: 156 [100/6658 (2%)]\tLoss: 1.124124\n",
      "Train Epoch: 156 [200/6658 (3%)]\tLoss: 0.084603\n",
      "Train Epoch: 156 [300/6658 (5%)]\tLoss: 0.143410\n",
      "Train Epoch: 156 [400/6658 (6%)]\tLoss: 0.157548\n",
      "Train Epoch: 156 [500/6658 (8%)]\tLoss: 0.235894\n",
      "Train Epoch: 156 [600/6658 (9%)]\tLoss: 0.032666\n",
      "Train Epoch: 156 [700/6658 (11%)]\tLoss: 5.574939\n",
      "Train Epoch: 156 [800/6658 (12%)]\tLoss: 0.094129\n",
      "Train Epoch: 156 [900/6658 (14%)]\tLoss: 0.374735\n",
      "Train Epoch: 156 [1000/6658 (15%)]\tLoss: 0.176189\n",
      "Train Epoch: 156 [1100/6658 (17%)]\tLoss: 1.052015\n",
      "Train Epoch: 156 [1200/6658 (18%)]\tLoss: 0.152335\n",
      "Train Epoch: 156 [1300/6658 (20%)]\tLoss: 4.175478\n",
      "Train Epoch: 156 [1400/6658 (21%)]\tLoss: 1.862505\n",
      "Train Epoch: 156 [1500/6658 (23%)]\tLoss: 0.346533\n",
      "Train Epoch: 156 [1600/6658 (24%)]\tLoss: 2.714598\n",
      "Train Epoch: 156 [1700/6658 (26%)]\tLoss: 0.015269\n",
      "Train Epoch: 156 [1800/6658 (27%)]\tLoss: 0.189363\n",
      "Train Epoch: 156 [1900/6658 (29%)]\tLoss: 0.983654\n",
      "Train Epoch: 156 [2000/6658 (30%)]\tLoss: 0.174942\n",
      "Train Epoch: 156 [2100/6658 (32%)]\tLoss: 0.015824\n",
      "Train Epoch: 156 [2200/6658 (33%)]\tLoss: 0.580554\n",
      "Train Epoch: 156 [2300/6658 (35%)]\tLoss: 0.079353\n",
      "Train Epoch: 156 [2400/6658 (36%)]\tLoss: 0.837201\n",
      "Train Epoch: 156 [2500/6658 (38%)]\tLoss: 0.212851\n",
      "Train Epoch: 156 [2600/6658 (39%)]\tLoss: 0.224033\n",
      "Train Epoch: 156 [2700/6658 (41%)]\tLoss: 0.164815\n",
      "Train Epoch: 156 [2800/6658 (42%)]\tLoss: 0.013330\n",
      "Train Epoch: 156 [2900/6658 (44%)]\tLoss: 0.000008\n",
      "Train Epoch: 156 [3000/6658 (45%)]\tLoss: 0.255648\n",
      "Train Epoch: 156 [3100/6658 (47%)]\tLoss: 0.000004\n",
      "Train Epoch: 156 [3200/6658 (48%)]\tLoss: 0.007014\n",
      "Train Epoch: 156 [3300/6658 (50%)]\tLoss: 0.214778\n",
      "Train Epoch: 156 [3400/6658 (51%)]\tLoss: 0.004225\n",
      "Train Epoch: 156 [3500/6658 (53%)]\tLoss: 0.464820\n",
      "Train Epoch: 156 [3600/6658 (54%)]\tLoss: 0.494895\n",
      "Train Epoch: 156 [3700/6658 (56%)]\tLoss: 0.018143\n",
      "Train Epoch: 156 [3800/6658 (57%)]\tLoss: 0.191427\n",
      "Train Epoch: 156 [3900/6658 (59%)]\tLoss: 0.061115\n",
      "Train Epoch: 156 [4000/6658 (60%)]\tLoss: 1.299963\n",
      "Train Epoch: 156 [4100/6658 (62%)]\tLoss: 0.166765\n",
      "Train Epoch: 156 [4200/6658 (63%)]\tLoss: 1.636063\n",
      "Train Epoch: 156 [4300/6658 (65%)]\tLoss: 0.001304\n",
      "Train Epoch: 156 [4400/6658 (66%)]\tLoss: 0.004327\n",
      "Train Epoch: 156 [4500/6658 (68%)]\tLoss: 0.022167\n",
      "Train Epoch: 156 [4600/6658 (69%)]\tLoss: 0.005699\n",
      "Train Epoch: 156 [4700/6658 (71%)]\tLoss: 0.138741\n",
      "Train Epoch: 156 [4800/6658 (72%)]\tLoss: 0.702712\n",
      "Train Epoch: 156 [4900/6658 (74%)]\tLoss: 0.131404\n",
      "Train Epoch: 156 [5000/6658 (75%)]\tLoss: 0.058507\n",
      "Train Epoch: 156 [5100/6658 (77%)]\tLoss: 0.005913\n",
      "Train Epoch: 156 [5200/6658 (78%)]\tLoss: 2.794325\n",
      "Train Epoch: 156 [5300/6658 (80%)]\tLoss: 0.219349\n",
      "Train Epoch: 156 [5400/6658 (81%)]\tLoss: 0.091504\n",
      "Train Epoch: 156 [5500/6658 (83%)]\tLoss: 0.445272\n",
      "Train Epoch: 156 [5600/6658 (84%)]\tLoss: 0.001897\n",
      "Train Epoch: 156 [5700/6658 (86%)]\tLoss: 0.003112\n",
      "Train Epoch: 156 [5800/6658 (87%)]\tLoss: 0.511693\n",
      "Train Epoch: 156 [5900/6658 (89%)]\tLoss: 0.001384\n",
      "Train Epoch: 156 [6000/6658 (90%)]\tLoss: 0.055811\n",
      "Train Epoch: 156 [6100/6658 (92%)]\tLoss: 0.084799\n",
      "Train Epoch: 156 [6200/6658 (93%)]\tLoss: 0.115278\n",
      "Train Epoch: 156 [6300/6658 (95%)]\tLoss: 0.463155\n",
      "Train Epoch: 156 [6400/6658 (96%)]\tLoss: 0.067123\n",
      "Train Epoch: 156 [6500/6658 (98%)]\tLoss: 0.129895\n",
      "Train Epoch: 156 [6600/6658 (99%)]\tLoss: 0.003259\n",
      "train loss average =  0.7079881029293328\n",
      "\n",
      "Test set: Average loss: 0.7029\n",
      "\n",
      "EarlyStopping counter: 9 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3275, 6.1084, 5.8537, 5.8603, 6.2357, 6.0452], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 157 [0/6658 (0%)]\tLoss: 0.334992\n",
      "Train Epoch: 157 [100/6658 (2%)]\tLoss: 0.076035\n",
      "Train Epoch: 157 [200/6658 (3%)]\tLoss: 0.004827\n",
      "Train Epoch: 157 [300/6658 (5%)]\tLoss: 0.020339\n",
      "Train Epoch: 157 [400/6658 (6%)]\tLoss: 0.104171\n",
      "Train Epoch: 157 [500/6658 (8%)]\tLoss: 1.693366\n",
      "Train Epoch: 157 [600/6658 (9%)]\tLoss: 1.186167\n",
      "Train Epoch: 157 [700/6658 (11%)]\tLoss: 0.008939\n",
      "Train Epoch: 157 [800/6658 (12%)]\tLoss: 0.001205\n",
      "Train Epoch: 157 [900/6658 (14%)]\tLoss: 1.243557\n",
      "Train Epoch: 157 [1000/6658 (15%)]\tLoss: 0.150748\n",
      "Train Epoch: 157 [1100/6658 (17%)]\tLoss: 0.228780\n",
      "Train Epoch: 157 [1200/6658 (18%)]\tLoss: 0.789858\n",
      "Train Epoch: 157 [1300/6658 (20%)]\tLoss: 0.001975\n",
      "Train Epoch: 157 [1400/6658 (21%)]\tLoss: 0.314561\n",
      "Train Epoch: 157 [1500/6658 (23%)]\tLoss: 0.265274\n",
      "Train Epoch: 157 [1600/6658 (24%)]\tLoss: 0.631851\n",
      "Train Epoch: 157 [1700/6658 (26%)]\tLoss: 0.000806\n",
      "Train Epoch: 157 [1800/6658 (27%)]\tLoss: 0.425409\n",
      "Train Epoch: 157 [1900/6658 (29%)]\tLoss: 0.215699\n",
      "Train Epoch: 157 [2000/6658 (30%)]\tLoss: 0.004332\n",
      "Train Epoch: 157 [2100/6658 (32%)]\tLoss: 0.040267\n",
      "Train Epoch: 157 [2200/6658 (33%)]\tLoss: 0.123063\n",
      "Train Epoch: 157 [2300/6658 (35%)]\tLoss: 0.206632\n",
      "Train Epoch: 157 [2400/6658 (36%)]\tLoss: 0.560817\n",
      "Train Epoch: 157 [2500/6658 (38%)]\tLoss: 0.019729\n",
      "Train Epoch: 157 [2600/6658 (39%)]\tLoss: 1.853034\n",
      "Train Epoch: 157 [2700/6658 (41%)]\tLoss: 0.191155\n",
      "Train Epoch: 157 [2800/6658 (42%)]\tLoss: 0.782970\n",
      "Train Epoch: 157 [2900/6658 (44%)]\tLoss: 0.670221\n",
      "Train Epoch: 157 [3000/6658 (45%)]\tLoss: 0.745703\n",
      "Train Epoch: 157 [3100/6658 (47%)]\tLoss: 0.085879\n",
      "Train Epoch: 157 [3200/6658 (48%)]\tLoss: 0.580508\n",
      "Train Epoch: 157 [3300/6658 (50%)]\tLoss: 0.000061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 157 [3400/6658 (51%)]\tLoss: 2.891970\n",
      "Train Epoch: 157 [3500/6658 (53%)]\tLoss: 0.797927\n",
      "Train Epoch: 157 [3600/6658 (54%)]\tLoss: 0.288218\n",
      "Train Epoch: 157 [3700/6658 (56%)]\tLoss: 0.000022\n",
      "Train Epoch: 157 [3800/6658 (57%)]\tLoss: 0.667253\n",
      "Train Epoch: 157 [3900/6658 (59%)]\tLoss: 0.330301\n",
      "Train Epoch: 157 [4000/6658 (60%)]\tLoss: 0.371100\n",
      "Train Epoch: 157 [4100/6658 (62%)]\tLoss: 0.163917\n",
      "Train Epoch: 157 [4200/6658 (63%)]\tLoss: 0.239535\n",
      "Train Epoch: 157 [4300/6658 (65%)]\tLoss: 0.510885\n",
      "Train Epoch: 157 [4400/6658 (66%)]\tLoss: 1.901120\n",
      "Train Epoch: 157 [4500/6658 (68%)]\tLoss: 0.518091\n",
      "Train Epoch: 157 [4600/6658 (69%)]\tLoss: 0.441824\n",
      "Train Epoch: 157 [4700/6658 (71%)]\tLoss: 0.024407\n",
      "Train Epoch: 157 [4800/6658 (72%)]\tLoss: 1.334141\n",
      "Train Epoch: 157 [4900/6658 (74%)]\tLoss: 0.000603\n",
      "Train Epoch: 157 [5000/6658 (75%)]\tLoss: 0.387217\n",
      "Train Epoch: 157 [5100/6658 (77%)]\tLoss: 0.373389\n",
      "Train Epoch: 157 [5200/6658 (78%)]\tLoss: 0.630239\n",
      "Train Epoch: 157 [5300/6658 (80%)]\tLoss: 0.025016\n",
      "Train Epoch: 157 [5400/6658 (81%)]\tLoss: 0.159740\n",
      "Train Epoch: 157 [5500/6658 (83%)]\tLoss: 0.049204\n",
      "Train Epoch: 157 [5600/6658 (84%)]\tLoss: 1.595761\n",
      "Train Epoch: 157 [5700/6658 (86%)]\tLoss: 1.420121\n",
      "Train Epoch: 157 [5800/6658 (87%)]\tLoss: 0.162733\n",
      "Train Epoch: 157 [5900/6658 (89%)]\tLoss: 1.995892\n",
      "Train Epoch: 157 [6000/6658 (90%)]\tLoss: 0.502527\n",
      "Train Epoch: 157 [6100/6658 (92%)]\tLoss: 0.016946\n",
      "Train Epoch: 157 [6200/6658 (93%)]\tLoss: 0.208098\n",
      "Train Epoch: 157 [6300/6658 (95%)]\tLoss: 0.109044\n",
      "Train Epoch: 157 [6400/6658 (96%)]\tLoss: 0.024188\n",
      "Train Epoch: 157 [6500/6658 (98%)]\tLoss: 1.392814\n",
      "Train Epoch: 157 [6600/6658 (99%)]\tLoss: 0.047520\n",
      "train loss average =  0.7068272255019211\n",
      "\n",
      "Test set: Average loss: 0.7065\n",
      "\n",
      "EarlyStopping counter: 10 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3298, 6.1092, 5.8535, 5.8599, 6.2377, 6.0452], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 158 [0/6658 (0%)]\tLoss: 0.109065\n",
      "Train Epoch: 158 [100/6658 (2%)]\tLoss: 0.923301\n",
      "Train Epoch: 158 [200/6658 (3%)]\tLoss: 0.095221\n",
      "Train Epoch: 158 [300/6658 (5%)]\tLoss: 0.272929\n",
      "Train Epoch: 158 [400/6658 (6%)]\tLoss: 0.150879\n",
      "Train Epoch: 158 [500/6658 (8%)]\tLoss: 0.008920\n",
      "Train Epoch: 158 [600/6658 (9%)]\tLoss: 0.466317\n",
      "Train Epoch: 158 [700/6658 (11%)]\tLoss: 0.376267\n",
      "Train Epoch: 158 [800/6658 (12%)]\tLoss: 0.110348\n",
      "Train Epoch: 158 [900/6658 (14%)]\tLoss: 0.491639\n",
      "Train Epoch: 158 [1000/6658 (15%)]\tLoss: 1.605152\n",
      "Train Epoch: 158 [1100/6658 (17%)]\tLoss: 0.152786\n",
      "Train Epoch: 158 [1200/6658 (18%)]\tLoss: 0.303729\n",
      "Train Epoch: 158 [1300/6658 (20%)]\tLoss: 0.002904\n",
      "Train Epoch: 158 [1400/6658 (21%)]\tLoss: 6.389366\n",
      "Train Epoch: 158 [1500/6658 (23%)]\tLoss: 0.004385\n",
      "Train Epoch: 158 [1600/6658 (24%)]\tLoss: 0.020755\n",
      "Train Epoch: 158 [1700/6658 (26%)]\tLoss: 0.333789\n",
      "Train Epoch: 158 [1800/6658 (27%)]\tLoss: 0.593956\n",
      "Train Epoch: 158 [1900/6658 (29%)]\tLoss: 0.215751\n",
      "Train Epoch: 158 [2000/6658 (30%)]\tLoss: 0.001344\n",
      "Train Epoch: 158 [2100/6658 (32%)]\tLoss: 0.038030\n",
      "Train Epoch: 158 [2200/6658 (33%)]\tLoss: 0.005289\n",
      "Train Epoch: 158 [2300/6658 (35%)]\tLoss: 0.017193\n",
      "Train Epoch: 158 [2400/6658 (36%)]\tLoss: 1.000728\n",
      "Train Epoch: 158 [2500/6658 (38%)]\tLoss: 0.011362\n",
      "Train Epoch: 158 [2600/6658 (39%)]\tLoss: 0.287601\n",
      "Train Epoch: 158 [2700/6658 (41%)]\tLoss: 0.382409\n",
      "Train Epoch: 158 [2800/6658 (42%)]\tLoss: 0.998972\n",
      "Train Epoch: 158 [2900/6658 (44%)]\tLoss: 1.799459\n",
      "Train Epoch: 158 [3000/6658 (45%)]\tLoss: 0.052150\n",
      "Train Epoch: 158 [3100/6658 (47%)]\tLoss: 1.556322\n",
      "Train Epoch: 158 [3200/6658 (48%)]\tLoss: 0.528235\n",
      "Train Epoch: 158 [3300/6658 (50%)]\tLoss: 0.008749\n",
      "Train Epoch: 158 [3400/6658 (51%)]\tLoss: 0.103003\n",
      "Train Epoch: 158 [3500/6658 (53%)]\tLoss: 4.159518\n",
      "Train Epoch: 158 [3600/6658 (54%)]\tLoss: 0.985977\n",
      "Train Epoch: 158 [3700/6658 (56%)]\tLoss: 2.133049\n",
      "Train Epoch: 158 [3800/6658 (57%)]\tLoss: 0.412032\n",
      "Train Epoch: 158 [3900/6658 (59%)]\tLoss: 0.059645\n",
      "Train Epoch: 158 [4000/6658 (60%)]\tLoss: 1.050817\n",
      "Train Epoch: 158 [4100/6658 (62%)]\tLoss: 0.005762\n",
      "Train Epoch: 158 [4200/6658 (63%)]\tLoss: 0.655674\n",
      "Train Epoch: 158 [4300/6658 (65%)]\tLoss: 1.128886\n",
      "Train Epoch: 158 [4400/6658 (66%)]\tLoss: 0.011458\n",
      "Train Epoch: 158 [4500/6658 (68%)]\tLoss: 0.916457\n",
      "Train Epoch: 158 [4600/6658 (69%)]\tLoss: 0.400547\n",
      "Train Epoch: 158 [4700/6658 (71%)]\tLoss: 0.504495\n",
      "Train Epoch: 158 [4800/6658 (72%)]\tLoss: 0.340838\n",
      "Train Epoch: 158 [4900/6658 (74%)]\tLoss: 0.417017\n",
      "Train Epoch: 158 [5000/6658 (75%)]\tLoss: 1.492895\n",
      "Train Epoch: 158 [5100/6658 (77%)]\tLoss: 1.256552\n",
      "Train Epoch: 158 [5200/6658 (78%)]\tLoss: 0.002570\n",
      "Train Epoch: 158 [5300/6658 (80%)]\tLoss: 0.418488\n",
      "Train Epoch: 158 [5400/6658 (81%)]\tLoss: 0.488816\n",
      "Train Epoch: 158 [5500/6658 (83%)]\tLoss: 0.880655\n",
      "Train Epoch: 158 [5600/6658 (84%)]\tLoss: 6.582126\n",
      "Train Epoch: 158 [5700/6658 (86%)]\tLoss: 0.552522\n",
      "Train Epoch: 158 [5800/6658 (87%)]\tLoss: 0.103955\n",
      "Train Epoch: 158 [5900/6658 (89%)]\tLoss: 0.045385\n",
      "Train Epoch: 158 [6000/6658 (90%)]\tLoss: 0.018897\n",
      "Train Epoch: 158 [6100/6658 (92%)]\tLoss: 0.080961\n",
      "Train Epoch: 158 [6200/6658 (93%)]\tLoss: 0.550771\n",
      "Train Epoch: 158 [6300/6658 (95%)]\tLoss: 0.131522\n",
      "Train Epoch: 158 [6400/6658 (96%)]\tLoss: 4.144155\n",
      "Train Epoch: 158 [6500/6658 (98%)]\tLoss: 0.000062\n",
      "Train Epoch: 158 [6600/6658 (99%)]\tLoss: 0.086651\n",
      "train loss average =  0.7007622285350383\n",
      "\n",
      "Test set: Average loss: 0.7068\n",
      "\n",
      "EarlyStopping counter: 11 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3317, 6.1106, 5.8538, 5.8596, 6.2389, 6.0450], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 159 [0/6658 (0%)]\tLoss: 0.744435\n",
      "Train Epoch: 159 [100/6658 (2%)]\tLoss: 0.001872\n",
      "Train Epoch: 159 [200/6658 (3%)]\tLoss: 0.226457\n",
      "Train Epoch: 159 [300/6658 (5%)]\tLoss: 0.048049\n",
      "Train Epoch: 159 [400/6658 (6%)]\tLoss: 0.615095\n",
      "Train Epoch: 159 [500/6658 (8%)]\tLoss: 0.535097\n",
      "Train Epoch: 159 [600/6658 (9%)]\tLoss: 1.512728\n",
      "Train Epoch: 159 [700/6658 (11%)]\tLoss: 0.203709\n",
      "Train Epoch: 159 [800/6658 (12%)]\tLoss: 0.129459\n",
      "Train Epoch: 159 [900/6658 (14%)]\tLoss: 0.424684\n",
      "Train Epoch: 159 [1000/6658 (15%)]\tLoss: 0.023714\n",
      "Train Epoch: 159 [1100/6658 (17%)]\tLoss: 0.304472\n",
      "Train Epoch: 159 [1200/6658 (18%)]\tLoss: 1.147757\n",
      "Train Epoch: 159 [1300/6658 (20%)]\tLoss: 0.019845\n",
      "Train Epoch: 159 [1400/6658 (21%)]\tLoss: 0.070563\n",
      "Train Epoch: 159 [1500/6658 (23%)]\tLoss: 0.134249\n",
      "Train Epoch: 159 [1600/6658 (24%)]\tLoss: 0.235819\n",
      "Train Epoch: 159 [1700/6658 (26%)]\tLoss: 0.243806\n",
      "Train Epoch: 159 [1800/6658 (27%)]\tLoss: 0.144790\n",
      "Train Epoch: 159 [1900/6658 (29%)]\tLoss: 0.238742\n",
      "Train Epoch: 159 [2000/6658 (30%)]\tLoss: 0.840816\n",
      "Train Epoch: 159 [2100/6658 (32%)]\tLoss: 0.068578\n",
      "Train Epoch: 159 [2200/6658 (33%)]\tLoss: 0.708148\n",
      "Train Epoch: 159 [2300/6658 (35%)]\tLoss: 0.397455\n",
      "Train Epoch: 159 [2400/6658 (36%)]\tLoss: 0.000000\n",
      "Train Epoch: 159 [2500/6658 (38%)]\tLoss: 0.388901\n",
      "Train Epoch: 159 [2600/6658 (39%)]\tLoss: 0.735972\n",
      "Train Epoch: 159 [2700/6658 (41%)]\tLoss: 0.642376\n",
      "Train Epoch: 159 [2800/6658 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 159 [2900/6658 (44%)]\tLoss: 0.740215\n",
      "Train Epoch: 159 [3000/6658 (45%)]\tLoss: 0.724352\n",
      "Train Epoch: 159 [3100/6658 (47%)]\tLoss: 0.615556\n",
      "Train Epoch: 159 [3200/6658 (48%)]\tLoss: 0.395198\n",
      "Train Epoch: 159 [3300/6658 (50%)]\tLoss: 0.128520\n",
      "Train Epoch: 159 [3400/6658 (51%)]\tLoss: 0.117666\n",
      "Train Epoch: 159 [3500/6658 (53%)]\tLoss: 3.097253\n",
      "Train Epoch: 159 [3600/6658 (54%)]\tLoss: 0.070457\n",
      "Train Epoch: 159 [3700/6658 (56%)]\tLoss: 0.202662\n",
      "Train Epoch: 159 [3800/6658 (57%)]\tLoss: 0.747327\n",
      "Train Epoch: 159 [3900/6658 (59%)]\tLoss: 0.284064\n",
      "Train Epoch: 159 [4000/6658 (60%)]\tLoss: 0.011090\n",
      "Train Epoch: 159 [4100/6658 (62%)]\tLoss: 0.009563\n",
      "Train Epoch: 159 [4200/6658 (63%)]\tLoss: 0.106471\n",
      "Train Epoch: 159 [4300/6658 (65%)]\tLoss: 0.025669\n",
      "Train Epoch: 159 [4400/6658 (66%)]\tLoss: 1.206983\n",
      "Train Epoch: 159 [4500/6658 (68%)]\tLoss: 1.003629\n",
      "Train Epoch: 159 [4600/6658 (69%)]\tLoss: 5.116717\n",
      "Train Epoch: 159 [4700/6658 (71%)]\tLoss: 0.199959\n",
      "Train Epoch: 159 [4800/6658 (72%)]\tLoss: 0.012533\n",
      "Train Epoch: 159 [4900/6658 (74%)]\tLoss: 0.125267\n",
      "Train Epoch: 159 [5000/6658 (75%)]\tLoss: 0.764656\n",
      "Train Epoch: 159 [5100/6658 (77%)]\tLoss: 0.677844\n",
      "Train Epoch: 159 [5200/6658 (78%)]\tLoss: 0.834417\n",
      "Train Epoch: 159 [5300/6658 (80%)]\tLoss: 0.440921\n",
      "Train Epoch: 159 [5400/6658 (81%)]\tLoss: 1.384547\n",
      "Train Epoch: 159 [5500/6658 (83%)]\tLoss: 0.676074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 159 [5600/6658 (84%)]\tLoss: 0.798339\n",
      "Train Epoch: 159 [5700/6658 (86%)]\tLoss: 0.612311\n",
      "Train Epoch: 159 [5800/6658 (87%)]\tLoss: 0.442198\n",
      "Train Epoch: 159 [5900/6658 (89%)]\tLoss: 1.191547\n",
      "Train Epoch: 159 [6000/6658 (90%)]\tLoss: 0.624711\n",
      "Train Epoch: 159 [6100/6658 (92%)]\tLoss: 0.021549\n",
      "Train Epoch: 159 [6200/6658 (93%)]\tLoss: 0.075832\n",
      "Train Epoch: 159 [6300/6658 (95%)]\tLoss: 0.099630\n",
      "Train Epoch: 159 [6400/6658 (96%)]\tLoss: 0.092241\n",
      "Train Epoch: 159 [6500/6658 (98%)]\tLoss: 0.286866\n",
      "Train Epoch: 159 [6600/6658 (99%)]\tLoss: 0.507376\n",
      "train loss average =  0.7111607691915128\n",
      "\n",
      "Test set: Average loss: 0.6848\n",
      "\n",
      "EarlyStopping counter: 12 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3338, 6.1117, 5.8541, 5.8592, 6.2403, 6.0449], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 160 [0/6658 (0%)]\tLoss: 0.217799\n",
      "Train Epoch: 160 [100/6658 (2%)]\tLoss: 0.488023\n",
      "Train Epoch: 160 [200/6658 (3%)]\tLoss: 0.099835\n",
      "Train Epoch: 160 [300/6658 (5%)]\tLoss: 0.459971\n",
      "Train Epoch: 160 [400/6658 (6%)]\tLoss: 0.038271\n",
      "Train Epoch: 160 [500/6658 (8%)]\tLoss: 0.815510\n",
      "Train Epoch: 160 [600/6658 (9%)]\tLoss: 0.168194\n",
      "Train Epoch: 160 [700/6658 (11%)]\tLoss: 0.002096\n",
      "Train Epoch: 160 [800/6658 (12%)]\tLoss: 0.385379\n",
      "Train Epoch: 160 [900/6658 (14%)]\tLoss: 0.122280\n",
      "Train Epoch: 160 [1000/6658 (15%)]\tLoss: 0.085716\n",
      "Train Epoch: 160 [1100/6658 (17%)]\tLoss: 0.230746\n",
      "Train Epoch: 160 [1200/6658 (18%)]\tLoss: 0.351839\n",
      "Train Epoch: 160 [1300/6658 (20%)]\tLoss: 0.076517\n",
      "Train Epoch: 160 [1400/6658 (21%)]\tLoss: 1.686594\n",
      "Train Epoch: 160 [1500/6658 (23%)]\tLoss: 0.015436\n",
      "Train Epoch: 160 [1600/6658 (24%)]\tLoss: 0.592609\n",
      "Train Epoch: 160 [1700/6658 (26%)]\tLoss: 0.369318\n",
      "Train Epoch: 160 [1800/6658 (27%)]\tLoss: 0.288617\n",
      "Train Epoch: 160 [1900/6658 (29%)]\tLoss: 0.067324\n",
      "Train Epoch: 160 [2000/6658 (30%)]\tLoss: 0.140525\n",
      "Train Epoch: 160 [2100/6658 (32%)]\tLoss: 0.127356\n",
      "Train Epoch: 160 [2200/6658 (33%)]\tLoss: 0.044002\n",
      "Train Epoch: 160 [2300/6658 (35%)]\tLoss: 0.393235\n",
      "Train Epoch: 160 [2400/6658 (36%)]\tLoss: 0.222885\n",
      "Train Epoch: 160 [2500/6658 (38%)]\tLoss: 3.044539\n",
      "Train Epoch: 160 [2600/6658 (39%)]\tLoss: 0.004831\n",
      "Train Epoch: 160 [2700/6658 (41%)]\tLoss: 0.164288\n",
      "Train Epoch: 160 [2800/6658 (42%)]\tLoss: 0.029147\n",
      "Train Epoch: 160 [2900/6658 (44%)]\tLoss: 0.145037\n",
      "Train Epoch: 160 [3000/6658 (45%)]\tLoss: 0.807889\n",
      "Train Epoch: 160 [3100/6658 (47%)]\tLoss: 3.080810\n",
      "Train Epoch: 160 [3200/6658 (48%)]\tLoss: 1.016894\n",
      "Train Epoch: 160 [3300/6658 (50%)]\tLoss: 0.022340\n",
      "Train Epoch: 160 [3400/6658 (51%)]\tLoss: 0.100941\n",
      "Train Epoch: 160 [3500/6658 (53%)]\tLoss: 0.110792\n",
      "Train Epoch: 160 [3600/6658 (54%)]\tLoss: 0.001248\n",
      "Train Epoch: 160 [3700/6658 (56%)]\tLoss: 0.124798\n",
      "Train Epoch: 160 [3800/6658 (57%)]\tLoss: 0.001095\n",
      "Train Epoch: 160 [3900/6658 (59%)]\tLoss: 0.347144\n",
      "Train Epoch: 160 [4000/6658 (60%)]\tLoss: 0.097348\n",
      "Train Epoch: 160 [4100/6658 (62%)]\tLoss: 0.162291\n",
      "Train Epoch: 160 [4200/6658 (63%)]\tLoss: 0.104085\n",
      "Train Epoch: 160 [4300/6658 (65%)]\tLoss: 0.144557\n",
      "Train Epoch: 160 [4400/6658 (66%)]\tLoss: 0.298277\n",
      "Train Epoch: 160 [4500/6658 (68%)]\tLoss: 0.401003\n",
      "Train Epoch: 160 [4600/6658 (69%)]\tLoss: 0.744954\n",
      "Train Epoch: 160 [4700/6658 (71%)]\tLoss: 2.712127\n",
      "Train Epoch: 160 [4800/6658 (72%)]\tLoss: 0.052836\n",
      "Train Epoch: 160 [4900/6658 (74%)]\tLoss: 0.287409\n",
      "Train Epoch: 160 [5000/6658 (75%)]\tLoss: 0.318895\n",
      "Train Epoch: 160 [5100/6658 (77%)]\tLoss: 0.521305\n",
      "Train Epoch: 160 [5200/6658 (78%)]\tLoss: 0.170855\n",
      "Train Epoch: 160 [5300/6658 (80%)]\tLoss: 0.074259\n",
      "Train Epoch: 160 [5400/6658 (81%)]\tLoss: 0.694739\n",
      "Train Epoch: 160 [5500/6658 (83%)]\tLoss: 0.011849\n",
      "Train Epoch: 160 [5600/6658 (84%)]\tLoss: 0.440999\n",
      "Train Epoch: 160 [5700/6658 (86%)]\tLoss: 0.041223\n",
      "Train Epoch: 160 [5800/6658 (87%)]\tLoss: 1.034073\n",
      "Train Epoch: 160 [5900/6658 (89%)]\tLoss: 0.119320\n",
      "Train Epoch: 160 [6000/6658 (90%)]\tLoss: 0.007954\n",
      "Train Epoch: 160 [6100/6658 (92%)]\tLoss: 0.077882\n",
      "Train Epoch: 160 [6200/6658 (93%)]\tLoss: 0.099133\n",
      "Train Epoch: 160 [6300/6658 (95%)]\tLoss: 3.376853\n",
      "Train Epoch: 160 [6400/6658 (96%)]\tLoss: 0.024961\n",
      "Train Epoch: 160 [6500/6658 (98%)]\tLoss: 0.800395\n",
      "Train Epoch: 160 [6600/6658 (99%)]\tLoss: 0.779545\n",
      "train loss average =  0.7121412298031484\n",
      "\n",
      "Test set: Average loss: 0.6965\n",
      "\n",
      "EarlyStopping counter: 13 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3360, 6.1120, 5.8538, 5.8594, 6.2417, 6.0448], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 161 [0/6658 (0%)]\tLoss: 0.246409\n",
      "Train Epoch: 161 [100/6658 (2%)]\tLoss: 0.245548\n",
      "Train Epoch: 161 [200/6658 (3%)]\tLoss: 2.799891\n",
      "Train Epoch: 161 [300/6658 (5%)]\tLoss: 0.678579\n",
      "Train Epoch: 161 [400/6658 (6%)]\tLoss: 0.091563\n",
      "Train Epoch: 161 [500/6658 (8%)]\tLoss: 0.538244\n",
      "Train Epoch: 161 [600/6658 (9%)]\tLoss: 0.001633\n",
      "Train Epoch: 161 [700/6658 (11%)]\tLoss: 0.003094\n",
      "Train Epoch: 161 [800/6658 (12%)]\tLoss: 0.506122\n",
      "Train Epoch: 161 [900/6658 (14%)]\tLoss: 0.141120\n",
      "Train Epoch: 161 [1000/6658 (15%)]\tLoss: 0.070010\n",
      "Train Epoch: 161 [1100/6658 (17%)]\tLoss: 0.257438\n",
      "Train Epoch: 161 [1200/6658 (18%)]\tLoss: 0.591315\n",
      "Train Epoch: 161 [1300/6658 (20%)]\tLoss: 0.000103\n",
      "Train Epoch: 161 [1400/6658 (21%)]\tLoss: 0.786439\n",
      "Train Epoch: 161 [1500/6658 (23%)]\tLoss: 0.822779\n",
      "Train Epoch: 161 [1600/6658 (24%)]\tLoss: 0.703155\n",
      "Train Epoch: 161 [1700/6658 (26%)]\tLoss: 0.193546\n",
      "Train Epoch: 161 [1800/6658 (27%)]\tLoss: 0.000030\n",
      "Train Epoch: 161 [1900/6658 (29%)]\tLoss: 0.001134\n",
      "Train Epoch: 161 [2000/6658 (30%)]\tLoss: 0.162920\n",
      "Train Epoch: 161 [2100/6658 (32%)]\tLoss: 0.166813\n",
      "Train Epoch: 161 [2200/6658 (33%)]\tLoss: 0.494166\n",
      "Train Epoch: 161 [2300/6658 (35%)]\tLoss: 0.246772\n",
      "Train Epoch: 161 [2400/6658 (36%)]\tLoss: 0.530915\n",
      "Train Epoch: 161 [2500/6658 (38%)]\tLoss: 0.223348\n",
      "Train Epoch: 161 [2600/6658 (39%)]\tLoss: 0.011245\n",
      "Train Epoch: 161 [2700/6658 (41%)]\tLoss: 1.377599\n",
      "Train Epoch: 161 [2800/6658 (42%)]\tLoss: 0.020404\n",
      "Train Epoch: 161 [2900/6658 (44%)]\tLoss: 0.036570\n",
      "Train Epoch: 161 [3000/6658 (45%)]\tLoss: 0.069915\n",
      "Train Epoch: 161 [3100/6658 (47%)]\tLoss: 0.164561\n",
      "Train Epoch: 161 [3200/6658 (48%)]\tLoss: 0.001635\n",
      "Train Epoch: 161 [3300/6658 (50%)]\tLoss: 0.000831\n",
      "Train Epoch: 161 [3400/6658 (51%)]\tLoss: 0.283608\n",
      "Train Epoch: 161 [3500/6658 (53%)]\tLoss: 0.139407\n",
      "Train Epoch: 161 [3600/6658 (54%)]\tLoss: 0.031716\n",
      "Train Epoch: 161 [3700/6658 (56%)]\tLoss: 0.061675\n",
      "Train Epoch: 161 [3800/6658 (57%)]\tLoss: 18.745163\n",
      "Train Epoch: 161 [3900/6658 (59%)]\tLoss: 0.018655\n",
      "Train Epoch: 161 [4000/6658 (60%)]\tLoss: 0.268259\n",
      "Train Epoch: 161 [4100/6658 (62%)]\tLoss: 0.658021\n",
      "Train Epoch: 161 [4200/6658 (63%)]\tLoss: 1.114657\n",
      "Train Epoch: 161 [4300/6658 (65%)]\tLoss: 0.132065\n",
      "Train Epoch: 161 [4400/6658 (66%)]\tLoss: 0.365520\n",
      "Train Epoch: 161 [4500/6658 (68%)]\tLoss: 1.206239\n",
      "Train Epoch: 161 [4600/6658 (69%)]\tLoss: 0.475529\n",
      "Train Epoch: 161 [4700/6658 (71%)]\tLoss: 0.147405\n",
      "Train Epoch: 161 [4800/6658 (72%)]\tLoss: 0.021496\n",
      "Train Epoch: 161 [4900/6658 (74%)]\tLoss: 0.053222\n",
      "Train Epoch: 161 [5000/6658 (75%)]\tLoss: 0.116289\n",
      "Train Epoch: 161 [5100/6658 (77%)]\tLoss: 0.354829\n",
      "Train Epoch: 161 [5200/6658 (78%)]\tLoss: 0.015265\n",
      "Train Epoch: 161 [5300/6658 (80%)]\tLoss: 0.019053\n",
      "Train Epoch: 161 [5400/6658 (81%)]\tLoss: 0.000864\n",
      "Train Epoch: 161 [5500/6658 (83%)]\tLoss: 0.017500\n",
      "Train Epoch: 161 [5600/6658 (84%)]\tLoss: 2.122776\n",
      "Train Epoch: 161 [5700/6658 (86%)]\tLoss: 0.601044\n",
      "Train Epoch: 161 [5800/6658 (87%)]\tLoss: 0.345323\n",
      "Train Epoch: 161 [5900/6658 (89%)]\tLoss: 0.269960\n",
      "Train Epoch: 161 [6000/6658 (90%)]\tLoss: 0.000584\n",
      "Train Epoch: 161 [6100/6658 (92%)]\tLoss: 0.048586\n",
      "Train Epoch: 161 [6200/6658 (93%)]\tLoss: 0.815541\n",
      "Train Epoch: 161 [6300/6658 (95%)]\tLoss: 0.087306\n",
      "Train Epoch: 161 [6400/6658 (96%)]\tLoss: 0.074708\n",
      "Train Epoch: 161 [6500/6658 (98%)]\tLoss: 0.702425\n",
      "Train Epoch: 161 [6600/6658 (99%)]\tLoss: 1.369190\n",
      "train loss average =  0.7046915655271712\n",
      "\n",
      "Test set: Average loss: 0.6985\n",
      "\n",
      "EarlyStopping counter: 14 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3380, 6.1129, 5.8531, 5.8583, 6.2435, 6.0447], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 162 [0/6658 (0%)]\tLoss: 0.123411\n",
      "Train Epoch: 162 [100/6658 (2%)]\tLoss: 0.006606\n",
      "Train Epoch: 162 [200/6658 (3%)]\tLoss: 0.337112\n",
      "Train Epoch: 162 [300/6658 (5%)]\tLoss: 0.014713\n",
      "Train Epoch: 162 [400/6658 (6%)]\tLoss: 0.170803\n",
      "Train Epoch: 162 [500/6658 (8%)]\tLoss: 0.034337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 162 [600/6658 (9%)]\tLoss: 0.058773\n",
      "Train Epoch: 162 [700/6658 (11%)]\tLoss: 0.614208\n",
      "Train Epoch: 162 [800/6658 (12%)]\tLoss: 0.053955\n",
      "Train Epoch: 162 [900/6658 (14%)]\tLoss: 0.389441\n",
      "Train Epoch: 162 [1000/6658 (15%)]\tLoss: 0.377068\n",
      "Train Epoch: 162 [1100/6658 (17%)]\tLoss: 0.442304\n",
      "Train Epoch: 162 [1200/6658 (18%)]\tLoss: 0.025119\n",
      "Train Epoch: 162 [1300/6658 (20%)]\tLoss: 0.480719\n",
      "Train Epoch: 162 [1400/6658 (21%)]\tLoss: 1.449585\n",
      "Train Epoch: 162 [1500/6658 (23%)]\tLoss: 0.033021\n",
      "Train Epoch: 162 [1600/6658 (24%)]\tLoss: 0.053692\n",
      "Train Epoch: 162 [1700/6658 (26%)]\tLoss: 0.099373\n",
      "Train Epoch: 162 [1800/6658 (27%)]\tLoss: 1.725819\n",
      "Train Epoch: 162 [1900/6658 (29%)]\tLoss: 0.276881\n",
      "Train Epoch: 162 [2000/6658 (30%)]\tLoss: 0.663675\n",
      "Train Epoch: 162 [2100/6658 (32%)]\tLoss: 0.189383\n",
      "Train Epoch: 162 [2200/6658 (33%)]\tLoss: 0.075748\n",
      "Train Epoch: 162 [2300/6658 (35%)]\tLoss: 0.146130\n",
      "Train Epoch: 162 [2400/6658 (36%)]\tLoss: 0.901725\n",
      "Train Epoch: 162 [2500/6658 (38%)]\tLoss: 0.461057\n",
      "Train Epoch: 162 [2600/6658 (39%)]\tLoss: 2.263699\n",
      "Train Epoch: 162 [2700/6658 (41%)]\tLoss: 0.295862\n",
      "Train Epoch: 162 [2800/6658 (42%)]\tLoss: 0.338628\n",
      "Train Epoch: 162 [2900/6658 (44%)]\tLoss: 0.001774\n",
      "Train Epoch: 162 [3000/6658 (45%)]\tLoss: 0.000049\n",
      "Train Epoch: 162 [3100/6658 (47%)]\tLoss: 0.199318\n",
      "Train Epoch: 162 [3200/6658 (48%)]\tLoss: 0.059418\n",
      "Train Epoch: 162 [3300/6658 (50%)]\tLoss: 0.340823\n",
      "Train Epoch: 162 [3400/6658 (51%)]\tLoss: 1.895659\n",
      "Train Epoch: 162 [3500/6658 (53%)]\tLoss: 0.047751\n",
      "Train Epoch: 162 [3600/6658 (54%)]\tLoss: 0.199826\n",
      "Train Epoch: 162 [3700/6658 (56%)]\tLoss: 0.026521\n",
      "Train Epoch: 162 [3800/6658 (57%)]\tLoss: 0.025753\n",
      "Train Epoch: 162 [3900/6658 (59%)]\tLoss: 0.811660\n",
      "Train Epoch: 162 [4000/6658 (60%)]\tLoss: 0.184997\n",
      "Train Epoch: 162 [4100/6658 (62%)]\tLoss: 0.063674\n",
      "Train Epoch: 162 [4200/6658 (63%)]\tLoss: 0.032746\n",
      "Train Epoch: 162 [4300/6658 (65%)]\tLoss: 0.724090\n",
      "Train Epoch: 162 [4400/6658 (66%)]\tLoss: 0.004096\n",
      "Train Epoch: 162 [4500/6658 (68%)]\tLoss: 2.034811\n",
      "Train Epoch: 162 [4600/6658 (69%)]\tLoss: 0.043679\n",
      "Train Epoch: 162 [4700/6658 (71%)]\tLoss: 1.127419\n",
      "Train Epoch: 162 [4800/6658 (72%)]\tLoss: 0.065532\n",
      "Train Epoch: 162 [4900/6658 (74%)]\tLoss: 0.078499\n",
      "Train Epoch: 162 [5000/6658 (75%)]\tLoss: 0.124644\n",
      "Train Epoch: 162 [5100/6658 (77%)]\tLoss: 0.009142\n",
      "Train Epoch: 162 [5200/6658 (78%)]\tLoss: 0.358629\n",
      "Train Epoch: 162 [5300/6658 (80%)]\tLoss: 10.781701\n",
      "Train Epoch: 162 [5400/6658 (81%)]\tLoss: 1.663277\n",
      "Train Epoch: 162 [5500/6658 (83%)]\tLoss: 0.358814\n",
      "Train Epoch: 162 [5600/6658 (84%)]\tLoss: 0.106162\n",
      "Train Epoch: 162 [5700/6658 (86%)]\tLoss: 0.297642\n",
      "Train Epoch: 162 [5800/6658 (87%)]\tLoss: 1.295906\n",
      "Train Epoch: 162 [5900/6658 (89%)]\tLoss: 1.336033\n",
      "Train Epoch: 162 [6000/6658 (90%)]\tLoss: 0.362387\n",
      "Train Epoch: 162 [6100/6658 (92%)]\tLoss: 0.012675\n",
      "Train Epoch: 162 [6200/6658 (93%)]\tLoss: 0.006668\n",
      "Train Epoch: 162 [6300/6658 (95%)]\tLoss: 0.241834\n",
      "Train Epoch: 162 [6400/6658 (96%)]\tLoss: 0.065995\n",
      "Train Epoch: 162 [6500/6658 (98%)]\tLoss: 0.052777\n",
      "Train Epoch: 162 [6600/6658 (99%)]\tLoss: 0.088582\n",
      "train loss average =  0.7086376961187615\n",
      "\n",
      "Test set: Average loss: 0.7077\n",
      "\n",
      "EarlyStopping counter: 15 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3399, 6.1140, 5.8516, 5.8573, 6.2454, 6.0451], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 163 [0/6658 (0%)]\tLoss: 2.207965\n",
      "Train Epoch: 163 [100/6658 (2%)]\tLoss: 0.143131\n",
      "Train Epoch: 163 [200/6658 (3%)]\tLoss: 2.119983\n",
      "Train Epoch: 163 [300/6658 (5%)]\tLoss: 0.073419\n",
      "Train Epoch: 163 [400/6658 (6%)]\tLoss: 0.787025\n",
      "Train Epoch: 163 [500/6658 (8%)]\tLoss: 0.228595\n",
      "Train Epoch: 163 [600/6658 (9%)]\tLoss: 1.071496\n",
      "Train Epoch: 163 [700/6658 (11%)]\tLoss: 0.040928\n",
      "Train Epoch: 163 [800/6658 (12%)]\tLoss: 0.456672\n",
      "Train Epoch: 163 [900/6658 (14%)]\tLoss: 0.356421\n",
      "Train Epoch: 163 [1000/6658 (15%)]\tLoss: 0.279198\n",
      "Train Epoch: 163 [1100/6658 (17%)]\tLoss: 0.108526\n",
      "Train Epoch: 163 [1200/6658 (18%)]\tLoss: 0.043342\n",
      "Train Epoch: 163 [1300/6658 (20%)]\tLoss: 0.290945\n",
      "Train Epoch: 163 [1400/6658 (21%)]\tLoss: 0.484136\n",
      "Train Epoch: 163 [1500/6658 (23%)]\tLoss: 0.462237\n",
      "Train Epoch: 163 [1600/6658 (24%)]\tLoss: 0.150475\n",
      "Train Epoch: 163 [1700/6658 (26%)]\tLoss: 0.367249\n",
      "Train Epoch: 163 [1800/6658 (27%)]\tLoss: 0.237832\n",
      "Train Epoch: 163 [1900/6658 (29%)]\tLoss: 0.380751\n",
      "Train Epoch: 163 [2000/6658 (30%)]\tLoss: 0.095656\n",
      "Train Epoch: 163 [2100/6658 (32%)]\tLoss: 0.553079\n",
      "Train Epoch: 163 [2200/6658 (33%)]\tLoss: 0.356584\n",
      "Train Epoch: 163 [2300/6658 (35%)]\tLoss: 0.053441\n",
      "Train Epoch: 163 [2400/6658 (36%)]\tLoss: 0.010429\n",
      "Train Epoch: 163 [2500/6658 (38%)]\tLoss: 0.007864\n",
      "Train Epoch: 163 [2600/6658 (39%)]\tLoss: 0.232650\n",
      "Train Epoch: 163 [2700/6658 (41%)]\tLoss: 0.263387\n",
      "Train Epoch: 163 [2800/6658 (42%)]\tLoss: 0.141220\n",
      "Train Epoch: 163 [2900/6658 (44%)]\tLoss: 5.318931\n",
      "Train Epoch: 163 [3000/6658 (45%)]\tLoss: 0.053666\n",
      "Train Epoch: 163 [3100/6658 (47%)]\tLoss: 0.048709\n",
      "Train Epoch: 163 [3200/6658 (48%)]\tLoss: 0.042554\n",
      "Train Epoch: 163 [3300/6658 (50%)]\tLoss: 0.015852\n",
      "Train Epoch: 163 [3400/6658 (51%)]\tLoss: 0.470470\n",
      "Train Epoch: 163 [3500/6658 (53%)]\tLoss: 2.609802\n",
      "Train Epoch: 163 [3600/6658 (54%)]\tLoss: 2.139365\n",
      "Train Epoch: 163 [3700/6658 (56%)]\tLoss: 0.000002\n",
      "Train Epoch: 163 [3800/6658 (57%)]\tLoss: 0.049614\n",
      "Train Epoch: 163 [3900/6658 (59%)]\tLoss: 2.077038\n",
      "Train Epoch: 163 [4000/6658 (60%)]\tLoss: 0.283353\n",
      "Train Epoch: 163 [4100/6658 (62%)]\tLoss: 0.003431\n",
      "Train Epoch: 163 [4200/6658 (63%)]\tLoss: 0.185405\n",
      "Train Epoch: 163 [4300/6658 (65%)]\tLoss: 0.476530\n",
      "Train Epoch: 163 [4400/6658 (66%)]\tLoss: 0.257462\n",
      "Train Epoch: 163 [4500/6658 (68%)]\tLoss: 0.002378\n",
      "Train Epoch: 163 [4600/6658 (69%)]\tLoss: 0.119927\n",
      "Train Epoch: 163 [4700/6658 (71%)]\tLoss: 0.008798\n",
      "Train Epoch: 163 [4800/6658 (72%)]\tLoss: 0.495779\n",
      "Train Epoch: 163 [4900/6658 (74%)]\tLoss: 0.223938\n",
      "Train Epoch: 163 [5000/6658 (75%)]\tLoss: 2.139733\n",
      "Train Epoch: 163 [5100/6658 (77%)]\tLoss: 0.763834\n",
      "Train Epoch: 163 [5200/6658 (78%)]\tLoss: 0.057778\n",
      "Train Epoch: 163 [5300/6658 (80%)]\tLoss: 0.738452\n",
      "Train Epoch: 163 [5400/6658 (81%)]\tLoss: 0.128632\n",
      "Train Epoch: 163 [5500/6658 (83%)]\tLoss: 0.008432\n",
      "Train Epoch: 163 [5600/6658 (84%)]\tLoss: 0.110856\n",
      "Train Epoch: 163 [5700/6658 (86%)]\tLoss: 0.532838\n",
      "Train Epoch: 163 [5800/6658 (87%)]\tLoss: 0.000358\n",
      "Train Epoch: 163 [5900/6658 (89%)]\tLoss: 0.505875\n",
      "Train Epoch: 163 [6000/6658 (90%)]\tLoss: 0.047901\n",
      "Train Epoch: 163 [6100/6658 (92%)]\tLoss: 0.566425\n",
      "Train Epoch: 163 [6200/6658 (93%)]\tLoss: 0.426343\n",
      "Train Epoch: 163 [6300/6658 (95%)]\tLoss: 0.017633\n",
      "Train Epoch: 163 [6400/6658 (96%)]\tLoss: 0.181156\n",
      "Train Epoch: 163 [6500/6658 (98%)]\tLoss: 0.953291\n",
      "Train Epoch: 163 [6600/6658 (99%)]\tLoss: 0.081005\n",
      "train loss average =  0.7054524514367319\n",
      "\n",
      "Test set: Average loss: 0.6942\n",
      "\n",
      "EarlyStopping counter: 16 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3418, 6.1152, 5.8501, 5.8572, 6.2463, 6.0449], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 164 [0/6658 (0%)]\tLoss: 0.307957\n",
      "Train Epoch: 164 [100/6658 (2%)]\tLoss: 0.019301\n",
      "Train Epoch: 164 [200/6658 (3%)]\tLoss: 0.024806\n",
      "Train Epoch: 164 [300/6658 (5%)]\tLoss: 0.014428\n",
      "Train Epoch: 164 [400/6658 (6%)]\tLoss: 0.354655\n",
      "Train Epoch: 164 [500/6658 (8%)]\tLoss: 0.365634\n",
      "Train Epoch: 164 [600/6658 (9%)]\tLoss: 0.005880\n",
      "Train Epoch: 164 [700/6658 (11%)]\tLoss: 0.567732\n",
      "Train Epoch: 164 [800/6658 (12%)]\tLoss: 0.223741\n",
      "Train Epoch: 164 [900/6658 (14%)]\tLoss: 1.696176\n",
      "Train Epoch: 164 [1000/6658 (15%)]\tLoss: 6.311443\n",
      "Train Epoch: 164 [1100/6658 (17%)]\tLoss: 0.797505\n",
      "Train Epoch: 164 [1200/6658 (18%)]\tLoss: 0.041560\n",
      "Train Epoch: 164 [1300/6658 (20%)]\tLoss: 0.405670\n",
      "Train Epoch: 164 [1400/6658 (21%)]\tLoss: 0.581280\n",
      "Train Epoch: 164 [1500/6658 (23%)]\tLoss: 1.173180\n",
      "Train Epoch: 164 [1600/6658 (24%)]\tLoss: 0.010037\n",
      "Train Epoch: 164 [1700/6658 (26%)]\tLoss: 0.090275\n",
      "Train Epoch: 164 [1800/6658 (27%)]\tLoss: 0.006800\n",
      "Train Epoch: 164 [1900/6658 (29%)]\tLoss: 0.006077\n",
      "Train Epoch: 164 [2000/6658 (30%)]\tLoss: 1.004269\n",
      "Train Epoch: 164 [2100/6658 (32%)]\tLoss: 0.684858\n",
      "Train Epoch: 164 [2200/6658 (33%)]\tLoss: 0.157216\n",
      "Train Epoch: 164 [2300/6658 (35%)]\tLoss: 0.022448\n",
      "Train Epoch: 164 [2400/6658 (36%)]\tLoss: 0.445407\n",
      "Train Epoch: 164 [2500/6658 (38%)]\tLoss: 0.157201\n",
      "Train Epoch: 164 [2600/6658 (39%)]\tLoss: 0.001498\n",
      "Train Epoch: 164 [2700/6658 (41%)]\tLoss: 0.983143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 164 [2800/6658 (42%)]\tLoss: 0.061335\n",
      "Train Epoch: 164 [2900/6658 (44%)]\tLoss: 0.100264\n",
      "Train Epoch: 164 [3000/6658 (45%)]\tLoss: 1.358514\n",
      "Train Epoch: 164 [3100/6658 (47%)]\tLoss: 0.224152\n",
      "Train Epoch: 164 [3200/6658 (48%)]\tLoss: 0.093344\n",
      "Train Epoch: 164 [3300/6658 (50%)]\tLoss: 0.335468\n",
      "Train Epoch: 164 [3400/6658 (51%)]\tLoss: 0.127719\n",
      "Train Epoch: 164 [3500/6658 (53%)]\tLoss: 0.018548\n",
      "Train Epoch: 164 [3600/6658 (54%)]\tLoss: 0.004149\n",
      "Train Epoch: 164 [3700/6658 (56%)]\tLoss: 1.016906\n",
      "Train Epoch: 164 [3800/6658 (57%)]\tLoss: 0.006943\n",
      "Train Epoch: 164 [3900/6658 (59%)]\tLoss: 0.006679\n",
      "Train Epoch: 164 [4000/6658 (60%)]\tLoss: 0.002396\n",
      "Train Epoch: 164 [4100/6658 (62%)]\tLoss: 0.009690\n",
      "Train Epoch: 164 [4200/6658 (63%)]\tLoss: 0.916572\n",
      "Train Epoch: 164 [4300/6658 (65%)]\tLoss: 0.094947\n",
      "Train Epoch: 164 [4400/6658 (66%)]\tLoss: 0.056006\n",
      "Train Epoch: 164 [4500/6658 (68%)]\tLoss: 0.280806\n",
      "Train Epoch: 164 [4600/6658 (69%)]\tLoss: 0.554224\n",
      "Train Epoch: 164 [4700/6658 (71%)]\tLoss: 1.195613\n",
      "Train Epoch: 164 [4800/6658 (72%)]\tLoss: 4.437743\n",
      "Train Epoch: 164 [4900/6658 (74%)]\tLoss: 2.007740\n",
      "Train Epoch: 164 [5000/6658 (75%)]\tLoss: 0.003475\n",
      "Train Epoch: 164 [5100/6658 (77%)]\tLoss: 0.096128\n",
      "Train Epoch: 164 [5200/6658 (78%)]\tLoss: 0.163851\n",
      "Train Epoch: 164 [5300/6658 (80%)]\tLoss: 0.106504\n",
      "Train Epoch: 164 [5400/6658 (81%)]\tLoss: 3.132780\n",
      "Train Epoch: 164 [5500/6658 (83%)]\tLoss: 1.470566\n",
      "Train Epoch: 164 [5600/6658 (84%)]\tLoss: 0.008205\n",
      "Train Epoch: 164 [5700/6658 (86%)]\tLoss: 2.771657\n",
      "Train Epoch: 164 [5800/6658 (87%)]\tLoss: 0.636683\n",
      "Train Epoch: 164 [5900/6658 (89%)]\tLoss: 0.000020\n",
      "Train Epoch: 164 [6000/6658 (90%)]\tLoss: 0.179349\n",
      "Train Epoch: 164 [6100/6658 (92%)]\tLoss: 0.253622\n",
      "Train Epoch: 164 [6200/6658 (93%)]\tLoss: 0.072440\n",
      "Train Epoch: 164 [6300/6658 (95%)]\tLoss: 0.893615\n",
      "Train Epoch: 164 [6400/6658 (96%)]\tLoss: 0.058001\n",
      "Train Epoch: 164 [6500/6658 (98%)]\tLoss: 1.361308\n",
      "Train Epoch: 164 [6600/6658 (99%)]\tLoss: 0.053450\n",
      "train loss average =  0.7114441928763126\n",
      "\n",
      "Test set: Average loss: 0.6738\n",
      "\n",
      "Validation loss decreased (0.676924 --> 0.673794).  Saving model ...\n",
      "Parameter containing:\n",
      "tensor([6.3439, 6.1162, 5.8497, 5.8565, 6.2476, 6.0447], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 165 [0/6658 (0%)]\tLoss: 0.162314\n",
      "Train Epoch: 165 [100/6658 (2%)]\tLoss: 0.970684\n",
      "Train Epoch: 165 [200/6658 (3%)]\tLoss: 0.027530\n",
      "Train Epoch: 165 [300/6658 (5%)]\tLoss: 5.526389\n",
      "Train Epoch: 165 [400/6658 (6%)]\tLoss: 0.118862\n",
      "Train Epoch: 165 [500/6658 (8%)]\tLoss: 0.376639\n",
      "Train Epoch: 165 [600/6658 (9%)]\tLoss: 0.001215\n",
      "Train Epoch: 165 [700/6658 (11%)]\tLoss: 0.471527\n",
      "Train Epoch: 165 [800/6658 (12%)]\tLoss: 0.349542\n",
      "Train Epoch: 165 [900/6658 (14%)]\tLoss: 0.160468\n",
      "Train Epoch: 165 [1000/6658 (15%)]\tLoss: 0.386712\n",
      "Train Epoch: 165 [1100/6658 (17%)]\tLoss: 1.198890\n",
      "Train Epoch: 165 [1200/6658 (18%)]\tLoss: 1.499919\n",
      "Train Epoch: 165 [1300/6658 (20%)]\tLoss: 1.206479\n",
      "Train Epoch: 165 [1400/6658 (21%)]\tLoss: 0.019580\n",
      "Train Epoch: 165 [1500/6658 (23%)]\tLoss: 0.098923\n",
      "Train Epoch: 165 [1600/6658 (24%)]\tLoss: 0.000008\n",
      "Train Epoch: 165 [1700/6658 (26%)]\tLoss: 0.423221\n",
      "Train Epoch: 165 [1800/6658 (27%)]\tLoss: 0.112774\n",
      "Train Epoch: 165 [1900/6658 (29%)]\tLoss: 0.034922\n",
      "Train Epoch: 165 [2000/6658 (30%)]\tLoss: 0.195488\n",
      "Train Epoch: 165 [2100/6658 (32%)]\tLoss: 0.004564\n",
      "Train Epoch: 165 [2200/6658 (33%)]\tLoss: 0.105150\n",
      "Train Epoch: 165 [2300/6658 (35%)]\tLoss: 0.052658\n",
      "Train Epoch: 165 [2400/6658 (36%)]\tLoss: 1.640219\n",
      "Train Epoch: 165 [2500/6658 (38%)]\tLoss: 0.125897\n",
      "Train Epoch: 165 [2600/6658 (39%)]\tLoss: 1.534559\n",
      "Train Epoch: 165 [2700/6658 (41%)]\tLoss: 0.744632\n",
      "Train Epoch: 165 [2800/6658 (42%)]\tLoss: 0.007716\n",
      "Train Epoch: 165 [2900/6658 (44%)]\tLoss: 0.053621\n",
      "Train Epoch: 165 [3000/6658 (45%)]\tLoss: 2.878051\n",
      "Train Epoch: 165 [3100/6658 (47%)]\tLoss: 0.117048\n",
      "Train Epoch: 165 [3200/6658 (48%)]\tLoss: 2.507889\n",
      "Train Epoch: 165 [3300/6658 (50%)]\tLoss: 0.000719\n",
      "Train Epoch: 165 [3400/6658 (51%)]\tLoss: 0.296794\n",
      "Train Epoch: 165 [3500/6658 (53%)]\tLoss: 0.056533\n",
      "Train Epoch: 165 [3600/6658 (54%)]\tLoss: 0.478405\n",
      "Train Epoch: 165 [3700/6658 (56%)]\tLoss: 0.167845\n",
      "Train Epoch: 165 [3800/6658 (57%)]\tLoss: 2.979562\n",
      "Train Epoch: 165 [3900/6658 (59%)]\tLoss: 0.007420\n",
      "Train Epoch: 165 [4000/6658 (60%)]\tLoss: 1.419200\n",
      "Train Epoch: 165 [4100/6658 (62%)]\tLoss: 1.092939\n",
      "Train Epoch: 165 [4200/6658 (63%)]\tLoss: 0.044896\n",
      "Train Epoch: 165 [4300/6658 (65%)]\tLoss: 0.103900\n",
      "Train Epoch: 165 [4400/6658 (66%)]\tLoss: 0.231788\n",
      "Train Epoch: 165 [4500/6658 (68%)]\tLoss: 0.851367\n",
      "Train Epoch: 165 [4600/6658 (69%)]\tLoss: 0.097907\n",
      "Train Epoch: 165 [4700/6658 (71%)]\tLoss: 0.365599\n",
      "Train Epoch: 165 [4800/6658 (72%)]\tLoss: 0.364138\n",
      "Train Epoch: 165 [4900/6658 (74%)]\tLoss: 0.782385\n",
      "Train Epoch: 165 [5000/6658 (75%)]\tLoss: 0.242477\n",
      "Train Epoch: 165 [5100/6658 (77%)]\tLoss: 0.006224\n",
      "Train Epoch: 165 [5200/6658 (78%)]\tLoss: 1.196209\n",
      "Train Epoch: 165 [5300/6658 (80%)]\tLoss: 0.000588\n",
      "Train Epoch: 165 [5400/6658 (81%)]\tLoss: 0.466978\n",
      "Train Epoch: 165 [5500/6658 (83%)]\tLoss: 0.750619\n",
      "Train Epoch: 165 [5600/6658 (84%)]\tLoss: 10.010333\n",
      "Train Epoch: 165 [5700/6658 (86%)]\tLoss: 0.343435\n",
      "Train Epoch: 165 [5800/6658 (87%)]\tLoss: 0.372158\n",
      "Train Epoch: 165 [5900/6658 (89%)]\tLoss: 0.874106\n",
      "Train Epoch: 165 [6000/6658 (90%)]\tLoss: 0.372364\n",
      "Train Epoch: 165 [6100/6658 (92%)]\tLoss: 0.914695\n",
      "Train Epoch: 165 [6200/6658 (93%)]\tLoss: 0.000199\n",
      "Train Epoch: 165 [6300/6658 (95%)]\tLoss: 0.254544\n",
      "Train Epoch: 165 [6400/6658 (96%)]\tLoss: 0.308883\n",
      "Train Epoch: 165 [6500/6658 (98%)]\tLoss: 0.004306\n",
      "Train Epoch: 165 [6600/6658 (99%)]\tLoss: 1.605603\n",
      "train loss average =  0.70312742979368\n",
      "\n",
      "Test set: Average loss: 0.7021\n",
      "\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3452, 6.1175, 5.8492, 5.8549, 6.2493, 6.0448], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 166 [0/6658 (0%)]\tLoss: 0.031402\n",
      "Train Epoch: 166 [100/6658 (2%)]\tLoss: 0.329804\n",
      "Train Epoch: 166 [200/6658 (3%)]\tLoss: 0.000005\n",
      "Train Epoch: 166 [300/6658 (5%)]\tLoss: 0.693490\n",
      "Train Epoch: 166 [400/6658 (6%)]\tLoss: 0.194697\n",
      "Train Epoch: 166 [500/6658 (8%)]\tLoss: 0.331714\n",
      "Train Epoch: 166 [600/6658 (9%)]\tLoss: 0.043172\n",
      "Train Epoch: 166 [700/6658 (11%)]\tLoss: 3.437776\n",
      "Train Epoch: 166 [800/6658 (12%)]\tLoss: 0.267139\n",
      "Train Epoch: 166 [900/6658 (14%)]\tLoss: 0.025635\n",
      "Train Epoch: 166 [1000/6658 (15%)]\tLoss: 0.026870\n",
      "Train Epoch: 166 [1100/6658 (17%)]\tLoss: 0.336887\n",
      "Train Epoch: 166 [1200/6658 (18%)]\tLoss: 0.111834\n",
      "Train Epoch: 166 [1300/6658 (20%)]\tLoss: 0.825738\n",
      "Train Epoch: 166 [1400/6658 (21%)]\tLoss: 0.212063\n",
      "Train Epoch: 166 [1500/6658 (23%)]\tLoss: 0.012429\n",
      "Train Epoch: 166 [1600/6658 (24%)]\tLoss: 0.017409\n",
      "Train Epoch: 166 [1700/6658 (26%)]\tLoss: 1.493217\n",
      "Train Epoch: 166 [1800/6658 (27%)]\tLoss: 0.014593\n",
      "Train Epoch: 166 [1900/6658 (29%)]\tLoss: 0.107043\n",
      "Train Epoch: 166 [2000/6658 (30%)]\tLoss: 0.401041\n",
      "Train Epoch: 166 [2100/6658 (32%)]\tLoss: 0.362474\n",
      "Train Epoch: 166 [2200/6658 (33%)]\tLoss: 0.000814\n",
      "Train Epoch: 166 [2300/6658 (35%)]\tLoss: 0.028525\n",
      "Train Epoch: 166 [2400/6658 (36%)]\tLoss: 1.414298\n",
      "Train Epoch: 166 [2500/6658 (38%)]\tLoss: 4.811646\n",
      "Train Epoch: 166 [2600/6658 (39%)]\tLoss: 0.519500\n",
      "Train Epoch: 166 [2700/6658 (41%)]\tLoss: 0.110498\n",
      "Train Epoch: 166 [2800/6658 (42%)]\tLoss: 0.846839\n",
      "Train Epoch: 166 [2900/6658 (44%)]\tLoss: 0.394881\n",
      "Train Epoch: 166 [3000/6658 (45%)]\tLoss: 0.071295\n",
      "Train Epoch: 166 [3100/6658 (47%)]\tLoss: 0.199916\n",
      "Train Epoch: 166 [3200/6658 (48%)]\tLoss: 0.132148\n",
      "Train Epoch: 166 [3300/6658 (50%)]\tLoss: 0.535315\n",
      "Train Epoch: 166 [3400/6658 (51%)]\tLoss: 0.338568\n",
      "Train Epoch: 166 [3500/6658 (53%)]\tLoss: 0.001392\n",
      "Train Epoch: 166 [3600/6658 (54%)]\tLoss: 0.000832\n",
      "Train Epoch: 166 [3700/6658 (56%)]\tLoss: 2.355744\n",
      "Train Epoch: 166 [3800/6658 (57%)]\tLoss: 0.034865\n",
      "Train Epoch: 166 [3900/6658 (59%)]\tLoss: 0.010542\n",
      "Train Epoch: 166 [4000/6658 (60%)]\tLoss: 0.000517\n",
      "Train Epoch: 166 [4100/6658 (62%)]\tLoss: 0.296802\n",
      "Train Epoch: 166 [4200/6658 (63%)]\tLoss: 0.417741\n",
      "Train Epoch: 166 [4300/6658 (65%)]\tLoss: 0.047265\n",
      "Train Epoch: 166 [4400/6658 (66%)]\tLoss: 0.274062\n",
      "Train Epoch: 166 [4500/6658 (68%)]\tLoss: 1.282076\n",
      "Train Epoch: 166 [4600/6658 (69%)]\tLoss: 1.223740\n",
      "Train Epoch: 166 [4700/6658 (71%)]\tLoss: 0.571414\n",
      "Train Epoch: 166 [4800/6658 (72%)]\tLoss: 0.584383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 166 [4900/6658 (74%)]\tLoss: 0.037154\n",
      "Train Epoch: 166 [5000/6658 (75%)]\tLoss: 0.254652\n",
      "Train Epoch: 166 [5100/6658 (77%)]\tLoss: 0.134413\n",
      "Train Epoch: 166 [5200/6658 (78%)]\tLoss: 1.703513\n",
      "Train Epoch: 166 [5300/6658 (80%)]\tLoss: 0.109339\n",
      "Train Epoch: 166 [5400/6658 (81%)]\tLoss: 0.127345\n",
      "Train Epoch: 166 [5500/6658 (83%)]\tLoss: 0.424745\n",
      "Train Epoch: 166 [5600/6658 (84%)]\tLoss: 1.122495\n",
      "Train Epoch: 166 [5700/6658 (86%)]\tLoss: 0.356442\n",
      "Train Epoch: 166 [5800/6658 (87%)]\tLoss: 0.053048\n",
      "Train Epoch: 166 [5900/6658 (89%)]\tLoss: 0.215366\n",
      "Train Epoch: 166 [6000/6658 (90%)]\tLoss: 0.325230\n",
      "Train Epoch: 166 [6100/6658 (92%)]\tLoss: 0.443725\n",
      "Train Epoch: 166 [6200/6658 (93%)]\tLoss: 0.160476\n",
      "Train Epoch: 166 [6300/6658 (95%)]\tLoss: 0.128077\n",
      "Train Epoch: 166 [6400/6658 (96%)]\tLoss: 0.120826\n",
      "Train Epoch: 166 [6500/6658 (98%)]\tLoss: 0.501597\n",
      "Train Epoch: 166 [6600/6658 (99%)]\tLoss: 0.187594\n",
      "train loss average =  0.7080374715191755\n",
      "\n",
      "Test set: Average loss: 0.6923\n",
      "\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3470, 6.1185, 5.8485, 5.8556, 6.2506, 6.0446], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 167 [0/6658 (0%)]\tLoss: 1.012799\n",
      "Train Epoch: 167 [100/6658 (2%)]\tLoss: 0.988487\n",
      "Train Epoch: 167 [200/6658 (3%)]\tLoss: 0.001343\n",
      "Train Epoch: 167 [300/6658 (5%)]\tLoss: 0.931943\n",
      "Train Epoch: 167 [400/6658 (6%)]\tLoss: 0.000434\n",
      "Train Epoch: 167 [500/6658 (8%)]\tLoss: 1.429938\n",
      "Train Epoch: 167 [600/6658 (9%)]\tLoss: 0.008295\n",
      "Train Epoch: 167 [700/6658 (11%)]\tLoss: 0.102394\n",
      "Train Epoch: 167 [800/6658 (12%)]\tLoss: 0.115583\n",
      "Train Epoch: 167 [900/6658 (14%)]\tLoss: 0.018222\n",
      "Train Epoch: 167 [1000/6658 (15%)]\tLoss: 0.709812\n",
      "Train Epoch: 167 [1100/6658 (17%)]\tLoss: 0.311032\n",
      "Train Epoch: 167 [1200/6658 (18%)]\tLoss: 2.850810\n",
      "Train Epoch: 167 [1300/6658 (20%)]\tLoss: 0.006929\n",
      "Train Epoch: 167 [1400/6658 (21%)]\tLoss: 1.004446\n",
      "Train Epoch: 167 [1500/6658 (23%)]\tLoss: 0.534052\n",
      "Train Epoch: 167 [1600/6658 (24%)]\tLoss: 0.171356\n",
      "Train Epoch: 167 [1700/6658 (26%)]\tLoss: 0.888176\n",
      "Train Epoch: 167 [1800/6658 (27%)]\tLoss: 0.031761\n",
      "Train Epoch: 167 [1900/6658 (29%)]\tLoss: 1.434121\n",
      "Train Epoch: 167 [2000/6658 (30%)]\tLoss: 0.467121\n",
      "Train Epoch: 167 [2100/6658 (32%)]\tLoss: 0.341825\n",
      "Train Epoch: 167 [2200/6658 (33%)]\tLoss: 0.001611\n",
      "Train Epoch: 167 [2300/6658 (35%)]\tLoss: 1.399940\n",
      "Train Epoch: 167 [2400/6658 (36%)]\tLoss: 0.210849\n",
      "Train Epoch: 167 [2500/6658 (38%)]\tLoss: 0.019887\n",
      "Train Epoch: 167 [2600/6658 (39%)]\tLoss: 0.192615\n",
      "Train Epoch: 167 [2700/6658 (41%)]\tLoss: 0.006012\n",
      "Train Epoch: 167 [2800/6658 (42%)]\tLoss: 1.840513\n",
      "Train Epoch: 167 [2900/6658 (44%)]\tLoss: 0.007650\n",
      "Train Epoch: 167 [3000/6658 (45%)]\tLoss: 0.066230\n",
      "Train Epoch: 167 [3100/6658 (47%)]\tLoss: 0.556791\n",
      "Train Epoch: 167 [3200/6658 (48%)]\tLoss: 0.028281\n",
      "Train Epoch: 167 [3300/6658 (50%)]\tLoss: 0.083570\n",
      "Train Epoch: 167 [3400/6658 (51%)]\tLoss: 0.091374\n",
      "Train Epoch: 167 [3500/6658 (53%)]\tLoss: 0.066466\n",
      "Train Epoch: 167 [3600/6658 (54%)]\tLoss: 10.248884\n",
      "Train Epoch: 167 [3700/6658 (56%)]\tLoss: 0.661505\n",
      "Train Epoch: 167 [3800/6658 (57%)]\tLoss: 0.583521\n",
      "Train Epoch: 167 [3900/6658 (59%)]\tLoss: 0.299068\n",
      "Train Epoch: 167 [4000/6658 (60%)]\tLoss: 0.140195\n",
      "Train Epoch: 167 [4100/6658 (62%)]\tLoss: 0.228886\n",
      "Train Epoch: 167 [4200/6658 (63%)]\tLoss: 1.418714\n",
      "Train Epoch: 167 [4300/6658 (65%)]\tLoss: 0.584288\n",
      "Train Epoch: 167 [4400/6658 (66%)]\tLoss: 0.212002\n",
      "Train Epoch: 167 [4500/6658 (68%)]\tLoss: 0.568269\n",
      "Train Epoch: 167 [4600/6658 (69%)]\tLoss: 3.102809\n",
      "Train Epoch: 167 [4700/6658 (71%)]\tLoss: 0.705559\n",
      "Train Epoch: 167 [4800/6658 (72%)]\tLoss: 0.399357\n",
      "Train Epoch: 167 [4900/6658 (74%)]\tLoss: 0.303473\n",
      "Train Epoch: 167 [5000/6658 (75%)]\tLoss: 0.238655\n",
      "Train Epoch: 167 [5100/6658 (77%)]\tLoss: 0.076882\n",
      "Train Epoch: 167 [5200/6658 (78%)]\tLoss: 0.195515\n",
      "Train Epoch: 167 [5300/6658 (80%)]\tLoss: 0.011525\n",
      "Train Epoch: 167 [5400/6658 (81%)]\tLoss: 0.867501\n",
      "Train Epoch: 167 [5500/6658 (83%)]\tLoss: 0.008687\n",
      "Train Epoch: 167 [5600/6658 (84%)]\tLoss: 0.653848\n",
      "Train Epoch: 167 [5700/6658 (86%)]\tLoss: 1.285153\n",
      "Train Epoch: 167 [5800/6658 (87%)]\tLoss: 0.109209\n",
      "Train Epoch: 167 [5900/6658 (89%)]\tLoss: 0.479964\n",
      "Train Epoch: 167 [6000/6658 (90%)]\tLoss: 0.035920\n",
      "Train Epoch: 167 [6100/6658 (92%)]\tLoss: 0.049734\n",
      "Train Epoch: 167 [6200/6658 (93%)]\tLoss: 0.131870\n",
      "Train Epoch: 167 [6300/6658 (95%)]\tLoss: 2.584773\n",
      "Train Epoch: 167 [6400/6658 (96%)]\tLoss: 0.114053\n",
      "Train Epoch: 167 [6500/6658 (98%)]\tLoss: 1.448851\n",
      "Train Epoch: 167 [6600/6658 (99%)]\tLoss: 0.001783\n",
      "train loss average =  0.7089903110507205\n",
      "\n",
      "Test set: Average loss: 0.7039\n",
      "\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3493, 6.1199, 5.8479, 5.8551, 6.2528, 6.0443], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 168 [0/6658 (0%)]\tLoss: 0.095901\n",
      "Train Epoch: 168 [100/6658 (2%)]\tLoss: 0.448463\n",
      "Train Epoch: 168 [200/6658 (3%)]\tLoss: 0.099717\n",
      "Train Epoch: 168 [300/6658 (5%)]\tLoss: 1.300190\n",
      "Train Epoch: 168 [400/6658 (6%)]\tLoss: 0.102097\n",
      "Train Epoch: 168 [500/6658 (8%)]\tLoss: 0.306335\n",
      "Train Epoch: 168 [600/6658 (9%)]\tLoss: 0.000069\n",
      "Train Epoch: 168 [700/6658 (11%)]\tLoss: 0.136214\n",
      "Train Epoch: 168 [800/6658 (12%)]\tLoss: 0.084000\n",
      "Train Epoch: 168 [900/6658 (14%)]\tLoss: 0.717915\n",
      "Train Epoch: 168 [1000/6658 (15%)]\tLoss: 0.258034\n",
      "Train Epoch: 168 [1100/6658 (17%)]\tLoss: 0.204987\n",
      "Train Epoch: 168 [1200/6658 (18%)]\tLoss: 0.212867\n",
      "Train Epoch: 168 [1300/6658 (20%)]\tLoss: 0.577616\n",
      "Train Epoch: 168 [1400/6658 (21%)]\tLoss: 0.007029\n",
      "Train Epoch: 168 [1500/6658 (23%)]\tLoss: 0.000354\n",
      "Train Epoch: 168 [1600/6658 (24%)]\tLoss: 0.801956\n",
      "Train Epoch: 168 [1700/6658 (26%)]\tLoss: 0.445284\n",
      "Train Epoch: 168 [1800/6658 (27%)]\tLoss: 1.148562\n",
      "Train Epoch: 168 [1900/6658 (29%)]\tLoss: 0.178824\n",
      "Train Epoch: 168 [2000/6658 (30%)]\tLoss: 2.179256\n",
      "Train Epoch: 168 [2100/6658 (32%)]\tLoss: 0.081085\n",
      "Train Epoch: 168 [2200/6658 (33%)]\tLoss: 0.012645\n",
      "Train Epoch: 168 [2300/6658 (35%)]\tLoss: 0.120035\n",
      "Train Epoch: 168 [2400/6658 (36%)]\tLoss: 0.639213\n",
      "Train Epoch: 168 [2500/6658 (38%)]\tLoss: 0.364886\n",
      "Train Epoch: 168 [2600/6658 (39%)]\tLoss: 1.390686\n",
      "Train Epoch: 168 [2700/6658 (41%)]\tLoss: 1.373379\n",
      "Train Epoch: 168 [2800/6658 (42%)]\tLoss: 0.423159\n",
      "Train Epoch: 168 [2900/6658 (44%)]\tLoss: 1.107031\n",
      "Train Epoch: 168 [3000/6658 (45%)]\tLoss: 0.000296\n",
      "Train Epoch: 168 [3100/6658 (47%)]\tLoss: 0.943013\n",
      "Train Epoch: 168 [3200/6658 (48%)]\tLoss: 0.019771\n",
      "Train Epoch: 168 [3300/6658 (50%)]\tLoss: 0.009855\n",
      "Train Epoch: 168 [3400/6658 (51%)]\tLoss: 0.527408\n",
      "Train Epoch: 168 [3500/6658 (53%)]\tLoss: 0.293909\n",
      "Train Epoch: 168 [3600/6658 (54%)]\tLoss: 0.000963\n",
      "Train Epoch: 168 [3700/6658 (56%)]\tLoss: 17.321630\n",
      "Train Epoch: 168 [3800/6658 (57%)]\tLoss: 0.004031\n",
      "Train Epoch: 168 [3900/6658 (59%)]\tLoss: 0.265073\n",
      "Train Epoch: 168 [4000/6658 (60%)]\tLoss: 0.158514\n",
      "Train Epoch: 168 [4100/6658 (62%)]\tLoss: 1.809682\n",
      "Train Epoch: 168 [4200/6658 (63%)]\tLoss: 0.003440\n",
      "Train Epoch: 168 [4300/6658 (65%)]\tLoss: 5.431549\n",
      "Train Epoch: 168 [4400/6658 (66%)]\tLoss: 0.778512\n",
      "Train Epoch: 168 [4500/6658 (68%)]\tLoss: 0.138325\n",
      "Train Epoch: 168 [4600/6658 (69%)]\tLoss: 0.277925\n",
      "Train Epoch: 168 [4700/6658 (71%)]\tLoss: 0.000701\n",
      "Train Epoch: 168 [4800/6658 (72%)]\tLoss: 0.656269\n",
      "Train Epoch: 168 [4900/6658 (74%)]\tLoss: 1.283418\n",
      "Train Epoch: 168 [5000/6658 (75%)]\tLoss: 0.052792\n",
      "Train Epoch: 168 [5100/6658 (77%)]\tLoss: 0.364567\n",
      "Train Epoch: 168 [5200/6658 (78%)]\tLoss: 1.072850\n",
      "Train Epoch: 168 [5300/6658 (80%)]\tLoss: 0.369546\n",
      "Train Epoch: 168 [5400/6658 (81%)]\tLoss: 0.225596\n",
      "Train Epoch: 168 [5500/6658 (83%)]\tLoss: 0.405736\n",
      "Train Epoch: 168 [5600/6658 (84%)]\tLoss: 0.410588\n",
      "Train Epoch: 168 [5700/6658 (86%)]\tLoss: 1.788436\n",
      "Train Epoch: 168 [5800/6658 (87%)]\tLoss: 0.326587\n",
      "Train Epoch: 168 [5900/6658 (89%)]\tLoss: 0.014376\n",
      "Train Epoch: 168 [6000/6658 (90%)]\tLoss: 0.305624\n",
      "Train Epoch: 168 [6100/6658 (92%)]\tLoss: 0.134438\n",
      "Train Epoch: 168 [6200/6658 (93%)]\tLoss: 0.816447\n",
      "Train Epoch: 168 [6300/6658 (95%)]\tLoss: 0.146901\n",
      "Train Epoch: 168 [6400/6658 (96%)]\tLoss: 0.448155\n",
      "Train Epoch: 168 [6500/6658 (98%)]\tLoss: 0.114372\n",
      "Train Epoch: 168 [6600/6658 (99%)]\tLoss: 0.003057\n",
      "train loss average =  0.7136272321626157\n",
      "\n",
      "Test set: Average loss: 0.7084\n",
      "\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3510, 6.1208, 5.8473, 5.8535, 6.2541, 6.0444], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 169 [0/6658 (0%)]\tLoss: 0.526820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 169 [100/6658 (2%)]\tLoss: 0.870100\n",
      "Train Epoch: 169 [200/6658 (3%)]\tLoss: 0.111293\n",
      "Train Epoch: 169 [300/6658 (5%)]\tLoss: 0.602339\n",
      "Train Epoch: 169 [400/6658 (6%)]\tLoss: 0.085001\n",
      "Train Epoch: 169 [500/6658 (8%)]\tLoss: 0.529869\n",
      "Train Epoch: 169 [600/6658 (9%)]\tLoss: 0.107772\n",
      "Train Epoch: 169 [700/6658 (11%)]\tLoss: 0.069506\n",
      "Train Epoch: 169 [800/6658 (12%)]\tLoss: 0.075526\n",
      "Train Epoch: 169 [900/6658 (14%)]\tLoss: 0.071437\n",
      "Train Epoch: 169 [1000/6658 (15%)]\tLoss: 0.012432\n",
      "Train Epoch: 169 [1100/6658 (17%)]\tLoss: 0.260296\n",
      "Train Epoch: 169 [1200/6658 (18%)]\tLoss: 0.159071\n",
      "Train Epoch: 169 [1300/6658 (20%)]\tLoss: 0.031805\n",
      "Train Epoch: 169 [1400/6658 (21%)]\tLoss: 0.241351\n",
      "Train Epoch: 169 [1500/6658 (23%)]\tLoss: 0.050157\n",
      "Train Epoch: 169 [1600/6658 (24%)]\tLoss: 0.429931\n",
      "Train Epoch: 169 [1700/6658 (26%)]\tLoss: 0.000401\n",
      "Train Epoch: 169 [1800/6658 (27%)]\tLoss: 0.764386\n",
      "Train Epoch: 169 [1900/6658 (29%)]\tLoss: 0.009823\n",
      "Train Epoch: 169 [2000/6658 (30%)]\tLoss: 0.380971\n",
      "Train Epoch: 169 [2100/6658 (32%)]\tLoss: 0.091135\n",
      "Train Epoch: 169 [2200/6658 (33%)]\tLoss: 0.000007\n",
      "Train Epoch: 169 [2300/6658 (35%)]\tLoss: 7.298307\n",
      "Train Epoch: 169 [2400/6658 (36%)]\tLoss: 0.160798\n",
      "Train Epoch: 169 [2500/6658 (38%)]\tLoss: 0.146226\n",
      "Train Epoch: 169 [2600/6658 (39%)]\tLoss: 0.328698\n",
      "Train Epoch: 169 [2700/6658 (41%)]\tLoss: 2.741659\n",
      "Train Epoch: 169 [2800/6658 (42%)]\tLoss: 1.903276\n",
      "Train Epoch: 169 [2900/6658 (44%)]\tLoss: 0.341891\n",
      "Train Epoch: 169 [3000/6658 (45%)]\tLoss: 0.296774\n",
      "Train Epoch: 169 [3100/6658 (47%)]\tLoss: 0.502090\n",
      "Train Epoch: 169 [3200/6658 (48%)]\tLoss: 0.214222\n",
      "Train Epoch: 169 [3300/6658 (50%)]\tLoss: 0.170698\n",
      "Train Epoch: 169 [3400/6658 (51%)]\tLoss: 0.110885\n",
      "Train Epoch: 169 [3500/6658 (53%)]\tLoss: 0.950485\n",
      "Train Epoch: 169 [3600/6658 (54%)]\tLoss: 0.136198\n",
      "Train Epoch: 169 [3700/6658 (56%)]\tLoss: 0.000413\n",
      "Train Epoch: 169 [3800/6658 (57%)]\tLoss: 0.857344\n",
      "Train Epoch: 169 [3900/6658 (59%)]\tLoss: 0.000263\n",
      "Train Epoch: 169 [4000/6658 (60%)]\tLoss: 0.569522\n",
      "Train Epoch: 169 [4100/6658 (62%)]\tLoss: 0.398671\n",
      "Train Epoch: 169 [4200/6658 (63%)]\tLoss: 0.162820\n",
      "Train Epoch: 169 [4300/6658 (65%)]\tLoss: 0.354433\n",
      "Train Epoch: 169 [4400/6658 (66%)]\tLoss: 0.000001\n",
      "Train Epoch: 169 [4500/6658 (68%)]\tLoss: 0.735044\n",
      "Train Epoch: 169 [4600/6658 (69%)]\tLoss: 0.201184\n",
      "Train Epoch: 169 [4700/6658 (71%)]\tLoss: 2.478160\n",
      "Train Epoch: 169 [4800/6658 (72%)]\tLoss: 0.187217\n",
      "Train Epoch: 169 [4900/6658 (74%)]\tLoss: 1.272015\n",
      "Train Epoch: 169 [5000/6658 (75%)]\tLoss: 0.097558\n",
      "Train Epoch: 169 [5100/6658 (77%)]\tLoss: 0.777787\n",
      "Train Epoch: 169 [5200/6658 (78%)]\tLoss: 0.175916\n",
      "Train Epoch: 169 [5300/6658 (80%)]\tLoss: 1.553050\n",
      "Train Epoch: 169 [5400/6658 (81%)]\tLoss: 0.002136\n",
      "Train Epoch: 169 [5500/6658 (83%)]\tLoss: 2.382315\n",
      "Train Epoch: 169 [5600/6658 (84%)]\tLoss: 0.292474\n",
      "Train Epoch: 169 [5700/6658 (86%)]\tLoss: 3.221990\n",
      "Train Epoch: 169 [5800/6658 (87%)]\tLoss: 1.169862\n",
      "Train Epoch: 169 [5900/6658 (89%)]\tLoss: 0.033159\n",
      "Train Epoch: 169 [6000/6658 (90%)]\tLoss: 0.322364\n",
      "Train Epoch: 169 [6100/6658 (92%)]\tLoss: 0.342012\n",
      "Train Epoch: 169 [6200/6658 (93%)]\tLoss: 0.014349\n",
      "Train Epoch: 169 [6300/6658 (95%)]\tLoss: 1.343251\n",
      "Train Epoch: 169 [6400/6658 (96%)]\tLoss: 0.885422\n",
      "Train Epoch: 169 [6500/6658 (98%)]\tLoss: 0.024756\n",
      "Train Epoch: 169 [6600/6658 (99%)]\tLoss: 0.441181\n",
      "train loss average =  0.7064156238694794\n",
      "\n",
      "Test set: Average loss: 0.7052\n",
      "\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3528, 6.1219, 5.8474, 5.8537, 6.2561, 6.0444], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 170 [0/6658 (0%)]\tLoss: 0.222203\n",
      "Train Epoch: 170 [100/6658 (2%)]\tLoss: 0.039512\n",
      "Train Epoch: 170 [200/6658 (3%)]\tLoss: 0.529413\n",
      "Train Epoch: 170 [300/6658 (5%)]\tLoss: 1.486306\n",
      "Train Epoch: 170 [400/6658 (6%)]\tLoss: 0.954461\n",
      "Train Epoch: 170 [500/6658 (8%)]\tLoss: 0.418955\n",
      "Train Epoch: 170 [600/6658 (9%)]\tLoss: 0.229920\n",
      "Train Epoch: 170 [700/6658 (11%)]\tLoss: 0.537874\n",
      "Train Epoch: 170 [800/6658 (12%)]\tLoss: 0.000059\n",
      "Train Epoch: 170 [900/6658 (14%)]\tLoss: 0.001075\n",
      "Train Epoch: 170 [1000/6658 (15%)]\tLoss: 0.279330\n",
      "Train Epoch: 170 [1100/6658 (17%)]\tLoss: 1.454511\n",
      "Train Epoch: 170 [1200/6658 (18%)]\tLoss: 11.399586\n",
      "Train Epoch: 170 [1300/6658 (20%)]\tLoss: 0.580855\n",
      "Train Epoch: 170 [1400/6658 (21%)]\tLoss: 0.144137\n",
      "Train Epoch: 170 [1500/6658 (23%)]\tLoss: 0.110846\n",
      "Train Epoch: 170 [1600/6658 (24%)]\tLoss: 0.125317\n",
      "Train Epoch: 170 [1700/6658 (26%)]\tLoss: 0.010931\n",
      "Train Epoch: 170 [1800/6658 (27%)]\tLoss: 0.064041\n",
      "Train Epoch: 170 [1900/6658 (29%)]\tLoss: 0.016884\n",
      "Train Epoch: 170 [2000/6658 (30%)]\tLoss: 0.668297\n",
      "Train Epoch: 170 [2100/6658 (32%)]\tLoss: 0.259730\n",
      "Train Epoch: 170 [2200/6658 (33%)]\tLoss: 0.480126\n",
      "Train Epoch: 170 [2300/6658 (35%)]\tLoss: 0.124205\n",
      "Train Epoch: 170 [2400/6658 (36%)]\tLoss: 0.237018\n",
      "Train Epoch: 170 [2500/6658 (38%)]\tLoss: 3.565071\n",
      "Train Epoch: 170 [2600/6658 (39%)]\tLoss: 0.197416\n",
      "Train Epoch: 170 [2700/6658 (41%)]\tLoss: 0.326546\n",
      "Train Epoch: 170 [2800/6658 (42%)]\tLoss: 0.728108\n",
      "Train Epoch: 170 [2900/6658 (44%)]\tLoss: 0.424240\n",
      "Train Epoch: 170 [3000/6658 (45%)]\tLoss: 5.059944\n",
      "Train Epoch: 170 [3100/6658 (47%)]\tLoss: 0.850050\n",
      "Train Epoch: 170 [3200/6658 (48%)]\tLoss: 0.011497\n",
      "Train Epoch: 170 [3300/6658 (50%)]\tLoss: 0.069693\n",
      "Train Epoch: 170 [3400/6658 (51%)]\tLoss: 0.215576\n",
      "Train Epoch: 170 [3500/6658 (53%)]\tLoss: 0.229059\n",
      "Train Epoch: 170 [3600/6658 (54%)]\tLoss: 0.113064\n",
      "Train Epoch: 170 [3700/6658 (56%)]\tLoss: 0.126615\n",
      "Train Epoch: 170 [3800/6658 (57%)]\tLoss: 0.619090\n",
      "Train Epoch: 170 [3900/6658 (59%)]\tLoss: 1.980330\n",
      "Train Epoch: 170 [4000/6658 (60%)]\tLoss: 0.366458\n",
      "Train Epoch: 170 [4100/6658 (62%)]\tLoss: 0.001904\n",
      "Train Epoch: 170 [4200/6658 (63%)]\tLoss: 0.105197\n",
      "Train Epoch: 170 [4300/6658 (65%)]\tLoss: 0.022576\n",
      "Train Epoch: 170 [4400/6658 (66%)]\tLoss: 0.020035\n",
      "Train Epoch: 170 [4500/6658 (68%)]\tLoss: 0.399577\n",
      "Train Epoch: 170 [4600/6658 (69%)]\tLoss: 0.011372\n",
      "Train Epoch: 170 [4700/6658 (71%)]\tLoss: 0.290755\n",
      "Train Epoch: 170 [4800/6658 (72%)]\tLoss: 0.207264\n",
      "Train Epoch: 170 [4900/6658 (74%)]\tLoss: 5.841567\n",
      "Train Epoch: 170 [5000/6658 (75%)]\tLoss: 0.382540\n",
      "Train Epoch: 170 [5100/6658 (77%)]\tLoss: 0.341049\n",
      "Train Epoch: 170 [5200/6658 (78%)]\tLoss: 0.005215\n",
      "Train Epoch: 170 [5300/6658 (80%)]\tLoss: 0.365649\n",
      "Train Epoch: 170 [5400/6658 (81%)]\tLoss: 0.041366\n",
      "Train Epoch: 170 [5500/6658 (83%)]\tLoss: 0.677364\n",
      "Train Epoch: 170 [5600/6658 (84%)]\tLoss: 0.127300\n",
      "Train Epoch: 170 [5700/6658 (86%)]\tLoss: 0.265438\n",
      "Train Epoch: 170 [5800/6658 (87%)]\tLoss: 1.225115\n",
      "Train Epoch: 170 [5900/6658 (89%)]\tLoss: 0.208525\n",
      "Train Epoch: 170 [6000/6658 (90%)]\tLoss: 0.320613\n",
      "Train Epoch: 170 [6100/6658 (92%)]\tLoss: 0.014876\n",
      "Train Epoch: 170 [6200/6658 (93%)]\tLoss: 0.023167\n",
      "Train Epoch: 170 [6300/6658 (95%)]\tLoss: 0.493952\n",
      "Train Epoch: 170 [6400/6658 (96%)]\tLoss: 0.040162\n",
      "Train Epoch: 170 [6500/6658 (98%)]\tLoss: 0.105907\n",
      "Train Epoch: 170 [6600/6658 (99%)]\tLoss: 1.322324\n",
      "train loss average =  0.7056421066552835\n",
      "\n",
      "Test set: Average loss: 0.6900\n",
      "\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3553, 6.1232, 5.8475, 5.8529, 6.2571, 6.0442], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 171 [0/6658 (0%)]\tLoss: 3.735839\n",
      "Train Epoch: 171 [100/6658 (2%)]\tLoss: 0.729904\n",
      "Train Epoch: 171 [200/6658 (3%)]\tLoss: 0.194410\n",
      "Train Epoch: 171 [300/6658 (5%)]\tLoss: 0.292948\n",
      "Train Epoch: 171 [400/6658 (6%)]\tLoss: 0.356401\n",
      "Train Epoch: 171 [500/6658 (8%)]\tLoss: 0.154405\n",
      "Train Epoch: 171 [600/6658 (9%)]\tLoss: 0.043654\n",
      "Train Epoch: 171 [700/6658 (11%)]\tLoss: 0.733368\n",
      "Train Epoch: 171 [800/6658 (12%)]\tLoss: 0.164450\n",
      "Train Epoch: 171 [900/6658 (14%)]\tLoss: 0.149914\n",
      "Train Epoch: 171 [1000/6658 (15%)]\tLoss: 0.004234\n",
      "Train Epoch: 171 [1100/6658 (17%)]\tLoss: 0.002448\n",
      "Train Epoch: 171 [1200/6658 (18%)]\tLoss: 0.143794\n",
      "Train Epoch: 171 [1300/6658 (20%)]\tLoss: 0.546101\n",
      "Train Epoch: 171 [1400/6658 (21%)]\tLoss: 1.804899\n",
      "Train Epoch: 171 [1500/6658 (23%)]\tLoss: 0.023284\n",
      "Train Epoch: 171 [1600/6658 (24%)]\tLoss: 2.025791\n",
      "Train Epoch: 171 [1700/6658 (26%)]\tLoss: 0.054540\n",
      "Train Epoch: 171 [1800/6658 (27%)]\tLoss: 0.106035\n",
      "Train Epoch: 171 [1900/6658 (29%)]\tLoss: 0.262763\n",
      "Train Epoch: 171 [2000/6658 (30%)]\tLoss: 0.881324\n",
      "Train Epoch: 171 [2100/6658 (32%)]\tLoss: 0.085423\n",
      "Train Epoch: 171 [2200/6658 (33%)]\tLoss: 0.069407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 171 [2300/6658 (35%)]\tLoss: 0.000146\n",
      "Train Epoch: 171 [2400/6658 (36%)]\tLoss: 1.151358\n",
      "Train Epoch: 171 [2500/6658 (38%)]\tLoss: 0.001324\n",
      "Train Epoch: 171 [2600/6658 (39%)]\tLoss: 0.905339\n",
      "Train Epoch: 171 [2700/6658 (41%)]\tLoss: 0.010370\n",
      "Train Epoch: 171 [2800/6658 (42%)]\tLoss: 0.018869\n",
      "Train Epoch: 171 [2900/6658 (44%)]\tLoss: 14.195217\n",
      "Train Epoch: 171 [3000/6658 (45%)]\tLoss: 0.175195\n",
      "Train Epoch: 171 [3100/6658 (47%)]\tLoss: 1.924878\n",
      "Train Epoch: 171 [3200/6658 (48%)]\tLoss: 0.208207\n",
      "Train Epoch: 171 [3300/6658 (50%)]\tLoss: 0.006969\n",
      "Train Epoch: 171 [3400/6658 (51%)]\tLoss: 0.000393\n",
      "Train Epoch: 171 [3500/6658 (53%)]\tLoss: 0.115037\n",
      "Train Epoch: 171 [3600/6658 (54%)]\tLoss: 0.374106\n",
      "Train Epoch: 171 [3700/6658 (56%)]\tLoss: 0.004720\n",
      "Train Epoch: 171 [3800/6658 (57%)]\tLoss: 0.642745\n",
      "Train Epoch: 171 [3900/6658 (59%)]\tLoss: 0.163968\n",
      "Train Epoch: 171 [4000/6658 (60%)]\tLoss: 0.115881\n",
      "Train Epoch: 171 [4100/6658 (62%)]\tLoss: 0.005703\n",
      "Train Epoch: 171 [4200/6658 (63%)]\tLoss: 0.376736\n",
      "Train Epoch: 171 [4300/6658 (65%)]\tLoss: 0.284509\n",
      "Train Epoch: 171 [4400/6658 (66%)]\tLoss: 0.592147\n",
      "Train Epoch: 171 [4500/6658 (68%)]\tLoss: 0.489673\n",
      "Train Epoch: 171 [4600/6658 (69%)]\tLoss: 0.723172\n",
      "Train Epoch: 171 [4700/6658 (71%)]\tLoss: 6.723573\n",
      "Train Epoch: 171 [4800/6658 (72%)]\tLoss: 0.013539\n",
      "Train Epoch: 171 [4900/6658 (74%)]\tLoss: 1.181031\n",
      "Train Epoch: 171 [5000/6658 (75%)]\tLoss: 0.560764\n",
      "Train Epoch: 171 [5100/6658 (77%)]\tLoss: 3.530913\n",
      "Train Epoch: 171 [5200/6658 (78%)]\tLoss: 0.915770\n",
      "Train Epoch: 171 [5300/6658 (80%)]\tLoss: 0.056083\n",
      "Train Epoch: 171 [5400/6658 (81%)]\tLoss: 4.490863\n",
      "Train Epoch: 171 [5500/6658 (83%)]\tLoss: 0.128418\n",
      "Train Epoch: 171 [5600/6658 (84%)]\tLoss: 0.036536\n",
      "Train Epoch: 171 [5700/6658 (86%)]\tLoss: 1.043472\n",
      "Train Epoch: 171 [5800/6658 (87%)]\tLoss: 0.173830\n",
      "Train Epoch: 171 [5900/6658 (89%)]\tLoss: 0.004292\n",
      "Train Epoch: 171 [6000/6658 (90%)]\tLoss: 2.014763\n",
      "Train Epoch: 171 [6100/6658 (92%)]\tLoss: 0.180900\n",
      "Train Epoch: 171 [6200/6658 (93%)]\tLoss: 0.581683\n",
      "Train Epoch: 171 [6300/6658 (95%)]\tLoss: 0.009054\n",
      "Train Epoch: 171 [6400/6658 (96%)]\tLoss: 0.256216\n",
      "Train Epoch: 171 [6500/6658 (98%)]\tLoss: 0.159727\n",
      "Train Epoch: 171 [6600/6658 (99%)]\tLoss: 0.302927\n",
      "train loss average =  0.7119169837258543\n",
      "\n",
      "Test set: Average loss: 0.6840\n",
      "\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3572, 6.1240, 5.8466, 5.8515, 6.2588, 6.0439], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 172 [0/6658 (0%)]\tLoss: 0.512490\n",
      "Train Epoch: 172 [100/6658 (2%)]\tLoss: 0.045980\n",
      "Train Epoch: 172 [200/6658 (3%)]\tLoss: 0.055556\n",
      "Train Epoch: 172 [300/6658 (5%)]\tLoss: 0.112580\n",
      "Train Epoch: 172 [400/6658 (6%)]\tLoss: 0.002144\n",
      "Train Epoch: 172 [500/6658 (8%)]\tLoss: 0.001900\n",
      "Train Epoch: 172 [600/6658 (9%)]\tLoss: 0.036045\n",
      "Train Epoch: 172 [700/6658 (11%)]\tLoss: 0.701959\n",
      "Train Epoch: 172 [800/6658 (12%)]\tLoss: 0.252887\n",
      "Train Epoch: 172 [900/6658 (14%)]\tLoss: 0.196113\n",
      "Train Epoch: 172 [1000/6658 (15%)]\tLoss: 0.504398\n",
      "Train Epoch: 172 [1100/6658 (17%)]\tLoss: 0.056905\n",
      "Train Epoch: 172 [1200/6658 (18%)]\tLoss: 0.545908\n",
      "Train Epoch: 172 [1300/6658 (20%)]\tLoss: 1.608574\n",
      "Train Epoch: 172 [1400/6658 (21%)]\tLoss: 0.391212\n",
      "Train Epoch: 172 [1500/6658 (23%)]\tLoss: 0.371439\n",
      "Train Epoch: 172 [1600/6658 (24%)]\tLoss: 0.578124\n",
      "Train Epoch: 172 [1700/6658 (26%)]\tLoss: 0.935452\n",
      "Train Epoch: 172 [1800/6658 (27%)]\tLoss: 0.416076\n",
      "Train Epoch: 172 [1900/6658 (29%)]\tLoss: 0.002316\n",
      "Train Epoch: 172 [2000/6658 (30%)]\tLoss: 0.276503\n",
      "Train Epoch: 172 [2100/6658 (32%)]\tLoss: 0.017092\n",
      "Train Epoch: 172 [2200/6658 (33%)]\tLoss: 0.484158\n",
      "Train Epoch: 172 [2300/6658 (35%)]\tLoss: 2.787027\n",
      "Train Epoch: 172 [2400/6658 (36%)]\tLoss: 0.338994\n",
      "Train Epoch: 172 [2500/6658 (38%)]\tLoss: 1.418170\n",
      "Train Epoch: 172 [2600/6658 (39%)]\tLoss: 0.000006\n",
      "Train Epoch: 172 [2700/6658 (41%)]\tLoss: 0.037178\n",
      "Train Epoch: 172 [2800/6658 (42%)]\tLoss: 0.071217\n",
      "Train Epoch: 172 [2900/6658 (44%)]\tLoss: 0.020674\n",
      "Train Epoch: 172 [3000/6658 (45%)]\tLoss: 0.124001\n",
      "Train Epoch: 172 [3100/6658 (47%)]\tLoss: 0.318332\n",
      "Train Epoch: 172 [3200/6658 (48%)]\tLoss: 0.576330\n",
      "Train Epoch: 172 [3300/6658 (50%)]\tLoss: 0.137649\n",
      "Train Epoch: 172 [3400/6658 (51%)]\tLoss: 0.376538\n",
      "Train Epoch: 172 [3500/6658 (53%)]\tLoss: 0.896096\n",
      "Train Epoch: 172 [3600/6658 (54%)]\tLoss: 10.050710\n",
      "Train Epoch: 172 [3700/6658 (56%)]\tLoss: 0.000006\n",
      "Train Epoch: 172 [3800/6658 (57%)]\tLoss: 0.211445\n",
      "Train Epoch: 172 [3900/6658 (59%)]\tLoss: 0.008685\n",
      "Train Epoch: 172 [4000/6658 (60%)]\tLoss: 0.111385\n",
      "Train Epoch: 172 [4100/6658 (62%)]\tLoss: 0.009162\n",
      "Train Epoch: 172 [4200/6658 (63%)]\tLoss: 0.132620\n",
      "Train Epoch: 172 [4300/6658 (65%)]\tLoss: 1.248733\n",
      "Train Epoch: 172 [4400/6658 (66%)]\tLoss: 0.012350\n",
      "Train Epoch: 172 [4500/6658 (68%)]\tLoss: 0.290801\n",
      "Train Epoch: 172 [4600/6658 (69%)]\tLoss: 0.221014\n",
      "Train Epoch: 172 [4700/6658 (71%)]\tLoss: 0.001570\n",
      "Train Epoch: 172 [4800/6658 (72%)]\tLoss: 0.562566\n",
      "Train Epoch: 172 [4900/6658 (74%)]\tLoss: 0.059832\n",
      "Train Epoch: 172 [5000/6658 (75%)]\tLoss: 18.695679\n",
      "Train Epoch: 172 [5100/6658 (77%)]\tLoss: 0.855876\n",
      "Train Epoch: 172 [5200/6658 (78%)]\tLoss: 0.001722\n",
      "Train Epoch: 172 [5300/6658 (80%)]\tLoss: 0.149856\n",
      "Train Epoch: 172 [5400/6658 (81%)]\tLoss: 0.982654\n",
      "Train Epoch: 172 [5500/6658 (83%)]\tLoss: 1.753970\n",
      "Train Epoch: 172 [5600/6658 (84%)]\tLoss: 1.151167\n",
      "Train Epoch: 172 [5700/6658 (86%)]\tLoss: 0.319592\n",
      "Train Epoch: 172 [5800/6658 (87%)]\tLoss: 0.291422\n",
      "Train Epoch: 172 [5900/6658 (89%)]\tLoss: 0.323785\n",
      "Train Epoch: 172 [6000/6658 (90%)]\tLoss: 0.005427\n",
      "Train Epoch: 172 [6100/6658 (92%)]\tLoss: 0.174563\n",
      "Train Epoch: 172 [6200/6658 (93%)]\tLoss: 0.653496\n",
      "Train Epoch: 172 [6300/6658 (95%)]\tLoss: 0.363814\n",
      "Train Epoch: 172 [6400/6658 (96%)]\tLoss: 2.248942\n",
      "Train Epoch: 172 [6500/6658 (98%)]\tLoss: 1.284011\n",
      "Train Epoch: 172 [6600/6658 (99%)]\tLoss: 0.457055\n",
      "train loss average =  0.7098204937521663\n",
      "\n",
      "Test set: Average loss: 0.7001\n",
      "\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3585, 6.1250, 5.8468, 5.8515, 6.2604, 6.0436], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 173 [0/6658 (0%)]\tLoss: 0.210720\n",
      "Train Epoch: 173 [100/6658 (2%)]\tLoss: 0.000004\n",
      "Train Epoch: 173 [200/6658 (3%)]\tLoss: 0.448652\n",
      "Train Epoch: 173 [300/6658 (5%)]\tLoss: 0.013150\n",
      "Train Epoch: 173 [400/6658 (6%)]\tLoss: 0.667091\n",
      "Train Epoch: 173 [500/6658 (8%)]\tLoss: 0.781307\n",
      "Train Epoch: 173 [600/6658 (9%)]\tLoss: 0.093310\n",
      "Train Epoch: 173 [700/6658 (11%)]\tLoss: 0.336691\n",
      "Train Epoch: 173 [800/6658 (12%)]\tLoss: 0.000516\n",
      "Train Epoch: 173 [900/6658 (14%)]\tLoss: 0.109983\n",
      "Train Epoch: 173 [1000/6658 (15%)]\tLoss: 0.311156\n",
      "Train Epoch: 173 [1100/6658 (17%)]\tLoss: 0.007624\n",
      "Train Epoch: 173 [1200/6658 (18%)]\tLoss: 0.344004\n",
      "Train Epoch: 173 [1300/6658 (20%)]\tLoss: 0.901278\n",
      "Train Epoch: 173 [1400/6658 (21%)]\tLoss: 0.000001\n",
      "Train Epoch: 173 [1500/6658 (23%)]\tLoss: 0.001460\n",
      "Train Epoch: 173 [1600/6658 (24%)]\tLoss: 0.006469\n",
      "Train Epoch: 173 [1700/6658 (26%)]\tLoss: 0.489110\n",
      "Train Epoch: 173 [1800/6658 (27%)]\tLoss: 0.695742\n",
      "Train Epoch: 173 [1900/6658 (29%)]\tLoss: 1.594008\n",
      "Train Epoch: 173 [2000/6658 (30%)]\tLoss: 0.043429\n",
      "Train Epoch: 173 [2100/6658 (32%)]\tLoss: 2.255084\n",
      "Train Epoch: 173 [2200/6658 (33%)]\tLoss: 0.109827\n",
      "Train Epoch: 173 [2300/6658 (35%)]\tLoss: 0.774838\n",
      "Train Epoch: 173 [2400/6658 (36%)]\tLoss: 0.119011\n",
      "Train Epoch: 173 [2500/6658 (38%)]\tLoss: 0.000680\n",
      "Train Epoch: 173 [2600/6658 (39%)]\tLoss: 0.463012\n",
      "Train Epoch: 173 [2700/6658 (41%)]\tLoss: 0.004342\n",
      "Train Epoch: 173 [2800/6658 (42%)]\tLoss: 0.181429\n",
      "Train Epoch: 173 [2900/6658 (44%)]\tLoss: 0.135873\n",
      "Train Epoch: 173 [3000/6658 (45%)]\tLoss: 0.536184\n",
      "Train Epoch: 173 [3100/6658 (47%)]\tLoss: 1.953964\n",
      "Train Epoch: 173 [3200/6658 (48%)]\tLoss: 0.222104\n",
      "Train Epoch: 173 [3300/6658 (50%)]\tLoss: 0.307650\n",
      "Train Epoch: 173 [3400/6658 (51%)]\tLoss: 0.748680\n",
      "Train Epoch: 173 [3500/6658 (53%)]\tLoss: 0.016134\n",
      "Train Epoch: 173 [3600/6658 (54%)]\tLoss: 0.105441\n",
      "Train Epoch: 173 [3700/6658 (56%)]\tLoss: 3.473786\n",
      "Train Epoch: 173 [3800/6658 (57%)]\tLoss: 1.375818\n",
      "Train Epoch: 173 [3900/6658 (59%)]\tLoss: 4.753049\n",
      "Train Epoch: 173 [4000/6658 (60%)]\tLoss: 0.018901\n",
      "Train Epoch: 173 [4100/6658 (62%)]\tLoss: 0.159076\n",
      "Train Epoch: 173 [4200/6658 (63%)]\tLoss: 0.000334\n",
      "Train Epoch: 173 [4300/6658 (65%)]\tLoss: 1.677617\n",
      "Train Epoch: 173 [4400/6658 (66%)]\tLoss: 0.812496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 173 [4500/6658 (68%)]\tLoss: 0.000045\n",
      "Train Epoch: 173 [4600/6658 (69%)]\tLoss: 0.157399\n",
      "Train Epoch: 173 [4700/6658 (71%)]\tLoss: 0.000680\n",
      "Train Epoch: 173 [4800/6658 (72%)]\tLoss: 0.601129\n",
      "Train Epoch: 173 [4900/6658 (74%)]\tLoss: 0.082642\n",
      "Train Epoch: 173 [5000/6658 (75%)]\tLoss: 5.168941\n",
      "Train Epoch: 173 [5100/6658 (77%)]\tLoss: 0.766789\n",
      "Train Epoch: 173 [5200/6658 (78%)]\tLoss: 0.130335\n",
      "Train Epoch: 173 [5300/6658 (80%)]\tLoss: 0.295074\n",
      "Train Epoch: 173 [5400/6658 (81%)]\tLoss: 0.001682\n",
      "Train Epoch: 173 [5500/6658 (83%)]\tLoss: 0.304455\n",
      "Train Epoch: 173 [5600/6658 (84%)]\tLoss: 0.640655\n",
      "Train Epoch: 173 [5700/6658 (86%)]\tLoss: 0.001272\n",
      "Train Epoch: 173 [5800/6658 (87%)]\tLoss: 3.794792\n",
      "Train Epoch: 173 [5900/6658 (89%)]\tLoss: 0.673370\n",
      "Train Epoch: 173 [6000/6658 (90%)]\tLoss: 0.006329\n",
      "Train Epoch: 173 [6100/6658 (92%)]\tLoss: 0.172130\n",
      "Train Epoch: 173 [6200/6658 (93%)]\tLoss: 0.259158\n",
      "Train Epoch: 173 [6300/6658 (95%)]\tLoss: 1.658540\n",
      "Train Epoch: 173 [6400/6658 (96%)]\tLoss: 0.000656\n",
      "Train Epoch: 173 [6500/6658 (98%)]\tLoss: 0.022984\n",
      "Train Epoch: 173 [6600/6658 (99%)]\tLoss: 0.139500\n",
      "train loss average =  0.7124343631370452\n",
      "\n",
      "Test set: Average loss: 0.6846\n",
      "\n",
      "EarlyStopping counter: 9 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3607, 6.1267, 5.8466, 5.8516, 6.2616, 6.0437], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 174 [0/6658 (0%)]\tLoss: 1.187967\n",
      "Train Epoch: 174 [100/6658 (2%)]\tLoss: 0.088778\n",
      "Train Epoch: 174 [200/6658 (3%)]\tLoss: 0.033004\n",
      "Train Epoch: 174 [300/6658 (5%)]\tLoss: 0.455505\n",
      "Train Epoch: 174 [400/6658 (6%)]\tLoss: 0.340558\n",
      "Train Epoch: 174 [500/6658 (8%)]\tLoss: 0.237193\n",
      "Train Epoch: 174 [600/6658 (9%)]\tLoss: 0.573140\n",
      "Train Epoch: 174 [700/6658 (11%)]\tLoss: 1.599174\n",
      "Train Epoch: 174 [800/6658 (12%)]\tLoss: 0.952917\n",
      "Train Epoch: 174 [900/6658 (14%)]\tLoss: 0.055774\n",
      "Train Epoch: 174 [1000/6658 (15%)]\tLoss: 0.024558\n",
      "Train Epoch: 174 [1100/6658 (17%)]\tLoss: 0.235634\n",
      "Train Epoch: 174 [1200/6658 (18%)]\tLoss: 0.094376\n",
      "Train Epoch: 174 [1300/6658 (20%)]\tLoss: 0.011811\n",
      "Train Epoch: 174 [1400/6658 (21%)]\tLoss: 0.000935\n",
      "Train Epoch: 174 [1500/6658 (23%)]\tLoss: 0.146491\n",
      "Train Epoch: 174 [1600/6658 (24%)]\tLoss: 0.122262\n",
      "Train Epoch: 174 [1700/6658 (26%)]\tLoss: 0.909975\n",
      "Train Epoch: 174 [1800/6658 (27%)]\tLoss: 0.075607\n",
      "Train Epoch: 174 [1900/6658 (29%)]\tLoss: 0.125682\n",
      "Train Epoch: 174 [2000/6658 (30%)]\tLoss: 0.266779\n",
      "Train Epoch: 174 [2100/6658 (32%)]\tLoss: 0.617385\n",
      "Train Epoch: 174 [2200/6658 (33%)]\tLoss: 0.832231\n",
      "Train Epoch: 174 [2300/6658 (35%)]\tLoss: 0.094314\n",
      "Train Epoch: 174 [2400/6658 (36%)]\tLoss: 0.188583\n",
      "Train Epoch: 174 [2500/6658 (38%)]\tLoss: 1.410001\n",
      "Train Epoch: 174 [2600/6658 (39%)]\tLoss: 0.423025\n",
      "Train Epoch: 174 [2700/6658 (41%)]\tLoss: 3.192235\n",
      "Train Epoch: 174 [2800/6658 (42%)]\tLoss: 0.024917\n",
      "Train Epoch: 174 [2900/6658 (44%)]\tLoss: 0.032241\n",
      "Train Epoch: 174 [3000/6658 (45%)]\tLoss: 0.430410\n",
      "Train Epoch: 174 [3100/6658 (47%)]\tLoss: 0.135320\n",
      "Train Epoch: 174 [3200/6658 (48%)]\tLoss: 0.387421\n",
      "Train Epoch: 174 [3300/6658 (50%)]\tLoss: 0.106351\n",
      "Train Epoch: 174 [3400/6658 (51%)]\tLoss: 0.430672\n",
      "Train Epoch: 174 [3500/6658 (53%)]\tLoss: 0.149872\n",
      "Train Epoch: 174 [3600/6658 (54%)]\tLoss: 2.716652\n",
      "Train Epoch: 174 [3700/6658 (56%)]\tLoss: 0.157076\n",
      "Train Epoch: 174 [3800/6658 (57%)]\tLoss: 0.178673\n",
      "Train Epoch: 174 [3900/6658 (59%)]\tLoss: 0.036686\n",
      "Train Epoch: 174 [4000/6658 (60%)]\tLoss: 0.402256\n",
      "Train Epoch: 174 [4100/6658 (62%)]\tLoss: 0.183297\n",
      "Train Epoch: 174 [4200/6658 (63%)]\tLoss: 0.288539\n",
      "Train Epoch: 174 [4300/6658 (65%)]\tLoss: 0.059912\n",
      "Train Epoch: 174 [4400/6658 (66%)]\tLoss: 0.110519\n",
      "Train Epoch: 174 [4500/6658 (68%)]\tLoss: 0.596690\n",
      "Train Epoch: 174 [4600/6658 (69%)]\tLoss: 0.775546\n",
      "Train Epoch: 174 [4700/6658 (71%)]\tLoss: 0.242085\n",
      "Train Epoch: 174 [4800/6658 (72%)]\tLoss: 0.004296\n",
      "Train Epoch: 174 [4900/6658 (74%)]\tLoss: 1.539316\n",
      "Train Epoch: 174 [5000/6658 (75%)]\tLoss: 0.526154\n",
      "Train Epoch: 174 [5100/6658 (77%)]\tLoss: 0.311906\n",
      "Train Epoch: 174 [5200/6658 (78%)]\tLoss: 0.864789\n",
      "Train Epoch: 174 [5300/6658 (80%)]\tLoss: 0.106103\n",
      "Train Epoch: 174 [5400/6658 (81%)]\tLoss: 0.294219\n",
      "Train Epoch: 174 [5500/6658 (83%)]\tLoss: 0.027602\n",
      "Train Epoch: 174 [5600/6658 (84%)]\tLoss: 0.212146\n",
      "Train Epoch: 174 [5700/6658 (86%)]\tLoss: 0.140218\n",
      "Train Epoch: 174 [5800/6658 (87%)]\tLoss: 1.142102\n",
      "Train Epoch: 174 [5900/6658 (89%)]\tLoss: 0.179643\n",
      "Train Epoch: 174 [6000/6658 (90%)]\tLoss: 0.173889\n",
      "Train Epoch: 174 [6100/6658 (92%)]\tLoss: 2.980948\n",
      "Train Epoch: 174 [6200/6658 (93%)]\tLoss: 0.005866\n",
      "Train Epoch: 174 [6300/6658 (95%)]\tLoss: 0.145544\n",
      "Train Epoch: 174 [6400/6658 (96%)]\tLoss: 1.339280\n",
      "Train Epoch: 174 [6500/6658 (98%)]\tLoss: 0.126746\n",
      "Train Epoch: 174 [6600/6658 (99%)]\tLoss: 0.121279\n",
      "train loss average =  0.7051409544191569\n",
      "\n",
      "Test set: Average loss: 0.7044\n",
      "\n",
      "EarlyStopping counter: 10 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3627, 6.1265, 5.8464, 5.8509, 6.2634, 6.0436], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 175 [0/6658 (0%)]\tLoss: 0.699084\n",
      "Train Epoch: 175 [100/6658 (2%)]\tLoss: 0.382154\n",
      "Train Epoch: 175 [200/6658 (3%)]\tLoss: 0.052000\n",
      "Train Epoch: 175 [300/6658 (5%)]\tLoss: 0.127828\n",
      "Train Epoch: 175 [400/6658 (6%)]\tLoss: 10.483053\n",
      "Train Epoch: 175 [500/6658 (8%)]\tLoss: 0.058172\n",
      "Train Epoch: 175 [600/6658 (9%)]\tLoss: 0.114092\n",
      "Train Epoch: 175 [700/6658 (11%)]\tLoss: 0.298520\n",
      "Train Epoch: 175 [800/6658 (12%)]\tLoss: 0.188454\n",
      "Train Epoch: 175 [900/6658 (14%)]\tLoss: 0.935079\n",
      "Train Epoch: 175 [1000/6658 (15%)]\tLoss: 0.804570\n",
      "Train Epoch: 175 [1100/6658 (17%)]\tLoss: 1.245492\n",
      "Train Epoch: 175 [1200/6658 (18%)]\tLoss: 0.360072\n",
      "Train Epoch: 175 [1300/6658 (20%)]\tLoss: 1.225767\n",
      "Train Epoch: 175 [1400/6658 (21%)]\tLoss: 0.580401\n",
      "Train Epoch: 175 [1500/6658 (23%)]\tLoss: 0.143501\n",
      "Train Epoch: 175 [1600/6658 (24%)]\tLoss: 0.189033\n",
      "Train Epoch: 175 [1700/6658 (26%)]\tLoss: 0.126035\n",
      "Train Epoch: 175 [1800/6658 (27%)]\tLoss: 0.714447\n",
      "Train Epoch: 175 [1900/6658 (29%)]\tLoss: 0.240162\n",
      "Train Epoch: 175 [2000/6658 (30%)]\tLoss: 0.143150\n",
      "Train Epoch: 175 [2100/6658 (32%)]\tLoss: 0.200450\n",
      "Train Epoch: 175 [2200/6658 (33%)]\tLoss: 0.024569\n",
      "Train Epoch: 175 [2300/6658 (35%)]\tLoss: 0.001009\n",
      "Train Epoch: 175 [2400/6658 (36%)]\tLoss: 0.004490\n",
      "Train Epoch: 175 [2500/6658 (38%)]\tLoss: 0.151779\n",
      "Train Epoch: 175 [2600/6658 (39%)]\tLoss: 0.161461\n",
      "Train Epoch: 175 [2700/6658 (41%)]\tLoss: 0.369277\n",
      "Train Epoch: 175 [2800/6658 (42%)]\tLoss: 1.229933\n",
      "Train Epoch: 175 [2900/6658 (44%)]\tLoss: 0.132921\n",
      "Train Epoch: 175 [3000/6658 (45%)]\tLoss: 0.086974\n",
      "Train Epoch: 175 [3100/6658 (47%)]\tLoss: 0.071559\n",
      "Train Epoch: 175 [3200/6658 (48%)]\tLoss: 0.009662\n",
      "Train Epoch: 175 [3300/6658 (50%)]\tLoss: 4.336850\n",
      "Train Epoch: 175 [3400/6658 (51%)]\tLoss: 0.129469\n",
      "Train Epoch: 175 [3500/6658 (53%)]\tLoss: 0.050612\n",
      "Train Epoch: 175 [3600/6658 (54%)]\tLoss: 0.078792\n",
      "Train Epoch: 175 [3700/6658 (56%)]\tLoss: 0.561557\n",
      "Train Epoch: 175 [3800/6658 (57%)]\tLoss: 0.579432\n",
      "Train Epoch: 175 [3900/6658 (59%)]\tLoss: 0.400092\n",
      "Train Epoch: 175 [4000/6658 (60%)]\tLoss: 0.054666\n",
      "Train Epoch: 175 [4100/6658 (62%)]\tLoss: 0.031983\n",
      "Train Epoch: 175 [4200/6658 (63%)]\tLoss: 0.030362\n",
      "Train Epoch: 175 [4300/6658 (65%)]\tLoss: 0.091349\n",
      "Train Epoch: 175 [4400/6658 (66%)]\tLoss: 0.189345\n",
      "Train Epoch: 175 [4500/6658 (68%)]\tLoss: 0.302336\n",
      "Train Epoch: 175 [4600/6658 (69%)]\tLoss: 0.369255\n",
      "Train Epoch: 175 [4700/6658 (71%)]\tLoss: 1.158185\n",
      "Train Epoch: 175 [4800/6658 (72%)]\tLoss: 0.554941\n",
      "Train Epoch: 175 [4900/6658 (74%)]\tLoss: 0.016456\n",
      "Train Epoch: 175 [5000/6658 (75%)]\tLoss: 0.658329\n",
      "Train Epoch: 175 [5100/6658 (77%)]\tLoss: 0.446645\n",
      "Train Epoch: 175 [5200/6658 (78%)]\tLoss: 0.446854\n",
      "Train Epoch: 175 [5300/6658 (80%)]\tLoss: 0.319839\n",
      "Train Epoch: 175 [5400/6658 (81%)]\tLoss: 1.311248\n",
      "Train Epoch: 175 [5500/6658 (83%)]\tLoss: 0.071940\n",
      "Train Epoch: 175 [5600/6658 (84%)]\tLoss: 0.304639\n",
      "Train Epoch: 175 [5700/6658 (86%)]\tLoss: 0.085981\n",
      "Train Epoch: 175 [5800/6658 (87%)]\tLoss: 0.019200\n",
      "Train Epoch: 175 [5900/6658 (89%)]\tLoss: 0.338380\n",
      "Train Epoch: 175 [6000/6658 (90%)]\tLoss: 1.450075\n",
      "Train Epoch: 175 [6100/6658 (92%)]\tLoss: 0.013212\n",
      "Train Epoch: 175 [6200/6658 (93%)]\tLoss: 0.000011\n",
      "Train Epoch: 175 [6300/6658 (95%)]\tLoss: 0.437162\n",
      "Train Epoch: 175 [6400/6658 (96%)]\tLoss: 0.573405\n",
      "Train Epoch: 175 [6500/6658 (98%)]\tLoss: 0.421318\n",
      "Train Epoch: 175 [6600/6658 (99%)]\tLoss: 4.309725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss average =  0.7043680806819718\n",
      "\n",
      "Test set: Average loss: 0.6762\n",
      "\n",
      "EarlyStopping counter: 11 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3649, 6.1271, 5.8460, 5.8502, 6.2651, 6.0434], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 176 [0/6658 (0%)]\tLoss: 0.048104\n",
      "Train Epoch: 176 [100/6658 (2%)]\tLoss: 0.003250\n",
      "Train Epoch: 176 [200/6658 (3%)]\tLoss: 0.130702\n",
      "Train Epoch: 176 [300/6658 (5%)]\tLoss: 0.131383\n",
      "Train Epoch: 176 [400/6658 (6%)]\tLoss: 0.042678\n",
      "Train Epoch: 176 [500/6658 (8%)]\tLoss: 0.577180\n",
      "Train Epoch: 176 [600/6658 (9%)]\tLoss: 0.056799\n",
      "Train Epoch: 176 [700/6658 (11%)]\tLoss: 1.015686\n",
      "Train Epoch: 176 [800/6658 (12%)]\tLoss: 0.162998\n",
      "Train Epoch: 176 [900/6658 (14%)]\tLoss: 0.050259\n",
      "Train Epoch: 176 [1000/6658 (15%)]\tLoss: 0.326469\n",
      "Train Epoch: 176 [1100/6658 (17%)]\tLoss: 0.967263\n",
      "Train Epoch: 176 [1200/6658 (18%)]\tLoss: 0.164849\n",
      "Train Epoch: 176 [1300/6658 (20%)]\tLoss: 0.610920\n",
      "Train Epoch: 176 [1400/6658 (21%)]\tLoss: 8.022722\n",
      "Train Epoch: 176 [1500/6658 (23%)]\tLoss: 0.364690\n",
      "Train Epoch: 176 [1600/6658 (24%)]\tLoss: 0.000957\n",
      "Train Epoch: 176 [1700/6658 (26%)]\tLoss: 0.391976\n",
      "Train Epoch: 176 [1800/6658 (27%)]\tLoss: 0.749628\n",
      "Train Epoch: 176 [1900/6658 (29%)]\tLoss: 0.652921\n",
      "Train Epoch: 176 [2000/6658 (30%)]\tLoss: 0.047931\n",
      "Train Epoch: 176 [2100/6658 (32%)]\tLoss: 1.081135\n",
      "Train Epoch: 176 [2200/6658 (33%)]\tLoss: 0.052108\n",
      "Train Epoch: 176 [2300/6658 (35%)]\tLoss: 0.069382\n",
      "Train Epoch: 176 [2400/6658 (36%)]\tLoss: 0.011816\n",
      "Train Epoch: 176 [2500/6658 (38%)]\tLoss: 0.476595\n",
      "Train Epoch: 176 [2600/6658 (39%)]\tLoss: 0.049752\n",
      "Train Epoch: 176 [2700/6658 (41%)]\tLoss: 0.033943\n",
      "Train Epoch: 176 [2800/6658 (42%)]\tLoss: 0.040809\n",
      "Train Epoch: 176 [2900/6658 (44%)]\tLoss: 0.171828\n",
      "Train Epoch: 176 [3000/6658 (45%)]\tLoss: 0.349314\n",
      "Train Epoch: 176 [3100/6658 (47%)]\tLoss: 0.409659\n",
      "Train Epoch: 176 [3200/6658 (48%)]\tLoss: 4.843402\n",
      "Train Epoch: 176 [3300/6658 (50%)]\tLoss: 0.000169\n",
      "Train Epoch: 176 [3400/6658 (51%)]\tLoss: 6.768382\n",
      "Train Epoch: 176 [3500/6658 (53%)]\tLoss: 0.495857\n",
      "Train Epoch: 176 [3600/6658 (54%)]\tLoss: 0.351015\n",
      "Train Epoch: 176 [3700/6658 (56%)]\tLoss: 0.239275\n",
      "Train Epoch: 176 [3800/6658 (57%)]\tLoss: 0.126933\n",
      "Train Epoch: 176 [3900/6658 (59%)]\tLoss: 0.628711\n",
      "Train Epoch: 176 [4000/6658 (60%)]\tLoss: 0.022001\n",
      "Train Epoch: 176 [4100/6658 (62%)]\tLoss: 0.138230\n",
      "Train Epoch: 176 [4200/6658 (63%)]\tLoss: 0.147610\n",
      "Train Epoch: 176 [4300/6658 (65%)]\tLoss: 0.877707\n",
      "Train Epoch: 176 [4400/6658 (66%)]\tLoss: 1.606492\n",
      "Train Epoch: 176 [4500/6658 (68%)]\tLoss: 0.150849\n",
      "Train Epoch: 176 [4600/6658 (69%)]\tLoss: 1.935585\n",
      "Train Epoch: 176 [4700/6658 (71%)]\tLoss: 1.356679\n",
      "Train Epoch: 176 [4800/6658 (72%)]\tLoss: 0.132734\n",
      "Train Epoch: 176 [4900/6658 (74%)]\tLoss: 0.135584\n",
      "Train Epoch: 176 [5000/6658 (75%)]\tLoss: 0.024933\n",
      "Train Epoch: 176 [5100/6658 (77%)]\tLoss: 0.076924\n",
      "Train Epoch: 176 [5200/6658 (78%)]\tLoss: 1.367617\n",
      "Train Epoch: 176 [5300/6658 (80%)]\tLoss: 0.004879\n",
      "Train Epoch: 176 [5400/6658 (81%)]\tLoss: 1.405201\n",
      "Train Epoch: 176 [5500/6658 (83%)]\tLoss: 1.945843\n",
      "Train Epoch: 176 [5600/6658 (84%)]\tLoss: 0.000468\n",
      "Train Epoch: 176 [5700/6658 (86%)]\tLoss: 0.934833\n",
      "Train Epoch: 176 [5800/6658 (87%)]\tLoss: 0.772643\n",
      "Train Epoch: 176 [5900/6658 (89%)]\tLoss: 0.088236\n",
      "Train Epoch: 176 [6000/6658 (90%)]\tLoss: 0.039708\n",
      "Train Epoch: 176 [6100/6658 (92%)]\tLoss: 1.494834\n",
      "Train Epoch: 176 [6200/6658 (93%)]\tLoss: 0.618930\n",
      "Train Epoch: 176 [6300/6658 (95%)]\tLoss: 0.474542\n",
      "Train Epoch: 176 [6400/6658 (96%)]\tLoss: 0.069975\n",
      "Train Epoch: 176 [6500/6658 (98%)]\tLoss: 0.445599\n",
      "Train Epoch: 176 [6600/6658 (99%)]\tLoss: 1.479592\n",
      "train loss average =  0.706313768395152\n",
      "\n",
      "Test set: Average loss: 0.6966\n",
      "\n",
      "EarlyStopping counter: 12 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3672, 6.1272, 5.8452, 5.8493, 6.2665, 6.0435], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 177 [0/6658 (0%)]\tLoss: 2.945299\n",
      "Train Epoch: 177 [100/6658 (2%)]\tLoss: 1.053472\n",
      "Train Epoch: 177 [200/6658 (3%)]\tLoss: 3.072131\n",
      "Train Epoch: 177 [300/6658 (5%)]\tLoss: 0.021349\n",
      "Train Epoch: 177 [400/6658 (6%)]\tLoss: 0.474575\n",
      "Train Epoch: 177 [500/6658 (8%)]\tLoss: 0.053797\n",
      "Train Epoch: 177 [600/6658 (9%)]\tLoss: 1.557922\n",
      "Train Epoch: 177 [700/6658 (11%)]\tLoss: 0.395392\n",
      "Train Epoch: 177 [800/6658 (12%)]\tLoss: 0.020871\n",
      "Train Epoch: 177 [900/6658 (14%)]\tLoss: 0.000011\n",
      "Train Epoch: 177 [1000/6658 (15%)]\tLoss: 0.399960\n",
      "Train Epoch: 177 [1100/6658 (17%)]\tLoss: 0.019149\n",
      "Train Epoch: 177 [1200/6658 (18%)]\tLoss: 1.310248\n",
      "Train Epoch: 177 [1300/6658 (20%)]\tLoss: 0.026681\n",
      "Train Epoch: 177 [1400/6658 (21%)]\tLoss: 0.035600\n",
      "Train Epoch: 177 [1500/6658 (23%)]\tLoss: 0.004879\n",
      "Train Epoch: 177 [1600/6658 (24%)]\tLoss: 0.054229\n",
      "Train Epoch: 177 [1700/6658 (26%)]\tLoss: 0.069012\n",
      "Train Epoch: 177 [1800/6658 (27%)]\tLoss: 0.028761\n",
      "Train Epoch: 177 [1900/6658 (29%)]\tLoss: 0.066328\n",
      "Train Epoch: 177 [2000/6658 (30%)]\tLoss: 0.156636\n",
      "Train Epoch: 177 [2100/6658 (32%)]\tLoss: 0.019039\n",
      "Train Epoch: 177 [2200/6658 (33%)]\tLoss: 1.338767\n",
      "Train Epoch: 177 [2300/6658 (35%)]\tLoss: 0.139772\n",
      "Train Epoch: 177 [2400/6658 (36%)]\tLoss: 0.334326\n",
      "Train Epoch: 177 [2500/6658 (38%)]\tLoss: 0.138561\n",
      "Train Epoch: 177 [2600/6658 (39%)]\tLoss: 0.005190\n",
      "Train Epoch: 177 [2700/6658 (41%)]\tLoss: 1.160439\n",
      "Train Epoch: 177 [2800/6658 (42%)]\tLoss: 0.356262\n",
      "Train Epoch: 177 [2900/6658 (44%)]\tLoss: 1.150421\n",
      "Train Epoch: 177 [3000/6658 (45%)]\tLoss: 0.919037\n",
      "Train Epoch: 177 [3100/6658 (47%)]\tLoss: 0.663494\n",
      "Train Epoch: 177 [3200/6658 (48%)]\tLoss: 0.890142\n",
      "Train Epoch: 177 [3300/6658 (50%)]\tLoss: 1.243909\n",
      "Train Epoch: 177 [3400/6658 (51%)]\tLoss: 0.078074\n",
      "Train Epoch: 177 [3500/6658 (53%)]\tLoss: 0.148760\n",
      "Train Epoch: 177 [3600/6658 (54%)]\tLoss: 0.191226\n",
      "Train Epoch: 177 [3700/6658 (56%)]\tLoss: 0.755412\n",
      "Train Epoch: 177 [3800/6658 (57%)]\tLoss: 0.240811\n",
      "Train Epoch: 177 [3900/6658 (59%)]\tLoss: 0.348106\n",
      "Train Epoch: 177 [4000/6658 (60%)]\tLoss: 0.566015\n",
      "Train Epoch: 177 [4100/6658 (62%)]\tLoss: 0.256953\n",
      "Train Epoch: 177 [4200/6658 (63%)]\tLoss: 1.477548\n",
      "Train Epoch: 177 [4300/6658 (65%)]\tLoss: 0.000040\n",
      "Train Epoch: 177 [4400/6658 (66%)]\tLoss: 0.397645\n",
      "Train Epoch: 177 [4500/6658 (68%)]\tLoss: 0.885337\n",
      "Train Epoch: 177 [4600/6658 (69%)]\tLoss: 3.002573\n",
      "Train Epoch: 177 [4700/6658 (71%)]\tLoss: 1.127033\n",
      "Train Epoch: 177 [4800/6658 (72%)]\tLoss: 0.760046\n",
      "Train Epoch: 177 [4900/6658 (74%)]\tLoss: 0.047210\n",
      "Train Epoch: 177 [5000/6658 (75%)]\tLoss: 0.376708\n",
      "Train Epoch: 177 [5100/6658 (77%)]\tLoss: 0.001298\n",
      "Train Epoch: 177 [5200/6658 (78%)]\tLoss: 0.237618\n",
      "Train Epoch: 177 [5300/6658 (80%)]\tLoss: 0.815461\n",
      "Train Epoch: 177 [5400/6658 (81%)]\tLoss: 0.154441\n",
      "Train Epoch: 177 [5500/6658 (83%)]\tLoss: 0.082543\n",
      "Train Epoch: 177 [5600/6658 (84%)]\tLoss: 0.056763\n",
      "Train Epoch: 177 [5700/6658 (86%)]\tLoss: 0.479763\n",
      "Train Epoch: 177 [5800/6658 (87%)]\tLoss: 0.203152\n",
      "Train Epoch: 177 [5900/6658 (89%)]\tLoss: 0.130100\n",
      "Train Epoch: 177 [6000/6658 (90%)]\tLoss: 2.363584\n",
      "Train Epoch: 177 [6100/6658 (92%)]\tLoss: 0.003273\n",
      "Train Epoch: 177 [6200/6658 (93%)]\tLoss: 0.087579\n",
      "Train Epoch: 177 [6300/6658 (95%)]\tLoss: 0.111258\n",
      "Train Epoch: 177 [6400/6658 (96%)]\tLoss: 0.022876\n",
      "Train Epoch: 177 [6500/6658 (98%)]\tLoss: 0.173956\n",
      "Train Epoch: 177 [6600/6658 (99%)]\tLoss: 0.101438\n",
      "train loss average =  0.7001549847687237\n",
      "\n",
      "Test set: Average loss: 0.7064\n",
      "\n",
      "EarlyStopping counter: 13 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3694, 6.1279, 5.8451, 5.8492, 6.2674, 6.0432], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 178 [0/6658 (0%)]\tLoss: 0.037400\n",
      "Train Epoch: 178 [100/6658 (2%)]\tLoss: 0.460193\n",
      "Train Epoch: 178 [200/6658 (3%)]\tLoss: 0.014758\n",
      "Train Epoch: 178 [300/6658 (5%)]\tLoss: 0.566521\n",
      "Train Epoch: 178 [400/6658 (6%)]\tLoss: 0.092760\n",
      "Train Epoch: 178 [500/6658 (8%)]\tLoss: 1.042120\n",
      "Train Epoch: 178 [600/6658 (9%)]\tLoss: 3.007342\n",
      "Train Epoch: 178 [700/6658 (11%)]\tLoss: 0.316600\n",
      "Train Epoch: 178 [800/6658 (12%)]\tLoss: 6.138467\n",
      "Train Epoch: 178 [900/6658 (14%)]\tLoss: 0.315084\n",
      "Train Epoch: 178 [1000/6658 (15%)]\tLoss: 0.231878\n",
      "Train Epoch: 178 [1100/6658 (17%)]\tLoss: 0.498565\n",
      "Train Epoch: 178 [1200/6658 (18%)]\tLoss: 0.100579\n",
      "Train Epoch: 178 [1300/6658 (20%)]\tLoss: 1.915135\n",
      "Train Epoch: 178 [1400/6658 (21%)]\tLoss: 0.328781\n",
      "Train Epoch: 178 [1500/6658 (23%)]\tLoss: 0.000165\n",
      "Train Epoch: 178 [1600/6658 (24%)]\tLoss: 0.330296\n",
      "Train Epoch: 178 [1700/6658 (26%)]\tLoss: 0.023942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 178 [1800/6658 (27%)]\tLoss: 0.514465\n",
      "Train Epoch: 178 [1900/6658 (29%)]\tLoss: 0.291836\n",
      "Train Epoch: 178 [2000/6658 (30%)]\tLoss: 0.034412\n",
      "Train Epoch: 178 [2100/6658 (32%)]\tLoss: 2.025322\n",
      "Train Epoch: 178 [2200/6658 (33%)]\tLoss: 0.368790\n",
      "Train Epoch: 178 [2300/6658 (35%)]\tLoss: 0.194672\n",
      "Train Epoch: 178 [2400/6658 (36%)]\tLoss: 0.834292\n",
      "Train Epoch: 178 [2500/6658 (38%)]\tLoss: 0.400145\n",
      "Train Epoch: 178 [2600/6658 (39%)]\tLoss: 0.753471\n",
      "Train Epoch: 178 [2700/6658 (41%)]\tLoss: 1.590002\n",
      "Train Epoch: 178 [2800/6658 (42%)]\tLoss: 0.197199\n",
      "Train Epoch: 178 [2900/6658 (44%)]\tLoss: 0.028814\n",
      "Train Epoch: 178 [3000/6658 (45%)]\tLoss: 0.236494\n",
      "Train Epoch: 178 [3100/6658 (47%)]\tLoss: 0.017908\n",
      "Train Epoch: 178 [3200/6658 (48%)]\tLoss: 1.047637\n",
      "Train Epoch: 178 [3300/6658 (50%)]\tLoss: 0.096541\n",
      "Train Epoch: 178 [3400/6658 (51%)]\tLoss: 0.416916\n",
      "Train Epoch: 178 [3500/6658 (53%)]\tLoss: 0.474474\n",
      "Train Epoch: 178 [3600/6658 (54%)]\tLoss: 0.048827\n",
      "Train Epoch: 178 [3700/6658 (56%)]\tLoss: 1.293961\n",
      "Train Epoch: 178 [3800/6658 (57%)]\tLoss: 0.025871\n",
      "Train Epoch: 178 [3900/6658 (59%)]\tLoss: 1.310210\n",
      "Train Epoch: 178 [4000/6658 (60%)]\tLoss: 0.098112\n",
      "Train Epoch: 178 [4100/6658 (62%)]\tLoss: 0.146178\n",
      "Train Epoch: 178 [4200/6658 (63%)]\tLoss: 2.920224\n",
      "Train Epoch: 178 [4300/6658 (65%)]\tLoss: 0.023822\n",
      "Train Epoch: 178 [4400/6658 (66%)]\tLoss: 0.548235\n",
      "Train Epoch: 178 [4500/6658 (68%)]\tLoss: 1.479349\n",
      "Train Epoch: 178 [4600/6658 (69%)]\tLoss: 0.631323\n",
      "Train Epoch: 178 [4700/6658 (71%)]\tLoss: 0.262891\n",
      "Train Epoch: 178 [4800/6658 (72%)]\tLoss: 1.101174\n",
      "Train Epoch: 178 [4900/6658 (74%)]\tLoss: 1.509373\n",
      "Train Epoch: 178 [5000/6658 (75%)]\tLoss: 0.360605\n",
      "Train Epoch: 178 [5100/6658 (77%)]\tLoss: 9.825386\n",
      "Train Epoch: 178 [5200/6658 (78%)]\tLoss: 0.006409\n",
      "Train Epoch: 178 [5300/6658 (80%)]\tLoss: 3.201748\n",
      "Train Epoch: 178 [5400/6658 (81%)]\tLoss: 0.717146\n",
      "Train Epoch: 178 [5500/6658 (83%)]\tLoss: 0.013148\n",
      "Train Epoch: 178 [5600/6658 (84%)]\tLoss: 0.456665\n",
      "Train Epoch: 178 [5700/6658 (86%)]\tLoss: 0.135210\n",
      "Train Epoch: 178 [5800/6658 (87%)]\tLoss: 7.429201\n",
      "Train Epoch: 178 [5900/6658 (89%)]\tLoss: 0.921682\n",
      "Train Epoch: 178 [6000/6658 (90%)]\tLoss: 0.240661\n",
      "Train Epoch: 178 [6100/6658 (92%)]\tLoss: 0.005196\n",
      "Train Epoch: 178 [6200/6658 (93%)]\tLoss: 0.016911\n",
      "Train Epoch: 178 [6300/6658 (95%)]\tLoss: 0.180410\n",
      "Train Epoch: 178 [6400/6658 (96%)]\tLoss: 5.982180\n",
      "Train Epoch: 178 [6500/6658 (98%)]\tLoss: 0.467897\n",
      "Train Epoch: 178 [6600/6658 (99%)]\tLoss: 0.282809\n",
      "train loss average =  0.7081272985978988\n",
      "\n",
      "Test set: Average loss: 0.7034\n",
      "\n",
      "EarlyStopping counter: 14 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3713, 6.1289, 5.8439, 5.8486, 6.2691, 6.0431], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 179 [0/6658 (0%)]\tLoss: 0.157731\n",
      "Train Epoch: 179 [100/6658 (2%)]\tLoss: 0.670409\n",
      "Train Epoch: 179 [200/6658 (3%)]\tLoss: 0.914233\n",
      "Train Epoch: 179 [300/6658 (5%)]\tLoss: 0.313747\n",
      "Train Epoch: 179 [400/6658 (6%)]\tLoss: 0.717261\n",
      "Train Epoch: 179 [500/6658 (8%)]\tLoss: 0.586509\n",
      "Train Epoch: 179 [600/6658 (9%)]\tLoss: 5.011609\n",
      "Train Epoch: 179 [700/6658 (11%)]\tLoss: 0.321982\n",
      "Train Epoch: 179 [800/6658 (12%)]\tLoss: 3.362348\n",
      "Train Epoch: 179 [900/6658 (14%)]\tLoss: 0.137566\n",
      "Train Epoch: 179 [1000/6658 (15%)]\tLoss: 0.625227\n",
      "Train Epoch: 179 [1100/6658 (17%)]\tLoss: 0.029040\n",
      "Train Epoch: 179 [1200/6658 (18%)]\tLoss: 0.001795\n",
      "Train Epoch: 179 [1300/6658 (20%)]\tLoss: 2.612153\n",
      "Train Epoch: 179 [1400/6658 (21%)]\tLoss: 0.006066\n",
      "Train Epoch: 179 [1500/6658 (23%)]\tLoss: 3.270591\n",
      "Train Epoch: 179 [1600/6658 (24%)]\tLoss: 0.195977\n",
      "Train Epoch: 179 [1700/6658 (26%)]\tLoss: 0.009725\n",
      "Train Epoch: 179 [1800/6658 (27%)]\tLoss: 0.014840\n",
      "Train Epoch: 179 [1900/6658 (29%)]\tLoss: 1.420626\n",
      "Train Epoch: 179 [2000/6658 (30%)]\tLoss: 0.027272\n",
      "Train Epoch: 179 [2100/6658 (32%)]\tLoss: 0.049065\n",
      "Train Epoch: 179 [2200/6658 (33%)]\tLoss: 0.000000\n",
      "Train Epoch: 179 [2300/6658 (35%)]\tLoss: 0.776090\n",
      "Train Epoch: 179 [2400/6658 (36%)]\tLoss: 0.095594\n",
      "Train Epoch: 179 [2500/6658 (38%)]\tLoss: 0.412060\n",
      "Train Epoch: 179 [2600/6658 (39%)]\tLoss: 0.352916\n",
      "Train Epoch: 179 [2700/6658 (41%)]\tLoss: 0.673318\n",
      "Train Epoch: 179 [2800/6658 (42%)]\tLoss: 0.033942\n",
      "Train Epoch: 179 [2900/6658 (44%)]\tLoss: 0.074444\n",
      "Train Epoch: 179 [3000/6658 (45%)]\tLoss: 9.461939\n",
      "Train Epoch: 179 [3100/6658 (47%)]\tLoss: 1.097264\n",
      "Train Epoch: 179 [3200/6658 (48%)]\tLoss: 0.002416\n",
      "Train Epoch: 179 [3300/6658 (50%)]\tLoss: 0.005286\n",
      "Train Epoch: 179 [3400/6658 (51%)]\tLoss: 0.010479\n",
      "Train Epoch: 179 [3500/6658 (53%)]\tLoss: 3.183905\n",
      "Train Epoch: 179 [3600/6658 (54%)]\tLoss: 0.652004\n",
      "Train Epoch: 179 [3700/6658 (56%)]\tLoss: 0.179147\n",
      "Train Epoch: 179 [3800/6658 (57%)]\tLoss: 0.011943\n",
      "Train Epoch: 179 [3900/6658 (59%)]\tLoss: 0.942468\n",
      "Train Epoch: 179 [4000/6658 (60%)]\tLoss: 0.040226\n",
      "Train Epoch: 179 [4100/6658 (62%)]\tLoss: 0.875360\n",
      "Train Epoch: 179 [4200/6658 (63%)]\tLoss: 0.125438\n",
      "Train Epoch: 179 [4300/6658 (65%)]\tLoss: 0.713635\n",
      "Train Epoch: 179 [4400/6658 (66%)]\tLoss: 0.856068\n",
      "Train Epoch: 179 [4500/6658 (68%)]\tLoss: 0.229193\n",
      "Train Epoch: 179 [4600/6658 (69%)]\tLoss: 0.007448\n",
      "Train Epoch: 179 [4700/6658 (71%)]\tLoss: 1.569427\n",
      "Train Epoch: 179 [4800/6658 (72%)]\tLoss: 0.569410\n",
      "Train Epoch: 179 [4900/6658 (74%)]\tLoss: 0.076416\n",
      "Train Epoch: 179 [5000/6658 (75%)]\tLoss: 3.588959\n",
      "Train Epoch: 179 [5100/6658 (77%)]\tLoss: 5.449005\n",
      "Train Epoch: 179 [5200/6658 (78%)]\tLoss: 0.020468\n",
      "Train Epoch: 179 [5300/6658 (80%)]\tLoss: 1.398274\n",
      "Train Epoch: 179 [5400/6658 (81%)]\tLoss: 0.000000\n",
      "Train Epoch: 179 [5500/6658 (83%)]\tLoss: 0.896114\n",
      "Train Epoch: 179 [5600/6658 (84%)]\tLoss: 0.295240\n",
      "Train Epoch: 179 [5700/6658 (86%)]\tLoss: 0.000000\n",
      "Train Epoch: 179 [5800/6658 (87%)]\tLoss: 0.432374\n",
      "Train Epoch: 179 [5900/6658 (89%)]\tLoss: 1.146445\n",
      "Train Epoch: 179 [6000/6658 (90%)]\tLoss: 0.695533\n",
      "Train Epoch: 179 [6100/6658 (92%)]\tLoss: 0.000432\n",
      "Train Epoch: 179 [6200/6658 (93%)]\tLoss: 0.683909\n",
      "Train Epoch: 179 [6300/6658 (95%)]\tLoss: 0.090955\n",
      "Train Epoch: 179 [6400/6658 (96%)]\tLoss: 0.630460\n",
      "Train Epoch: 179 [6500/6658 (98%)]\tLoss: 0.065127\n",
      "Train Epoch: 179 [6600/6658 (99%)]\tLoss: 0.121693\n",
      "train loss average =  0.7099243873368695\n",
      "\n",
      "Test set: Average loss: 0.7033\n",
      "\n",
      "EarlyStopping counter: 15 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3734, 6.1305, 5.8445, 5.8480, 6.2702, 6.0433], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 180 [0/6658 (0%)]\tLoss: 0.744325\n",
      "Train Epoch: 180 [100/6658 (2%)]\tLoss: 6.966674\n",
      "Train Epoch: 180 [200/6658 (3%)]\tLoss: 0.013628\n",
      "Train Epoch: 180 [300/6658 (5%)]\tLoss: 0.221818\n",
      "Train Epoch: 180 [400/6658 (6%)]\tLoss: 0.515098\n",
      "Train Epoch: 180 [500/6658 (8%)]\tLoss: 0.690920\n",
      "Train Epoch: 180 [600/6658 (9%)]\tLoss: 0.002799\n",
      "Train Epoch: 180 [700/6658 (11%)]\tLoss: 1.221402\n",
      "Train Epoch: 180 [800/6658 (12%)]\tLoss: 2.020647\n",
      "Train Epoch: 180 [900/6658 (14%)]\tLoss: 0.237812\n",
      "Train Epoch: 180 [1000/6658 (15%)]\tLoss: 0.200940\n",
      "Train Epoch: 180 [1100/6658 (17%)]\tLoss: 0.227987\n",
      "Train Epoch: 180 [1200/6658 (18%)]\tLoss: 0.146261\n",
      "Train Epoch: 180 [1300/6658 (20%)]\tLoss: 0.001774\n",
      "Train Epoch: 180 [1400/6658 (21%)]\tLoss: 3.887144\n",
      "Train Epoch: 180 [1500/6658 (23%)]\tLoss: 0.179629\n",
      "Train Epoch: 180 [1600/6658 (24%)]\tLoss: 0.341032\n",
      "Train Epoch: 180 [1700/6658 (26%)]\tLoss: 0.075981\n",
      "Train Epoch: 180 [1800/6658 (27%)]\tLoss: 0.414706\n",
      "Train Epoch: 180 [1900/6658 (29%)]\tLoss: 0.104923\n",
      "Train Epoch: 180 [2000/6658 (30%)]\tLoss: 0.199970\n",
      "Train Epoch: 180 [2100/6658 (32%)]\tLoss: 3.624787\n",
      "Train Epoch: 180 [2200/6658 (33%)]\tLoss: 0.922480\n",
      "Train Epoch: 180 [2300/6658 (35%)]\tLoss: 1.945527\n",
      "Train Epoch: 180 [2400/6658 (36%)]\tLoss: 2.936579\n",
      "Train Epoch: 180 [2500/6658 (38%)]\tLoss: 0.004942\n",
      "Train Epoch: 180 [2600/6658 (39%)]\tLoss: 0.029289\n",
      "Train Epoch: 180 [2700/6658 (41%)]\tLoss: 0.260909\n",
      "Train Epoch: 180 [2800/6658 (42%)]\tLoss: 0.479845\n",
      "Train Epoch: 180 [2900/6658 (44%)]\tLoss: 0.207051\n",
      "Train Epoch: 180 [3000/6658 (45%)]\tLoss: 0.279349\n",
      "Train Epoch: 180 [3100/6658 (47%)]\tLoss: 0.000038\n",
      "Train Epoch: 180 [3200/6658 (48%)]\tLoss: 0.152628\n",
      "Train Epoch: 180 [3300/6658 (50%)]\tLoss: 0.163960\n",
      "Train Epoch: 180 [3400/6658 (51%)]\tLoss: 0.893318\n",
      "Train Epoch: 180 [3500/6658 (53%)]\tLoss: 1.461026\n",
      "Train Epoch: 180 [3600/6658 (54%)]\tLoss: 0.000039\n",
      "Train Epoch: 180 [3700/6658 (56%)]\tLoss: 0.506099\n",
      "Train Epoch: 180 [3800/6658 (57%)]\tLoss: 0.696429\n",
      "Train Epoch: 180 [3900/6658 (59%)]\tLoss: 0.135107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 180 [4000/6658 (60%)]\tLoss: 0.170158\n",
      "Train Epoch: 180 [4100/6658 (62%)]\tLoss: 0.077828\n",
      "Train Epoch: 180 [4200/6658 (63%)]\tLoss: 0.049926\n",
      "Train Epoch: 180 [4300/6658 (65%)]\tLoss: 0.536670\n",
      "Train Epoch: 180 [4400/6658 (66%)]\tLoss: 1.069355\n",
      "Train Epoch: 180 [4500/6658 (68%)]\tLoss: 0.776519\n",
      "Train Epoch: 180 [4600/6658 (69%)]\tLoss: 0.580295\n",
      "Train Epoch: 180 [4700/6658 (71%)]\tLoss: 5.490266\n",
      "Train Epoch: 180 [4800/6658 (72%)]\tLoss: 0.038047\n",
      "Train Epoch: 180 [4900/6658 (74%)]\tLoss: 0.132084\n",
      "Train Epoch: 180 [5000/6658 (75%)]\tLoss: 0.454639\n",
      "Train Epoch: 180 [5100/6658 (77%)]\tLoss: 0.001200\n",
      "Train Epoch: 180 [5200/6658 (78%)]\tLoss: 0.029198\n",
      "Train Epoch: 180 [5300/6658 (80%)]\tLoss: 0.064849\n",
      "Train Epoch: 180 [5400/6658 (81%)]\tLoss: 0.311452\n",
      "Train Epoch: 180 [5500/6658 (83%)]\tLoss: 0.170415\n",
      "Train Epoch: 180 [5600/6658 (84%)]\tLoss: 0.003541\n",
      "Train Epoch: 180 [5700/6658 (86%)]\tLoss: 0.179954\n",
      "Train Epoch: 180 [5800/6658 (87%)]\tLoss: 1.920414\n",
      "Train Epoch: 180 [5900/6658 (89%)]\tLoss: 0.126517\n",
      "Train Epoch: 180 [6000/6658 (90%)]\tLoss: 0.720623\n",
      "Train Epoch: 180 [6100/6658 (92%)]\tLoss: 0.070238\n",
      "Train Epoch: 180 [6200/6658 (93%)]\tLoss: 0.020859\n",
      "Train Epoch: 180 [6300/6658 (95%)]\tLoss: 0.004081\n",
      "Train Epoch: 180 [6400/6658 (96%)]\tLoss: 0.300157\n",
      "Train Epoch: 180 [6500/6658 (98%)]\tLoss: 0.001596\n",
      "Train Epoch: 180 [6600/6658 (99%)]\tLoss: 0.279925\n",
      "train loss average =  0.7062229282824743\n",
      "Train Epoch: 181 [800/6658 (12%)]\tLoss: 0.247880\n",
      "Train Epoch: 181 [900/6658 (14%)]\tLoss: 1.138644\n",
      "Train Epoch: 181 [1000/6658 (15%)]\tLoss: 0.019421\n",
      "Train Epoch: 181 [1100/6658 (17%)]\tLoss: 0.141424\n",
      "Train Epoch: 181 [1200/6658 (18%)]\tLoss: 0.605114\n",
      "Train Epoch: 181 [1300/6658 (20%)]\tLoss: 0.284915\n",
      "Train Epoch: 181 [1400/6658 (21%)]\tLoss: 0.733006\n",
      "Train Epoch: 181 [1500/6658 (23%)]\tLoss: 2.070670\n",
      "Train Epoch: 181 [1600/6658 (24%)]\tLoss: 0.066804\n",
      "Train Epoch: 181 [1700/6658 (26%)]\tLoss: 0.267383\n",
      "Train Epoch: 181 [1800/6658 (27%)]\tLoss: 1.181601\n",
      "Train Epoch: 181 [1900/6658 (29%)]\tLoss: 0.246640\n",
      "Train Epoch: 181 [2000/6658 (30%)]\tLoss: 0.104073\n",
      "Train Epoch: 181 [2100/6658 (32%)]\tLoss: 1.010799\n",
      "Train Epoch: 181 [2200/6658 (33%)]\tLoss: 0.537515\n",
      "Train Epoch: 181 [2300/6658 (35%)]\tLoss: 0.047203\n",
      "Train Epoch: 181 [2400/6658 (36%)]\tLoss: 0.456453\n",
      "Train Epoch: 181 [2500/6658 (38%)]\tLoss: 0.261484\n",
      "Train Epoch: 181 [2600/6658 (39%)]\tLoss: 0.169048\n",
      "Train Epoch: 181 [2700/6658 (41%)]\tLoss: 0.184633\n",
      "Train Epoch: 181 [2800/6658 (42%)]\tLoss: 0.561704\n",
      "Train Epoch: 181 [2900/6658 (44%)]\tLoss: 0.722791\n",
      "Train Epoch: 181 [3000/6658 (45%)]\tLoss: 0.047000\n",
      "Train Epoch: 181 [3100/6658 (47%)]\tLoss: 0.036285\n",
      "Train Epoch: 181 [3200/6658 (48%)]\tLoss: 0.617845\n",
      "Train Epoch: 181 [3300/6658 (50%)]\tLoss: 0.005296\n",
      "Train Epoch: 181 [3400/6658 (51%)]\tLoss: 0.374444\n",
      "Train Epoch: 181 [3500/6658 (53%)]\tLoss: 0.046804\n",
      "Train Epoch: 181 [3600/6658 (54%)]\tLoss: 0.498369\n",
      "Train Epoch: 181 [3700/6658 (56%)]\tLoss: 0.064146\n",
      "Train Epoch: 181 [3800/6658 (57%)]\tLoss: 40.262737\n",
      "Train Epoch: 181 [3900/6658 (59%)]\tLoss: 0.450955\n",
      "Train Epoch: 181 [4000/6658 (60%)]\tLoss: 0.190158\n",
      "Train Epoch: 181 [4100/6658 (62%)]\tLoss: 1.241225\n",
      "Train Epoch: 181 [4200/6658 (63%)]\tLoss: 13.311105\n",
      "Train Epoch: 181 [4300/6658 (65%)]\tLoss: 0.000439\n",
      "Train Epoch: 181 [4400/6658 (66%)]\tLoss: 0.683292\n",
      "Train Epoch: 181 [4500/6658 (68%)]\tLoss: 0.444604\n",
      "Train Epoch: 181 [4600/6658 (69%)]\tLoss: 0.164136\n",
      "Train Epoch: 181 [4700/6658 (71%)]\tLoss: 0.040860\n",
      "Train Epoch: 181 [4800/6658 (72%)]\tLoss: 0.548818\n",
      "Train Epoch: 181 [4900/6658 (74%)]\tLoss: 1.814302\n",
      "Train Epoch: 181 [5000/6658 (75%)]\tLoss: 0.903690\n",
      "Train Epoch: 181 [5100/6658 (77%)]\tLoss: 0.219719\n",
      "Train Epoch: 181 [5200/6658 (78%)]\tLoss: 1.399863\n",
      "Train Epoch: 181 [5300/6658 (80%)]\tLoss: 0.536564\n",
      "Train Epoch: 181 [5400/6658 (81%)]\tLoss: 1.183525\n",
      "Train Epoch: 181 [5500/6658 (83%)]\tLoss: 0.077199\n",
      "Train Epoch: 181 [5600/6658 (84%)]\tLoss: 0.681948\n",
      "Train Epoch: 181 [5700/6658 (86%)]\tLoss: 0.219998\n",
      "Train Epoch: 181 [5800/6658 (87%)]\tLoss: 0.001810\n",
      "Train Epoch: 181 [5900/6658 (89%)]\tLoss: 0.153106\n",
      "Train Epoch: 181 [6000/6658 (90%)]\tLoss: 1.407829\n",
      "Train Epoch: 181 [6100/6658 (92%)]\tLoss: 0.006975\n",
      "Train Epoch: 181 [6200/6658 (93%)]\tLoss: 0.010657\n",
      "Train Epoch: 181 [6300/6658 (95%)]\tLoss: 0.011199\n",
      "Train Epoch: 181 [6400/6658 (96%)]\tLoss: 0.096603\n",
      "Train Epoch: 181 [6500/6658 (98%)]\tLoss: 0.145265\n",
      "Train Epoch: 181 [6600/6658 (99%)]\tLoss: 1.068815\n",
      "train loss average =  0.7084506683207785\n",
      "\n",
      "Test set: Average loss: 0.6807\n",
      "\n",
      "EarlyStopping counter: 17 out of 25\n",
      "Parameter containing:\n",
      "tensor([6.3767, 6.1309, 5.8431, 5.8471, 6.2731, 6.0424], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Train Epoch: 182 [0/6658 (0%)]\tLoss: 0.414842\n",
      "Train Epoch: 182 [100/6658 (2%)]\tLoss: 0.326594\n",
      "Train Epoch: 182 [200/6658 (3%)]\tLoss: 0.140076\n",
      "Train Epoch: 182 [300/6658 (5%)]\tLoss: 0.099771\n",
      "Train Epoch: 182 [400/6658 (6%)]\tLoss: 0.015384\n",
      "Train Epoch: 182 [500/6658 (8%)]\tLoss: 0.011902\n",
      "Train Epoch: 182 [600/6658 (9%)]\tLoss: 0.041176\n",
      "Train Epoch: 182 [700/6658 (11%)]\tLoss: 0.050172\n",
      "Train Epoch: 182 [800/6658 (12%)]\tLoss: 0.219778\n",
      "Train Epoch: 182 [900/6658 (14%)]\tLoss: 1.154575\n",
      "Train Epoch: 182 [1000/6658 (15%)]\tLoss: 0.156505\n",
      "Train Epoch: 182 [1100/6658 (17%)]\tLoss: 0.005972\n",
      "Train Epoch: 182 [1200/6658 (18%)]\tLoss: 2.450680\n",
      "Train Epoch: 182 [1300/6658 (20%)]\tLoss: 0.002470\n",
      "Train Epoch: 182 [1400/6658 (21%)]\tLoss: 0.020647\n",
      "Train Epoch: 182 [1500/6658 (23%)]\tLoss: 0.000006\n",
      "Train Epoch: 182 [1600/6658 (24%)]\tLoss: 0.184792\n",
      "Train Epoch: 182 [1700/6658 (26%)]\tLoss: 0.318043\n",
      "Train Epoch: 182 [1800/6658 (27%)]\tLoss: 0.303554\n",
      "Train Epoch: 182 [1900/6658 (29%)]\tLoss: 0.249187\n",
      "Train Epoch: 182 [2000/6658 (30%)]\tLoss: 0.159937\n",
      "Train Epoch: 182 [2100/6658 (32%)]\tLoss: 0.012106\n",
      "Train Epoch: 182 [2200/6658 (33%)]\tLoss: 1.259730\n",
      "Train Epoch: 182 [2300/6658 (35%)]\tLoss: 3.543823\n",
      "Train Epoch: 182 [2400/6658 (36%)]\tLoss: 0.391330\n",
      "Train Epoch: 182 [2500/6658 (38%)]\tLoss: 0.473015\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=25, verbose=True)\n",
    "for epoch in range(1, 100000):\n",
    "        try:\n",
    "#            train(model, optimizer, train_generator_waves, epoch,device,f_auc=f_train_auc,f_loss=f_train_loss)\n",
    "#            test(model, test_generator_waves,epoch, device,f_auc=f_test_auc,f_loss=f_test_loss)\n",
    "            train(model, optimizer, train_generator, epoch, device)\n",
    "            test_loss = test(model, test_generator,epoch, device)\n",
    "        \n",
    "        except KeyError:\n",
    "            print('Key Error problem')\n",
    "        early_stopping(test_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print(model.sigma)    \n",
    "        torch.save(model.state_dict(), os.path.join('./', 'gauss_draft_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'Cl': 3.6, 'S': 3.7, 'N': 3, 'O': 2.8, 'C': -, 'H': 2.2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 6.0000, 6.0000, 6.0000, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 6.0000, 6.0000, 6.0000, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 6.0000, 6.0000, 6.0000, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 6.0000, 6.0000, 6.0000, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 6.0000, 6.0000, 6.0000, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 6.0000, 6.0000, 6.0000, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 6.0000, 6.0000, 6.0000, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 6.0000, 6.0000, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 6.0000, 6.0000, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 6.0000, 6.0000, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9999, 5.9999, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([6.0001, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 6.0000, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0002, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0001, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9998, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9997], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9998, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 5.9999, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9998], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9998, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9999, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9998, 6.0001, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9998, 6.0001, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9998, 6.0001, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9998, 6.0001, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9998, 6.0001, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9998, 6.0001, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9998, 6.0001, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9998, 6.0001, 6.0001, 6.0000, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9998, 6.0001, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0000, 6.0001, 6.0001, 5.9999], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0000, 6.0001, 6.0001, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0000, 6.0001, 6.0001, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0000, 6.0001, 6.0001, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0000, 6.0001, 6.0001, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0001, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0001, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0001, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0001, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0001, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([6.0000, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 6.0000, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 6.0000, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 6.0000, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 6.0000, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 5.9999, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0000], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9997, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0002, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0003, 6.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0003, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9999, 6.0004, 6.0002], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9999, 6.0004, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9999, 6.0004, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9999, 6.0004, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 5.9999, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0001, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 5.9999, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9997, 6.0000, 6.0001, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0001, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0001, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0001, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0001, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0005, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0001, 6.0000, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0001, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0001, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0001, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0001, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0001, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0001, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0001, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0001, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0001, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0001, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0001, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 6.0000, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 6.0000, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9999, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0003], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9999, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9998, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9997, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0006, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9999, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.9998, 5.9996, 5.9998, 5.9997, 6.0007, 6.0004], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data_inp, target) in enumerate(train_generator):\n",
    "        data_inp = data_inp.to(device)\n",
    "        target = target.to(device)\n",
    "        # set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_inp)\n",
    "#         f = output.sum()\n",
    "#         f.backward()\n",
    "#         print(model.sigma.grad)\n",
    "        mask = (target == target)\n",
    "        output_masked = torch.masked_select(output, mask).type_as(output)\n",
    "        target_masked = torch.masked_select(target, mask).type_as(output)\n",
    "        criterion=nn.MSELoss()\n",
    "        loss = criterion(output_masked, target_masked)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(model.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2010, -0.4394,  0.1859, -0.3289, -0.4254], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sigma = Variable(torch.ones(5).float().to(device),requires_grad=True)\n",
    "fc1 = nn.Sequential(nn.Linear(5, 5),nn.Linear(5,2)).to(device)\n",
    "x=fc1(sigma)\n",
    "f = x.sum()\n",
    "f.backward()\n",
    "print(sigma.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  += sigma[2]*torch.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 70, 70, 70])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[10][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualization as vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAD8CAYAAAD5TVjyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAajUlEQVR4nO3df5CdVZ3n8fcnnSCDIgkG2MgPwZqUg/MDcLoiFlMjPwQjy4BTq7ug60QLK+UUuDgztSPMbKHi/IFrlQxbMmqvZIhbDsigDFk2iqkA5boOmESCECJDjBS0yRAi4C8coNOf/eN5Gu+9/eM+3X3v7XuffF5Vp+59fp7T3eHLOc855zmyTUREHSxa6AJERHRKAlpE1EYCWkTURgJaRNRGAlpE1EYCWkTURgJaRHSNpOMl3SNpp6Qdkq6Y4hxJ+h+Sdkn6vqQ3NRxbI+mxMq1pm1/GoUVEt0haAayw/T1JhwPbgHfafqThnPOBDwPnA28Grrf9ZklHAluBYcDltb9v+9np8ptXDU3SakmPlpH1yvncKyLqx/Ze298rv/8c2Akc23LaRcCXXLgPWFoGwrcDm2w/UwaxTcDqmfJbPNeCShoCbgDOBUaBLZI2NEbeVodpuZdy4lyzjIg2nuNxnvd+zeceqyXvr3juNtgB/FvDrhHbI1OdK+lE4DTg/pZDxwJPNmyPlvum2z+tOQc0YBWwy/busrC3UETaaQPaUk5kLVvnkWVEzGSE4XnfYz9U/q9U8G+222Yq6VXAV4GP2P7Z5NtM4hn2T2s+Tc5K0VPSWklbJW19nqfnkV1E9MzQomqpAklLKILZl21/bYpTRoHjG7aPA/bMsH9a8wlolaKn7RHbw7aHD+OoeWQXET0hwSFD1VLbW0nAjcBO25+Z5rQNwJ+UvZ2nAz+1vRe4CzhP0jJJy4Dzyn3Tmk+Tc9bRMyIGgIDF83oM1+gM4H3AQ5K2l/v+CjgBwPbngY0UPZy7gOeBD5THnpH0SWBLed01tp+ZKbP5BLQtwEpJJwE/Bi4G3jOP+0VEPxCVm5Pt2P42U7fmGs8xcNk0x9YB66rmN+eAZntM0uUUVcAhYJ3tHXO9X0T0kaGO1dB6aj41NGxvpKguRkRdSB2rofXavAJaRNRQB5ucvZaAFhHNJno5B1ACWkRMdjA+Q4uIGhKwOE3OiKgDKTW0iKiRdApERC0sSqdARNRJamgRUQsiz9Aioi4yUyAi6iI1tIiojUx9iojayNSniKiV1NAiohbyDC0iaiPvQ4uIWulQDU3SOuACYJ/t35ni+H8F3ltuLgZOBo4q1xN4HPg5cAAYq7JcXgJaRDTr7NSnm4DPAl+a6qDtTwOfBpD0R8CftSyEcpZded3jBLSImELnFkn5VrliehWXADfPJ7/BbChHRPdMdApUSZ3KUjoMWE2xIPEEA9+UtE3S2ir3SQ0tIlrMqlNguaStDdsjtkfmkOkfAf+vpbl5hu09ko4GNkn6ge1vzXSTBLSIaDa7YRv7qzysr+BiWpqbtveUn/sk3Q6sAmYMaG3DsKR1kvZJerhh35GSNkl6rPxcNqcfISL6z8TUpyqpE9lJRwBvBe5o2PdKSYdPfAfOAx6e+g6/VqVEN1G0bRtdCWy2vRLYXG5HRB1IsGSoWmp7K90M/DPwBkmjki6V9CFJH2o47Y+Bb9r+ZcO+Y4BvS3oQ+C7wf2x/o11+bZuc0/RSXAScWX5fD9wLfLTdvSJiQHSul/OSCufcRFFxaty3GzhltvnN9RnaMbb3lhnvLR/aTansnVgLcAQnzDG7iOiZTH2aXtnjMQLwWg272/lFxHwdfFOfnpK0oqydrQD2dbJQEbGABriGNtcwvAFYU35fQ0PvRETUwKJF1VKfaVtDK3spzqQYQDcKfAy4FrhV0qXAE8C7u1nIiOihOi9jN0MvxTkdLktE9IsBbXJmpkBENJP6sjlZRQJaREyWGlpE1EJWfYqI2piY+jSAEtAiYrJFaXJGRB2kyRkR9aHU0CKiJlJDi4haSQ0tImohvZwRURtpckZEfaRTICLqQgzsXM7BLHVEdFeHFhqeatW4luNnSvqppO1lurrh2GpJj0raJanSQkypoUVEs86+beMm4LPAl2Y45//avqC5CBoCbgDOBUaBLZI22H5kpswS0CKimYAlHVv1aapV46pYBewqV39C0i0Uq83NGNDS5IyIyaq/gnu5pK0Nae0ccnuLpAclfV3Sb5f7jgWebDhntNw3o9TQIqKZxHj1Xs79tofnkdv3gNfZ/oWk84F/AlZS1BNbtV01LjW0iGhiYHzRokpp3nnZP7P9i/L7RmCJpOUUNbLjG049DtjT7n6poUXEJLOooc2LpH8HPGXbklZRVLJ+AjwHrJR0EvBj4GLgPe3ul4AWEU0s8VKHpj5Ns2rcEgDbnwfeBfyppDHgV8DFtg2MSbocuAsYAtbZ3tEuvwS0iGgmcIeGbcywatzE8c9SDOuY6thGYONs8mtbaknHS7pH0k5JOyRdUe4/UtImSY+Vn8tmk3FE9KfiGZoqpX5TJQyPAX9h+2TgdOAySW8ErgQ2214JbC63I2LQqVow68eAVmWh4b3A3vL7zyXtpBgPchFF2xhgPXAv8NGulDIiemail3MQzeoZWjni9zTgfuCYMthhe6+ko6e5Zi2wFuAITphPWSOiR/qx9lVF5YAm6VXAV4GP2P6ZVO0Htj0CjAC8VsNtB8ZFxMKyxEtDNX7Bo6QlFMHsy7a/Vu5+StKKsna2AtjXrUJGRG8Nag2tSi+ngBuBnbY/03BoA7Cm/L4GuKPzxYuIXnM5bKNK6jdVamhnAO8DHpK0vdz3V8C1wK2SLgWeAN7dnSJGRG/1Zw9mFVV6Ob/N1BNFAc7pbHEiYsHpIOnljIj6MzBesdOv3ySgRUQTS4wtrnEvZ0QcXA6khhYRdXDQzBSIiIOBcGpoEVELGtyBtQloEdHEwFidpz5FxEFEyrCNiKgHAwcGtFNgMEsdEV01XtbS2qV2JK2TtE/Sw9Mcf6+k75fpO5JOaTj2uKSHJG2XtLVKuVNDi4gmHZ4pcBPFmgFfmub4j4C32n5W0jsoXjX25objZ9neXzWzBLSIaCZ1cpGUb5Uvhp3u+HcaNu+jWH9zzhLQIqKJgbHqAW15S3NwpHyp61xcCny9pSjflGTgC1Xum4AWEZPMosm53/bwfPOTdBZFQPuDht1n2N5Tvt5/k6Qf2P7WTPdJp0BENLHEuBZVSp0g6feALwIX2f7Jy+Ww95Sf+4DbgVXt7pWAFhGTdKqXsx1JJwBfA95n+18a9r9S0uET34HzgCl7ShulyRkRTYpxaJ3p5ZR0M8Vyl8sljQIfA5YA2P48cDXwGuDvyoWXxsom7DHA7eW+xcA/2P5Gu/wS0CKimcSBRZ2Z+mT7kjbHPwh8cIr9u4FTJl8xswS0iGhiYHzat+73twS0iJgkczkjoibUsR7MXktAi4gmg7xISpWFhg+V9F1JD0raIekT5f6TJN0v6TFJX5F0SPeLGxFdp2JNgSqp31SpV74AnG37FOBUYLWk04FPAdfZXgk8SzHKNyIGnBFjGqqU+k3bgObCL8rNJWUycDZwW7l/PfDOrpQwInrOUqXUbyo9+ZM0JGk7sA/YBPwQeM72WHnKKHDsNNeulbRV0tbneboTZY6ILpp4htaLmQKdVimg2T5g+1SKV3usAk6e6rRprh2xPWx7+DCOmntJI6JnxlGl1G9m1ctp+zlJ9wKnA0slLS5raccBe7pQvlgAH2/4h/rxqf8/FTXmAR62UaWX8yhJS8vvvwG8DdgJ3AO8qzxtDXBHtwoZEb1V5xraCmC9pCGKAHir7TslPQLcIulvgAeAG7tYzojoEQteGtAaWtuAZvv7wGlT7N9NhfcTxeBpbGZq/M6Xv3vRBQtRnOixosnZf7WvKjJTICImcR82J6tIQIuISQa1UyABLYDpm5aN39P8PDjk9UERUSNibEDfzp+AFhFNDH058byKBLQApm9Cppl5cOpUk1PSOuACYJ/t35niuIDrgfOB54H32/5eeWwN8N/KU//G9vp2+Q1mvTIiusaIcRZVShXcBKye4fg7gJVlWgt8DkDSkRQLqryZYnjYxyQta5dZAlpETGJUKbW9T7Ew8DMznHIR8KXyrT73UUypXAG8Hdhk+xnbz1K8FGOmwAikyRkRU5hFk3O5pK0N2yO2R2aR1bHAkw3bE2/umW7/jBLQIqKJYTa9nPvLdTTnaqrI6Rn2zyhNzohoYsSBiqkDRoHjG7Yn3twz3f4ZJaDFjLzogpdTHDw69Qytgg3An6hwOvBT23uBu4DzJC0rOwPOK/fNKE3OiJikg8M2bgbOpHjWNkrRc7kEwPbngY0UQzZ2UQzb+EB57BlJnwS2lLe6xvZMnQtAAlpEtDBwwJ0JaLYvaXPcwGXTHFsHrJtNfgloMUkG00bmckZELRSdAv23RF0VCWgRMcl4h5qcvZaAFsD0zczG/Y3SFK0vQ6eGZPRcAlpEtBBODS0i6iAveIyBN10TMk3Lg48NL3kwx9wnoEXEJIPa5KwchiUNSXpA0p3l9kmS7pf0mKSvSDqke8WMiN6ptshwPzZLZ1OvvIJixfQJnwKus70SeBa4tJMFi4iFYYphG1VSv6kU0CQdB/x74IvltoCzgdvKU9YD7+xGASOi9w5YlVK/qfoM7W+BvwQOL7dfAzxne6zcnvbla5LWUrxalyM4Ye4ljYieGdSFhtvW0CRNLHCwrXH3FKdO+fI12yO2h20PH8ZRcyxmRPSKLV4aX1Qp9ZsqNbQzgAslnQ8cCryaosa2VNLispZW6eVrEdH/imdoC12KuWkbYm1fZfs42ycCFwN3234vcA/wrvK0NcAdXStlRPSUrUqp38ynzvhR4M8l7aJ4pnZjZ4oUEQtpkHs5ZzWw1va9wL3l990U6+VFRM304xizKjJTICKadPKNtb2WgBYRzSwOHOhcD6ak1cD1wBDwRdvXthy/Djir3DwMONr20vLYAeCh8tgTti+cKa8EtIho0skamqQh4AbgXIrxqlskbbD9yMv52X/WcP6HgdMabvEr26dWza//BpJExMJyRzsFVgG7bO+2/SJwC3DRDOdfAtw816InoEXEJLMYtrFc0taGtLblVscCTzZszzSr6HXAScDdDbsPLe97n6S20yvT5IyIJmZWQzL22x6e4XjlWUUU41xvs32gYd8JtvdIej1wt6SHbP9wuswS0CKiiQ0vHehYL+cocHzD9kyzii6mZY1O23vKz92S7qV4vjZtQEuTMyIm6eBMgS3AyvL9iYdQBK0NrSdJegOwDPjnhn3LJL2i/L6cYhrmI63XNkoNLSIm6dQsANtjki4H7qIYtrHO9g5J1wBbbU8Et0uAW8qV1CecDHxB0jhF5evaxt7RqSSgRUQTAwfGOzew1vZGYGPLvqtbtj8+xXXfAX53NnkloEVEsz6dp1lFAlpENDHg8YUuxdwkoEVEM8NYB6c+9VICWkQ0yeT0iKgVd7BToJcS0CKiySC/gjsBLSKaWR0dttFLCWgR0cTQ0feh9VICWkQ0M4xn2EZE1IGB8TQ5I6IW3NmpT72UgBYRTYzqXUOT9Djwc+AAMGZ7WNKRwFeAE4HHgf9o+9nuFDMiemlQpz7NpivjLNunNryd8kpgs+2VwOZyOyIGXPGCx0WVUr+ZT4kuAtaX39cDbd/3HRGDYXy8Wuo3VQOagW9K2tawCMIxtvcClJ9HT3WhpLUTCyg8z9PzL3FEdJeLqU9VUr+p2ilwRrlQwdHAJkk/qJqB7RFgBOC1Gh7QCRURB4/aD9toWKhgn6TbKdbae0rSCtt7Ja0A9nWxnBHRK4YDfdicrKJtk1PSKyUdPvEdOA94mGKhgzXlaWuAO7pVyIjonYlhG1VSFZJWS3pU0i5JkzoPJb1f0tOStpfpgw3H1kh6rExrWq9tVaWGdgxwu6SJ8//B9jckbQFulXQp8ATw7ko/XUT0NRvGXupMk1PSEHADcC7FknZbJG2YYrGTr9i+vOXaI4GPAcMULeFt5bXTDg9rG9Bs7wZOmWL/T4Bz2l0fEYOng8/QVgG7yjiCpFsoRkjMuHpT6e3AJtvPlNduAlYDN093Qf8NJImIheVZDdtYPjGKoUxrW+52LPBkw/Zoua/Vf5D0fUm3SZpYmLjqtS/L1KeImEQVa2iG/Q2D7ae81dSXNfnfwM22X5D0IYpxrWdXvLZJamgR0cwwdECVUgWjwPEN28cBe5qys39i+4Vy838Cv1/12lYJaBHRRBaLx6qlCrYAKyWdJOkQ4GKKERK/zq8Y9jXhQmBn+f0u4DxJyyQtoxhhcddMmaXJGRGT6EBn7mN7TNLlFIFoCFhne4eka4CttjcA/0XShcAY8Azw/vLaZyR9kiIoAlwz0UEwnQS0iGgiw1AHZwrY3ghsbNl3dcP3q4Crprl2HbCual4JaBExyaIBnSmQgBYRTWRYVO2Bf99JQIuISaoO2+g3CWgR0UQWSzo09anXEtAioplhUYd6OXstAS0imog0OSOiLgxDqaFFRB2IDNuIiLrIsI2IqAsZFqeXMyLqIr2cEVELMixKL2dE1EWn3rbRawloEdHMlV/e2HcS0CKiSdEpsNClmJsEtIhoZtCA1tAqvYJb0tJyNZYfSNop6S2SjpS0qVwAdFP5ityIGHCimClQJfWbqmsKXA98w/ZvUazRuRO4EthseyWwudyOiEFXTk6vkvpN24Am6dXAHwI3Ath+0fZzFIuFri9PWw+8s1uFjIjeEcVMgSqp0v2k1ZIelbRL0qSKj6Q/l/RIuS7nZkmvazh2QNL2Mm1ovbZVlWdorweeBv5e0inANuAK4BjbewFs75V09DQ/zFpgLcARnFAhu4hYUAZ1aC6npCHgBuBcimXptkjaYLtx5fQHgGHbz0v6U+C/A/+pPPYr26dWza9Kk3Mx8Cbgc7ZPA37JLJqXtkdsD9sePoyjql4WEQtEhiUvqlKqYBWwy/Zu2y8Ct1C07l5m+x7bz5eb91GsvzknVQLaKDBq+/5y+zaKAPfUxHp65ee+uRYiIvpIZ5+hHQs82bA9Wu6bzqXA1xu2D5W0VdJ9kto+1mrb5LT9r5KelPQG248C5wCPlGkNcG35eUe7e0VE/yueoVU+fbmkrQ3bI7ZHWm7XylPmK/1nYBh4a8PuE2zvkfR64G5JD9n+4XSFqToO7cPAl8uVj3cDH6Co3d0q6VLgCeDdFe8VEf1sdq8P2m97eIbjo8DxDdvHAXtaT5L0NuCvgbfafuHloth7ys/dku4FTgPmF9Bsb6eInK3OqXJ9RAyOWdbQ2tkCrJR0EvBj4GLgPU35SacBXwBW297XsH8Z8LztFyQtB86g6DCYVmYKRESzDi6SYntM0uXAXcAQsM72DknXAFttbwA+DbwK+EdJAE/YvhA4GfiCpHGKFuG1Lb2jkySgRUQTWSyu1oNZie2NwMaWfVc3fH/bNNd9B/jd2eSVgBYRzbKMXUTUhRLQIqJOEtAiohaUVZ8iojYMi19c6ELMTQJaRDTJM7SIqJUEtIiohTxDi4haSQ0tIuohz9Aioi6UXs6IqIv0ckZEfRgWjS10IeYmAS0iJkkvZ0TUQpqcEVErCWgRUQsaTy9nRNRIamgRUQuD/AytykLDEXEwKYdtVElVSFot6VFJuyRdOcXxV0j6Snn8fkknNhy7qtz/qKS3t8srAS0imkwsY9eJldMlDQE3AO8A3ghcIumNLaddCjxr+zeB64BPlde+kWLZu98GVgN/V95vWgloEdGsnPpUJVWwCthle7ftF4FbgItazrkIWF9+vw04R8V6dhcBt9h+wfaPgF3l/abV02doe9m2/xPol8D+XubbYPkC5r3Q+SfvgyPv1833BnvZdtfH0fKKpx8qaWvD9ojtkYbtY4EnG7ZHgTe33OPlc8p1PH8KvKbcf1/LtcfOVJieBjTbR0na2mbp+K5ZyLwXOv/kfXDlPR+2V3fwdlNNOXDFc6pc2yRNzojoplHg+Ibt44A9050jaTFwBPBMxWubJKBFRDdtAVZKOknSIRQP+Te0nLMBWFN+fxdwt22X+y8ue0FPAlYC350ps4UYhzbS/pRa5r3Q+SfvgyvvvlA+E7scuAsYAtbZ3iHpGmCr7Q3AjcD/krSLomZ2cXntDkm3Ao8AY8BltmfsW1URCCMiBl+anBFRGwloEVEbPQ1o7aZAdDivdZL2SXq4Yd+RkjZJeqz8XNalvI+XdI+knZJ2SLqiV/lLOlTSdyU9WOb9iXL/SeW0ksfKaSaHdDrvhjIMSXpA0p29zFvS45IekrR9YmxUD//mSyXdJukH5d/9Lb3KO36tZwGt4hSITrqJYrpEoyuBzbZXApvL7W4YA/7C9snA6cBl5c/ai/xfAM62fQpwKrBa0ukU00muK/N+lmK6SbdcAexs2O5l3mfZPrVh/Fev/ubXA9+w/VvAKRQ/f6/yjgm2e5KAtwB3NWxfBVzV5TxPBB5u2H4UWFF+XwE82qOf/Q7g3F7nDxwGfI9iZPZ+YPFUf4sO53kcxX+8ZwN3UgyO7FXejwPLW/Z1/XcOvBr4EWUn20L/ezuYUy+bnFNNgZhxGkMXHGN7L0D5eXS3MyzfHHAacH+v8i+bfNuBfcAm4IfAc7Yn3o/Qzd/93wJ/CYyX26/pYd4Gvilpm6S15b5e/M5fDzwN/H3Z1P6ipFf2KO9o0MuANutpDINO0quArwIfsf2zXuVr+4DtUylqS6uAk6c6rdP5SroA2Gd7W+PuXuRdOsP2mygea1wm6Q+7lE+rxcCbgM/ZPg34JWleLoheBrRZT2PogqckrQAoP/d1KyNJSyiC2Zdtf63X+QPYfg64l+I53tJyWgl073d/BnChpMcp3qpwNkWNrRd5Y3tP+bkPuJ0imPfidz4KjNq+v9y+jSLA9fTvHb0NaFWmQHRb4xSLNRTPtjqufPXJjcBO25/pZf6SjpK0tPz+G8DbKB5Q30MxraRredu+yvZxtk+k+Pvebfu9vchb0islHT7xHTgPeJge/M5t/yvwpKQ3lLvOoRjd3pN/b9Gglw/sgPOBf6F4pvPXXc7rZmAv8BLF/0EvpXiesxl4rPw8skt5/wFFs+r7wPYynd+L/IHfAx4o834YuLrc/3qKeXC7gH8EXtHl3/+ZwJ29yrvM48Ey7Zj499XDv/mpwNby9/5PwLJe5Z3065SpTxFRG5kpEBG1kYAWEbWRgBYRtZGAFhG1kYAWEbWRgBYRtZGAFhG18f8BgUjQfREOdDkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAD8CAYAAADJwUnTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAWuUlEQVR4nO3dfYxddZ3H8fen0yKiaMUC1hYtro2raxTIpGLYGAQ1lTVismBQo9XtponBXYxuFNxE0ewm+o+o0ZXMClqNKyA+0CUoshWi/mFlypOUylKRyCyVUgV8wIfOzGf/OGfIvfN0z8zce+49M59X8su959xzf7/fzNAvv8dzZJuIiCZZ1e8KREQsVAJXRDROAldENE4CV0Q0TgJXRDROAldENE4CV0T0jKSjJf1E0p2S9kn66CzXPEXS1ZIOSNojaVOnfBO4IqKX/gycZfvlwCnAVkmnT7tmO/Co7RcClwGf6JTpkgKXpK2S7i0j5cVLySsilh8Xfl8erinT9FXv5wI7y/fXAmdL0nz5rl5shSQNAZ8DXguMAbdK2mX7nrm+c4zWeS2bFltkRHTwGA/whA/P+4++k62SD1e8di/sA/7UcmrE9kjrNWWs2Au8EPic7T3TstkAPAhge1zS48CzgTmrsejABWwBDti+v6zcVRSRc87AtZZN7GB0CUVGxHxGGF5yHoeh8r9SwZ9sz1uo7QngFElrgW9Jeqntu9uzmfm1+fJcSlfxyShZGivPtZG0Q9KopNEneGQJxUVEbYZWVUsLYPsx4BZg67SPxoCTACStBp4J/Ga+vJYSuCpFSdsjtodtDx/D8UsoLiJqIcFRQ9VSx6x0fNnSQtJTgdcAP5t22S5gW/n+POD77nD3h6V0FZ+MkqWNwENLyC8iBoGA1UsaJmu1HthZjnOtAq6xfb2kjwGjtncBVwBfkXSAoqV1QadMlxK4bgU2SzoZ+L+ysLcuIb+IGARiwd3Audi+Czh1lvMfbnn/J+D8heS76MBVjv6/B7gRGAKutL1vsflFxAAZ6lqLqyeW0uLC9g3ADV2qS0QMAqlrLa5eWVLgiohlqItdxV5J4IqIdlOzigMsgSsiZlrOY1wRsQwJWJ2uYkQ0iZQWV0Q0UAbnI6JRVmVwPiKaKC2uiGgUkTGuiGiarJyPiKZJiysiGidbfiKicbLlJyIaKS2uiGiUjHFFROPkflwR0UhpcUVEo2TLT0Q0UrqKEdEoGZyPiObJ4HxENE0DWlwdw6qkKyUdknR3y7njJN0k6b7y9Vm9rWZE1GZqy0+V1CdVSv4SsHXauYuB3bY3A7vL44hYDiRYM1Qt9UnHwGX7B8Bvpp0+F9hZvt8JvKnL9YqIfloGLa7ZnGj7IED5esJcF0raIWlU0ugTPLLI4iKiNlNjXFVSp6ykkyTdLGm/pH2SLprlmjMlPS7pjjJ9uFO+PR+ctz0CjAA8V8PudXkRsVRdnVUcB95v+zZJxwJ7Jd1k+55p1/3Q9huqZrrY2j0saT1A+XpokflExKDpYovL9kHbt5XvfwfsBzYstYqLDVy7gG3l+23AdUutSEQMkFWrqqUFkLQJOBXYM8vHr5R0p6TvSPqbTnl17CpK+hpwJrBO0hjwEeDjwDWStgO/BM6vXPuIGGwL26u4TtJoy/FIOTzURtLTgW8A77X922kf3wY83/bvJZ0DfBvYPF+hHQOX7bfM8dHZnb4bEQ1VfQHqYdvD810gaQ1F0Pqq7W9O/7w1kNm+QdJ/SFpn+/BceWblfES0kxbcDZw7Kwm4Athv+5NzXPMc4GHblrSFYgjr1/Plm8AVETN1b8vPGcDbgZ9KuqM89yHgeQC2LwfOA94taRz4I3CB7XlXICRwRUS7Lj7lx/aPyhznu+azwGcXkm8CV0S0m9ryM8ASuCJiplWDfXeIBK6IaJcHwkZE8ygtrohomLS4IqKR0uKKiEbJrGJENE66ihHRPBmcj4imEV3bq9grCVwRMdOAP54sgSsi2nXx7hC9ksAVEe0ErEngioimSYsrIhpFYjKzihHRJAYm0+KKiKZJiysiGsUSR7LlJyIaReAB7yp2rJ2kkyTdLGm/pH2SLirPHyfpJkn3la/P6n11I6LXijEuVUr9UiWsjgPvt/1i4HTgQkkvAS4GdtveDOwujyOi6VQtaPUzcFV5IOxB4GD5/neS9gMbgHMpnnANsBO4BfhgT2oZEbVZdrOKkjYBpwJ7gBPLoIbtg5JOmOM7O4AdAM8sHqUWEQNu2cwqSno6xWO032v7t8UDajuzPQKMADxXw/M+5DEi+s8SR4aWwayipDUUQeurtr9Znn5Y0vqytbUeONSrSkZEvQa9xVVlVlHAFcB+259s+WgXsK18vw24rvvVi4i6uVwOUSX1S5UW1xnA24GfSrqjPPch4OPANZK2A78Ezu9NFSOiXstgr6LtH1Hc6GI2Z3e3OhHRdxr8WcXBrl1E1M7ApFQpdTLXAvZp10jSZyQdkHSXpNM65ZstPxHRxhLjq7s2qzi1gP02SccCeyXdZPuelmteD2wu0yuAz5evc0qLKyJmmJAqpU5sH7R9W/n+d8DUAvZW5wJfduHHwNpypcKc0uKKiDYLXDm/TtJoy/FIuXZzhmkL2FttAB5sOR4rzx2cq9AEroiYRrjiAnPgsO3hjjlOW8A+o8CZ5l2snsAVEe3U3QWocyxgbzUGnNRyvBF4aL48M8YVEW0MjA8NVUqdzLOAvdUu4B3l7OLpwONT+6DnkhZXRLSruNShorkWsD8PwPblwA3AOcAB4AngXZ0yTeCKiDYGJrq0ALXDAvapawxcuJB8E7giYoYutrh6IoErItpMrZwfZAlcK9jXD/z8yffnv/CvavtuDDhp4B+WkcAVEW0MjCdwRUTTpKsYy1K6h8uXJSaVFldENExaXBHRKMU6rgSuaJgqM4ZVZxUz+9hAEhOrlsFTfiJi5TAwOf9i975L4IqIGTLGFX3V2lWD9u7aXF23Kl26qt2+dA+bKLOKEdEwTdjyU+WBsEdL+omkO8undHy0PH+ypD2S7pN0taSjel/diOg5de+e871SpcX1Z+As278v72T4I0nfAd4HXGb7KkmXA9spns4RA2QxXbVLWwZmL225g25mCFcGI8Y12LOKHVtc5ZM3fl8erimTgbOAa8vzO4E39aSGEVE7S5VSv1QagZM0VN698BBwE/Bz4DHb4+UlU0/lmO27OySNShp9gke6UeeI6KFuPhC2VyoFLtsTtk+huIn9FuDFs102x3dHbA/bHj6G4xdf04iozSSqlPplQbOKth+TdAtwOsVDG1eXra6OT+WIwTPXmNWlczwZaqmr6KMZ3IDlEFVmFY+XtLZ8/1TgNRRPo70ZOK+8bBtwXa8qGRH1Wg4trvXATklDFIHuGtvXS7oHuErSvwG3UzyCKCIazoIjA97i6hi4bN9F8djs6efvpxjvigE2feV8q9Zu3VzdvYWej+YruoqDvQA1K+cjYgZnk3VENM2gD84ncC1zVe+VtdDvp3u4fOW2NhHRQGK82hLPvkngiog2hr5uoK4igStmyIxhdKurKOlK4A3AIdsvneXzMynWgP6iPPVN2x/rlG8CV0S0MWKye13FLwGfBb48zzU/tP2GhWSawBURM3RrOYTtH0ja1JXMWiRwrWBV9h7GyrSAruI6SaMtxyO2RxZY3Csl3Umx3/lfbO/r9IUErohoY1jIrOJh28NLKO424PnljUrPAb4NbO70pcGe84yI2hkxUTEtuSz7t1M3KrV9A7BG0rpO30vgiogZjCqlpZL0HKlYeyFpC0VM+nWn76WrGBEzdHE5xNeAMynGwsaAj1Dc/h3bl1PcGuvdksaBPwIX2J79hnAtErgioo2BCXdtVvEtHT7/LMVyiQVJ4Ip5ZTHqypS9ihHRKMXg/GA/niyBKyJmmOxSV7FXErhiUarcGXX6Z9EMhq4sdeilBK6ImEY4La6IaJLcSDD6br6u21zdvaV079I1bD4bjniw16YncEXEDIPeVawcViUNSbpd0vXl8cmS9ki6T9LVko7qXTUjoj7VHgbbz+7kQtqDF1E8wXrKJ4DLbG8GHgW2d7NiEdEfplgOUSX1S6WuoqSNwN8B/w68r9wUeRbw1vKSncClwOd7UMdYgvnGnHoxHpXlEMtDt7b89ErVMa5PAR8Aji2Pnw08Znu8PB4DNsz2RUk7gB0Az+R5i69pRNRm0B8I27GrKGnqRvd7W0/PcumsO7ptj9getj18DMcvspoRURdbHJlcVSn1S5UW1xnAG8u7Ex4NPIOiBbZW0uqy1bWR4rar0SBVNlAvdMlEuobNV4xx9bsW8+sYMm1fYnuj7U3ABcD3bb8NuJniXjoA2ygeMRQRy4CtSqlfltLW+yDFQP0BijGvK7pTpYjop2UzqzjF9i3ALeX7+4Et3a9S9FLurxVVZMtPRDRKN++A2isJXBHRzmJiInsVY4AstHuY7uTKkxZXRDSPcwfUiGigQb87RAJXRLQx/V3qUEUCV0S0seHIRAJXRDRMuooR0TjpKkZEoxiYmBzswDXYq8wion4V9ylWaZVJulLSIUl3z/G5JH1G0gFJd0k6rUoVE7gioo0BT1ZLFXwJ2DrP568HNpdpBxXvopyuYkS0M4x3acuP7R9I2jTPJecCX7Zt4MeS1kpab/vgfPkmcEVEmwVu+VknabTleMT2yAKK2wA82HI8dRv4BK6IWBhXH5w/bHt4CUVVvg18qwSuiGhT862bx4CTWo4r3QY+g/MR0c5iYrJa6oJdwDvK2cXTgcc7jW9BWlwRMY2ha/fjkvQ14EyKsbAx4CPAGgDblwM3AOcAB4AngHdVyTeBKyLaGSarLXXonJX9lg6fG7hwofkmcEVEGwOTA75yPoErItp58Lf8JHBFRBuj5dHikvQA8DtgAhi3PSzpOOBqYBPwAPBm24/2ppoRUaeK23n6ZiFTB6+2fUrLYrOLgd22NwO7y+OIaLjiRoKrKqV+WUrJ5wI7y/c7gTctvToRMQgmJ6ulfqkauAx8T9JeSTvKcydOLRQrX0+Y7YuSdkgalTT6BI8svcYR0VsutvxUSf1SdXD+DNsPSToBuEnSz6oWUG64HAF4robr20gQEYuybJZD2H6ofD0k6VvAFuDhqdtPSFoPHOphPSOiLoaJpg/OS3qapGOn3gOvA+6m2GO0rbxsG3BdryoZEfWZWg5RJfVLlRbXicC3JE1d/1+2vyvpVuAaSduBXwLn966aEVEXG8aPNLyraPt+4OWznP81cHYvKhUR/bUsxrgiYgXp4ibrXkngiogZVLHF1a9lAglcEdHOMDRRLXCN97gqc0ngiog2slg9nsAVEQ2jiX7XYH4JXBHRRoahzCpGRNOsyqxiRDSJDKsqDs73SwJXRMxQdTlEvyRwRUQbWaxp+pafiFhhDKsyqxgRTSLSVYyIpjEMpcUVEU0ishwiIpqmAcsh+vd8oYgYSDKsPqJKqVJ+0lZJ90o6IGnGYwwlvVPSI5LuKNM/dsozLa6ImKFbs4qShoDPAa8FxoBbJe2yfc+0S6+2/Z6q+SZwRUQbGVZ1b1ZxC3CgvJMykq6ieCbr9MC1IOkqRsQMmqiWKtgAPNhyPFaem+7vJd0l6VpJJ3XKNIErItpZDE1US8C6qQc+l2nHtNxma7pNv3HqfwObbL8M+B9gZ6cqpqsYEW2KwfnKlx+2PTzP52NAawtqI/BQ6wXlg3em/CfwiU6FpsUVEe0MmlClVMGtwGZJJ0s6CriA4pmsTyofKD3ljcD+TplWanFJWgt8AXhp8WPxD8C9wNXAJuAB4M22H62SX0QMLtG9lfO2xyW9B7gRGAKutL1P0seAUdu7gH+W9EaKO0H/Bnhnp3yrdhU/DXzX9nll1DwG+BCw2/bHy7UZFwMfXOgPFhEDpsubrG3fANww7dyHW95fAlyykDw7dhUlPQN4FXBFWchfbD9GMaU5NYi2E3jTQgqOiMEkipXzVVK/VBnjegHwCPBFSbdL+oKkpwEn2j4IUL6eMNuXJe2YmnF4gke6VvGI6BGDJqulfqkSuFYDpwGft30q8AeKbmEltkdsD9sePobjF1nNiKiLDGv+okqpX6oErjFgzPae8vhaikD28NRsQPl6qDdVjIhalWNcVVK/dAxctn8FPCjpReWpsymW6+8CtpXntgHX9aSGEVGrYoxrsANX1VnFfwK+Ws4o3g+8iyLoXSNpO/BL4PzeVDEiatWA29pUCly27wBmWx17dnerExH9NtXiGmTZ8hMR7fKwjIhoGlms7uOMYRUJXBHRLi2uiGgaJXBFRBMlcEVEo2i5LIeIiBXEsPov/a7E/BK4IqJNxrgiopESuCKiUTLGFRGNlBZXRDRLxrgiommUWcWIaJrMKkZE8xhWjfe7EvNL4IqIGTKrGBGNkq5iRDRSAldENIomM6sYEQ2UFldENEoTxriqPBA2IlaScjlElVSFpK2S7pV0QNLFs3z+FElXl5/vkbSpU54JXBHRppsPhJU0BHwOeD3wEuAtkl4y7bLtwKO2XwhcBnyiU74JXBHRrtzyUyVVsAU4YPt+238BrgLOnXbNucDO8v21wNmS5l1IVusY10H2Hv4o+gNwuM5yW6zrY9n9Lj9lr4yyn7/UDA6y98ZL0bqKlx8tabTleMT2SMvxBuDBluMx4BXT8njyGtvjkh4Hns08v7taA5ft4yWN2p7tqdg918+y+11+yl5ZZS+F7a1dzG62lpMXcU2bdBUjopfGgJNajjcCD811jaTVwDOB38yXaQJXRPTSrcBmSSdLOgq4ANg17ZpdwLby/XnA923P2+Lqxzqukc6XLMuy+11+yl5ZZQ+EcszqPcCNwBBwpe19kj4GjNreBVwBfEXSAYqW1gWd8lWHwBYRMXDSVYyIxkngiojGqTVwdVr63+WyrpR0SNLdLeeOk3STpPvK12f1qOyTJN0sab+kfZIuqqt8SUdL+omkO8uyP1qeP7ncTnFfub3iqG6X3VKHIUm3S7q+zrIlPSDpp5LumFpbVOPffK2kayX9rPy7v7Kuslei2gJXxaX/3fQlYPp6lIuB3bY3A7vL414YB95v+8XA6cCF5c9aR/l/Bs6y/XLgFGCrpNMptlFcVpb9KMU2i165CNjfclxn2a+2fUrL+qm6/uafBr5r+6+Bl1P8/HWVvfLYriUBrwRubDm+BLikx2VuAu5uOb4XWF++Xw/cW9PPfh3w2rrLB44BbqNYqXwYWD3b36LLZW6k+Ed6FnA9xeLCusp+AFg37VzPf+fAM4BfUE529fu/t5WQ6uwqzrb0f0ON5QOcaPsgQPl6Qq8LLHe6nwrsqav8sqt2B3AIuAn4OfCY7an9/L383X8K+AAwWR4/u8ayDXxP0l5JO8pzdfzOXwA8Anyx7CJ/QdLTaip7RaozcC14WX/TSXo68A3gvbZ/W1e5tidsn0LR+tkCvHi2y7pdrqQ3AIds7209XUfZpTNsn0YxHHGhpFf1qJzpVgOnAZ+3fSrwB9It7Kk6A1eVpf+99rCk9QDl66FeFSRpDUXQ+qrtb9ZdPoDtx4BbKMbZ1pbbKaB3v/szgDdKeoDiLgBnUbTA6igb2w+Vr4eAb1EE7Tp+52PAmO095fG1FIGs1r/3SlJn4Kqy9L/XWrcWbKMYe+q68pYcVwD7bX+yzvIlHS9pbfn+qcBrKAaKb6bYTtGzsm1fYnuj7U0Uf9/v235bHWVLepqkY6feA68D7qaG37ntXwEPSnpReeps4J46yl6x6hxQA84B/pdizOVfe1zW14CDwBGK/yNupxhv2Q3cV74e16Oy/5aiO3QXcEeZzqmjfOBlwO1l2XcDHy7PvwD4CXAA+DrwlB7//s8Erq+r7LKMO8u0b+q/rxr/5qcAo+Xv/dvAs+oqeyWmbPmJiMbJyvmIaJwErohonASuiGicBK6IaJwErohonASuiGicBK6IaJz/B+w8562I3glDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAD8CAYAAAD5TVjyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAbPUlEQVR4nO3df5CdVZ3n8fcnnSCDIgkG2MgPwdqUg/MDcFJRi6lRQDCyCm6t7oKuGy2slFPg4ji1I8xsIeLsFrNWybglo/ZKhrDlEBiUIbJRTAUo13XAJBqFEBlipKBNhhABf+EAnf7sH8/TeG93375Pd997+96nP6+qU/c+P8/p7vDlnOec8xzZJiKiDhbNdwEiIjolAS0iaiMBLSJqIwEtImojAS0iaiMBLSJqIwEtIrpG0omS7pG0W9IuSZdPcY4k/U9JeyT9QNLrGo6tlfRImda2zS/j0CKiWyStAFbY/q6kI4EdwDttP9RwzvnAh4HzgdcDn7H9eklHA9uBVYDLa//A9tOt8ptTDU3SGkkPl5H1irncKyLqx/Z+298tv/8C2A0cP+G0C4GbXLgPWFoGwrcCW2w/VQaxLcCa6fJbPNuCShoCrgfOBUaAbZI2NUbeiY7Qci/l5NlmGRFtPMOjPOuDmss91kg+WPHcHbAL+JeGXcO2h6c6V9LJwBnA/RMOHQ883rA9Uu5rtb+lWQc0YDWwx/besrAbKSJty4C2lJNZx/Y5ZBkR0xlm1ZzvcRAq/1cq+BfbbTOV9DLgy8BHbP988m0m8TT7W5pLk7NS9JS0TtJ2Sduf5ck5ZBcRPTO0qFqqQNISimD2JdtfmeKUEeDEhu0TgH3T7G9pLgGtUvS0PWx7le1VR3DMHLKLiJ6Q4LChaqntrSTgBmC37U+3OG0T8J/K3s43AD+zvR+4CzhP0jJJy4Dzyn0tzaXJOePoGREDQMDiOT2Ga3Qm8D7gAUk7y31/DpwEYPvzwGaKHs49wLPAB8pjT0n6JLCtvO4a209Nl9lcAto2YKWkU4CfABcB75nD/SKiH4jKzcl2bH+LqVtzjecYuLTFsfXA+qr5zTqg2R6VdBlFFXAIWG9712zvFxF9ZKhjNbSemksNDdubKaqLEVEXUsdqaL02p4AWETXUwSZnryWgRUSz8V7OAZSAFhGTLcRnaBFRQwIWp8kZEXUgpYYWETWSToGIqIVF6RSIiDpJDS0iakHkGVpE1EVmCkREXaSGFhG1kalPEVEbmfoUEbWSGlpE1EKeoUVEbeR9aBFRKx2qoUlaD7wdOGD7d6c4/l+A95abi4FTgWPK9QQeBX4BHAJGqyyXl4AWEc06O/XpRuCzwE1THbT9KeBTAJLeAfzJhIVQzrIrr3ucgBYRU+jcIinfLFdMr+Ji4Oa55DeYDeWI6J7xToEqqVNZSkcAaygWJB5n4BuSdkhaV+U+qaFFxAQz6hRYLml7w/aw7eFZZPoO4P9NaG6eaXufpGOBLZJ+aPub090kAS0ims1s2MbBKg/rK7iICc1N2/vKzwOSbgdWA9MGtLZhWNJ6SQckPdiw72hJWyQ9Un4um9WPEBH9Z3zqU5XUieyko4A3AXc07HuppCPHvwPnAQ9OfYffqFKiGynato2uALbaXglsLbcjog4kWDJULbW9lW4G/hF4jaQRSZdI+pCkDzWc9m+Bb9j+VcO+44BvSfo+8B3g/9j+erv82jY5W/RSXAi8ufy+AbgX+Fi7e0XEgOhcL+fFFc65kaLi1LhvL3DaTPOb7TO042zvLzPeXz60m1LZO7EO4ChOmmV2EdEzmfrUWtnjMQzwSq1yt/OLiLlaeFOfnpC0oqydrQAOdLJQETGPBriGNtswvAlYW35fS0PvRETUwKJF1VKfaVtDK3sp3kwxgG4E+DhwLXCrpEuAx4B3d7OQEdFDdV7GbppeinM6XJaI6BcD2uTMTIGIaCb1ZXOyigS0iJgsNbSIqIWs+hQRtTE+9WkAJaBFxGSL0uSMiDpIkzMi6kOpoUVETaSGFhG1khpaRNRCejkjojbS5IyI+kinQETUhRjYuZyDWeqI6K4OLTQ81apxE46/WdLPJO0s01UNx9ZIeljSHkmVFmJKDS0imnX2bRs3Ap8FbprmnP9r++3NRdAQcD1wLjACbJO0yfZD02WWgBYRzQQs6diqT1OtGlfFamBPufoTkjZSrDY3bUBLkzMiJqv+Cu7lkrY3pHWzyO2Nkr4v6WuSfqfcdzzweMM5I+W+aaWGFhHNJMaq93IetL1qDrl9F3iV7V9KOh/4B2AlRT1xorarxqWGFhFNDIwtWlQpzTkv++e2f1l+3wwskbScokZ2YsOpJwD72t0vNbSImGQGNbQ5kfSvgCdsW9JqikrWT4FngJWSTgF+AlwEvKfd/RLQIqKJJV7o0NSnFqvGLQGw/XngXcAfSxoFfg1cZNvAqKTLgLuAIWC97V3t8ktAi4hmAndo2MY0q8aNH/8sxbCOqY5tBjbPJL+2pZZ0oqR7JO2WtEvS5eX+oyVtkfRI+blsJhlHRH8qnqGpUuo3VcLwKPCntk8F3gBcKum1wBXAVtsrga3ldkQMOlULZv0Y0KosNLwf2F9+/4Wk3RTjQS6kaBsDbADuBT7WlVJGRM+M93IOohk9QytH/J4B3A8cVwY7bO+XdGyLa9YB6wCO4qS5lDUieqQfa19VVA5okl4GfBn4iO2fS9V+YNvDwDDAK7Wq7cC4iJhflnhhqMYveJS0hCKYfcn2V8rdT0haUdbOVgAHulXIiOitQa2hVenlFHADsNv2pxsObQLWlt/XAnd0vngR0Wsuh21USf2mSg3tTOB9wAOSdpb7/hy4FrhV0iXAY8C7u1PEiOit/uzBrKJKL+e3mHqiKMA5nS1ORMw7LZBezoioPwNjFTv9+k0CWkQ0scTo4hr3ckbEwnIoNbSIqIMFM1MgIhYC4dTQoo40dueL373o7dOcGbWhwR1Ym4AWEU0MjNZ56lNELCBShm3E4KnSnEwzc+ExcGhAOwUGs9QR0VVjZS2tXWpH0npJByQ92OL4eyX9oEzflnRaw7FHJT0gaaek7VXKnRpaRDTp8EyBGynWDLipxfEfA2+y/bSkt1G8auz1DcfPsn2wamYJaAtYq+ZkejYXOKmTi6R8s3wxbKvj327YvI9i/c1ZS0CLiCYGRqsHtOUTmoPD5UtdZ+MS4GsTivINSQa+UOW+CWgRMckMmpwHba+aa36SzqIIaH/YsPtM2/vK1/tvkfRD29+c7j4JaAtYq6Zlq2bm1Q1vkbqavE29riwxpt71F0r6feCLwNts//TFctj7ys8Dkm4HVgPTBrT0ckbEJJ3q5WxH0knAV4D32f6nhv0vlXTk+HfgPGDKntJGqaFFRJNiHFpnejkl3Uyx3OVySSPAx4ElALY/D1wFvAL4m3LhpdGyCXsccHu5bzHwd7a/3i6/BLQFrEovZ6OrF6WZuSBIHFrUmalPti9uc/yDwAen2L8XOG3yFdNLQIuIJgbGWr51v78loEXEJJnLGQOtU4NpJzZXMzB3EPW2l7OTEtAioskgL5JSZaHhwyV9R9L3Je2S9Ily/ymS7pf0iKRbJB3W/eJGRNepWFOgSuo3VWpozwFn2/6lpCXAtyR9DfgocJ3tjZI+TzHK93NdLGt00UybhjMdlBuDw4hRDeYLHtvW0Fz4Zbm5pEwGzgZuK/dvAN7ZlRJGRM9ZqpT6TaUnf5KGJO0EDgBbgB8Bz9geLU8ZAY5vce06SdslbX+WJztR5ojoovFnaL2YKdBplQKa7UO2T6d4tcdq4NSpTmtx7bDtVbZXHcExsy9pRPTMGKqU+s2MejltPyPpXuANwFJJi8ta2gnAvi6UL+ZoumEUrZ6DVZmEXuU+MZg8wMM2qvRyHiNpafn9t4C3ALuBe4B3laetBe7oViEjorfqXENbAWyQNEQRAG+1faekh4CNkv4S+B5wQxfLGRE9YsELA1pDaxvQbP8AOGOK/XspnqdFH/v4onc0bTc2IVu/9+w357SaqJ6mZX0VTc7+q31VkZkCETGJ+7A5WUUCWkRMMqidAgloNTfXV2Wnabnw5PVBEVEjYnRA386fgBYRTQx9OfG8igS0GppuoGurY3MZHJtmaf10qskpaT3wduCA7d+d4riAzwDnA88C77f93fLYWuC/lqf+pe0N7fIbzHplRHSNEWMsqpQquBFYM83xtwEry7SO8o09ko6mWFDl9RTDwz4uaVm7zBLQImISo0qp7X2KhYGfmuaUC4Gbyrf63EcxpXIF8FZgi+2nbD9N8VKM6QIjkCZnLc2mCVil+dlqkO1c847+M4Mm53JJ2xu2h20PzyCr44HHG7bH39zTav+0EtAioolhJr2cB8t1NGdrqsjpafZPK03OiGhixKGKqQNGgBMbtsff3NNq/7RSQ1tgWjUhG+d8foKvvvi96VVCWWh4wejh1KdNwGWSNlJ0APzM9n5JdwH/vaEj4DzgynY3S0CLiEk6OGzjZuDNFM/aRih6LpcA2P48sJliyMYeimEbHyiPPSXpk8C28lbX2J6ucwFIQIuICQwccmcCmu2L2xw3cGmLY+uB9TPJLwFtgWn9ltqG5mTDk9Uqc0Ebm6WfGPtq07H0eg6mzOWMiFooOgUGcxm7BLSImGSsQ03OXktAW2CqNAFn+pbaVs3VGEyGTg3J6LkEtIiYQDg1tIiog7zgMQZGqzU3Z/r6oKrzPdPLOXhseMGD+ewgAS0iJhnUJmflMCxpSNL3JN1Zbp8i6X5Jj0i6RdJh3StmRPROtUWG+7FZOpN65eUUK6aP+yvgOtsrgaeBSzpZsIiYH6YYtlEl9ZtKTU5JJwD/BvhvwEfL1+aeDbynPGUDcDXl2yajf7Ua+V/lvWeN57R6NpZnZvXQqalPvVb1GdpfA38GHFluvwJ4xvZoud3y5WuS1lG8WpejOGn2JY2InhnUhYbbNjkljS9wsKNx9xSnTvm/ftvDtlfZXnUEx8yymBHRK7Z4YWxRpdRvqtTQzgQukHQ+cDjwcooa21JJi8taWqWXr0X/6tSwjRh8xTO0+S7F7LQNsbavtH2C7ZOBi4C7bb8XuAd4V3naWuCOrpUyInrKVqXUb+ZSZ/wYRQfBHopnajd0pkgRMZ9q38s5zva9wL3l970U6+VFDbRqNrZqWqaZWW/9OMasiswUiIgmnXxjba8loEVEM4tDhzrXgylpDfAZYAj4ou1rJxy/Djir3DwCONb20vLYIeCB8thjti+YLq8EtJhWmpYLTydraJKGgOuBcynGq26TtMn2Qy/mZ/9Jw/kfBs5ouMWvbZ9eNb/+G0gSEfPLHe0UWA3ssb3X9vPARuDCac6/GLh5tkVPQIuISWYwbGO5pO0Nad2EWx0PPN6wPd2solcBpwB3N+w+vLzvfZLe2a7caXJGRBMzoyEZB22vmuZ45VlFFONcb7N9qGHfSbb3SXo1cLekB2z/qFVmCWgR0cSGFw51rJdzBDixYXu6WUUXMWGNTtv7ys+9ku6leL7WMqClyRkRk3RwpsA2YGX5/sTDKILWpoknSXoNsAz4x4Z9yyS9pPy+nGIa5kMTr22UGlpETNKpWQC2RyVdBtxFMWxjve1dkq4BttseD24XAxvLldTHnQp8QdIYReXr2sbe0akkoEVEEwOHxjo3sNb2ZmDzhH1XTdi+eorrvg383kzySkCLiGZ9Ok+zigS0iGhiwGPzXYrZSUCLiGaG0Q5OfeqlBLSIaJLJ6RFRK+5gp0AvJaBFRJNBfgV3AlpENLM6OmyjlxLQIqKJoaPvQ+ulBLSIaGYYy7CNiKgDA2NpckZELbizU596KQEtIpoY1buGJulR4BfAIWDU9ipJRwO3ACcDjwL/3vbT3SlmRPTSoE59mklXxlm2T294O+UVwFbbK4Gt5XZEDLjiBY+LKqV+M5cSXQhsKL9vANq+7zsiBsPYWLXUb6oGNAPfkLSjYRGE42zvByg/j53qQknrxhdQeJYn517iiOguF1OfqqR+U7VT4MxyoYJjgS2Sflg1A9vDwDDAK7VqQCdURCwctR+20bBQwQFJt1OstfeEpBW290taARzoYjkjolcMh/qwOVlF2yanpJdKOnL8O3Ae8CDFQgdry9PWAnd0q5AR0TvjwzaqpCokrZH0sKQ9kiZ1Hkp6v6QnJe0s0wcbjq2V9EiZ1k68dqIqNbTjgNsljZ//d7a/LmkbcKukS4DHgHdX+ukioq/ZMPpCZ5qckoaA64FzKZa02yZp0xSLndxi+7IJ1x4NfBxYRdES3lFe23J4WNuAZnsvcNoU+38KnNPu+ogYPB18hrYa2FPGESRtpBghMe3qTaW3AltsP1VeuwVYA9zc6oL+G0gSEfPLMxq2sXx8FEOZ1k242/HA4w3bI+W+if6dpB9Iuk3S+MLEVa99UaY+RcQkqlhDMxxsGGw/5a2mvqzJV4GbbT8n6UMU41rPrnhtk9TQIqKZYeiQKqUKRoATG7ZPAPY1ZWf/1PZz5eb/Av6g6rUTJaBFRBNZLB6tlirYBqyUdIqkw4CLKEZI/Ca/YtjXuAuA3eX3u4DzJC2TtIxihMVd02WWJmdETKJDnbmP7VFJl1EEoiFgve1dkq4BttveBPxnSRcAo8BTwPvLa5+S9EmKoAhwzXgHQSsJaBHRRIahDs4UsL0Z2Dxh31UN368Ermxx7XpgfdW8EtAiYpJFAzpTIAEtIprIsKjaA/++k4AWEZNUHbbRbxLQIqKJLJZ0aOpTryWgRUQzw6IO9XL2WgJaRDQRaXJGRF0YhlJDi4g6EBm2ERF1kWEbEVEXMixOL2dE1EV6OSOiFmRYlF7OiKiLTr1to9cS0CKimSu/vLHvJKBFRJOiU2C+SzE7CWgR0cygAa2hVXoFt6Sl5WosP5S0W9IbJR0taUu5AOiW8hW5ETHgRDFToErqN1XXFPgM8HXbv02xRudu4Apgq+2VwNZyOyIGXTk5vUrqN20DmqSXA38E3ABg+3nbz1AsFrqhPG0D8M5uFTIiekcUMwWqpEr3k9ZIeljSHkmTKj6SPirpoXJdzq2SXtVw7JCknWXaNPHaiao8Q3s18CTwt5JOA3YAlwPH2d4PYHu/pGNb/DDrgHUAR3FShewiYl4Z1KG5nJKGgOuBcymWpdsmaZPtxpXTvwessv2spD8G/gfwH8pjv7Z9etX8qjQ5FwOvAz5n+wzgV8ygeWl72PYq26uO4Jiql0XEPJFhyfOqlCpYDeyxvdf288BGitbdi2zfY/vZcvM+ivU3Z6VKQBsBRmzfX27fRhHgnhhfT6/8PDDbQkREH+nsM7TjgccbtkfKfa1cAnytYftwSdsl3Sep7WOttk1O2/8s6XFJr7H9MHAO8FCZ1gLXlp93tLtXRPS/4hla5dOXS9resD1se3jC7SbylPlK/xFYBbypYfdJtvdJejVwt6QHbP+oVWGqjkP7MPClcuXjvcAHKGp3t0q6BHgMeHfFe0VEP5vZ64MO2l41zfER4MSG7ROAfRNPkvQW4C+AN9l+7sWi2PvKz72S7gXOAOYW0GzvpIicE51T5fqIGBwzrKG1sw1YKekU4CfARcB7mvKTzgC+AKyxfaBh/zLgWdvPSVoOnEnRYdBSZgpERLMOLpJie1TSZcBdwBCw3vYuSdcA221vAj4FvAz4e0kAj9m+ADgV+IKkMYoW4bUTekcnSUCLiCayWFytB7MS25uBzRP2XdXw/S0trvs28HszySsBLSKaZRm7iKgLJaBFRJ0koEVELSirPkVEbRgWPz/fhZidBLSIaJJnaBFRKwloEVELeYYWEbWSGlpE1EOeoUVEXSi9nBFRF+nljIj6MCwane9CzE4CWkRMkl7OiKiFNDkjolYS0CKiFjSWXs6IqJHU0CKiFgb5GVqVhYYjYiEph21USVVIWiPpYUl7JF0xxfGXSLqlPH6/pJMbjl1Z7n9Y0lvb5ZWAFhFNxpex68TK6ZKGgOuBtwGvBS6W9NoJp10CPG37XwPXAX9VXvtaimXvfgdYA/xNeb+WEtAiolk59alKqmA1sMf2XtvPAxuBCyeccyGwofx+G3COivXsLgQ22n7O9o+BPeX9WurpM7T97Dj4CfQr4GAv822wfB7znu/8k/fCyPtVc73BfnbcdTVaXvH0wyVtb9getj3csH088HjD9gjw+gn3ePGcch3PnwGvKPffN+Ha46crTE8Dmu1jJG1vs3R818xn3vOdf/JeWHnPhe01HbzdVFMOXPGcKtc2SZMzIrppBDixYfsEYF+rcyQtBo4Cnqp4bZMEtIjopm3ASkmnSDqM4iH/pgnnbALWlt/fBdxt2+X+i8pe0FOAlcB3pstsPsahDbc/pZZ5z3f+yXth5d0XymdilwF3AUPAetu7JF0DbLe9CbgB+N+S9lDUzC4qr90l6VbgIWAUuNT2tH2rKgJhRMTgS5MzImojAS0iaqOnAa3dFIgO57Ve0gFJDzbsO1rSFkmPlJ/LupT3iZLukbRb0i5Jl/cqf0mHS/qOpO+XeX+i3H9KOa3kkXKayWGdzruhDEOSvifpzl7mLelRSQ9I2jk+NqqHf/Olkm6T9MPy7/7GXuUdv9GzgFZxCkQn3UgxXaLRFcBW2yuBreV2N4wCf2r7VOANwKXlz9qL/J8DzrZ9GnA6sEbSGyimk1xX5v00xXSTbrkc2N2w3cu8z7J9esP4r179zT8DfN32bwOnUfz8vco7xtnuSQLeCNzVsH0lcGWX8zwZeLBh+2FgRfl9BfBwj372O4Bze50/cATwXYqR2QeBxVP9LTqc5wkU//GeDdxJMTiyV3k/CiyfsK/rv3Pg5cCPKTvZ5vvf20JOvWxyTjUFYtppDF1wnO39AOXnsd3OsHxzwBnA/b3Kv2zy7QQOAFuAHwHP2B5/P0I3f/d/DfwZMFZuv6KHeRv4hqQdktaV+3rxO3818CTwt2VT+4uSXtqjvKNBLwPajKcxDDpJLwO+DHzE9s97la/tQ7ZPp6gtrQZOneq0Tucr6e3AAds7Gnf3Iu/SmbZfR/FY41JJf9SlfCZaDLwO+JztM4BfkeblvOhlQJvxNIYueELSCoDy80C3MpK0hCKYfcn2V3qdP4DtZ4B7KZ7jLS2nlUD3fvdnAhdIepTirQpnU9TYepE3tveVnweA2ymCeS9+5yPAiO37y+3bKAJcT//e0duAVmUKRLc1TrFYS/Fsq+PKV5/cAOy2/ele5i/pGElLy++/BbyF4gH1PRTTSrqWt+0rbZ9g+2SKv+/dtt/bi7wlvVTSkePfgfOAB+nB79z2PwOPS3pNuescitHtPfn3Fg16+cAOOB/4J4pnOn/R5bxuBvYDL1D8H/QSiuc5W4FHys+ju5T3H1I0q34A7CzT+b3IH/h94Htl3g8CV5X7X00xD24P8PfAS7r8+38zcGev8i7z+H6Zdo3/++rh3/x0YHv5e/8HYFmv8k76TcrUp4iojcwUiIjaSECLiNpIQIuI2khAi4jaSECLiNpIQIuI2khAi4ja+P8qFgm88lIVLgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAD8CAYAAAD5TVjyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAa9klEQVR4nO3df5CdVZ3n8fcnnSCDIgkG2MgPwdqUg/MDcFJRi6lRQDCyCk6t7oKuGy2slFPg4ji1I8xsIeLsFrNW6bglo/ZKhjjlEBmUIcNGMRWgXNcBEzQKITLESEGbDCEC/sIBOv3ZP56n4bndffs+3X3v7dtPPq+qU/c+P8/p7vDlnOec8xzZJiKiCRbNdwEiIrolAS0iGiMBLSIaIwEtIhojAS0iGiMBLSIaIwEtInpG0omS7pS0S9JOSZdPcY4k/S9JuyX9QNJrKsfWSnqoTGs75pdxaBHRK5JWACtsf1fSkcC9wNttP1A553zgg8D5wGuBT9t+raSjge3AKsDltb9n+8l2+c2phiZpjaQHy8h6xVzuFRHNY3uf7e+W338B7AKOn3DahcAXXbgbWFoGwjcDW2w/UQaxLcCa6fJbPNuCShoCrgPOBUaAbZI2VSPvREdouZdy8myzjIgOnuJhnvYBzeUeayQfqHnuvbAT+NfKrmHbw1OdK+lk4AzgngmHjgcerWyPlPva7W9r1gENWA3str2nLOxGikjbNqAt5WTWsX0OWUbEdIZZNed7HIDa/5UK/tV2x0wlvQT4CvAh2z+ffJtJPM3+tubS5KwVPSWtk7Rd0vaneXwO2UVE3wwtqpdqkLSEIph9yfZXpzhlBDixsn0CsHea/W3NJaDVip62h22vsr3qCI6ZQ3YR0RcSHDZUL3W8lQRcD+yy/ck2p20C/nPZ2/k64Ge29wG3A+dJWiZpGXBeua+tuTQ5Zxw9I2IBELB4To/hqs4E3gPcJ2lHue/PgJMAbH8O2EzRw7kbeBp4X3nsCUkfB7aV111j+4npMptLQNsGrJR0CvAT4CLgXXO4X0QMAlG7OdmJ7W8xdWuueo6BS9scWw+sr5vfrAOa7VFJl1FUAYeA9bZ3zvZ+ETFAhrpWQ+urudTQsL2ZoroYEU0hda2G1m9zCmgR0UBdbHL2WwJaRLQa7+VcgBLQImKyQ/EZWkQ0kIDFaXJGRBNIqaFFRIOkUyAiGmFROgUioklSQ4uIRhB5hhYRTZGZAhHRFKmhRURjZOpTRDRGpj5FRKOkhhYRjZBnaBHRGHkfWkQ0SpdqaJLWA28F9tv+7SmO/1fg3eXmYuBU4JhyPYGHgV8AB4HROsvlJaBFRKvuTn26AfgM8MWpDtr+BPAJAElvA/54wkIoZ9m11z1OQIuIKXRvkZRvlium13ExcONc8luYDeWI6J3xToE6qVtZSkcAaygWJB5n4BuS7pW0rs59UkOLiAlm1CmwXNL2yvaw7eFZZPo24P9NaG6eaXuvpGOBLZJ+aPub090kAS0iWs1s2MaBOg/ra7iICc1N23vLz/2SbgFWA9MGtI5hWNJ6Sfsl3V/Zd7SkLZIeKj+XzepHiIjBMz71qU7qRnbSUcAbgFsr+14s6cjx78B5wP1T3+EFdUp0A0XbtuoKYKvtlcDWcjsimkCCJUP1Usdb6Ubgn4BXSRqRdImkD0j6QOW0PwS+YftXlX3HAd+S9H3gO8D/sf31Tvl1bHK26aW4EHhj+X0DcBfwkU73iogFonu9nBfXOOcGiopTdd8e4LSZ5jfbZ2jH2d5XZryvfGg3pbJ3Yh3AUZw0y+wiom8y9am9ssdjGODlWuVe5xcRc3XoTX16TNKKsna2AtjfzUJFxDxawDW02YbhTcDa8vtaKr0TEdEAixbVSwOmYw2t7KV4I8UAuhHgo8C1wE2SLgEeAd7Zy0JGRB81eRm7aXopzulyWSJiUCzQJmdmCkREK2kgm5N1JKBFxGSpoUVEI2TVp4hojPGpTwtQAlpETLYoTc6IaII0OSOiOZQaWkQ0RGpoEdEoqaFFRCOklzMiGiNNzohojnQKRERTiAU7l3NhljoieqtLCw1PtWrchONvlPQzSTvKdFXl2BpJD0raLanWQkypoUVEq+6+beMG4DPAF6c55//afmtrETQEXAecC4wA2yRtsv3AdJkloEVEKwFLurbq01SrxtWxGthdrv6EpI0Uq81NG9DS5IyIyeq/gnu5pO2VtG4Wub1e0vclfU3Sb5X7jgcerZwzUu6bVmpoEdFKYqx+L+cB26vmkNt3gVfY/qWk84F/AFZS1BMn6rhqXGpoEdHCwNiiRbXSnPOyf277l+X3zcASScspamQnVk49Adjb6X6poUXEJDOooc2JpH8DPGbbklZTVLJ+CjwFrJR0CvAT4CLgXZ3ul4AWES0s8VyXpj61WTVuCYDtzwHvAP5I0ijwa+Ai2wZGJV0G3A4MAett7+yUXwJaRLQSuEvDNqZZNW78+GcohnVMdWwzsHkm+XUstaQTJd0paZeknZIuL/cfLWmLpIfKz2UzyTgiBlPxDE210qCpE4ZHgT+xfSrwOuBSSa8GrgC22l4JbC23I2KhU71gNogBrc5Cw/uAfeX3X0jaRTEe5EKKtjHABuAu4CM9KWVE9M14L+dCNKNnaOWI3zOAe4DjymCH7X2Sjm1zzTpgHcBRnDSXskZEnwxi7auO2gFN0kuArwAfsv1zqd4PbHsYGAZ4uVZ1HBgXEfPLEs8NNfgFj5KWUASzL9n+arn7MUkrytrZCmB/rwoZEf21UGtodXo5BVwP7LL9ycqhTcDa8vta4NbuFy8i+s3lsI06adDUqaGdCbwHuE/SjnLfnwHXAjdJugR4BHhnb4oYEf01mD2YddTp5fwWU08UBTinu8WJiHmnQ6SXMyKaz8BYzU6/QZOAFhEtLDG6uMG9nBFxaDmYGlpENMEhM1MgIg4FwqmhRUQjaOEOrE1Ai4gWBkabPPUpIg4hUoZtREQzGDi4QDsFFmapI6KnxspaWqfUiaT1kvZLur/N8XdL+kGZvi3ptMqxhyXdJ2mHpO11yp0aWkS06PJMgRso1gz4YpvjPwbeYPtJSW+heNXYayvHz7J9oG5mCWgR0Urq5iIp3yxfDNvu+Lcrm3dTrL85awloEdHCwGj9gLZ8QnNwuHyp62xcAnxtQlG+IcnA5+vcNwEtIiaZQZPzgO1Vc81P0lkUAe33K7vPtL23fL3/Fkk/tP3N6e6TToGIaGGJMS2qlbpB0u8CXwAutP3T58th7y0/9wO3AKs73SsBLSIm6VYvZyeSTgK+CrzH9j9X9r9Y0pHj34HzgCl7SqvS5IyIFsU4tO70ckq6kWK5y+WSRoCPAksAbH8OuAp4GfDX5cJLo2UT9jjglnLfYuDvbH+9U34JaDGJxm57/rsXvXUeSxLzQuLgou5MfbJ9cYfj7wfeP8X+PcBpk6+YXgJaRLQwMNb2rfuDLQEtIibJXM5ojDQzD3XqWg9mvyWgRUSLhbxISp2Fhg+X9B1J35e0U9LHyv2nSLpH0kOSvizpsN4XNyJ6TsWaAnXSoKlTQ3sGONv2LyUtAb4l6WvAh4FP2d4o6XMUo3w/28OyRp+06+Wc6f5YmIwY1cJ8wWPHGpoLvyw3l5TJwNnAzeX+DcDbe1LCiOg7S7XSoKn15E/SkKQdwH5gC/Aj4Cnbo+UpI8Dxba5dJ2m7pO1P83g3yhwRPTT+DK0fMwW6rVZAs33Q9ukUr/ZYDZw61Wltrh22vcr2qiM4ZvYljYi+GUO10qCZUS+n7ack3QW8DlgqaXFZSzsB2NuD8kWXXV35R/ixsX98/nv12Ve752B1no9Vn6fVvSYGixfwsI06vZzHSFpafv8N4E3ALuBO4B3laWuBW3tVyIjorybX0FYAGyQNUQTAm2zfJukBYKOkvwC+B1zfw3JGRJ9Y8NwCraF1DGi2fwCcMcX+PdR4P1EMlqurjzrb/JutNhs/uuhtU19bkWZlsxRNzsGrfdWRmQIRMYkHsDlZRwJaREyyUDsFEtAOYe1G+Fe/t2tm1rnnxHvFwpDXB0VEg4jRBfp2/gS0iGhhGMiJ53UkoMUkM51snsnpzdOtJqek9cBbgf22f3uK4wI+DZwPPA281/Z3y2Nrgf9WnvoXtjd0ym9h1isjomeMGGNRrVTDDcCaaY6/BVhZpnWUb+yRdDTFgiqvpRge9lFJyzplloAWEZMY1Uod71MsDPzENKdcCHyxfKvP3RRTKlcAbwa22H7C9pMUL8WYLjACaXIecqpzOa9e9EIPZp1mY8u1ld7PNDObZwZNzuWStle2h20PzyCr44FHK9vjb+5pt39aCWgR0cIwk17OA+U6mrM1VeT0NPunlSZnRLQw4mDN1AUjwImV7fE397TbP63U0A4xc5mPWb22XfMzmqGPU582AZdJ2kjRAfAz2/sk3Q78j0pHwHnAlZ1uloAWEZN0cdjGjcAbKZ61jVD0XC4BsP05YDPFkI3dFMM23lcee0LSx4Ft5a2usT1d5wKQgBYRExg46O4ENNsXdzhu4NI2x9YD62eSXwJaTKtd72eamc2WuZwR0QhFp8DCXMYuAS0iJhnrUpOz3xLQYpK5DL6Nhc/QrSEZfZeAFhETCKeGFhFNkBc8xoJXbU5+jBfW66zOJWnXzExTtFlseM4LcxJRAlpETLJQm5y1w7CkIUnfk3RbuX2KpHskPSTpy5IO610xI6J/6i0yPIjN0pnUKy+nWDF93F8Cn7K9EngSuKSbBYuI+WGKYRt10qCp1eSUdALw74D/Dny4fG3u2cC7ylM2AFdTvm0yBledlZ5mKs/NmqdbU5/6re4ztL8C/hQ4stx+GfCU7dFyu+3L1ySto3i1Lkdx0uxLGhF9s1AXGu7Y5JQ0vsDBvdXdU5w65eQ+28O2V9ledQTHzLKYEdEvtnhubFGtNGjq1NDOBC6QdD5wOPBSihrbUkmLy1parZevxfzLKk7RSfEMbb5LMTsdQ6ztK22fYPtk4CLgDtvvBu4E3lGetha4tWeljIi+slUrDZq51Bk/QtFBsJvimdr13SlSRMynxvdyjrN9F3BX+X0PxXp50TBpZsYgjjGrIzMFIqJFN99Y228JaBHRyuLgwe71YEpaA3waGAK+YPvaCcc/BZxVbh4BHGt7aXnsIHBfeewR2xdMl1cCWkS06GYNTdIQcB1wLsV41W2SNtl+4Pn87D+unP9B4IzKLX5t+/S6+Q3eQJKImF/uaqfAamC37T22nwU2AhdOc/7FwI2zLXoCWkRMMoNhG8slba+kdRNudTzwaGV7ullFrwBOAe6o7D68vO/dkt7eqdxpckZECzOjIRkHbK+a5njtWUUU41xvtn2wsu8k23slvRK4Q9J9tn/ULrMEtIhoYcNzB7vWyzkCnFjZnm5W0UVMWKPT9t7yc4+kuyier7UNaGlyRsQkXZwpsA1YWb4/8TCKoLVp4kmSXgUsA/6psm+ZpBeV35dTTMN8YOK1VamhRcQk3ZoFYHtU0mXA7RTDNtbb3inpGmC77fHgdjGwsVxJfdypwOcljVFUvq6t9o5OJQEtIloYODjWvYG1tjcDmyfsu2rC9tVTXPdt4HdmklcCWkS0GtB5mnUkoEVECwMem+9SzE4CWkS0Mox2cepTPyWgRUSLTE6PiEZxFzsF+ikBLSJaLORXcCegRUQrq6vDNvopAS0iWhi6+j60fkpAi4hWhrEM24iIJjAwliZnRDSCuzv1qZ8S0CKihVGza2iSHgZ+ARwERm2vknQ08GXgZOBh4D/YfrI3xYyIflqoU59m0pVxlu3TK2+nvALYanslsLXcjogFrnjB46JaadDMpUQXAhvK7xuAju/7joiFYWysXho0dQOagW9IureyCMJxtvcBlJ/HTnWhpHXjCyg8zeNzL3FE9JaLqU910qCp2ylwZrlQwbHAFkk/rJuB7WFgGODlWrVAJ1REHDoaP2yjslDBfkm3UKy195ikFbb3SVoB7O9hOSOiXwwHB7A5WUfHJqekF0s6cvw7cB5wP8VCB2vL09YCt/aqkBHRP+PDNuqkOiStkfSgpN2SJnUeSnqvpMcl7SjT+yvH1kp6qExrJ147UZ0a2nHALZLGz/8721+XtA24SdIlwCPAO2v9dBEx0GwYfa47TU5JQ8B1wLkUS9ptk7RpisVOvmz7sgnXHg18FFhF0RK+t7y27fCwjgHN9h7gtCn2/xQ4p9P1EbHwdPEZ2mpgdxlHkLSRYoTEtKs3ld4MbLH9RHntFmANcGO7CwZvIElEzC/PaNjG8vFRDGVaN+FuxwOPVrZHyn0T/XtJP5B0s6TxhYnrXvu8TH2KiElUs4ZmOFAZbD/lraa+rMU/AjfafkbSByjGtZ5d89oWqaFFRCvD0EHVSjWMACdWtk8A9rZkZ//U9jPl5v8Gfq/utRMloEVEC1ksHq2XatgGrJR0iqTDgIsoRki8kF8x7GvcBcCu8vvtwHmSlklaRjHC4vbpMkuTMyIm0cHu3Mf2qKTLKALRELDe9k5J1wDbbW8C/oukC4BR4AngveW1T0j6OEVQBLhmvIOgnQS0iGghw1AXZwrY3gxsnrDvqsr3K4Er21y7HlhfN68EtIiYZNECnSmQgBYRLWRYVO+B/8BJQIuISeoO2xg0CWgR0UIWS7o09anfEtAiopVhUZd6OfstAS0iWog0OSOiKQxDqaFFRBOIDNuIiKbIsI2IaAoZFqeXMyKaIr2cEdEIMixKL2dENEW33rbRbwloEdHKtV/eOHAS0CKiRdEpMN+lmJ0EtIhoZdACraHVegW3pKXlaiw/lLRL0uslHS1pS7kA6JbyFbkRscCJYqZAnTRo6q4p8Gng67Z/k2KNzl3AFcBW2yuBreV2RCx05eT0OmnQdAxokl4K/AFwPYDtZ20/RbFY6IbytA3A23tVyIjoH1HMFKiTat1PWiPpQUm7JU2q+Ej6sKQHynU5t0p6ReXYQUk7yrRp4rUT1XmG9krgceBvJJ0G3AtcDhxnex+A7X2Sjm3zw6wD1gEcxUk1souIeWVQl+ZyShoCrgPOpViWbpukTbarK6d/D1hl+2lJfwT8T+A/lsd+bfv0uvnVaXIuBl4DfNb2GcCvmEHz0vaw7VW2Vx3BMXUvi4h5IsOSZ1Ur1bAa2G17j+1ngY0Urbvn2b7T9tPl5t0U62/OSp2ANgKM2L6n3L6ZIsA9Nr6eXvm5f7aFiIgB0t1naMcDj1a2R8p97VwCfK2yfbik7ZLultTxsVbHJqftf5H0qKRX2X4QOAd4oExrgWvLz1s73SsiBl/xDK326cslba9sD9sennC7iTxlvtJ/AlYBb6jsPsn2XkmvBO6QdJ/tH7UrTN1xaB8EvlSufLwHeB9F7e4mSZcAjwDvrHmviBhkM3t90AHbq6Y5PgKcWNk+Adg78SRJbwL+HHiD7WeeL4q9t/zcI+ku4AxgbgHN9g6KyDnROXWuj4iFY4Y1tE62ASslnQL8BLgIeFdLftIZwOeBNbb3V/YvA562/Yyk5cCZFB0GbWWmQES06uIiKbZHJV0G3A4MAett75R0DbDd9ibgE8BLgL+XBPCI7QuAU4HPSxqjaBFeO6F3dJIEtIhoIYvF9Xowa7G9Gdg8Yd9Vle9vanPdt4HfmUleCWgR0SrL2EVEUygBLSKaJAEtIhpBWfUpIhrDsPjZ+S7E7CSgRUSLPEOLiEZJQIuIRsgztIholNTQIqIZ8gwtIppC6eWMiKZIL2dENIdh0eh8F2J2EtAiYpL0ckZEI6TJGRGNkoAWEY2gsfRyRkSDpIYWEY2wkJ+h1VloOCIOJeWwjTqpDklrJD0oabekK6Y4/iJJXy6P3yPp5MqxK8v9D0p6c6e8EtAiosX4MnbdWDld0hBwHfAW4NXAxZJePeG0S4Anbf9b4FPAX5bXvppi2bvfAtYAf13er60EtIhoVU59qpNqWA3str3H9rPARuDCCedcCGwov98MnKNiPbsLgY22n7H9Y2B3eb+2+voMbR/3HvgY+hVwoJ/5Viyfx7znO//kfWjk/Yq53mAf995+NVpe8/TDJW2vbA/bHq5sHw88WtkeAV474R7Pn1Ou4/kz4GXl/rsnXHv8dIXpa0CzfYyk7R2Wju+Z+cx7vvNP3odW3nNhe00XbzfVlAPXPKfOtS3S5IyIXhoBTqxsnwDsbXeOpMXAUcATNa9tkYAWEb20DVgp6RRJh1E85N804ZxNwNry+zuAO2y73H9R2Qt6CrAS+M50mc3HOLThzqc0Mu/5zj95H1p5D4TymdhlwO3AELDe9k5J1wDbbW8Crgf+VtJuiprZReW1OyXdBDwAjAKX2p62b1VFIIyIWPjS5IyIxkhAi4jG6GtA6zQFost5rZe0X9L9lX1HS9oi6aHyc1mP8j5R0p2SdknaKenyfuUv6XBJ35H0/TLvj5X7TymnlTxUTjM5rNt5V8owJOl7km7rZ96SHpZ0n6Qd42Oj+vg3XyrpZkk/LP/ur+9X3vGCvgW0mlMguukGiukSVVcAW22vBLaW270wCvyJ7VOB1wGXlj9rP/J/Bjjb9mnA6cAaSa+jmE7yqTLvJymmm/TK5cCuynY/8z7L9umV8V/9+pt/Gvi67d8ETqP4+fuVd4yz3ZcEvB64vbJ9JXBlj/M8Gbi/sv0gsKL8vgJ4sE8/+63Auf3OHzgC+C7FyOwDwOKp/hZdzvMEiv94zwZuoxgc2a+8HwaWT9jX89858FLgx5SdbPP97+1QTv1sck41BWLaaQw9cJztfQDl57G9zrB8c8AZwD39yr9s8u0A9gNbgB8BT9kefz9CL3/3fwX8KTBWbr+sj3kb+IakeyWtK/f143f+SuBx4G/KpvYXJL24T3lHRT8D2oynMSx0kl4CfAX4kO2f9ytf2wdtn05RW1oNnDrVad3OV9Jbgf22763u7kfepTNtv4biscalkv6gR/lMtBh4DfBZ22cAvyLNy3nRz4A242kMPfCYpBUA5ef+XmUkaQlFMPuS7a/2O38A208Bd1E8x1taTiuB3v3uzwQukPQwxVsVzqaosfUjb2zvLT/3A7dQBPN+/M5HgBHb95TbN1MEuL7+vaO/Aa3OFIheq06xWEvxbKvrylefXA/ssv3JfuYv6RhJS8vvvwG8ieIB9Z0U00p6lrftK22fYPtkir/vHbbf3Y+8Jb1Y0pHj34HzgPvpw+/c9r8Aj0p6VbnrHIrR7X359xYV/XxgB5wP/DPFM50/73FeNwL7gOco/g96CcXznK3AQ+Xn0T3K+/cpmlU/AHaU6fx+5A/8LvC9Mu/7gavK/a+kmAe3G/h74EU9/v2/EbitX3mXeXy/TDvH/3318W9+OrC9/L3/A7CsX3knvZAy9SkiGiMzBSKiMRLQIqIxEtAiojES0CKiMRLQIqIxEtAiojES0CKiMf4/UwvlwUULbCYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAD8CAYAAAD5TVjyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAayElEQVR4nO3df5CdVZ3n8fenO0EGRRIMsJEfgjUpB2dnAKcrajE1CghGFsGp1V3QdTMWVsopcHGc2hFmqlBxa4tZq3SckhF7JANuOUQGZciwUUwFKNd1wCQagRAZYqSgTYYQAX/hAJ3+7B/P03jv7R/36e57b9/75POqOnXv8/Oc7g5fznnOOc+RbSIi6mBosQsQEdEpCWgRURsJaBFRGwloEVEbCWgRURsJaBFRGwloEdE1kk6UdLekXZJ2SrpimnMk6a8l7ZZ0v6TXNRxbK+mRMq1tm1/GoUVEt0haCay0/V1JRwLbgXfYfqjhnPOBDwLnA68HPmP79ZKOBrYBI4DLa3/P9tMz5begGpqkNZIeLiPrlQu5V0TUj+19tr9bfv85sAs4vuW0i4AvunAvsKwMhG8FNtt+qgxim4E1s+W3ZL4FlTQMXAecC4wBWyVtbIy8rY7QCi/j5PlmGRFtPMOjPOsDWsg91kg+UPHc7bAT+LeGXaO2R6c7V9LJwBnAfS2Hjgceb9geK/fNtH9G8w5owGpgt+09ZWE3UETaGQPaMk5mHdsWkGVEzGaUkQXf4wBU/q9U8G+222Yq6WXAV4AP2f7Z1NtM4Vn2z2ghTc5K0VPSOknbJG17licXkF1E9MzwULVUgaSlFMHsS7a/Os0pY8CJDdsnAHtn2T+jhQS0StHT9qjtEdsjR3DMArKLiJ6Q4LDhaqntrSTgBmCX7U/NcNpG4L+WvZ1vAH5qex9wJ3CepOWSlgPnlftmtJAm55yjZ0QMAAFLFvQYrtGZwHuBByTtKPf9OXASgO3rgU0UPZy7gWeB95XHnpL0CWBred01tp+aLbOFBLStwCpJpwA/Bi4G3r2A+0VEPxCVm5Pt2P4W07fmGs8xcNkMx9YD66vmN++AZntc0uUUVcBhYL3tnfO9X0T0keGO1dB6aiE1NGxvoqguRkRdSB2rofXaggJaRNRQB5ucvZaAFhHNJns5B1ACWkRMdSg+Q4uIGhKwJE3OiKgDKTW0iKiRdApERC0MpVMgIuokNbSIqAWRZ2gRUReZKRARdZEaWkTURqY+RURtZOpTRNRKamgRUQt5hhYRtZH3oUVErXSohiZpPXABsN/2v5/m+H8H3lNuLgFOBY4p1xN4FPg5cBAYr7JcXgJaRDTr7NSnG4HPAl+c7qDtTwKfBJD0duBPWhZCOcuuvO5xAlpETKNzi6R8s1wxvYpLgJsXkt9gNpQjonsmOwWqpE5lKR0BrKFYkHiSgW9I2i5pXZX7pIYWES3m1CmwQtK2hu1R26PzyPTtwP9raW6eaXuvpGOBzZJ+YPubs90kAS0ims1t2MaBKg/rK7iYluam7b3l535JtwGrgVkDWtswLGm9pP2SHmzYd7SkzZIeKT+Xz+tHiIj+Mzn1qUrqRHbSUcCbgNsb9r1U0pGT34HzgAenv8OvVSnRjRRt20ZXAltsrwK2lNsRUQcSLB2ultreSjcD/wy8RtKYpEslfUDSBxpO+0PgG7Z/2bDvOOBbkr4PfAf4P7a/3i6/tk3OGXopLgLeXH6/CbgH+Ei7e0XEgOhcL+clFc65kaLi1LhvD3DaXPOb7zO042zvKzPeVz60m1bZO7EO4ChOmmd2EdEzmfo0s7LHYxTglRpxt/OLiIU69KY+PSFpZVk7Wwns72ShImIRDXANbb5heCOwtvy+lobeiYiogaGhaqnPtK2hlb0Ub6YYQDcGfBS4FrhF0qXAY8C7ulnIiOihOi9jN0svxTkdLktE9IsBbXJmpkBENJP6sjlZRQJaREyVGlpE1EJWfYqI2pic+jSAEtAiYqqhNDkjog7S5IyI+lBqaBFRE6mhRUStpIYWEbWQXs6IqI00OSOiPtIpEBF1IQZ2LudgljoiuqtDCw1Pt2pcy/E3S/qppB1lurrh2BpJD0vaLanSQkypoUVEs86+beNG4LPAF2c55//avqC5CBoGrgPOBcaArZI22n5otswS0CKimYClHVv1abpV46pYDewuV39C0gaK1eZmDWhpckbEVNVfwb1C0raGtG4eub1R0vclfU3Sb5f7jgcebzhnrNw3q9TQIqKZxET1Xs4DtkcWkNt3gVfZ/oWk84F/BFZR1BNbtV01LjW0iGhiYGJoqFJacF72z2z/ovy+CVgqaQVFjezEhlNPAPa2u19qaBExxRxqaAsi6d8BT9i2pNUUlayfAM8AqySdAvwYuBh4d7v7JaBFRBNLvNChqU8zrBq3FMD29cA7gT+WNA78CrjYtoFxSZcDdwLDwHrbO9vll4AWEc0E7tCwjVlWjZs8/lmKYR3THdsEbJpLfm1LLelESXdL2iVpp6Qryv1HS9os6ZHyc/lcMo6I/lQ8Q1Ol1G+qhOFx4E9tnwq8AbhM0muBK4EttlcBW8rtiBh0qhbM+jGgVVloeB+wr/z+c0m7KMaDXETRNga4CbgH+EhXShkRPTPZyzmI5vQMrRzxewZwH3BcGeywvU/SsTNcsw5YB3AUJy2krBHRI/1Y+6qickCT9DLgK8CHbP9MqvYD2x4FRgFeqZG2A+MiYnFZ4oXhGr/gUdJSimD2JdtfLXc/IWllWTtbCezvViEjorcGtYZWpZdTwA3ALtufaji0EVhbfl8L3N754kVEr7kctlEl9ZsqNbQzgfcCD0jaUe77c+Ba4BZJlwKPAe/qThEjorf6sweziiq9nN9i+omiAOd0tjgRseh0iPRyRkT9GZio2OnXbxLQIqKJJcaX1LiXMyIOLQdTQ4uIOjhkZgpExKFAODW0iKgFDe7A2gS0iGhiYLzOU58i4hAiZdhGRNSDgYMD2ikwmKWOiK6aKGtp7VI7ktZL2i/pwRmOv0fS/WX6tqTTGo49KukBSTskbatS7tTQIqJJh2cK3EixZsAXZzj+I+BNtp+W9DaKV429vuH4WbYPVM0sAS0imkmdXCTlm+WLYWc6/u2GzXsp1t+ctwS0iGhiYLx6QFvR0hwcLV/qOh+XAl9rKco3JBn4fJX7JqBFxBRzaHIesD2y0PwknUUR0H6/YfeZtveWr/ffLOkHtr85233SKRARTSwxoaFKqRMk/S7wBeAi2z95sRz23vJzP3AbsLrdvRLQImKKTvVytiPpJOCrwHtt/0vD/pdKOnLyO3AeMG1PaaM0OSOiSTEOrTO9nJJupljucoWkMeCjwFIA29cDVwOvAP6mXHhpvGzCHgfcVu5bAvy97a+3yy8BLSKaSRwc6szUJ9uXtDn+fuD90+zfA5w29YrZJaBFRBMDEzO+db+/JaBFxBSZyxkRNaGO9WD2WgJaRDQZ5EVSqiw0fLik70j6vqSdkj5e7j9F0n2SHpH0ZUmHdb+4EdF1KtYUqJL6TZUa2nPA2bZ/IWkp8C1JXwM+DHza9gZJ11OM8v1cF8saXaSJO6bd76EL2p4/0zkxmIwY12C+4LFtDc2FX5SbS8tk4Gzg1nL/TcA7ulLCiOg5S5VSv6n05E/SsKQdwH5gM/BD4Bnb4+UpY8DxM1y7TtI2Sdue5clOlDkiumjyGVovZgp0WqWAZvug7dMpXu2xGjh1utNmuHbU9ojtkSM4Zv4ljYiemUCVUr+ZUy+n7Wck3QO8AVgmaUlZSzsB2NuF8kWPVHlWFocGD/CwjSq9nMdIWlZ+/w3gLcAu4G7gneVpa4Hbu1XIiOitOtfQVgI3SRqmCIC32L5D0kPABkn/A/gecEMXyxkRPWLBCwNaQ2sb0GzfD5wxzf49VHg/UfSvmZqTHx16+4vfPzY07aPRNEVrrGhy9l/tq4rMFIiIKdyHzckqEtAiYopB7RRIQDvEVBnh/7GGETiZEXDoyeuDIqJGxPiAvp0/AS0imhj6cuJ5FYMZhmPReeiCF1PUT6fGoUlaL2m/pGkXOFHhryXtlnS/pNc1HFtbvs3nEUlrq5Q7AS0imhgxwVClVMGNwJpZjr8NWFWmdZRv7JF0NMWCKq+nGB72UUnL22WWgBYRUxhVSm3vUywM/NQsp1wEfLF8q8+9FFMqVwJvBTbbfsr20xQvxZgtMAJ5hnbImeuczbk2KVvvkybpYJpDL+cKSdsatkdtj84hq+OBxxu2J9/cM9P+WSWgRUQTw1x6OQ+U62jO13SR07Psn1WanBHRxIiDFVMHjAEnNmxPvrlnpv2zSg0tgIW9ajvzOuunh1OfNgKXS9pA0QHwU9v7JN0J/M+GjoDzgKva3SwBLSKm6NRMAUk3A2+meNY2RtFzuRTA9vXAJuB8YDfwLPC+8thTkj4BbC1vdY3t2ToXgAS0iGhh4KA7E9BsX9LmuIHLZji2Hlg/l/wS0A5hjU3FxlcGfXzin1783tjM7FRPaPS/zOWMiFooOgUGcxm7BLSImGKiQ03OXktAO4Q1NhUbXxk002Ce5vPV8L3t8KAYIIZODcnouQS0iGghnBpaRNRBXvAYtTXTwNo0M+vLhhc8mJOIEtAiYopBbXJWDsOShiV9T9Id5fYpku4rX772ZUmHda+YEdE71V7u2I/N0rnUK6+gWDF90l8Cn7a9CngauLSTBYuIxWGKYRtVUr+pFNAknQD8B+AL5baAs4Fby1NuAt7RjQLG4sqrtg9NB61Kqd9UfYb2V8CfAUeW268AnrE9Xm7P+PI1SesoXq3LUZw0/5JGRM8M6kLDbWtoki4A9tve3rh7mlOn7fayPWp7xPbIERwzz2JGRK/Y4oWJoUqp31SpoZ0JXCjpfOBw4OUUNbZlkpaUtbRKL1+LiP5XPENb7FLMT9sQa/sq2yfYPhm4GLjL9nuAu4F3lqetBW7vWikjoqdsVUr9ZiF1xo8AH5a0m+KZ2g2dKVJELKZB7uWc08Ba2/cA95Tf91CslxcRNdOPY8yqyEyBiGjSyTfW9loCWkQ0szh4sHM9mJLWAJ8BhoEv2L625fingbPKzSOAY20vK48dBB4ojz1m+8LZ8kpAi4gmnayhSRoGrgPOpRivulXSRtsPvZif/ScN538QOKPhFr+yfXrV/PpvIElELC53tFNgNbDb9h7bzwMbgItmOf8S4Ob5Fj0BLSKmmMOwjRWStjWkdS23Oh54vGF7tllFrwJOAe5q2H14ed97JbWdXpkmZ0Q0MXMaknHA9sgsxyvPKqIY53qr7YMN+06yvVfSq4G7JD1g+4czZZaAFhFNbHjhYMd6OceAExu2Z5tVdDEta3Ta3lt+7pF0D8XztRkDWpqcETFFB2cKbAVWle9PPIwiaG1sPUnSa4DlwD837Fsu6SXl9xUU0zAfar22UWpoETFFp2YB2B6XdDlwJ8WwjfW2d0q6BthmezK4XQJsKFdSn3Qq8HlJExSVr2sbe0enk4AWEU0MHJzo3MBa25uATS37rm7Z/tg0130b+J255JWAFhHN+nSeZhUJaBHRxIAnFrsU85OAFhHNDOMdnPrUSwloEdEkk9MjolbcwU6BXkpAi4gmg/wK7gS0iGhmdXTYRi8loEVEE0NH34fWSwloEdHMMJFhGxFRBwYm0uSMiFpwZ6c+9VICWkQ0Map3DU3So8DPgYPAuO0RSUcDXwZOBh4F/pPtp7tTzIjopUGd+jSXroyzbJ/e8HbKK4EttlcBW8rtiBhwxQsehyqlfrOQEl0E3FR+vwlo+77viBgMExPVUr+pGtAMfEPS9oZFEI6zvQ+g/Dx2ugslrZtcQOFZnlx4iSOiu1xMfaqS+k3VToEzy4UKjgU2S/pB1QxsjwKjAK/UyIBOqIg4dNR+2EbDQgX7Jd1GsdbeE5JW2t4naSWwv4vljIheMRzsw+ZkFW2bnJJeKunIye/AecCDFAsdrC1PWwvc3q1CRkTvTA7bqJKqkLRG0sOSdkua0nko6Y8kPSlpR5ne33BsraRHyrS29dpWVWpoxwG3SZo8/+9tf13SVuAWSZcCjwHvqvTTRURfs2H8hc40OSUNA9cB51IsabdV0sZpFjv5su3LW649GvgoMELREt5eXjvj8LC2Ac32HuC0afb/BDin3fURMXg6+AxtNbC7jCNI2kAxQmLW1ZtKbwU2236qvHYzsAa4eaYL+m8gSUQsLs9p2MaKyVEMZVrXcrfjgccbtsfKfa3+o6T7Jd0qaXJh4qrXvihTnyJiClWsoRkONAy2n/ZW01/W5J+Am20/J+kDFONaz654bZPU0CKimWH4oCqlCsaAExu2TwD2NmVn/8T2c+Xm3wK/V/XaVgloEdFEFkvGq6UKtgKrJJ0i6TDgYooREr/Orxj2NelCYFf5/U7gPEnLJS2nGGFx52yZpckZEVPoYGfuY3tc0uUUgWgYWG97p6RrgG22NwL/TdKFwDjwFPBH5bVPSfoERVAEuGayg2AmCWgR0USG4Q7OFLC9CdjUsu/qhu9XAVfNcO16YH3VvBLQImKKoQGdKZCAFhFNZBiq9sC/7ySgRcQUVYdt9JsEtIhoIoulHZr61GsJaBHRzDDUoV7OXktAi4gmIk3OiKgLw3BqaBFRByLDNiKiLjJsIyLqQoYl6eWMiLpIL2dE1IIMQ+nljIi66NTbNnotAS0imrnyyxv7TgJaRDQpOgUWuxTzk4AWEc0MGtAaWqVXcEtaVq7G8gNJuyS9UdLRkjaXC4BuLl+RGxEDThQzBaqkflN1TYHPAF+3/VsUa3TuAq4EttheBWwptyNi0JWT06ukftM2oEl6OfAHwA0Atp+3/QzFYqE3lafdBLyjW4WMiN4RxUyBKqnS/aQ1kh6WtFvSlIqPpA9Leqhcl3OLpFc1HDsoaUeZNrZe26rKM7RXA08CfyfpNGA7cAVwnO19ALb3STp2hh9mHbAO4ChOqpBdRCwqgzo0l1PSMHAdcC7FsnRbJW203bhy+veAEdvPSvpj4H8B/7k89ivbp1fNr0qTcwnwOuBzts8Afskcmpe2R22P2B45gmOqXhYRi0SGpc+rUqpgNbDb9h7bzwMbKFp3L7J9t+1ny817KdbfnJcqAW0MGLN9X7l9K0WAe2JyPb3yc/98CxERfaSzz9COBx5v2B4r983kUuBrDduHS9om6V5JbR9rtW1y2v5XSY9Leo3th4FzgIfKtBa4tvy8vd29IqL/Fc/QKp++QtK2hu1R26Mtt2vlafOV/gswArypYfdJtvdKejVwl6QHbP9wpsJUHYf2QeBL5crHe4D3UdTubpF0KfAY8K6K94qIfja31wcdsD0yy/Ex4MSG7ROAva0nSXoL8BfAm2w/92JR7L3l5x5J9wBnAAsLaLZ3UETOVudUuT4iBscca2jtbAVWSToF+DFwMfDupvykM4DPA2ts72/Yvxx41vZzklYAZ1J0GMwoMwUiolkHF0mxPS7pcuBOYBhYb3unpGuAbbY3Ap8EXgb8gySAx2xfCJwKfF7SBEWL8NqW3tEpEtAiooksllTrwazE9iZgU8u+qxu+v2WG674N/M5c8kpAi4hmWcYuIupCCWgRUScJaBFRC8qqTxFRG4Ylzy92IeYnAS0imuQZWkTUSgJaRNRCnqFFRK2khhYR9ZBnaBFRF0ovZ0TURXo5I6I+DEPji12I+UlAi4gp0ssZEbWQJmdE1EoCWkTUgibSyxkRNZIaWkTUwiA/Q6uy0HBEHErKYRtVUhWS1kh6WNJuSVdOc/wlkr5cHr9P0skNx64q9z8s6a3t8kpAi4gmk8vYdWLldEnDwHXA24DXApdIem3LaZcCT9v+TeDTwF+W176WYtm73wbWAH9T3m9GCWgR0ayc+lQlVbAa2G17j+3ngQ3ARS3nXATcVH6/FThHxXp2FwEbbD9n+0fA7vJ+M+rpM7R9bD/wcfRL4EAv822wYhHzXuz8k/ehkferFnqDfWy/82NoRcXTD5e0rWF71PZow/bxwOMN22PA61vu8eI55TqePwVeUe6/t+Xa42crTE8Dmu1jJG1rs3R81yxm3oudf/I+tPJeCNtrOni76aYcuOI5Va5tkiZnRHTTGHBiw/YJwN6ZzpG0BDgKeKritU0S0CKim7YCqySdIukwiof8G1vO2QisLb+/E7jLtsv9F5e9oKcAq4DvzJbZYoxDG21/Si3zXuz8k/ehlXdfKJ+JXQ7cCQwD623vlHQNsM32RuAG4H9L2k1RM7u4vHanpFuAh4Bx4DLbs/atqgiEERGDL03OiKiNBLSIqI2eBrR2UyA6nNd6SfslPdiw72hJmyU9Un4u71LeJ0q6W9IuSTslXdGr/CUdLuk7kr5f5v3xcv8p5bSSR8ppJod1Ou+GMgxL+p6kO3qZt6RHJT0gacfk2Kge/s2XSbpV0g/Kv/sbe5V3/FrPAlrFKRCddCPFdIlGVwJbbK8CtpTb3TAO/KntU4E3AJeVP2sv8n8OONv2acDpwBpJb6CYTvLpMu+nKaabdMsVwK6G7V7mfZbt0xvGf/Xqb/4Z4Ou2fws4jeLn71XeMcl2TxLwRuDOhu2rgKu6nOfJwIMN2w8DK8vvK4GHe/Sz3w6c2+v8gSOA71KMzD4ALJnub9HhPE+g+I/3bOAOisGRvcr7UWBFy76u/86BlwM/ouxkW+x/b4dy6mWTc7opELNOY+iC42zvAyg/j+12huWbA84A7utV/mWTbwewH9gM/BB4xvbk+xG6+bv/K+DPgIly+xU9zNvANyRtl7Su3NeL3/mrgSeBvyub2l+Q9NIe5R0NehnQ5jyNYdBJehnwFeBDtn/Wq3xtH7R9OkVtaTVw6nSndTpfSRcA+21vb9zdi7xLZ9p+HcVjjcsk/UGX8mm1BHgd8DnbZwC/JM3LRdHLgDbnaQxd8ISklQDl5/5uZSRpKUUw+5Ltr/Y6fwDbzwD3UDzHW1ZOK4Hu/e7PBC6U9CjFWxXOpqix9SJvbO8tP/cDt1EE8178zseAMdv3ldu3UgS4nv69o7cBrcoUiG5rnGKxluLZVseVrz65Adhl+1O9zF/SMZKWld9/A3gLxQPquymmlXQtb9tX2T7B9skUf9+7bL+nF3lLeqmkIye/A+cBD9KD37ntfwUel/Sactc5FKPbe/LvLRr08oEdcD7wLxTPdP6iy3ndDOwDXqD4P+ilFM9ztgCPlJ9Hdynv36doVt0P7CjT+b3IH/hd4Htl3g8CV5f7X00xD2438A/AS7r8+38zcEev8i7z+H6Zdk7+++rh3/x0YFv5e/9HYHmv8k76dcrUp4iojcwUiIjaSECLiNpIQIuI2khAi4jaSECLiNpIQIuI2khAi4ja+P806N8TA1CNMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    vis.molecule_visualization2D(np.expand_dims(train_set[i][0].numpy().sum(axis=0).sum(axis=0),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fa551c7eca8>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[[ 5.1371e-02,  3.8120e-02,  5.9627e-02],\n",
      "           [-5.5404e-02, -1.4286e-02, -9.0682e-03],\n",
      "           [-6.1930e-03,  4.4690e-02,  8.6186e-03]],\n",
      "\n",
      "          [[ 6.2113e-02, -1.7580e-02, -7.3847e-02],\n",
      "           [ 6.9574e-02,  3.9660e-02, -7.1597e-02],\n",
      "           [-1.9775e-02, -5.6930e-02,  3.1993e-02]],\n",
      "\n",
      "          [[ 6.2652e-02, -4.4029e-02, -6.2310e-02],\n",
      "           [-5.3702e-03, -6.9392e-02,  2.4187e-02],\n",
      "           [ 7.1772e-02, -2.4553e-02, -3.8751e-02]]],\n",
      "\n",
      "\n",
      "         [[[-3.4688e-03,  2.0741e-02,  6.1599e-02],\n",
      "           [ 2.7281e-02, -4.8626e-02, -5.5101e-02],\n",
      "           [ 2.1256e-02,  2.6283e-02, -7.0569e-02]],\n",
      "\n",
      "          [[-5.9242e-02,  6.8024e-02,  5.2904e-03],\n",
      "           [ 3.1777e-02, -3.3375e-02, -3.7471e-02],\n",
      "           [ 7.9789e-03,  4.7791e-02,  4.6652e-03]],\n",
      "\n",
      "          [[-7.0653e-03, -5.3583e-02,  6.6849e-02],\n",
      "           [-9.3354e-03,  9.9189e-03, -4.0019e-02],\n",
      "           [-1.7245e-02,  6.1240e-02, -1.1306e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 3.1290e-03,  1.6332e-02,  2.8929e-03],\n",
      "           [ 1.4548e-02,  1.7782e-02,  5.5001e-03],\n",
      "           [-6.4641e-02, -4.4329e-02, -7.3041e-02]],\n",
      "\n",
      "          [[-6.2714e-02,  2.5984e-02, -3.1542e-02],\n",
      "           [-2.6704e-02, -5.3637e-02, -6.8250e-02],\n",
      "           [ 1.9146e-02,  2.8923e-02,  1.5351e-02]],\n",
      "\n",
      "          [[ 3.2480e-02, -6.2029e-02, -5.0182e-02],\n",
      "           [-6.4931e-02, -2.2181e-02,  6.3511e-02],\n",
      "           [ 6.3089e-02, -6.2977e-02, -8.7559e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 3.7961e-02, -5.8066e-03, -6.1912e-03],\n",
      "           [ 5.4105e-02,  3.4775e-02,  7.0955e-02],\n",
      "           [ 3.0165e-02,  5.6440e-02,  7.3122e-03]],\n",
      "\n",
      "          [[ 6.6804e-04, -1.3385e-02,  5.9576e-02],\n",
      "           [ 2.5163e-02, -1.9136e-02,  3.2093e-02],\n",
      "           [-5.9867e-02,  1.3102e-02, -3.2185e-02]],\n",
      "\n",
      "          [[-3.2142e-03,  4.6481e-03, -6.2269e-02],\n",
      "           [-5.2645e-02,  2.1400e-02,  5.3864e-02],\n",
      "           [ 6.5554e-03,  5.7208e-02,  5.4478e-02]]],\n",
      "\n",
      "\n",
      "         [[[-1.7636e-02,  6.3714e-02,  5.7523e-02],\n",
      "           [ 1.6814e-02,  6.4132e-02, -2.0149e-02],\n",
      "           [-2.9827e-02,  4.2016e-02,  1.8945e-02]],\n",
      "\n",
      "          [[-3.7290e-02, -1.5218e-03, -2.0675e-02],\n",
      "           [-3.5742e-02,  5.7620e-02, -5.6652e-02],\n",
      "           [-6.4428e-03, -6.1075e-02,  1.3264e-02]],\n",
      "\n",
      "          [[ 1.7704e-03,  4.8024e-02, -4.4187e-02],\n",
      "           [-4.2906e-02,  1.9884e-02, -5.4052e-02],\n",
      "           [ 2.7947e-02, -1.2049e-02, -4.2656e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 5.5112e-02,  4.9208e-02, -3.6338e-02],\n",
      "           [-9.8013e-04,  7.2733e-02, -3.6904e-02],\n",
      "           [-3.2143e-02, -3.0872e-02, -4.2597e-02]],\n",
      "\n",
      "          [[ 1.1305e-02,  4.6599e-02,  2.0795e-02],\n",
      "           [-2.6889e-02,  8.7994e-03,  5.3977e-02],\n",
      "           [-3.4283e-02, -2.0079e-02, -5.5841e-02]],\n",
      "\n",
      "          [[-7.2400e-02,  7.0915e-02, -8.8965e-03],\n",
      "           [-3.3884e-02,  4.4180e-02, -4.4930e-02],\n",
      "           [ 2.9764e-02,  6.1521e-02,  4.2767e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-2.3640e-02,  1.3032e-02, -2.1995e-02],\n",
      "           [ 3.6913e-02,  1.3313e-03, -4.1221e-03],\n",
      "           [-5.4514e-03, -5.8501e-02, -1.0089e-02]],\n",
      "\n",
      "          [[ 1.3691e-02, -6.1088e-03, -2.5317e-02],\n",
      "           [-7.1530e-02, -9.4846e-05, -5.7121e-02],\n",
      "           [ 3.3481e-02, -5.3482e-02, -5.4274e-02]],\n",
      "\n",
      "          [[-5.7607e-02,  5.6750e-02,  1.4227e-03],\n",
      "           [ 4.7958e-02,  3.4942e-02, -1.7256e-02],\n",
      "           [ 6.1988e-02,  4.8161e-02, -2.7656e-02]]],\n",
      "\n",
      "\n",
      "         [[[-4.7686e-02,  5.7466e-02,  7.2759e-02],\n",
      "           [ 5.0378e-02,  3.9596e-02, -6.6878e-02],\n",
      "           [ 1.1561e-02, -3.3878e-02, -2.0676e-02]],\n",
      "\n",
      "          [[ 1.5513e-02, -6.7190e-02, -2.2548e-02],\n",
      "           [-2.4303e-02, -5.4953e-02,  8.1849e-03],\n",
      "           [ 5.4656e-02,  5.1485e-02,  1.1432e-02]],\n",
      "\n",
      "          [[-3.8066e-02, -1.3893e-02,  6.0566e-02],\n",
      "           [-5.1786e-02, -3.2465e-03, -2.6007e-02],\n",
      "           [-5.7800e-02,  3.7657e-02,  6.9276e-02]]],\n",
      "\n",
      "\n",
      "         [[[-2.2654e-02,  7.6274e-02,  7.5247e-02],\n",
      "           [-4.8237e-02,  6.2736e-02,  7.0574e-02],\n",
      "           [-8.0346e-03, -1.0101e-02,  7.0882e-02]],\n",
      "\n",
      "          [[-1.9121e-02, -1.0061e-02, -7.4899e-02],\n",
      "           [ 5.5775e-02,  6.1496e-03,  1.4311e-02],\n",
      "           [ 4.8510e-03,  4.4621e-02, -3.6315e-02]],\n",
      "\n",
      "          [[ 2.7515e-02, -3.5511e-02,  4.4572e-02],\n",
      "           [-1.0639e-02, -4.6605e-02, -5.8425e-02],\n",
      "           [ 4.1687e-02, -4.2518e-03,  3.6052e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 3.0590e-02, -6.6424e-02,  7.3282e-02],\n",
      "           [ 8.0133e-04, -7.1805e-02,  3.5540e-02],\n",
      "           [-5.5217e-02, -3.3414e-02,  5.7444e-02]],\n",
      "\n",
      "          [[ 1.9044e-02,  6.0908e-02,  6.8449e-02],\n",
      "           [-5.4493e-02, -2.7403e-02,  2.2959e-02],\n",
      "           [ 4.7872e-02, -3.3590e-02,  2.0598e-02]],\n",
      "\n",
      "          [[-5.0281e-02, -5.2998e-02,  1.5401e-02],\n",
      "           [-5.7102e-02, -1.8390e-02, -3.6663e-02],\n",
      "           [-5.4561e-03,  7.2518e-02, -7.0720e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 6.5668e-02,  6.3895e-02,  3.4463e-02],\n",
      "           [ 3.7201e-02, -6.9737e-02, -4.7390e-02],\n",
      "           [ 6.9574e-02,  4.3921e-02,  6.5587e-03]],\n",
      "\n",
      "          [[-1.2096e-02, -6.8598e-02,  3.3270e-04],\n",
      "           [ 7.4856e-02,  2.0268e-02,  1.0060e-02],\n",
      "           [ 7.1019e-02,  5.5940e-02, -3.6396e-02]],\n",
      "\n",
      "          [[ 4.1440e-02, -5.8251e-02,  4.4095e-02],\n",
      "           [ 2.4414e-03, -6.4332e-02,  5.7054e-02],\n",
      "           [-5.1592e-02,  1.7763e-02,  4.3896e-02]]],\n",
      "\n",
      "\n",
      "         [[[-3.5620e-02,  3.8922e-02,  1.7523e-02],\n",
      "           [ 2.1815e-02,  2.4580e-02, -4.5641e-02],\n",
      "           [ 4.4491e-02, -2.6054e-02,  7.0797e-02]],\n",
      "\n",
      "          [[-4.3483e-02,  5.6601e-02,  7.3840e-02],\n",
      "           [-2.6406e-02,  2.0718e-02, -1.0080e-02],\n",
      "           [ 3.4623e-02,  5.2247e-03,  3.1038e-02]],\n",
      "\n",
      "          [[-5.2639e-02, -7.2122e-02, -2.1091e-02],\n",
      "           [ 3.5599e-02, -5.3251e-03, -4.0331e-02],\n",
      "           [ 7.5842e-02, -1.7894e-02, -7.5274e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-4.9759e-02,  2.7244e-02,  7.0203e-02],\n",
      "           [-3.7064e-02,  6.8845e-02, -4.4261e-02],\n",
      "           [-6.6700e-02,  4.1613e-02,  2.3555e-02]],\n",
      "\n",
      "          [[-1.6452e-02, -5.6160e-02, -7.5144e-02],\n",
      "           [ 5.2857e-03, -1.4826e-02,  7.1992e-03],\n",
      "           [-2.1764e-02,  2.3018e-02, -4.2210e-02]],\n",
      "\n",
      "          [[-4.1783e-02,  5.0140e-02,  4.4116e-03],\n",
      "           [ 3.6470e-02, -5.3234e-02, -6.6991e-02],\n",
      "           [-6.1117e-02,  6.1262e-02, -7.6026e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 4.5560e-02, -7.3154e-02,  7.0228e-02],\n",
      "           [ 8.0968e-04, -2.8033e-02, -3.3142e-03],\n",
      "           [-2.5824e-02,  6.0826e-02, -2.0942e-02]],\n",
      "\n",
      "          [[-6.4984e-03,  3.5835e-02,  1.4253e-02],\n",
      "           [-7.1806e-02,  4.4995e-02, -2.8754e-02],\n",
      "           [-8.2469e-03,  3.5363e-02, -6.3810e-02]],\n",
      "\n",
      "          [[-9.9740e-03, -3.0256e-02,  1.9360e-02],\n",
      "           [-6.5026e-03, -2.5632e-02, -6.5583e-02],\n",
      "           [ 2.8961e-02,  2.5871e-02,  4.9726e-03]]],\n",
      "\n",
      "\n",
      "         [[[-5.4407e-02,  1.7858e-02, -4.1094e-02],\n",
      "           [ 1.2400e-03,  3.4025e-02, -1.1768e-02],\n",
      "           [-2.8681e-02, -7.1857e-02,  5.9160e-02]],\n",
      "\n",
      "          [[ 5.6238e-02, -1.7328e-02,  6.8396e-02],\n",
      "           [ 3.1967e-02, -3.9937e-02,  6.1968e-02],\n",
      "           [ 6.5048e-02, -4.3351e-03, -1.2568e-02]],\n",
      "\n",
      "          [[-1.6750e-02, -2.1435e-02,  1.0108e-02],\n",
      "           [ 2.3162e-02, -9.9546e-03, -2.8290e-02],\n",
      "           [ 2.4353e-02,  2.8528e-02, -8.1634e-03]]],\n",
      "\n",
      "\n",
      "         [[[-3.4799e-02,  7.2076e-02,  2.6469e-03],\n",
      "           [ 7.5491e-02, -4.2058e-02, -8.9293e-04],\n",
      "           [-5.2529e-02,  5.7252e-02, -6.6792e-02]],\n",
      "\n",
      "          [[ 3.0180e-03, -5.8283e-03, -3.2766e-02],\n",
      "           [ 2.8435e-02, -1.2991e-02, -5.5950e-02],\n",
      "           [ 2.1761e-02,  6.6281e-03,  4.4229e-02]],\n",
      "\n",
      "          [[-3.2260e-02, -5.3653e-04, -1.3608e-02],\n",
      "           [-6.6843e-02, -1.1601e-02,  3.7314e-02],\n",
      "           [ 9.7988e-03, -5.1031e-03,  5.1271e-02]]],\n",
      "\n",
      "\n",
      "         [[[-7.4732e-02, -3.9829e-02, -6.2105e-03],\n",
      "           [ 1.0364e-03, -2.5126e-02,  1.6140e-02],\n",
      "           [-6.0252e-02, -4.0536e-02, -7.3146e-02]],\n",
      "\n",
      "          [[ 6.0829e-02, -3.0452e-02, -6.8085e-02],\n",
      "           [-4.3055e-02,  2.0856e-02, -7.2837e-02],\n",
      "           [-6.6699e-02, -4.3278e-02,  3.5923e-02]],\n",
      "\n",
      "          [[ 6.6085e-02, -7.0568e-02,  1.5056e-02],\n",
      "           [ 1.7455e-02,  2.7433e-02,  5.1454e-02],\n",
      "           [-6.3687e-03, -4.7335e-02, -6.1903e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 4.3567e-02, -1.2806e-04, -6.4995e-02],\n",
      "           [ 6.0664e-02, -1.1648e-02,  1.9390e-03],\n",
      "           [ 7.2550e-02,  2.5610e-02, -9.9928e-03]],\n",
      "\n",
      "          [[-4.3748e-02, -4.8894e-02, -3.3126e-02],\n",
      "           [-3.2715e-02,  3.1179e-02,  1.1890e-02],\n",
      "           [ 7.5558e-02,  6.8151e-02, -2.7766e-03]],\n",
      "\n",
      "          [[-1.1008e-02, -2.0360e-02,  6.8571e-02],\n",
      "           [-4.8887e-02,  2.4110e-02, -4.4764e-03],\n",
      "           [ 9.6534e-03,  6.0684e-02, -4.6544e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 2.7656e-02, -6.1464e-02,  6.9386e-02],\n",
      "           [-7.2123e-02,  2.8081e-02,  3.4765e-03],\n",
      "           [-3.5685e-02,  2.6414e-02,  1.8880e-02]],\n",
      "\n",
      "          [[ 6.9366e-03,  6.5207e-02, -4.7608e-02],\n",
      "           [-2.2300e-02, -1.8460e-03, -4.6036e-02],\n",
      "           [-6.7439e-02, -1.0697e-02,  3.3235e-02]],\n",
      "\n",
      "          [[ 7.6196e-02, -9.4940e-03, -5.7739e-02],\n",
      "           [ 6.2046e-02, -3.2037e-05, -1.5492e-02],\n",
      "           [-3.8137e-02, -5.0095e-02, -1.0102e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 2.2400e-02,  5.6034e-02, -5.4008e-02],\n",
      "           [ 8.3174e-03, -7.5511e-03,  7.6444e-02],\n",
      "           [-2.0093e-02, -4.9426e-02,  1.5926e-02]],\n",
      "\n",
      "          [[-5.9488e-02, -5.7199e-02,  1.4057e-03],\n",
      "           [ 1.5579e-02,  1.5421e-02,  1.4001e-03],\n",
      "           [-2.2189e-02,  6.4261e-02,  4.9461e-02]],\n",
      "\n",
      "          [[ 1.4261e-02,  1.8148e-02, -5.5067e-02],\n",
      "           [ 2.8746e-03, -3.4977e-02,  4.6979e-02],\n",
      "           [-7.1826e-02, -1.9609e-02, -4.0516e-02]]],\n",
      "\n",
      "\n",
      "         [[[-7.3116e-03, -4.7048e-02, -3.6331e-02],\n",
      "           [-5.4764e-02,  6.8267e-04, -4.2538e-03],\n",
      "           [ 1.2974e-02, -7.0359e-02, -2.0297e-02]],\n",
      "\n",
      "          [[-5.1547e-02,  5.3439e-02, -6.9755e-02],\n",
      "           [-4.3015e-02,  4.9723e-02,  6.2657e-02],\n",
      "           [-6.7991e-02,  3.0162e-02, -2.1931e-03]],\n",
      "\n",
      "          [[ 1.7446e-02,  4.6900e-02,  1.9275e-02],\n",
      "           [-7.6400e-02, -5.4101e-02, -1.1712e-02],\n",
      "           [-4.1324e-02, -7.4063e-02, -4.6322e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 2.7991e-03,  5.1063e-02, -5.9443e-03],\n",
      "           [-2.5038e-02, -5.8206e-02,  6.9699e-02],\n",
      "           [-5.5521e-02, -6.6101e-02,  1.4592e-02]],\n",
      "\n",
      "          [[-3.1295e-02, -3.4812e-02, -5.5292e-02],\n",
      "           [ 1.2870e-02,  2.8189e-03, -3.0699e-04],\n",
      "           [ 3.9075e-02, -3.4600e-02, -4.9398e-02]],\n",
      "\n",
      "          [[ 5.9552e-02,  2.8512e-02, -7.6264e-02],\n",
      "           [-2.4220e-02, -4.7888e-02, -6.3957e-02],\n",
      "           [-6.6082e-03, -1.6264e-02,  1.8668e-02]]],\n",
      "\n",
      "\n",
      "         [[[-4.5377e-02,  1.0562e-02,  5.4776e-02],\n",
      "           [-7.0557e-02,  1.1092e-02, -3.5248e-02],\n",
      "           [ 6.6618e-02,  5.1577e-04, -1.9302e-02]],\n",
      "\n",
      "          [[ 6.8782e-02,  2.5986e-02, -3.9406e-02],\n",
      "           [-5.6729e-02,  2.3459e-02,  5.9632e-03],\n",
      "           [-3.4007e-02, -2.6613e-02,  6.1072e-02]],\n",
      "\n",
      "          [[ 3.9344e-03, -6.7168e-02, -2.3517e-02],\n",
      "           [ 8.5967e-03, -1.0419e-02, -6.8154e-02],\n",
      "           [-6.4773e-02,  4.1567e-02, -5.4859e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 3.3144e-02, -3.5666e-02,  4.8431e-02],\n",
      "           [-5.5712e-02,  5.4802e-02,  7.1342e-02],\n",
      "           [-4.9727e-03, -3.9154e-02,  4.9108e-02]],\n",
      "\n",
      "          [[-5.9662e-02,  2.0783e-02, -3.1844e-02],\n",
      "           [-3.9475e-02,  2.7011e-03, -3.8235e-02],\n",
      "           [ 6.5693e-03,  3.9350e-02, -6.0096e-02]],\n",
      "\n",
      "          [[-6.8989e-02, -1.5742e-02, -1.1964e-02],\n",
      "           [-6.0081e-02, -5.4726e-02, -7.4735e-02],\n",
      "           [-1.8927e-02, -4.3870e-02,  6.6433e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-4.4389e-02,  5.0625e-02, -6.4516e-02],\n",
      "           [ 3.9865e-02, -1.0047e-02, -1.8854e-02],\n",
      "           [ 2.3209e-02, -7.0996e-02,  4.6068e-03]],\n",
      "\n",
      "          [[-5.0806e-02,  6.8704e-02, -6.3779e-03],\n",
      "           [ 4.1349e-02, -6.3133e-02, -7.3277e-02],\n",
      "           [ 4.4125e-02,  2.4334e-02, -7.4405e-02]],\n",
      "\n",
      "          [[-1.0142e-02,  2.8519e-02, -2.9371e-02],\n",
      "           [ 6.2994e-02, -1.1294e-02, -4.4321e-02],\n",
      "           [-2.0812e-02, -1.0134e-02,  3.1529e-02]]],\n",
      "\n",
      "\n",
      "         [[[-3.4505e-02,  2.1751e-02,  1.6984e-03],\n",
      "           [ 2.5165e-02, -3.1851e-02, -6.5268e-02],\n",
      "           [-1.1270e-02,  4.0144e-04, -3.7111e-02]],\n",
      "\n",
      "          [[-5.0117e-02,  5.7144e-02,  9.0941e-03],\n",
      "           [ 7.0857e-02, -2.8799e-03, -4.1840e-02],\n",
      "           [-7.3507e-02,  3.1919e-02,  4.2208e-02]],\n",
      "\n",
      "          [[-1.5283e-02,  3.6419e-02,  1.7578e-02],\n",
      "           [ 1.5502e-02, -3.0723e-02, -6.5245e-02],\n",
      "           [ 5.5706e-02, -4.1006e-02,  4.3738e-02]]],\n",
      "\n",
      "\n",
      "         [[[-1.1307e-02, -4.7022e-02,  7.4921e-03],\n",
      "           [ 2.0097e-02,  5.0187e-03,  4.8290e-02],\n",
      "           [-7.3580e-02,  4.4987e-02,  2.0920e-02]],\n",
      "\n",
      "          [[-4.5690e-02, -6.7278e-03,  7.6188e-03],\n",
      "           [-6.0421e-02,  1.7830e-02, -3.4529e-02],\n",
      "           [ 2.3920e-02, -1.1507e-03, -6.1548e-02]],\n",
      "\n",
      "          [[ 2.0760e-02,  6.8006e-02,  2.5126e-03],\n",
      "           [ 6.4240e-02,  1.8418e-02, -3.4015e-04],\n",
      "           [ 3.9029e-02,  2.2661e-03,  3.2095e-02]]],\n",
      "\n",
      "\n",
      "         [[[-5.6381e-02,  5.4141e-02,  6.3995e-03],\n",
      "           [-3.8961e-02,  7.1805e-02, -4.3374e-02],\n",
      "           [ 7.3115e-02, -6.1678e-03,  7.1789e-02]],\n",
      "\n",
      "          [[-6.5862e-02, -2.3283e-02,  3.5632e-02],\n",
      "           [-6.6672e-02, -6.8901e-03, -4.5548e-02],\n",
      "           [ 3.4394e-02,  5.3302e-02,  7.2302e-02]],\n",
      "\n",
      "          [[ 7.5429e-02,  4.0702e-02, -5.0441e-02],\n",
      "           [-4.3999e-02,  1.4751e-03,  6.4905e-02],\n",
      "           [-9.0257e-03, -5.3154e-02,  2.5663e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 6.5697e-02, -5.2316e-02, -4.0214e-02],\n",
      "           [-6.0185e-02,  1.3366e-02,  7.4476e-02],\n",
      "           [ 7.1879e-02, -6.1693e-02, -5.2861e-02]],\n",
      "\n",
      "          [[ 3.9351e-02,  5.9792e-02, -3.0597e-02],\n",
      "           [ 2.3516e-02,  1.4265e-02, -5.8177e-03],\n",
      "           [-6.1394e-02,  2.2118e-02,  7.0328e-02]],\n",
      "\n",
      "          [[ 1.5578e-02,  7.1892e-02, -1.0514e-02],\n",
      "           [ 2.9059e-02,  3.3380e-02,  3.1548e-02],\n",
      "           [ 3.3514e-02, -7.5423e-02,  2.4995e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 4.9000e-02,  5.4280e-02,  3.6996e-02],\n",
      "           [-3.9020e-02, -6.5241e-02,  5.9856e-02],\n",
      "           [-4.6242e-02, -6.4787e-02, -6.5125e-02]],\n",
      "\n",
      "          [[ 3.2425e-02, -1.0465e-02,  4.8832e-02],\n",
      "           [-3.0197e-02, -6.3735e-02, -2.7055e-02],\n",
      "           [-1.3909e-03, -4.6500e-02,  7.6263e-02]],\n",
      "\n",
      "          [[ 2.2294e-02,  1.9740e-02, -7.0918e-02],\n",
      "           [ 6.9310e-02, -2.6853e-02, -1.1474e-02],\n",
      "           [ 4.5429e-02,  2.8722e-03,  4.1936e-03]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 5.7978e-02, -6.8173e-02,  7.1282e-02],\n",
      "           [-7.4047e-02, -4.4957e-02,  1.4025e-02],\n",
      "           [-2.0757e-02,  3.5384e-02,  4.8947e-02]],\n",
      "\n",
      "          [[ 1.3545e-02, -7.3705e-02, -4.7230e-02],\n",
      "           [ 1.5764e-02,  4.8362e-02, -5.1789e-02],\n",
      "           [-5.7275e-02, -5.4119e-03, -7.4546e-02]],\n",
      "\n",
      "          [[-5.7646e-02, -3.5044e-02,  8.5529e-03],\n",
      "           [ 2.2380e-02, -1.8929e-02,  5.6047e-02],\n",
      "           [-7.0703e-03,  1.7948e-02, -3.1442e-02]]],\n",
      "\n",
      "\n",
      "         [[[-1.7974e-02, -7.0892e-02, -6.4040e-02],\n",
      "           [ 5.9546e-02,  1.4528e-02,  2.9930e-02],\n",
      "           [-2.4717e-02, -4.6171e-02, -3.4193e-02]],\n",
      "\n",
      "          [[-2.0116e-02,  5.0158e-02, -1.9466e-02],\n",
      "           [-6.0219e-02, -2.4427e-02, -4.9808e-02],\n",
      "           [-1.4938e-02,  5.4641e-02,  6.2527e-02]],\n",
      "\n",
      "          [[ 2.3560e-02,  3.2440e-03,  2.4209e-02],\n",
      "           [-5.5987e-02,  5.2007e-02, -5.0497e-02],\n",
      "           [ 3.8700e-02,  6.3904e-02, -6.6345e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 2.3317e-02,  6.4930e-02,  4.3732e-02],\n",
      "           [ 3.7180e-02, -1.2064e-02,  7.4210e-02],\n",
      "           [-5.1465e-02,  6.1729e-02, -5.4026e-02]],\n",
      "\n",
      "          [[ 1.4674e-03, -7.1751e-02, -2.5448e-02],\n",
      "           [ 7.1360e-02, -5.2245e-02,  6.2123e-05],\n",
      "           [-7.8920e-03, -4.7268e-02,  6.1919e-02]],\n",
      "\n",
      "          [[-5.5544e-02, -3.2689e-02,  9.0528e-03],\n",
      "           [-6.1584e-02,  6.4205e-02,  4.6070e-02],\n",
      "           [ 2.5735e-02, -7.3139e-03, -2.5732e-02]]],\n",
      "\n",
      "\n",
      "         [[[-6.5975e-03, -4.6363e-02,  5.4299e-02],\n",
      "           [-1.1877e-02, -7.0515e-02,  5.2412e-03],\n",
      "           [-4.6029e-02, -6.5441e-02, -4.1569e-02]],\n",
      "\n",
      "          [[-5.5629e-02, -1.6960e-02, -4.5139e-02],\n",
      "           [ 1.0619e-03,  5.8370e-02,  3.4947e-02],\n",
      "           [ 6.1699e-02,  4.0101e-02, -4.1382e-02]],\n",
      "\n",
      "          [[-2.7770e-02, -4.9825e-02, -3.0127e-02],\n",
      "           [-4.8517e-02, -1.3990e-03, -4.5313e-02],\n",
      "           [ 5.4743e-02, -2.2227e-02,  6.1515e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 3.2941e-02, -1.1878e-02, -2.0061e-02],\n",
      "           [-2.4563e-02,  6.5416e-02,  2.7544e-02],\n",
      "           [ 2.6232e-02, -2.2626e-02,  6.1028e-02]],\n",
      "\n",
      "          [[ 4.6282e-02, -4.7877e-02,  1.4737e-02],\n",
      "           [-4.0802e-02, -6.0846e-02,  3.5181e-02],\n",
      "           [-4.9249e-02,  1.6176e-02,  7.0421e-02]],\n",
      "\n",
      "          [[ 1.0632e-02, -2.6397e-02, -6.9701e-02],\n",
      "           [-5.4137e-02,  5.6535e-02, -6.3311e-03],\n",
      "           [ 9.6373e-03, -2.4939e-02, -5.4011e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 6.3496e-02, -3.4529e-02, -2.5848e-02],\n",
      "           [-3.7617e-02,  6.3316e-02, -4.2509e-02],\n",
      "           [ 6.5800e-02, -2.6765e-02, -1.8527e-02]],\n",
      "\n",
      "          [[ 3.3884e-02,  6.5770e-02, -5.1125e-02],\n",
      "           [-6.4679e-03,  3.4411e-02,  7.4229e-04],\n",
      "           [-3.2531e-02, -1.6052e-02,  4.8493e-02]],\n",
      "\n",
      "          [[ 3.9552e-02, -1.2117e-02, -2.5791e-02],\n",
      "           [ 2.3773e-02,  1.6285e-02,  1.0173e-02],\n",
      "           [ 1.2828e-02,  7.3417e-02, -7.3169e-02]]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0100, 0.0099, 0.0099, 0.0099, 0.0100, 0.0100, 0.0099, 0.0099, 0.0100,\n",
      "        0.0101, 0.0101, 0.0101, 0.0101, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0099, 0.0100, 0.0100, 0.0099, 0.0100, 0.0099, 0.0099, 0.0100,\n",
      "        0.0100, 0.0099, 0.0099, 0.0100, 0.0099], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[[ 4.5525e-02, -5.1784e-04,  4.4883e-02],\n",
      "           [ 2.2487e-02, -2.2590e-02,  7.9247e-03],\n",
      "           [ 2.0811e-02,  1.9854e-02, -9.4020e-03]],\n",
      "\n",
      "          [[-4.1328e-02,  3.7719e-02,  3.1699e-02],\n",
      "           [ 3.8693e-02,  2.7887e-02,  1.9378e-02],\n",
      "           [ 3.5009e-02, -3.9186e-02,  1.5806e-03]],\n",
      "\n",
      "          [[-2.6161e-02,  3.2345e-02, -3.2177e-02],\n",
      "           [-2.7184e-02, -5.2144e-03,  5.8812e-03],\n",
      "           [-1.3012e-02, -2.1136e-02, -4.2196e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 1.4905e-02, -4.4821e-02,  9.4401e-03],\n",
      "           [ 1.1702e-02, -2.7014e-02,  3.0729e-02],\n",
      "           [ 4.6421e-02,  4.5754e-02, -7.7914e-03]],\n",
      "\n",
      "          [[-4.3210e-02,  2.7752e-02,  1.6158e-02],\n",
      "           [-3.3934e-02, -2.2857e-03, -4.4771e-02],\n",
      "           [-1.0686e-02,  3.1919e-02,  6.9115e-03]],\n",
      "\n",
      "          [[ 4.0838e-02, -3.6909e-02, -5.7108e-03],\n",
      "           [-4.7316e-03, -1.5042e-02, -2.2304e-04],\n",
      "           [ 3.3399e-02, -3.4782e-02, -6.6554e-03]]],\n",
      "\n",
      "\n",
      "         [[[-1.8091e-02,  1.5809e-02,  3.3676e-02],\n",
      "           [-2.7169e-02,  9.8798e-03,  3.4171e-02],\n",
      "           [ 1.7493e-02, -3.8719e-02,  3.3242e-02]],\n",
      "\n",
      "          [[-3.4882e-02,  2.3342e-02, -3.3357e-02],\n",
      "           [ 2.2721e-02, -7.9291e-04, -2.8226e-02],\n",
      "           [-3.4625e-02,  2.7850e-02, -3.6461e-02]],\n",
      "\n",
      "          [[ 2.6916e-02,  1.0900e-02,  1.2567e-02],\n",
      "           [ 1.4004e-02,  5.3541e-03,  2.3248e-03],\n",
      "           [ 3.4835e-02,  4.7830e-02, -2.9860e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 3.1291e-02, -1.8892e-02, -1.6882e-03],\n",
      "           [ 4.5696e-02,  3.1945e-02, -9.7620e-03],\n",
      "           [ 4.0890e-02,  1.6572e-02, -2.7099e-02]],\n",
      "\n",
      "          [[-3.7699e-02, -3.5285e-02, -2.7450e-02],\n",
      "           [-6.4373e-03, -2.0008e-02, -3.1355e-02],\n",
      "           [ 2.2499e-02,  1.7875e-02,  3.4024e-02]],\n",
      "\n",
      "          [[ 4.4382e-03,  2.5345e-02,  2.2942e-02],\n",
      "           [-3.6346e-03,  1.0998e-02, -8.0236e-03],\n",
      "           [-1.6976e-02,  9.2486e-03,  2.3610e-03]]],\n",
      "\n",
      "\n",
      "         [[[-1.2408e-02, -3.2927e-02, -1.7630e-02],\n",
      "           [ 8.9131e-03,  2.6305e-02,  4.7552e-02],\n",
      "           [ 4.1529e-02,  1.2236e-02, -4.7068e-02]],\n",
      "\n",
      "          [[-6.8483e-03, -1.4154e-02, -3.0299e-02],\n",
      "           [ 8.1711e-03, -4.7524e-03,  1.1483e-02],\n",
      "           [ 1.7439e-02, -6.2927e-03,  1.7954e-02]],\n",
      "\n",
      "          [[-3.5765e-02, -3.2239e-02,  4.1750e-02],\n",
      "           [ 4.1960e-02, -4.5811e-02, -2.9579e-02],\n",
      "           [ 3.8747e-02,  9.2230e-03,  1.1179e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 4.0751e-02,  3.2340e-02, -3.7730e-02],\n",
      "           [-2.5362e-02,  9.1920e-03, -3.1183e-04],\n",
      "           [-2.4104e-02, -2.3522e-02,  1.1304e-02]],\n",
      "\n",
      "          [[ 4.8019e-02,  1.2161e-02, -2.4947e-02],\n",
      "           [-2.9775e-02, -3.7528e-03,  7.9165e-03],\n",
      "           [ 3.2684e-02,  1.2476e-02, -3.7644e-02]],\n",
      "\n",
      "          [[ 4.5160e-02, -3.7762e-02,  4.7378e-02],\n",
      "           [-2.7535e-02,  1.5169e-02, -4.3820e-02],\n",
      "           [-2.0683e-02,  3.7069e-02, -5.1430e-03]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 4.1231e-02, -2.2366e-02, -1.7323e-02],\n",
      "           [-1.2217e-02, -2.8719e-02, -4.8137e-03],\n",
      "           [ 2.6553e-02, -3.2918e-02, -3.5004e-02]],\n",
      "\n",
      "          [[ 4.7218e-02,  9.5780e-03, -3.9914e-02],\n",
      "           [-1.4094e-02, -2.1252e-02, -4.6431e-02],\n",
      "           [ 1.3488e-02, -1.4318e-02, -2.7320e-02]],\n",
      "\n",
      "          [[ 1.8105e-02,  1.6489e-02,  7.3627e-03],\n",
      "           [-4.0600e-02,  3.1447e-02,  4.1250e-02],\n",
      "           [-4.0497e-02, -1.3142e-02,  2.1895e-03]]],\n",
      "\n",
      "\n",
      "         [[[-4.4872e-02, -2.3745e-02, -1.1406e-02],\n",
      "           [-2.4823e-02,  1.6321e-02,  4.2247e-02],\n",
      "           [ 1.6420e-02,  6.1007e-03,  1.6754e-02]],\n",
      "\n",
      "          [[-2.7263e-02, -4.3037e-02,  6.0660e-03],\n",
      "           [-3.1125e-02, -2.0258e-02,  1.0415e-03],\n",
      "           [-4.2413e-02,  1.1862e-02,  2.7712e-02]],\n",
      "\n",
      "          [[-3.8591e-02,  2.1584e-02,  2.6909e-05],\n",
      "           [ 4.4168e-02, -1.5372e-02, -3.4344e-02],\n",
      "           [ 3.7631e-02, -3.3024e-02, -2.5440e-02]]],\n",
      "\n",
      "\n",
      "         [[[-4.1408e-02, -4.5991e-02, -3.9500e-02],\n",
      "           [ 2.8469e-02, -3.7896e-02, -2.4967e-02],\n",
      "           [ 2.7873e-02,  2.9333e-02,  3.7172e-02]],\n",
      "\n",
      "          [[-3.5200e-02, -1.1023e-02,  3.6470e-02],\n",
      "           [-4.5475e-02, -1.3764e-02, -4.5079e-02],\n",
      "           [ 4.0087e-02, -3.2315e-03,  4.4116e-02]],\n",
      "\n",
      "          [[ 4.1820e-02, -3.6901e-02, -3.4181e-02],\n",
      "           [-1.4425e-02,  1.8497e-02, -4.0028e-02],\n",
      "           [-4.2835e-02,  4.4738e-02,  2.2088e-03]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 3.6804e-02,  7.5012e-03, -1.6586e-02],\n",
      "           [-4.2702e-02,  1.4437e-02,  1.3293e-02],\n",
      "           [ 7.7514e-03,  2.3439e-02,  1.1621e-02]],\n",
      "\n",
      "          [[ 4.7331e-02,  1.5947e-02, -4.7141e-02],\n",
      "           [-2.0057e-02, -2.0568e-02,  8.7456e-03],\n",
      "           [-3.3092e-02,  2.9781e-02,  1.1086e-02]],\n",
      "\n",
      "          [[-7.2704e-03, -2.6582e-03,  3.6399e-02],\n",
      "           [ 3.6739e-02, -1.2560e-02, -3.3350e-02],\n",
      "           [ 4.7420e-02,  3.6967e-02, -7.1075e-04]]],\n",
      "\n",
      "\n",
      "         [[[-6.5961e-03,  1.2378e-02,  2.6860e-02],\n",
      "           [-3.4555e-02,  2.8191e-02, -2.4204e-02],\n",
      "           [-3.8229e-02, -1.6672e-02,  3.9628e-02]],\n",
      "\n",
      "          [[ 2.9401e-02,  3.6882e-03,  2.1398e-02],\n",
      "           [ 5.2544e-03,  8.1498e-04,  2.1493e-02],\n",
      "           [-4.7595e-02, -3.2419e-02, -2.2212e-02]],\n",
      "\n",
      "          [[ 3.5647e-02, -1.8953e-02,  2.5792e-02],\n",
      "           [-1.0512e-02,  2.6900e-02, -1.4274e-03],\n",
      "           [-2.3458e-02,  6.2624e-04, -2.1117e-02]]],\n",
      "\n",
      "\n",
      "         [[[-1.2196e-02, -3.0827e-03,  3.1774e-03],\n",
      "           [-1.5841e-02, -5.5781e-03,  2.5857e-02],\n",
      "           [ 5.8508e-03, -2.2593e-02, -3.0611e-02]],\n",
      "\n",
      "          [[ 2.9316e-02, -3.0021e-02,  3.0339e-03],\n",
      "           [ 2.2319e-02, -9.7577e-03, -2.8833e-02],\n",
      "           [-1.8138e-02, -4.2081e-02, -3.0413e-02]],\n",
      "\n",
      "          [[-2.6583e-02,  3.0919e-02,  1.1023e-02],\n",
      "           [ 4.2038e-02, -3.3744e-02, -8.7212e-03],\n",
      "           [-3.8343e-02, -1.2894e-02, -1.3497e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-4.6108e-02,  3.5410e-02, -2.1218e-02],\n",
      "           [-4.1469e-02, -4.3467e-02,  4.5185e-02],\n",
      "           [ 4.4789e-02, -4.1631e-02,  1.1227e-02]],\n",
      "\n",
      "          [[-8.8973e-03,  1.9917e-02, -3.2170e-03],\n",
      "           [ 3.2547e-03, -3.8478e-02, -3.6533e-02],\n",
      "           [-2.7142e-02, -1.2517e-02, -1.6611e-02]],\n",
      "\n",
      "          [[ 3.0162e-02, -1.8601e-02, -3.3216e-02],\n",
      "           [-2.5838e-02,  2.8296e-02,  3.1268e-03],\n",
      "           [-3.4882e-02, -4.4702e-02, -2.7251e-02]]],\n",
      "\n",
      "\n",
      "         [[[-4.1412e-02, -4.0956e-02, -1.5186e-02],\n",
      "           [ 2.5242e-02, -4.7316e-02, -6.3801e-03],\n",
      "           [-3.1245e-02, -3.4321e-02, -4.6916e-02]],\n",
      "\n",
      "          [[ 4.7396e-02, -2.5116e-02,  2.1937e-02],\n",
      "           [ 4.6831e-02,  2.4644e-02,  2.2663e-02],\n",
      "           [-3.1328e-02,  2.6525e-02, -3.9982e-02]],\n",
      "\n",
      "          [[-4.3302e-02,  4.1321e-02, -2.3925e-02],\n",
      "           [-3.6806e-03, -5.4280e-03,  4.1444e-02],\n",
      "           [-4.1940e-02, -3.3973e-02,  9.7549e-03]]],\n",
      "\n",
      "\n",
      "         [[[-4.2958e-02, -3.4389e-02,  3.2168e-02],\n",
      "           [ 3.9933e-02, -1.3487e-02, -1.3168e-02],\n",
      "           [-1.6313e-02, -5.1064e-03,  2.8334e-02]],\n",
      "\n",
      "          [[ 3.2368e-02, -4.3455e-03,  3.7639e-02],\n",
      "           [ 3.2937e-02,  1.8034e-02,  3.5151e-02],\n",
      "           [ 4.0242e-02, -2.6612e-03,  3.1719e-02]],\n",
      "\n",
      "          [[-4.4638e-02,  2.2489e-03,  4.5143e-02],\n",
      "           [ 3.9174e-02,  4.6627e-02, -2.7342e-02],\n",
      "           [-3.0526e-02,  1.4295e-02,  4.0306e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 8.7344e-03,  2.5258e-02, -4.7560e-02],\n",
      "           [ 1.2727e-02, -4.5675e-02,  2.9699e-02],\n",
      "           [-4.4606e-02,  1.6282e-02,  4.8260e-03]],\n",
      "\n",
      "          [[-2.6476e-02, -1.9007e-03,  2.8857e-02],\n",
      "           [-2.2454e-02, -2.7129e-02, -3.6442e-02],\n",
      "           [ 1.9923e-02, -4.2089e-02, -3.1571e-02]],\n",
      "\n",
      "          [[-2.9359e-02,  3.2500e-02, -2.9913e-02],\n",
      "           [-3.2523e-02,  4.2470e-02,  3.3315e-02],\n",
      "           [-1.0874e-02,  8.9582e-03, -2.8374e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 6.5996e-03,  3.3230e-02,  2.7959e-02],\n",
      "           [ 1.0237e-02, -4.8974e-03, -1.9451e-02],\n",
      "           [ 2.1695e-02, -2.4957e-02,  4.7153e-02]],\n",
      "\n",
      "          [[-2.9961e-02, -3.1161e-02,  1.4463e-02],\n",
      "           [-1.4479e-02,  4.5188e-02,  3.4069e-02],\n",
      "           [-2.8729e-02, -1.3279e-02, -2.7772e-02]],\n",
      "\n",
      "          [[-1.7228e-02,  2.7091e-02,  1.8678e-02],\n",
      "           [-4.7700e-02, -3.0661e-03, -8.1463e-03],\n",
      "           [-1.5733e-02, -1.0005e-02,  2.0581e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 3.9925e-02,  4.6229e-02,  2.7826e-02],\n",
      "           [ 4.2272e-03,  4.1209e-02,  2.0704e-02],\n",
      "           [ 3.0544e-03,  4.4800e-02,  1.7108e-03]],\n",
      "\n",
      "          [[-2.2066e-03, -2.7625e-02,  2.6896e-03],\n",
      "           [-3.3585e-02, -4.1152e-02, -2.5514e-02],\n",
      "           [-2.7068e-02, -1.1911e-02,  4.0535e-02]],\n",
      "\n",
      "          [[ 2.8619e-02, -4.5965e-02,  2.0191e-02],\n",
      "           [ 4.2689e-02,  4.2201e-02,  3.8267e-02],\n",
      "           [ 1.9966e-02,  1.0052e-03, -4.0349e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 3.5982e-02, -1.6790e-02, -4.4349e-02],\n",
      "           [ 4.1101e-02,  5.3139e-03, -4.5118e-02],\n",
      "           [-4.2435e-02,  1.7818e-02,  6.6145e-03]],\n",
      "\n",
      "          [[-2.2731e-02, -4.6847e-02, -3.4123e-02],\n",
      "           [-2.7638e-02, -4.7965e-02, -1.8888e-03],\n",
      "           [ 1.8090e-02, -4.3132e-02, -1.3513e-02]],\n",
      "\n",
      "          [[-2.6147e-02,  1.5779e-02, -3.2304e-02],\n",
      "           [ 2.0273e-02, -3.1693e-02, -4.7300e-02],\n",
      "           [ 5.6325e-03, -1.9739e-02, -3.9551e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 3.9381e-02,  2.1396e-03,  2.1785e-02],\n",
      "           [ 5.0634e-04,  3.8623e-02, -2.3496e-02],\n",
      "           [ 1.3352e-02,  4.1461e-02, -4.4942e-02]],\n",
      "\n",
      "          [[ 1.4887e-02,  4.2976e-02,  4.5020e-02],\n",
      "           [-4.7388e-02,  3.8955e-02, -1.7348e-02],\n",
      "           [ 7.2223e-03,  3.8389e-02,  1.6190e-03]],\n",
      "\n",
      "          [[ 3.4827e-02, -9.6871e-03,  2.8145e-02],\n",
      "           [ 3.8258e-03, -2.1979e-02,  4.2719e-02],\n",
      "           [ 3.0122e-02,  4.0494e-02, -1.7186e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 3.9069e-02, -1.6251e-02,  1.9752e-02],\n",
      "           [-1.5769e-02,  3.3837e-02, -2.5080e-02],\n",
      "           [ 1.9413e-02,  2.5037e-02, -2.4444e-02]],\n",
      "\n",
      "          [[ 1.1291e-02, -3.7333e-02,  4.8357e-03],\n",
      "           [ 1.5666e-02,  3.9329e-02,  1.7117e-02],\n",
      "           [ 2.9019e-02, -2.7859e-03,  2.5901e-02]],\n",
      "\n",
      "          [[ 8.0506e-03, -2.1633e-04,  1.4898e-02],\n",
      "           [ 4.7704e-02, -3.5401e-02, -4.3937e-02],\n",
      "           [ 9.3290e-05,  2.5562e-02, -3.2098e-03]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-2.2422e-02, -2.1121e-02,  3.5105e-02],\n",
      "           [-1.5398e-03, -3.8482e-03, -1.3330e-02],\n",
      "           [-4.4336e-02,  2.0215e-02, -2.6846e-02]],\n",
      "\n",
      "          [[ 2.6305e-02, -6.0531e-03,  2.3772e-02],\n",
      "           [-3.6482e-02,  4.5148e-02,  1.0184e-04],\n",
      "           [ 1.7364e-02,  3.6995e-02,  1.5721e-02]],\n",
      "\n",
      "          [[-1.9787e-02, -4.0580e-02, -4.0961e-02],\n",
      "           [ 3.1546e-03, -9.0935e-03, -2.4595e-02],\n",
      "           [-1.1822e-02,  2.4758e-02,  4.3412e-03]]],\n",
      "\n",
      "\n",
      "         [[[-1.4751e-02, -2.6355e-02, -2.4496e-02],\n",
      "           [ 2.2046e-02, -3.7418e-02,  4.2956e-02],\n",
      "           [-1.6985e-02, -2.6121e-02,  4.3390e-02]],\n",
      "\n",
      "          [[-4.0193e-03, -6.9801e-03, -4.3009e-02],\n",
      "           [-2.1371e-02,  4.3067e-02,  3.6216e-02],\n",
      "           [ 3.3225e-02, -6.5162e-03,  5.0802e-03]],\n",
      "\n",
      "          [[ 3.5937e-02,  1.1788e-02,  4.4628e-02],\n",
      "           [-4.5356e-02, -1.7052e-02, -4.2942e-02],\n",
      "           [-4.2552e-02, -7.9172e-03,  3.8863e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 2.9386e-02,  2.2983e-02,  1.7084e-03],\n",
      "           [ 3.7277e-02, -3.3847e-02,  2.1151e-02],\n",
      "           [ 3.7894e-02, -2.0851e-03,  1.9086e-03]],\n",
      "\n",
      "          [[ 3.6073e-02, -9.7639e-03, -3.6682e-02],\n",
      "           [-3.4822e-02,  5.0011e-03, -3.6741e-02],\n",
      "           [ 4.8065e-02,  8.8559e-03, -1.8279e-02]],\n",
      "\n",
      "          [[-2.5694e-03, -3.5765e-02, -9.2105e-03],\n",
      "           [-1.9721e-02,  2.2996e-02, -3.8233e-02],\n",
      "           [-4.3431e-02,  4.5545e-02,  3.5873e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.9618e-02,  4.6220e-02, -1.4971e-02],\n",
      "           [-1.0628e-02, -4.0784e-02,  4.8136e-03],\n",
      "           [ 4.3208e-02,  4.0036e-02, -4.5394e-02]],\n",
      "\n",
      "          [[-3.6384e-02,  2.3193e-02,  2.9238e-02],\n",
      "           [ 4.4722e-02,  2.0431e-03,  1.3207e-02],\n",
      "           [ 4.1375e-02, -2.5327e-02,  2.4283e-02]],\n",
      "\n",
      "          [[-1.0357e-02,  1.7328e-02,  1.2751e-04],\n",
      "           [ 8.8304e-03, -3.7530e-02,  2.3253e-02],\n",
      "           [-2.3837e-03, -4.0518e-02,  9.3794e-03]]],\n",
      "\n",
      "\n",
      "         [[[-3.2439e-02, -3.2358e-02,  4.0305e-02],\n",
      "           [-1.2127e-02,  3.5592e-02,  1.7692e-02],\n",
      "           [ 2.5351e-02, -4.1198e-02,  3.7087e-02]],\n",
      "\n",
      "          [[-9.7637e-03, -3.9295e-02,  4.0605e-02],\n",
      "           [-3.3577e-02, -5.6360e-03,  9.1135e-03],\n",
      "           [-2.6794e-02,  4.6618e-02, -8.7700e-03]],\n",
      "\n",
      "          [[-5.6581e-03, -4.2746e-02,  3.4325e-02],\n",
      "           [ 1.3256e-03, -1.5749e-02,  1.1785e-02],\n",
      "           [-1.0736e-02,  2.6152e-03,  4.0046e-02]]],\n",
      "\n",
      "\n",
      "         [[[-3.6023e-02, -5.8478e-03, -2.0018e-02],\n",
      "           [ 3.9688e-02,  8.3811e-03, -1.3869e-02],\n",
      "           [-1.9989e-02, -3.1278e-04,  2.4059e-02]],\n",
      "\n",
      "          [[-1.9602e-03,  3.0174e-02, -1.8524e-02],\n",
      "           [-1.7541e-02, -2.1027e-02, -1.2958e-02],\n",
      "           [ 4.0116e-02,  1.5801e-02, -3.3962e-02]],\n",
      "\n",
      "          [[-2.2238e-02, -4.2877e-02,  3.7875e-02],\n",
      "           [ 7.7196e-03, -1.0388e-02,  4.1894e-02],\n",
      "           [-3.7610e-02,  3.2684e-02,  3.7037e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 3.9577e-02, -1.6873e-02,  1.0397e-02],\n",
      "           [-1.7076e-02, -3.6132e-02, -2.7095e-02],\n",
      "           [-1.6120e-04, -5.3773e-03, -4.5355e-02]],\n",
      "\n",
      "          [[ 1.7489e-02,  1.2688e-02, -4.5019e-02],\n",
      "           [-3.7611e-02,  1.4557e-02, -3.3437e-02],\n",
      "           [ 2.6766e-02, -3.7075e-02, -4.3703e-02]],\n",
      "\n",
      "          [[ 1.2554e-02,  1.7765e-02, -1.8572e-02],\n",
      "           [ 3.0788e-02, -3.9639e-02,  1.0312e-02],\n",
      "           [ 4.5577e-02, -3.2523e-02, -8.4576e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 4.3814e-02,  4.3693e-02, -1.3948e-02],\n",
      "           [ 1.8214e-02, -2.9277e-02,  3.5504e-03],\n",
      "           [-4.1522e-03, -4.4716e-02, -3.8497e-03]],\n",
      "\n",
      "          [[-8.0975e-03,  2.7505e-02,  2.4007e-02],\n",
      "           [-1.7554e-02, -1.9729e-02, -2.7378e-02],\n",
      "           [ 2.5752e-02, -2.9455e-02,  3.9683e-02]],\n",
      "\n",
      "          [[-3.9143e-02, -3.3121e-02, -3.2307e-02],\n",
      "           [ 1.6774e-02, -4.3010e-02,  4.0183e-02],\n",
      "           [ 3.8319e-02,  8.2810e-03, -3.6867e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 4.3552e-02, -2.3590e-02, -2.6034e-02],\n",
      "           [ 1.6795e-03, -3.4488e-02, -9.0699e-03],\n",
      "           [-1.4029e-03,  4.6215e-02,  1.7283e-02]],\n",
      "\n",
      "          [[-2.4830e-02, -1.5719e-02,  3.2890e-03],\n",
      "           [ 3.5551e-02,  3.7506e-02, -4.7006e-02],\n",
      "           [-2.9416e-02,  3.8546e-03, -2.9374e-02]],\n",
      "\n",
      "          [[-3.2407e-02,  2.7133e-02,  4.7296e-02],\n",
      "           [ 4.6847e-02,  1.4303e-02,  3.3746e-02],\n",
      "           [-6.5836e-04, -3.7564e-02,  2.5492e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-3.5892e-02, -4.3015e-02,  1.5997e-04],\n",
      "           [-4.0304e-02,  3.1425e-03,  6.2455e-04],\n",
      "           [ 4.3533e-02, -1.3877e-02,  1.5640e-02]],\n",
      "\n",
      "          [[ 3.9512e-04, -4.7575e-02, -4.3414e-02],\n",
      "           [-2.1897e-03, -2.7017e-02,  1.7493e-02],\n",
      "           [ 1.5436e-02,  2.8638e-02,  3.5976e-02]],\n",
      "\n",
      "          [[-3.8847e-02, -1.5865e-02, -4.4950e-02],\n",
      "           [-3.3614e-02,  2.7864e-02,  2.9077e-02],\n",
      "           [ 4.5830e-03,  4.0457e-04, -2.9297e-02]]],\n",
      "\n",
      "\n",
      "         [[[-3.0908e-02,  1.1366e-02, -6.7526e-03],\n",
      "           [-4.6405e-02,  3.4758e-02,  4.5295e-02],\n",
      "           [-4.2124e-02, -1.1105e-02, -3.3246e-02]],\n",
      "\n",
      "          [[-1.4293e-02,  4.5142e-02,  3.3137e-02],\n",
      "           [-1.5543e-02, -7.8395e-03,  2.0533e-02],\n",
      "           [ 2.1733e-02,  4.5555e-02,  2.8645e-02]],\n",
      "\n",
      "          [[-4.1494e-02,  1.1258e-02,  2.9373e-02],\n",
      "           [ 2.6675e-02, -1.2495e-02, -4.7798e-02],\n",
      "           [ 3.2266e-02,  4.1234e-02, -3.7752e-02]]],\n",
      "\n",
      "\n",
      "         [[[-1.0224e-02, -7.0876e-03, -3.3809e-02],\n",
      "           [-2.2784e-02, -4.4663e-02, -4.5539e-02],\n",
      "           [-2.6108e-02,  3.1862e-02, -1.5207e-02]],\n",
      "\n",
      "          [[-9.4371e-03, -1.8142e-02,  3.4100e-02],\n",
      "           [ 4.7433e-02,  1.8300e-02,  3.7115e-02],\n",
      "           [-4.2826e-02, -1.6989e-02,  1.1210e-02]],\n",
      "\n",
      "          [[ 2.2327e-03,  3.1555e-02, -3.2454e-02],\n",
      "           [ 4.2658e-02, -3.3128e-02, -1.5449e-02],\n",
      "           [-2.5465e-02, -2.3579e-02,  2.3439e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 3.4606e-02, -1.1337e-02,  1.3302e-02],\n",
      "           [ 1.7508e-02,  3.8359e-02,  7.1170e-03],\n",
      "           [-1.1070e-02,  2.3602e-02,  3.4067e-02]],\n",
      "\n",
      "          [[-9.1379e-03, -2.5665e-02,  1.7908e-02],\n",
      "           [ 3.6872e-03,  2.0808e-02, -8.7839e-03],\n",
      "           [ 3.6888e-02, -4.1347e-02,  4.1299e-02]],\n",
      "\n",
      "          [[-2.8413e-02, -2.2779e-02,  3.5362e-02],\n",
      "           [ 1.4471e-03, -1.3831e-02,  4.2205e-02],\n",
      "           [ 2.4139e-02, -4.7201e-02,  2.7697e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 1.6754e-02,  2.3627e-02,  4.5625e-02],\n",
      "           [-2.2105e-02, -1.7930e-02,  1.5402e-02],\n",
      "           [ 2.9355e-02, -1.0513e-02,  2.3066e-02]],\n",
      "\n",
      "          [[ 3.2092e-02, -9.6432e-03, -3.2801e-02],\n",
      "           [ 2.5656e-02,  2.6242e-02, -3.4231e-02],\n",
      "           [-6.5870e-03,  4.7158e-02, -2.6437e-02]],\n",
      "\n",
      "          [[-2.0543e-03,  3.6229e-02,  2.4074e-02],\n",
      "           [ 2.7645e-02, -2.8180e-02, -1.0082e-02],\n",
      "           [ 3.4015e-02, -2.4441e-02,  2.7271e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 4.3804e-02, -2.0904e-02, -3.3427e-02],\n",
      "           [ 3.5231e-02, -4.2325e-02,  2.6377e-02],\n",
      "           [-2.8157e-02, -1.8622e-02,  3.5567e-02]],\n",
      "\n",
      "          [[-2.2563e-02, -2.3946e-03, -1.6013e-02],\n",
      "           [ 1.5464e-02, -2.2848e-02,  1.2816e-02],\n",
      "           [ 3.8457e-02,  4.1823e-02, -3.3462e-02]],\n",
      "\n",
      "          [[-1.0665e-02, -2.3959e-02,  7.0939e-03],\n",
      "           [ 3.0801e-02, -4.4321e-02,  2.7968e-02],\n",
      "           [ 4.9579e-03, -3.7271e-02,  4.2175e-02]]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0100, 0.0099, 0.0101, 0.0099, 0.0099, 0.0101, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0101, 0.0101, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0101,\n",
      "        0.0099, 0.0101, 0.0100, 0.0099, 0.0100, 0.0101, 0.0100, 0.0100, 0.0101,\n",
      "        0.0099, 0.0099, 0.0101, 0.0101, 0.0099, 0.0101, 0.0100, 0.0100, 0.0099,\n",
      "        0.0101, 0.0101, 0.0099, 0.0100, 0.0100, 0.0100, 0.0099, 0.0099, 0.0100,\n",
      "        0.0101, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0101,\n",
      "        0.0100, 0.0099, 0.0101, 0.0099, 0.0100, 0.0101, 0.0101, 0.0100, 0.0101,\n",
      "        0.0100], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[[ 3.0337e-02,  2.2360e-02, -4.6284e-03],\n",
      "           [ 2.8375e-02, -6.3517e-03, -2.5153e-02],\n",
      "           [-3.0763e-02, -8.0297e-03, -1.6227e-02]],\n",
      "\n",
      "          [[-2.5380e-02,  3.3315e-02,  1.5141e-02],\n",
      "           [ 3.1709e-02,  8.4615e-03,  1.9656e-04],\n",
      "           [ 3.7477e-03,  1.6665e-02,  2.0677e-02]],\n",
      "\n",
      "          [[-6.8806e-03, -1.0269e-02, -1.2292e-02],\n",
      "           [-5.2518e-03, -2.2258e-03, -1.6230e-02],\n",
      "           [ 2.7927e-02,  1.0798e-02, -1.8194e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 1.2051e-02,  3.3410e-02,  1.8377e-03],\n",
      "           [ 1.3718e-02, -2.1461e-02,  1.1029e-02],\n",
      "           [-1.8504e-02, -1.0783e-03,  8.7938e-03]],\n",
      "\n",
      "          [[ 1.2623e-02,  9.7105e-03,  2.4504e-03],\n",
      "           [ 1.7269e-02,  8.3312e-03, -2.3768e-02],\n",
      "           [-1.1234e-02, -4.2314e-03, -6.3775e-03]],\n",
      "\n",
      "          [[ 2.9078e-02,  1.6368e-02,  2.1672e-02],\n",
      "           [ 2.5527e-02, -2.4878e-03,  2.4756e-03],\n",
      "           [ 2.2453e-02, -4.5939e-03, -1.6160e-03]]],\n",
      "\n",
      "\n",
      "         [[[-2.6996e-03, -9.3684e-03,  1.1830e-02],\n",
      "           [ 4.2514e-03, -7.2591e-03,  7.2989e-03],\n",
      "           [ 2.1592e-02, -2.9693e-02, -2.1690e-02]],\n",
      "\n",
      "          [[-1.4859e-02, -1.4725e-02, -4.0250e-04],\n",
      "           [-7.1079e-04,  1.2487e-02,  1.0012e-02],\n",
      "           [-3.3679e-02,  3.9213e-03,  1.2425e-02]],\n",
      "\n",
      "          [[-3.0174e-02,  8.9251e-03,  1.6342e-02],\n",
      "           [-2.9567e-02, -6.3508e-03, -6.8051e-03],\n",
      "           [ 3.3863e-02,  2.0973e-02,  1.0422e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 2.7440e-03, -1.7804e-02, -3.8492e-03],\n",
      "           [-1.3764e-03,  9.9897e-04, -2.2905e-02],\n",
      "           [ 1.1776e-02, -2.0362e-02,  1.1562e-02]],\n",
      "\n",
      "          [[ 1.7540e-02, -1.4632e-02, -1.2193e-02],\n",
      "           [-1.0055e-02,  9.4010e-03, -1.5843e-02],\n",
      "           [-3.9319e-03, -2.2052e-02, -1.2902e-02]],\n",
      "\n",
      "          [[ 2.1473e-02, -2.5047e-02,  1.9772e-02],\n",
      "           [-2.2747e-02,  2.0873e-02, -2.3411e-02],\n",
      "           [-2.1984e-02,  1.3411e-02, -5.2775e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 1.1929e-02, -3.4663e-03,  6.9672e-03],\n",
      "           [-2.6008e-02, -3.5495e-03, -5.1492e-03],\n",
      "           [-8.2045e-03, -3.5500e-03,  2.4044e-02]],\n",
      "\n",
      "          [[-1.5678e-02, -7.6417e-03,  1.3234e-02],\n",
      "           [-3.9217e-03, -2.3335e-02, -1.0777e-03],\n",
      "           [ 7.5952e-03, -3.2990e-02,  3.3607e-02]],\n",
      "\n",
      "          [[ 1.7898e-02,  2.8686e-03, -2.7060e-02],\n",
      "           [ 2.0634e-02,  2.4865e-02, -5.0506e-03],\n",
      "           [-1.3698e-02, -2.9377e-04,  3.9846e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 3.6716e-03, -2.6593e-02, -2.3627e-02],\n",
      "           [ 8.3427e-03, -2.2953e-03, -2.4044e-02],\n",
      "           [ 8.7525e-03,  2.6821e-02,  1.7178e-02]],\n",
      "\n",
      "          [[ 8.6726e-03,  1.8854e-03,  1.7791e-02],\n",
      "           [ 3.1502e-02, -5.7358e-03,  3.1101e-02],\n",
      "           [ 1.5593e-02,  2.9429e-02, -1.1270e-02]],\n",
      "\n",
      "          [[-7.9463e-03, -7.3447e-03,  2.4753e-02],\n",
      "           [ 1.5713e-02,  1.2745e-03, -1.2684e-02],\n",
      "           [ 3.1616e-02,  6.9828e-03, -2.2727e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.9325e-02,  2.1900e-02,  2.8350e-02],\n",
      "           [ 8.3621e-03, -1.8167e-02, -2.6464e-03],\n",
      "           [ 1.4239e-02,  1.8465e-02,  2.1471e-04]],\n",
      "\n",
      "          [[-2.0390e-02, -9.2560e-03, -3.2152e-02],\n",
      "           [ 3.2270e-02, -4.7960e-03,  2.2704e-02],\n",
      "           [-7.9349e-03, -1.7879e-02,  1.2216e-02]],\n",
      "\n",
      "          [[-3.0079e-02, -1.6714e-02,  3.3947e-02],\n",
      "           [ 1.6454e-02,  2.3114e-02,  1.5607e-02],\n",
      "           [ 3.0048e-02, -9.6649e-03,  1.2773e-02]]],\n",
      "\n",
      "\n",
      "         [[[-3.2870e-02,  3.3360e-02, -2.1777e-02],\n",
      "           [ 3.0030e-02,  2.4102e-02,  1.1460e-02],\n",
      "           [ 1.4268e-02,  2.3388e-02,  1.1125e-02]],\n",
      "\n",
      "          [[-1.2720e-03,  2.6598e-02, -1.0347e-02],\n",
      "           [-8.1261e-03, -2.2339e-03, -2.9116e-02],\n",
      "           [ 2.8855e-02, -1.8895e-02, -3.1126e-02]],\n",
      "\n",
      "          [[-3.1728e-02,  2.0575e-02, -1.4119e-02],\n",
      "           [-2.4711e-02,  1.9015e-02,  3.3151e-02],\n",
      "           [ 3.3099e-02,  1.8047e-03, -1.7651e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 2.2116e-02, -2.1784e-02,  3.3594e-02],\n",
      "           [-1.3164e-02,  1.3594e-02, -1.4660e-02],\n",
      "           [-2.6283e-02,  1.4634e-02,  2.8760e-02]],\n",
      "\n",
      "          [[ 1.6090e-02, -1.4685e-02,  2.0203e-02],\n",
      "           [-1.1607e-02,  1.2019e-02,  1.6994e-02],\n",
      "           [ 1.8733e-02,  2.0269e-02,  2.2715e-02]],\n",
      "\n",
      "          [[-2.8487e-02, -2.0896e-02,  1.6981e-03],\n",
      "           [-2.8556e-02, -1.5753e-02, -7.6762e-03],\n",
      "           [-1.6716e-02,  4.5874e-03,  1.9160e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.5317e-02, -3.1368e-02, -2.4136e-02],\n",
      "           [ 3.2519e-02, -2.2574e-02, -2.1574e-02],\n",
      "           [ 6.8214e-05,  1.7714e-02, -1.8998e-03]],\n",
      "\n",
      "          [[-1.8555e-02,  5.8268e-03, -3.0045e-02],\n",
      "           [-3.2943e-02,  2.7908e-02,  2.0710e-02],\n",
      "           [-3.0279e-02, -1.7012e-02, -6.4845e-03]],\n",
      "\n",
      "          [[ 1.9109e-02,  1.6979e-02,  4.1833e-03],\n",
      "           [-1.0798e-03, -2.1240e-02,  3.3444e-02],\n",
      "           [ 1.5714e-02, -1.1664e-02,  5.9279e-03]]],\n",
      "\n",
      "\n",
      "         [[[-1.8991e-02,  3.3265e-02,  2.5367e-02],\n",
      "           [-2.7738e-02, -2.3054e-02,  1.2118e-02],\n",
      "           [-2.3462e-02, -2.4979e-02, -1.5491e-02]],\n",
      "\n",
      "          [[-8.3004e-03, -2.7047e-02,  1.6465e-02],\n",
      "           [-3.0724e-02, -6.6447e-04,  1.4672e-02],\n",
      "           [ 2.4723e-02, -8.0844e-03,  7.1490e-03]],\n",
      "\n",
      "          [[-7.5744e-03, -2.8454e-02,  7.6422e-03],\n",
      "           [ 2.8633e-03,  4.2100e-04, -9.9721e-03],\n",
      "           [-1.1712e-02,  1.4514e-02,  2.1847e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 1.6850e-02, -1.2271e-02, -1.9217e-04],\n",
      "           [ 2.5683e-02,  1.0085e-03,  2.8096e-02],\n",
      "           [ 1.0779e-02,  2.6390e-02, -3.0433e-02]],\n",
      "\n",
      "          [[-1.1132e-03, -2.5759e-02, -4.8989e-03],\n",
      "           [-1.3085e-02, -3.0477e-02, -1.0857e-02],\n",
      "           [-4.9024e-03,  2.8025e-02, -2.6995e-02]],\n",
      "\n",
      "          [[-2.8543e-02, -3.4050e-03, -1.6579e-02],\n",
      "           [-3.0141e-02, -1.6753e-02,  1.4292e-02],\n",
      "           [-3.2302e-02,  4.8919e-03,  1.3039e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-2.8265e-02,  1.2665e-02, -3.3833e-03],\n",
      "           [ 3.6128e-03, -1.3846e-02,  2.0007e-02],\n",
      "           [-1.0718e-02,  1.9762e-03,  7.7596e-03]],\n",
      "\n",
      "          [[-1.1040e-02,  2.3633e-02,  2.9955e-02],\n",
      "           [ 1.9893e-03,  2.6358e-02, -6.4115e-03],\n",
      "           [-2.7540e-02,  2.7607e-02,  9.3903e-03]],\n",
      "\n",
      "          [[ 6.1177e-04,  2.0999e-02,  7.9283e-03],\n",
      "           [-1.6902e-02, -2.7624e-02, -3.0504e-02],\n",
      "           [ 9.5315e-03, -1.0881e-02, -3.9115e-03]]],\n",
      "\n",
      "\n",
      "         [[[-2.2940e-02, -1.3239e-02, -1.3200e-02],\n",
      "           [ 2.3223e-02, -2.3068e-02,  8.5708e-03],\n",
      "           [-7.9416e-03,  3.1389e-02, -3.3910e-02]],\n",
      "\n",
      "          [[ 3.1772e-02,  1.0992e-02, -1.3632e-02],\n",
      "           [ 3.1126e-03, -3.0706e-03, -9.6478e-03],\n",
      "           [-1.4339e-02,  7.6948e-03,  9.8260e-04]],\n",
      "\n",
      "          [[-7.1912e-03,  1.5835e-02,  8.3336e-03],\n",
      "           [ 1.6677e-02,  1.2618e-02,  9.2618e-03],\n",
      "           [-2.1483e-03,  2.6016e-02,  1.4790e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 3.1180e-02, -6.5443e-03, -3.3349e-02],\n",
      "           [ 3.3955e-02,  2.9152e-02, -3.0517e-02],\n",
      "           [-2.4476e-02, -2.2566e-02,  8.4688e-03]],\n",
      "\n",
      "          [[ 5.4921e-03, -2.0518e-02,  8.2104e-03],\n",
      "           [ 2.4106e-02,  2.0156e-02,  1.2330e-02],\n",
      "           [-1.0163e-02,  2.4840e-02,  1.2871e-02]],\n",
      "\n",
      "          [[ 8.5401e-04,  1.0550e-02,  2.6282e-02],\n",
      "           [-1.5358e-02, -2.9626e-02,  3.0527e-02],\n",
      "           [-1.4467e-02, -5.4964e-03, -2.1310e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-2.3749e-02, -1.8838e-02, -9.0790e-03],\n",
      "           [-2.9329e-02, -2.0885e-02, -1.0893e-02],\n",
      "           [ 1.5813e-02,  1.0233e-02, -8.5939e-03]],\n",
      "\n",
      "          [[-2.9488e-02, -1.2718e-02,  3.2310e-02],\n",
      "           [ 1.2413e-02,  1.6420e-02, -1.9573e-02],\n",
      "           [-1.8605e-02,  2.4384e-02,  5.1159e-03]],\n",
      "\n",
      "          [[ 1.8067e-02,  1.6705e-02, -8.6796e-03],\n",
      "           [-2.6198e-02,  8.2706e-03,  1.3358e-02],\n",
      "           [-3.1993e-02,  3.0168e-02, -7.0983e-03]]],\n",
      "\n",
      "\n",
      "         [[[-1.1436e-03,  1.6035e-02,  2.5513e-02],\n",
      "           [-1.5523e-02, -1.7037e-03,  1.9515e-03],\n",
      "           [ 8.3743e-03, -2.1848e-02,  1.3539e-02]],\n",
      "\n",
      "          [[-5.5811e-03,  3.2114e-02, -1.9088e-02],\n",
      "           [-2.4647e-02,  1.4959e-02, -6.9882e-03],\n",
      "           [-3.2877e-02,  2.9632e-02, -1.1868e-02]],\n",
      "\n",
      "          [[-1.1267e-02,  1.4254e-02, -1.2902e-02],\n",
      "           [-1.8126e-02, -2.5716e-02,  1.2243e-02],\n",
      "           [-1.6495e-02, -1.2940e-02, -9.2427e-03]]],\n",
      "\n",
      "\n",
      "         [[[-2.0038e-02,  2.2883e-02, -2.5312e-02],\n",
      "           [ 4.8869e-03, -2.1444e-02,  1.8159e-02],\n",
      "           [ 1.9254e-02, -1.4905e-02, -2.7647e-02]],\n",
      "\n",
      "          [[ 2.0109e-02, -1.6702e-02,  4.4854e-03],\n",
      "           [-1.6059e-02, -9.3206e-03, -2.5914e-02],\n",
      "           [ 1.3292e-02,  1.9843e-02,  1.0872e-02]],\n",
      "\n",
      "          [[-1.4836e-02, -1.8203e-02, -7.9149e-03],\n",
      "           [ 3.1402e-02,  2.7650e-02, -2.7028e-02],\n",
      "           [ 1.1111e-02, -4.5116e-03,  1.9185e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.6817e-03, -1.9310e-02, -2.7428e-02],\n",
      "           [ 2.2958e-02, -1.4226e-02, -3.0114e-02],\n",
      "           [-2.1417e-02,  1.9731e-02,  1.0581e-02]],\n",
      "\n",
      "          [[-3.3293e-02,  1.2315e-02,  8.9105e-03],\n",
      "           [-2.2005e-02,  1.2489e-02,  1.1960e-02],\n",
      "           [ 6.7106e-03,  5.2295e-04,  7.0651e-03]],\n",
      "\n",
      "          [[-2.1784e-03,  1.9422e-02, -3.3479e-02],\n",
      "           [ 8.0545e-03,  2.8385e-02, -2.5092e-02],\n",
      "           [ 1.9812e-02, -2.7090e-02, -2.7963e-02]]],\n",
      "\n",
      "\n",
      "         [[[-1.1691e-02,  1.6878e-02, -2.4157e-02],\n",
      "           [-1.8737e-02,  2.7915e-02, -2.0083e-03],\n",
      "           [-8.8408e-03, -8.9809e-03, -2.4885e-02]],\n",
      "\n",
      "          [[-2.1184e-02, -2.2453e-02,  3.0237e-02],\n",
      "           [ 5.5764e-03,  2.4164e-02, -2.1167e-02],\n",
      "           [-2.2365e-02,  4.9918e-03,  3.1994e-02]],\n",
      "\n",
      "          [[-1.9687e-02, -1.7196e-02, -1.6218e-02],\n",
      "           [-6.5322e-03,  3.0202e-02, -5.7931e-03],\n",
      "           [-3.3041e-02,  3.0767e-03, -3.3007e-02]]],\n",
      "\n",
      "\n",
      "         [[[-2.9871e-02, -1.8814e-02, -1.6173e-02],\n",
      "           [ 1.3790e-02, -8.4626e-03,  3.1374e-02],\n",
      "           [ 2.0636e-02, -4.2490e-03, -1.8519e-03]],\n",
      "\n",
      "          [[ 9.9373e-03,  8.8947e-03,  2.7192e-02],\n",
      "           [ 3.2267e-02,  2.2029e-02, -2.8636e-02],\n",
      "           [ 2.7567e-02,  2.7144e-02, -2.5426e-02]],\n",
      "\n",
      "          [[ 1.2599e-02,  1.1447e-02, -7.0575e-03],\n",
      "           [ 2.8267e-02, -2.8322e-02, -5.5576e-04],\n",
      "           [-5.9487e-03, -2.5559e-02,  6.7448e-03]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-2.9956e-02,  3.1174e-02,  2.8917e-02],\n",
      "           [-5.5660e-03,  1.2951e-02, -2.8543e-02],\n",
      "           [ 2.4885e-02, -3.3594e-02,  5.8916e-03]],\n",
      "\n",
      "          [[-2.3609e-02, -2.4672e-02, -1.2351e-02],\n",
      "           [ 3.0008e-02, -3.3103e-02,  2.2784e-02],\n",
      "           [ 1.5334e-03,  2.5512e-02,  1.1367e-03]],\n",
      "\n",
      "          [[ 1.2321e-02, -3.2885e-02, -2.2926e-02],\n",
      "           [ 1.6411e-02,  8.7480e-03,  7.0505e-03],\n",
      "           [ 1.1992e-02, -3.0744e-02, -1.2310e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 2.4017e-02,  2.6090e-02,  1.4871e-03],\n",
      "           [ 1.6354e-02, -9.1179e-03,  1.7677e-02],\n",
      "           [-4.1854e-03, -1.6510e-02, -3.3968e-02]],\n",
      "\n",
      "          [[ 7.3727e-03, -2.9062e-02, -3.8386e-03],\n",
      "           [ 4.0577e-03,  8.3031e-03,  1.3174e-03],\n",
      "           [-2.3590e-02, -1.8884e-02, -3.1183e-02]],\n",
      "\n",
      "          [[ 6.4191e-03, -2.4668e-02,  4.5186e-03],\n",
      "           [ 2.6462e-02,  6.4392e-03,  5.1719e-03],\n",
      "           [ 2.7997e-02, -4.6648e-03, -1.4335e-02]]],\n",
      "\n",
      "\n",
      "         [[[-2.7713e-02, -1.1904e-02,  3.1566e-02],\n",
      "           [-3.1698e-02, -1.9598e-02,  1.0222e-02],\n",
      "           [-1.8714e-02, -3.1735e-02, -2.2993e-02]],\n",
      "\n",
      "          [[ 2.1289e-02, -2.3499e-02, -1.0680e-03],\n",
      "           [-1.4692e-02,  8.2649e-03,  3.1091e-02],\n",
      "           [ 1.5369e-02, -1.4501e-02,  2.8716e-02]],\n",
      "\n",
      "          [[ 5.6720e-03, -2.9425e-02,  1.0632e-02],\n",
      "           [-1.9434e-02, -3.8086e-03,  2.2632e-02],\n",
      "           [-2.7716e-02, -2.6224e-02, -2.2225e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-2.0632e-02,  2.4615e-02, -3.6128e-03],\n",
      "           [ 2.1872e-02,  2.0103e-02,  1.5110e-02],\n",
      "           [-3.1485e-03,  2.1633e-02,  6.6020e-03]],\n",
      "\n",
      "          [[-2.1219e-02,  3.0365e-02, -1.6856e-02],\n",
      "           [ 1.2930e-02, -2.1723e-02, -1.4481e-02],\n",
      "           [-1.8465e-03,  2.8963e-02, -1.8413e-02]],\n",
      "\n",
      "          [[ 3.3215e-02, -2.3598e-02,  2.4055e-02],\n",
      "           [-1.8852e-02,  1.2227e-03, -2.8571e-02],\n",
      "           [-3.3787e-02,  2.8317e-02, -3.1221e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 3.1649e-02,  1.7082e-02,  2.9977e-02],\n",
      "           [ 1.8320e-03, -2.6105e-02, -1.2711e-02],\n",
      "           [ 1.1318e-02,  3.3356e-02, -3.0186e-02]],\n",
      "\n",
      "          [[ 1.3405e-02, -1.3755e-03,  9.0659e-03],\n",
      "           [ 3.2079e-02, -1.1350e-02,  2.2325e-02],\n",
      "           [-1.6596e-02,  5.2980e-03, -5.2490e-03]],\n",
      "\n",
      "          [[-3.1951e-02, -2.9820e-02, -1.1760e-02],\n",
      "           [-9.4017e-03, -2.4306e-02, -2.2016e-02],\n",
      "           [-2.9220e-02, -1.5399e-02,  1.7016e-02]]],\n",
      "\n",
      "\n",
      "         [[[-2.6103e-02,  8.0430e-03,  8.6819e-03],\n",
      "           [ 2.4132e-02, -1.2285e-02, -4.8209e-03],\n",
      "           [ 1.0351e-02, -2.0167e-02,  3.8223e-03]],\n",
      "\n",
      "          [[-1.9521e-02, -2.4661e-02, -1.5134e-02],\n",
      "           [-2.6301e-02, -2.3149e-02,  2.5319e-03],\n",
      "           [-2.0892e-02,  7.5648e-03, -2.4244e-02]],\n",
      "\n",
      "          [[ 2.7842e-02, -1.4324e-02,  1.0211e-03],\n",
      "           [ 6.2089e-03,  8.0166e-03, -2.9336e-02],\n",
      "           [ 2.0768e-02, -1.9989e-02,  3.2606e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-6.9897e-03, -1.2224e-02,  7.2013e-03],\n",
      "           [-3.1682e-02, -1.1153e-02,  2.3693e-02],\n",
      "           [-2.2202e-03, -1.1516e-02,  1.0062e-02]],\n",
      "\n",
      "          [[-2.7449e-02,  2.3646e-02,  2.5310e-02],\n",
      "           [ 4.3562e-03, -4.6791e-03, -2.7881e-02],\n",
      "           [-3.0632e-02, -1.7928e-02, -1.8549e-03]],\n",
      "\n",
      "          [[-3.1532e-02, -8.7563e-04, -2.3435e-02],\n",
      "           [ 2.2301e-02, -1.1666e-02, -2.9910e-02],\n",
      "           [ 1.1177e-02,  1.4781e-02,  2.8804e-02]]],\n",
      "\n",
      "\n",
      "         [[[-2.4876e-02, -2.6596e-02, -8.7313e-03],\n",
      "           [-2.7959e-02,  1.2046e-02, -3.1528e-02],\n",
      "           [ 3.1311e-02,  1.5526e-02, -5.7630e-03]],\n",
      "\n",
      "          [[ 2.1004e-02,  7.6422e-03,  1.0174e-02],\n",
      "           [ 1.9998e-03, -3.3782e-02,  2.5322e-02],\n",
      "           [-1.0953e-02,  1.3059e-02,  4.6582e-03]],\n",
      "\n",
      "          [[ 7.9984e-04, -4.3929e-03,  2.6469e-02],\n",
      "           [-5.2213e-04, -2.9603e-02,  2.7362e-02],\n",
      "           [-1.0484e-02, -8.2959e-03,  4.7521e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 2.7792e-02, -3.0217e-02,  5.1983e-03],\n",
      "           [-1.8875e-02, -5.2791e-03, -2.8782e-02],\n",
      "           [-2.9941e-02,  2.0320e-02, -1.0038e-02]],\n",
      "\n",
      "          [[ 2.5592e-02, -2.4484e-02,  2.6466e-02],\n",
      "           [ 3.2174e-02,  6.6012e-03, -2.7125e-02],\n",
      "           [ 5.3818e-03, -3.3736e-02,  2.3777e-02]],\n",
      "\n",
      "          [[ 3.3592e-02, -2.6207e-02,  7.4107e-03],\n",
      "           [-8.4972e-03, -2.1524e-02, -3.2474e-02],\n",
      "           [-7.7742e-03,  5.9836e-03, -2.3206e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.2965e-02,  1.8268e-02,  2.6598e-02],\n",
      "           [ 1.5774e-03,  2.7052e-02,  9.6697e-03],\n",
      "           [-2.0595e-02,  2.9439e-02, -1.9910e-02]],\n",
      "\n",
      "          [[-1.9821e-02, -1.5276e-02,  7.7738e-03],\n",
      "           [ 1.2696e-02,  3.1614e-02,  1.9638e-02],\n",
      "           [-2.6110e-02, -3.0268e-02, -1.3386e-02]],\n",
      "\n",
      "          [[-2.7963e-02,  4.6521e-03,  9.4757e-03],\n",
      "           [ 2.7060e-02, -3.8478e-03,  2.7414e-02],\n",
      "           [-1.4235e-02,  6.0240e-03,  2.7733e-02]]],\n",
      "\n",
      "\n",
      "         [[[-4.1442e-03, -2.9440e-02, -1.6182e-02],\n",
      "           [-6.4573e-03, -2.4918e-02, -2.2091e-02],\n",
      "           [-8.3811e-03, -2.6457e-02, -3.3967e-02]],\n",
      "\n",
      "          [[-3.0072e-02, -2.4828e-02,  1.8881e-02],\n",
      "           [ 3.1125e-02,  1.0544e-02,  3.6707e-03],\n",
      "           [ 1.4953e-02,  2.4391e-02, -2.0289e-02]],\n",
      "\n",
      "          [[-3.1810e-03,  1.1002e-02, -1.8144e-02],\n",
      "           [-4.8920e-03, -4.7736e-03, -3.0655e-02],\n",
      "           [-1.9271e-02,  1.2607e-02,  1.8870e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 2.8879e-02, -1.7184e-02, -9.2502e-03],\n",
      "           [-1.8214e-02, -3.7987e-04,  3.3353e-02],\n",
      "           [ 2.2056e-02, -1.5354e-02,  2.1742e-03]],\n",
      "\n",
      "          [[ 3.5711e-03, -2.3216e-02,  5.9188e-03],\n",
      "           [-9.9060e-03,  1.3063e-02, -2.2549e-02],\n",
      "           [ 3.2280e-02, -1.8563e-02,  8.8655e-03]],\n",
      "\n",
      "          [[ 2.8488e-02, -1.9589e-02,  1.5247e-02],\n",
      "           [-2.6448e-02, -7.2702e-03, -1.1324e-02],\n",
      "           [-1.3373e-02,  3.6635e-03,  2.0573e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.9179e-03, -3.2872e-02,  1.1332e-02],\n",
      "           [-7.6937e-03, -5.7629e-03,  2.5888e-02],\n",
      "           [ 1.9003e-02,  1.6236e-02, -5.1368e-03]],\n",
      "\n",
      "          [[-4.8620e-03,  2.9552e-02,  2.7162e-02],\n",
      "           [ 2.5744e-02, -1.8265e-02, -2.6565e-02],\n",
      "           [ 2.2867e-02, -1.4021e-02,  2.0721e-02]],\n",
      "\n",
      "          [[-1.5799e-03, -6.2974e-03, -2.3091e-03],\n",
      "           [-1.4726e-02,  1.5643e-02,  2.3098e-02],\n",
      "           [ 3.3039e-02,  3.9842e-03,  6.0791e-03]]],\n",
      "\n",
      "\n",
      "         [[[-3.0794e-02,  1.5229e-02,  2.2450e-02],\n",
      "           [ 7.9968e-05, -2.4102e-02,  1.6310e-02],\n",
      "           [-2.9185e-02, -3.2140e-03,  1.0036e-02]],\n",
      "\n",
      "          [[-6.7952e-03, -2.5116e-02,  8.3136e-04],\n",
      "           [-1.1255e-02, -8.2734e-03,  2.9890e-02],\n",
      "           [-3.1390e-02, -1.1062e-02,  1.1045e-02]],\n",
      "\n",
      "          [[ 2.6690e-02, -1.3533e-02,  1.3966e-02],\n",
      "           [ 3.2881e-02,  1.9504e-03,  1.1573e-02],\n",
      "           [ 3.8249e-03, -3.3051e-03, -2.5237e-02]]],\n",
      "\n",
      "\n",
      "         [[[-5.3993e-03,  2.4993e-02, -1.7877e-02],\n",
      "           [ 3.1781e-02, -3.9246e-03,  1.9674e-02],\n",
      "           [-2.1385e-02,  3.2841e-02, -1.2511e-02]],\n",
      "\n",
      "          [[-1.5031e-02, -1.1109e-02, -2.5345e-02],\n",
      "           [ 2.5076e-02, -1.5686e-02,  2.0845e-02],\n",
      "           [-9.5981e-03,  1.9196e-02, -2.8200e-02]],\n",
      "\n",
      "          [[-2.2266e-02, -1.8615e-02, -2.4155e-02],\n",
      "           [ 2.5537e-02, -1.2538e-02, -1.2394e-02],\n",
      "           [ 6.0341e-03,  2.1322e-02, -2.7457e-02]]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0101, 0.0099, 0.0101, 0.0100, 0.0100,\n",
      "        0.0100, 0.0099, 0.0100, 0.0101, 0.0100, 0.0099, 0.0100, 0.0100, 0.0099,\n",
      "        0.0101, 0.0099, 0.0099, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0101, 0.0101, 0.0099, 0.0101, 0.0100, 0.0100, 0.0101,\n",
      "        0.0101, 0.0100, 0.0101, 0.0100, 0.0100, 0.0099, 0.0099, 0.0100, 0.0100,\n",
      "        0.0100, 0.0101, 0.0100, 0.0101, 0.0101, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0101, 0.0102, 0.0101, 0.0101, 0.0099, 0.0099, 0.0101,\n",
      "        0.0099, 0.0100, 0.0100, 0.0099, 0.0100, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0101, 0.0100, 0.0100, 0.0101, 0.0101,\n",
      "        0.0100, 0.0099, 0.0099, 0.0099, 0.0100, 0.0101, 0.0099, 0.0100, 0.0100,\n",
      "        0.0100, 0.0099, 0.0101, 0.0100, 0.0099, 0.0100, 0.0101, 0.0100, 0.0098,\n",
      "        0.0100, 0.0100, 0.0099, 0.0099, 0.0099, 0.0099, 0.0100, 0.0101, 0.0101,\n",
      "        0.0101, 0.0099, 0.0101, 0.0099, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0099, 0.0099, 0.0100, 0.0101, 0.0100, 0.0099, 0.0101, 0.0100,\n",
      "        0.0101, 0.0100], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[[ 8.1267e-03,  1.5377e-03, -1.7147e-02],\n",
      "           [ 1.7807e-03,  2.2839e-02,  8.0383e-03],\n",
      "           [-1.5739e-02,  1.9378e-02, -9.6404e-03]],\n",
      "\n",
      "          [[ 1.8170e-02, -1.1469e-02,  1.8799e-02],\n",
      "           [-1.3749e-02,  2.4050e-03,  3.9233e-03],\n",
      "           [-3.9617e-03, -6.1847e-03,  3.6018e-03]],\n",
      "\n",
      "          [[-1.6714e-02, -9.7218e-03, -8.7636e-03],\n",
      "           [ 6.8166e-03,  3.4820e-03, -1.3595e-02],\n",
      "           [-2.3155e-02,  2.1278e-02,  8.8313e-03]]],\n",
      "\n",
      "\n",
      "         [[[-1.6815e-02, -1.2867e-02,  2.1427e-02],\n",
      "           [-1.4979e-02, -4.2849e-03,  3.2790e-03],\n",
      "           [-1.9402e-02,  2.7780e-03, -7.7726e-04]],\n",
      "\n",
      "          [[ 1.4431e-02, -1.7235e-02, -1.0078e-02],\n",
      "           [-3.4908e-03, -1.3754e-02,  1.5185e-02],\n",
      "           [ 2.0092e-02,  1.7176e-02,  1.4616e-02]],\n",
      "\n",
      "          [[-1.3903e-02, -4.9103e-03, -3.1672e-03],\n",
      "           [-1.5717e-02, -1.5684e-02, -3.3895e-03],\n",
      "           [-1.3626e-02,  1.7519e-02, -5.8937e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 2.0127e-02,  3.4723e-04, -1.4867e-02],\n",
      "           [-6.2693e-03, -1.9007e-02,  2.4263e-03],\n",
      "           [ 1.1121e-02,  1.9778e-02,  2.2259e-02]],\n",
      "\n",
      "          [[-7.8719e-03,  9.8564e-03, -9.8177e-03],\n",
      "           [ 9.4697e-03,  4.0423e-03,  7.4970e-03],\n",
      "           [ 1.1457e-02, -2.3434e-03, -1.0620e-02]],\n",
      "\n",
      "          [[ 2.0834e-02,  2.2779e-02,  5.8872e-03],\n",
      "           [ 1.0971e-02,  1.8001e-02,  9.9552e-03],\n",
      "           [-1.2823e-02, -2.7513e-03,  1.5982e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-2.3657e-03, -5.2115e-03,  1.4372e-02],\n",
      "           [-2.0334e-02, -1.2740e-02,  9.0570e-03],\n",
      "           [-1.2179e-02, -1.5167e-02, -7.7951e-04]],\n",
      "\n",
      "          [[ 2.3735e-02, -2.7755e-03, -1.4625e-02],\n",
      "           [ 5.4328e-03, -2.0717e-02,  1.0007e-02],\n",
      "           [ 1.6512e-02, -1.1309e-02,  2.0619e-02]],\n",
      "\n",
      "          [[ 2.1390e-02, -5.5996e-03, -2.5443e-03],\n",
      "           [-8.6005e-03, -1.8033e-02,  1.4220e-02],\n",
      "           [ 4.1639e-03, -2.2193e-02, -4.9590e-03]]],\n",
      "\n",
      "\n",
      "         [[[-2.2276e-02,  9.1047e-03,  5.9518e-03],\n",
      "           [-1.6692e-03, -5.5650e-03,  1.6000e-02],\n",
      "           [-8.2819e-03,  2.2443e-02, -7.8042e-03]],\n",
      "\n",
      "          [[-6.3515e-03, -4.7738e-03,  6.0128e-03],\n",
      "           [ 5.9824e-03,  1.9219e-02, -4.8225e-04],\n",
      "           [ 2.2878e-02, -4.4076e-03, -6.2238e-03]],\n",
      "\n",
      "          [[ 1.5488e-02, -9.2961e-03, -1.1213e-02],\n",
      "           [ 9.8835e-05, -1.9513e-02,  4.4276e-03],\n",
      "           [-1.7142e-02, -2.9858e-03,  9.2826e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3944e-02, -2.3189e-04, -1.3848e-02],\n",
      "           [-1.1668e-03, -2.1761e-02, -4.8419e-03],\n",
      "           [-6.1453e-03,  1.0170e-02, -4.1450e-03]],\n",
      "\n",
      "          [[-1.3093e-03,  2.1064e-02, -2.0225e-02],\n",
      "           [-5.2188e-03, -3.8353e-03,  7.5170e-03],\n",
      "           [ 1.5351e-02,  2.1550e-02,  1.5657e-02]],\n",
      "\n",
      "          [[-1.9854e-02, -1.1495e-02, -5.9279e-03],\n",
      "           [ 1.2423e-02,  2.3757e-02,  6.2005e-03],\n",
      "           [ 8.4647e-03, -3.8446e-03,  1.7080e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-8.0758e-03, -3.9579e-03, -1.2310e-02],\n",
      "           [-1.4403e-02,  1.3891e-02, -5.1758e-03],\n",
      "           [-2.1789e-03, -9.6521e-03, -9.3560e-04]],\n",
      "\n",
      "          [[ 3.5081e-04, -1.4537e-02,  1.0327e-02],\n",
      "           [-1.1332e-02, -2.3351e-02, -1.8143e-02],\n",
      "           [ 9.2910e-03, -4.2806e-03,  2.0385e-02]],\n",
      "\n",
      "          [[-2.2175e-02,  1.3097e-02,  2.0076e-02],\n",
      "           [ 5.5648e-03, -2.1525e-02, -1.9836e-02],\n",
      "           [ 1.4285e-02,  2.9775e-03, -2.3232e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 1.8996e-02,  1.5380e-02,  1.0213e-02],\n",
      "           [-2.3698e-02, -1.6001e-02, -1.9413e-02],\n",
      "           [ 7.1719e-03,  1.1889e-02,  2.2476e-02]],\n",
      "\n",
      "          [[-2.3013e-02, -4.0366e-03, -4.0676e-03],\n",
      "           [-2.2195e-02, -5.1446e-03, -6.1845e-03],\n",
      "           [-1.1481e-02,  1.7964e-03,  2.3840e-02]],\n",
      "\n",
      "          [[-3.9607e-03, -1.5058e-02, -1.4017e-02],\n",
      "           [-9.3032e-03,  2.1720e-02, -1.8153e-02],\n",
      "           [ 8.5435e-03,  1.4629e-02, -1.0351e-02]]],\n",
      "\n",
      "\n",
      "         [[[-2.7357e-03,  1.3181e-02, -1.5887e-02],\n",
      "           [-1.0410e-02,  2.2906e-02,  8.5806e-03],\n",
      "           [ 1.4695e-02, -1.0004e-02, -1.2859e-02]],\n",
      "\n",
      "          [[ 1.9988e-02, -7.1143e-03,  1.4096e-02],\n",
      "           [ 2.3907e-02, -1.9579e-02, -1.3354e-02],\n",
      "           [ 1.2311e-03,  1.2879e-02,  2.7060e-03]],\n",
      "\n",
      "          [[-4.4507e-03,  1.4252e-02, -7.3096e-03],\n",
      "           [-1.0244e-02,  1.2578e-03, -9.7153e-03],\n",
      "           [-1.7447e-02,  1.2555e-02, -1.9674e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 5.1855e-03,  1.8088e-02,  3.9110e-03],\n",
      "           [-1.9369e-02, -1.4851e-02, -5.2631e-03],\n",
      "           [ 1.1888e-02,  2.3174e-02,  2.0730e-02]],\n",
      "\n",
      "          [[-1.9524e-03, -1.8265e-02, -1.1189e-02],\n",
      "           [ 1.8254e-02,  1.5959e-02,  1.7455e-02],\n",
      "           [ 1.2445e-02, -8.4620e-03, -1.3469e-02]],\n",
      "\n",
      "          [[-1.2108e-02,  8.2890e-03, -6.5920e-03],\n",
      "           [-1.9841e-02, -2.0241e-02,  2.2912e-02],\n",
      "           [ 7.1319e-03, -1.8536e-02, -1.5371e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 7.8934e-03,  1.7386e-02, -1.4431e-02],\n",
      "           [ 1.3574e-02, -6.3603e-03,  1.3187e-02],\n",
      "           [-6.0174e-03,  2.7027e-03, -1.1539e-02]],\n",
      "\n",
      "          [[-2.0097e-02, -4.8821e-03,  2.2317e-02],\n",
      "           [-2.3290e-02, -2.8932e-03,  8.2419e-04],\n",
      "           [-1.8704e-02,  4.5056e-03,  1.0386e-02]],\n",
      "\n",
      "          [[-3.4999e-03,  2.4838e-03,  4.5986e-03],\n",
      "           [ 5.3362e-03,  1.6307e-02,  2.3896e-02],\n",
      "           [-3.1083e-03, -4.4496e-03, -1.8078e-02]]],\n",
      "\n",
      "\n",
      "         [[[-1.6792e-03, -8.5725e-03,  9.0747e-03],\n",
      "           [-1.8327e-02,  1.5859e-02,  3.6398e-03],\n",
      "           [ 1.8255e-02, -2.2907e-02, -1.2391e-02]],\n",
      "\n",
      "          [[ 2.9954e-03,  1.8848e-02,  1.5211e-02],\n",
      "           [ 1.2439e-02,  4.0783e-03,  3.9437e-03],\n",
      "           [-3.3772e-03, -1.2971e-02, -6.4959e-03]],\n",
      "\n",
      "          [[ 1.3794e-02, -2.2726e-02,  1.8392e-02],\n",
      "           [ 2.2681e-02,  5.1216e-03,  9.9469e-03],\n",
      "           [ 1.7779e-02,  1.5179e-02,  1.5745e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 2.5122e-03,  1.3924e-02, -6.1364e-04],\n",
      "           [ 6.0772e-03, -2.1338e-02,  7.8241e-03],\n",
      "           [ 5.2900e-03, -1.2309e-02, -3.7821e-03]],\n",
      "\n",
      "          [[-2.4869e-03,  1.4576e-02, -1.2493e-02],\n",
      "           [-4.6030e-06,  1.3525e-02, -1.7739e-02],\n",
      "           [-1.4168e-02,  1.5620e-02,  1.2369e-02]],\n",
      "\n",
      "          [[-1.4093e-02, -5.3912e-03,  2.1204e-02],\n",
      "           [ 1.6897e-02,  1.1180e-02, -1.2258e-02],\n",
      "           [ 3.3289e-03,  3.3830e-04, -2.2191e-02]]],\n",
      "\n",
      "\n",
      "         [[[-1.7169e-02, -1.2033e-03, -1.0671e-02],\n",
      "           [-1.9857e-03, -1.9817e-02,  3.7549e-03],\n",
      "           [-1.2842e-02,  1.0173e-02, -1.3291e-02]],\n",
      "\n",
      "          [[ 1.6498e-02,  2.2275e-02,  1.1686e-02],\n",
      "           [-2.0809e-02,  3.6164e-03, -1.9438e-02],\n",
      "           [-1.0895e-02, -2.2596e-03, -9.0186e-03]],\n",
      "\n",
      "          [[-5.2131e-03, -6.6614e-03,  1.4077e-02],\n",
      "           [ 5.1375e-03,  7.1614e-03, -2.0900e-02],\n",
      "           [-8.3555e-03, -1.1628e-02,  5.1356e-04]]],\n",
      "\n",
      "\n",
      "         [[[ 2.3382e-02, -1.4124e-02, -1.4903e-02],\n",
      "           [-1.5319e-03,  6.8376e-03, -7.1038e-03],\n",
      "           [ 2.0331e-03,  9.3778e-03, -5.1950e-03]],\n",
      "\n",
      "          [[ 9.3095e-03, -1.6351e-02,  2.3730e-02],\n",
      "           [ 6.5087e-04,  1.0779e-02,  2.0438e-02],\n",
      "           [ 1.2740e-02,  1.5764e-02, -1.5284e-02]],\n",
      "\n",
      "          [[ 1.4063e-02,  1.6227e-02, -1.5202e-02],\n",
      "           [-2.0112e-02,  2.0279e-03,  6.4391e-03],\n",
      "           [ 1.3227e-02, -2.3488e-02, -1.7741e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-2.4881e-03,  1.1454e-03,  1.6981e-02],\n",
      "           [-1.5342e-02,  1.7519e-02, -3.1655e-03],\n",
      "           [-2.3891e-02,  1.4443e-02, -1.3499e-02]],\n",
      "\n",
      "          [[-1.5837e-02,  1.7741e-02, -1.9242e-02],\n",
      "           [-1.9309e-02,  1.9498e-02, -1.9929e-02],\n",
      "           [ 1.4242e-02,  1.1131e-02,  9.5253e-03]],\n",
      "\n",
      "          [[-1.5677e-02, -6.0736e-04,  9.1189e-03],\n",
      "           [ 1.8581e-02,  3.9886e-03,  2.0176e-03],\n",
      "           [-1.1769e-02,  1.6673e-02,  1.7104e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 3.3047e-03,  8.2565e-03,  1.9660e-02],\n",
      "           [ 5.9586e-03,  3.8263e-03, -1.5157e-02],\n",
      "           [ 7.0140e-03, -8.5488e-03,  8.0371e-03]],\n",
      "\n",
      "          [[-1.9087e-02,  1.6191e-02, -1.0459e-02],\n",
      "           [ 1.7439e-02,  9.5727e-03,  1.5599e-03],\n",
      "           [ 9.0948e-03,  4.8392e-03,  9.3997e-03]],\n",
      "\n",
      "          [[ 2.0117e-03,  2.1318e-02, -1.7425e-02],\n",
      "           [ 7.7829e-03, -1.4136e-02, -1.9008e-02],\n",
      "           [ 4.0182e-03,  2.3962e-04, -1.9611e-02]]],\n",
      "\n",
      "\n",
      "         [[[-4.5699e-03, -7.0191e-03,  1.7327e-02],\n",
      "           [-1.8729e-02,  2.2215e-02, -1.4049e-02],\n",
      "           [-4.9878e-03, -4.9572e-03,  6.5126e-03]],\n",
      "\n",
      "          [[ 8.1290e-03, -1.8847e-02, -1.1252e-03],\n",
      "           [ 1.6937e-02,  9.6433e-03, -2.5445e-03],\n",
      "           [-2.2151e-02,  2.4838e-03, -1.8712e-02]],\n",
      "\n",
      "          [[-1.0515e-02, -4.2056e-03,  2.3878e-02],\n",
      "           [-1.7579e-02, -1.6730e-02,  1.7193e-02],\n",
      "           [-3.6816e-03, -9.1881e-03, -4.7045e-03]]]],\n",
      "\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 2.3902e-04, -1.6197e-02, -1.9714e-02],\n",
      "           [-1.7683e-02,  2.2566e-02,  2.2890e-02],\n",
      "           [-5.5244e-03,  1.9763e-02, -8.2276e-03]],\n",
      "\n",
      "          [[ 6.8981e-03, -2.0543e-02, -8.6990e-03],\n",
      "           [-3.9112e-03, -2.2032e-03, -1.3308e-04],\n",
      "           [ 5.5191e-03,  9.4719e-03, -2.1824e-02]],\n",
      "\n",
      "          [[ 1.8643e-02,  2.1458e-02,  3.1881e-03],\n",
      "           [-1.2176e-02, -3.1071e-03, -1.9807e-02],\n",
      "           [ 1.3416e-02, -2.0946e-02, -2.0201e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 8.3656e-03, -1.9093e-02, -1.8486e-02],\n",
      "           [ 1.3965e-02,  1.4411e-02,  8.1147e-03],\n",
      "           [ 4.7352e-03,  2.1517e-02, -1.9585e-02]],\n",
      "\n",
      "          [[-3.1875e-03,  1.1285e-02, -2.2634e-02],\n",
      "           [ 1.0924e-02,  2.3377e-02,  1.6512e-02],\n",
      "           [-7.8424e-03, -6.6879e-03,  1.0894e-02]],\n",
      "\n",
      "          [[-1.1458e-02,  1.7198e-02,  2.4041e-03],\n",
      "           [ 1.4600e-02,  2.3421e-02, -1.7462e-02],\n",
      "           [ 1.3125e-02, -2.1930e-02, -2.7886e-03]]],\n",
      "\n",
      "\n",
      "         [[[-3.0044e-04,  1.5937e-02, -1.6409e-02],\n",
      "           [-1.1534e-02,  1.0928e-02, -4.2601e-03],\n",
      "           [-1.6254e-02, -1.5445e-02,  1.2432e-02]],\n",
      "\n",
      "          [[-5.4466e-03,  1.9628e-02, -1.4430e-02],\n",
      "           [ 7.1329e-04, -5.6041e-03,  9.8991e-03],\n",
      "           [ 9.3934e-03,  1.2936e-03,  3.0152e-03]],\n",
      "\n",
      "          [[ 8.0153e-03, -1.6800e-02,  2.2274e-03],\n",
      "           [ 1.2606e-04,  1.8588e-02,  6.6089e-03],\n",
      "           [ 1.1187e-02,  6.5947e-03,  1.8003e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 5.1853e-03,  1.3432e-02, -1.1081e-02],\n",
      "           [ 1.4690e-02, -1.8549e-02, -7.4048e-03],\n",
      "           [ 1.4073e-04,  1.5835e-02, -7.3516e-03]],\n",
      "\n",
      "          [[-1.1775e-02,  1.8573e-02,  7.1974e-03],\n",
      "           [ 1.3433e-02,  7.6299e-03, -7.9489e-03],\n",
      "           [-1.2933e-03,  2.2466e-02,  1.6011e-02]],\n",
      "\n",
      "          [[ 2.0448e-02,  1.3532e-02,  2.1606e-02],\n",
      "           [ 6.9991e-03,  2.0613e-02,  7.6160e-03],\n",
      "           [ 2.0603e-02,  2.0270e-02, -2.1348e-03]]],\n",
      "\n",
      "\n",
      "         [[[-1.9541e-02,  2.3895e-02, -1.1350e-02],\n",
      "           [-1.4938e-02, -1.9467e-02,  1.6402e-02],\n",
      "           [-1.0518e-02, -1.0340e-02,  9.1494e-03]],\n",
      "\n",
      "          [[ 1.6923e-03,  1.4203e-02, -9.4139e-03],\n",
      "           [ 1.8869e-03, -1.7081e-02, -1.2593e-02],\n",
      "           [ 1.9678e-02, -1.5751e-02, -6.0475e-03]],\n",
      "\n",
      "          [[-1.8708e-02, -5.3349e-03,  1.1318e-02],\n",
      "           [ 2.3751e-02,  7.1843e-03,  2.3266e-02],\n",
      "           [-1.6954e-02, -1.8151e-02,  1.1786e-02]]],\n",
      "\n",
      "\n",
      "         [[[-1.8063e-02, -1.6882e-02, -1.2209e-02],\n",
      "           [-1.2357e-02, -8.5374e-03,  3.2457e-03],\n",
      "           [ 1.4935e-02, -8.9958e-03, -1.8965e-02]],\n",
      "\n",
      "          [[-3.4992e-03,  2.0355e-02, -1.4957e-04],\n",
      "           [ 8.5623e-03,  1.2415e-02, -9.3634e-03],\n",
      "           [ 1.9249e-03,  1.4671e-02, -8.3701e-03]],\n",
      "\n",
      "          [[-1.4985e-02,  1.2623e-02, -1.5063e-02],\n",
      "           [-2.2793e-02,  1.0542e-02,  4.0825e-03],\n",
      "           [-2.7924e-03,  1.6688e-02,  9.4250e-03]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 6.6509e-03, -1.9519e-02, -2.5606e-03],\n",
      "           [ 2.0584e-02,  1.6219e-02,  2.2330e-02],\n",
      "           [ 4.2394e-03,  1.2965e-02,  1.8871e-02]],\n",
      "\n",
      "          [[-5.2342e-03,  6.3469e-03,  2.1642e-02],\n",
      "           [ 5.6749e-03,  1.6354e-02, -1.4498e-02],\n",
      "           [ 4.7651e-03, -8.7727e-03, -2.2535e-02]],\n",
      "\n",
      "          [[ 3.2617e-03, -1.6092e-02,  1.4671e-02],\n",
      "           [-6.3541e-04, -3.1012e-03,  1.3640e-02],\n",
      "           [-1.0760e-02, -1.0001e-02, -6.0873e-03]]],\n",
      "\n",
      "\n",
      "         [[[-1.4757e-02,  1.9767e-02, -2.9642e-03],\n",
      "           [ 1.1042e-02,  8.0765e-03, -1.9592e-02],\n",
      "           [ 1.1857e-02,  1.9450e-02, -9.5774e-03]],\n",
      "\n",
      "          [[ 3.8793e-03,  2.1673e-03, -1.1555e-02],\n",
      "           [-1.9259e-02,  1.5826e-03, -8.9144e-03],\n",
      "           [-4.4398e-03,  1.0631e-02, -1.0775e-02]],\n",
      "\n",
      "          [[ 3.4494e-03, -2.3569e-02,  1.4196e-02],\n",
      "           [-2.2253e-02,  2.1877e-03, -9.4616e-03],\n",
      "           [ 1.7131e-02,  1.6407e-02,  2.0721e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 1.0555e-02, -1.3457e-02,  1.4885e-02],\n",
      "           [ 1.5428e-02, -1.8440e-02,  5.5095e-04],\n",
      "           [-1.1241e-03,  1.9328e-02, -4.4030e-03]],\n",
      "\n",
      "          [[-4.0066e-03,  1.0564e-02, -4.8850e-03],\n",
      "           [ 4.0004e-03, -1.0406e-02,  1.5968e-02],\n",
      "           [-1.2191e-02, -9.3322e-03, -3.4105e-03]],\n",
      "\n",
      "          [[-1.3024e-02,  2.3180e-02, -1.2009e-02],\n",
      "           [-1.2156e-02,  1.4253e-02, -3.3945e-03],\n",
      "           [-5.7363e-03, -7.5529e-03, -2.5448e-03]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.2553e-02,  2.3158e-02, -2.3366e-02],\n",
      "           [-1.4650e-02, -4.0743e-04,  1.1381e-02],\n",
      "           [ 2.3080e-02,  2.3883e-02,  1.6548e-02]],\n",
      "\n",
      "          [[-1.2594e-02, -2.0771e-02, -5.9623e-03],\n",
      "           [-2.0894e-02,  4.9089e-03, -1.8588e-02],\n",
      "           [-1.3617e-02, -1.4406e-02, -2.3762e-03]],\n",
      "\n",
      "          [[ 1.2641e-02, -7.4492e-03,  4.0487e-03],\n",
      "           [-1.6608e-02,  4.4496e-03, -1.6500e-03],\n",
      "           [-4.4328e-03, -5.5146e-03,  1.5279e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 1.9220e-02, -1.3180e-02, -2.0724e-02],\n",
      "           [-2.1843e-02,  2.3259e-02, -4.4200e-03],\n",
      "           [ 1.0454e-02, -2.4639e-03, -1.6326e-02]],\n",
      "\n",
      "          [[ 2.1002e-02,  2.2508e-02, -2.3862e-02],\n",
      "           [-1.1973e-02, -1.8685e-02,  1.8333e-02],\n",
      "           [-1.9890e-02,  6.7878e-03,  1.5418e-02]],\n",
      "\n",
      "          [[ 1.1327e-02,  1.8246e-02,  2.3405e-02],\n",
      "           [ 8.4257e-03, -9.8497e-03, -1.2056e-02],\n",
      "           [ 2.2385e-02,  5.2488e-03, -3.2798e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 1.9400e-02, -9.1257e-03, -1.2484e-02],\n",
      "           [ 2.2384e-02,  6.6039e-03, -8.5809e-03],\n",
      "           [-1.2575e-02, -2.2227e-02,  1.4160e-02]],\n",
      "\n",
      "          [[-1.1456e-02, -2.2650e-02,  1.2454e-02],\n",
      "           [-1.3227e-03,  7.4594e-03, -1.1222e-02],\n",
      "           [-1.4220e-02,  4.4389e-04, -1.3592e-02]],\n",
      "\n",
      "          [[ 1.7083e-02,  6.8118e-03, -6.3038e-03],\n",
      "           [-1.6086e-02,  1.9574e-02,  1.4065e-03],\n",
      "           [ 1.5231e-02, -1.3753e-02,  8.5567e-03]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-8.9957e-03,  1.6567e-02,  5.1820e-03],\n",
      "           [-2.3509e-02, -1.2534e-02, -1.5237e-02],\n",
      "           [ 7.0063e-03, -1.5956e-02, -2.9166e-03]],\n",
      "\n",
      "          [[-1.8461e-02, -1.8691e-02, -1.5875e-02],\n",
      "           [-2.1454e-02,  1.3816e-02,  1.7352e-02],\n",
      "           [ 2.0550e-03, -1.4393e-02,  1.9163e-02]],\n",
      "\n",
      "          [[-1.2349e-02,  3.0613e-04, -1.9883e-02],\n",
      "           [-2.6254e-03,  2.2308e-02,  1.0072e-02],\n",
      "           [-1.7055e-02,  1.1073e-02,  1.5604e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 4.5060e-03,  2.2568e-02, -8.8054e-04],\n",
      "           [-7.8148e-03, -2.1201e-02, -7.3343e-03],\n",
      "           [-7.5059e-03,  8.8911e-03, -1.1232e-02]],\n",
      "\n",
      "          [[ 1.5750e-02,  6.1013e-03,  1.8295e-02],\n",
      "           [ 1.3037e-02,  2.0704e-02, -8.0545e-03],\n",
      "           [ 8.0142e-04,  5.1396e-03, -1.2958e-02]],\n",
      "\n",
      "          [[-1.3838e-02, -1.2628e-02,  5.3057e-03],\n",
      "           [-1.9640e-02, -2.7693e-03,  5.8140e-03],\n",
      "           [ 3.2786e-03, -8.8813e-03, -1.6203e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 2.1880e-03,  4.5217e-03, -4.7266e-03],\n",
      "           [-1.3808e-02,  3.1637e-03, -1.1813e-02],\n",
      "           [-5.4334e-03,  2.3761e-02, -1.4818e-02]],\n",
      "\n",
      "          [[-2.0184e-02,  1.4489e-02, -1.7566e-02],\n",
      "           [ 1.5964e-02, -1.3696e-02, -8.0609e-03],\n",
      "           [-5.6468e-03,  2.2229e-02, -5.8083e-05]],\n",
      "\n",
      "          [[-9.9158e-03,  4.1041e-03,  1.0510e-02],\n",
      "           [-1.3581e-02,  3.7975e-03,  1.6723e-02],\n",
      "           [-9.8775e-03,  2.3647e-02,  1.4974e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-5.8777e-03,  9.3966e-03,  2.3135e-02],\n",
      "           [-1.2367e-02, -1.2103e-02, -3.3487e-03],\n",
      "           [ 9.7564e-03, -7.0513e-03, -2.2565e-02]],\n",
      "\n",
      "          [[ 1.7718e-02, -2.5483e-03, -4.8837e-03],\n",
      "           [ 1.0199e-02,  1.3151e-02, -1.8610e-02],\n",
      "           [-8.2485e-03,  4.5107e-03, -5.6326e-03]],\n",
      "\n",
      "          [[-8.0587e-04,  4.1616e-03, -1.3146e-02],\n",
      "           [ 2.4096e-02,  2.2161e-02,  2.0595e-02],\n",
      "           [-3.5888e-03,  1.3960e-02, -1.1861e-02]]],\n",
      "\n",
      "\n",
      "         [[[-1.2853e-02,  8.3351e-03, -1.0270e-02],\n",
      "           [-2.0579e-02, -1.4510e-02, -1.1000e-02],\n",
      "           [ 2.2863e-02,  4.3395e-03,  1.8189e-02]],\n",
      "\n",
      "          [[ 3.4571e-03, -1.6019e-02, -8.6273e-03],\n",
      "           [ 1.7913e-02,  6.5594e-03,  5.4948e-03],\n",
      "           [-1.5465e-02,  4.3921e-04,  1.1025e-02]],\n",
      "\n",
      "          [[-3.7732e-03,  5.0093e-03,  1.7057e-02],\n",
      "           [-2.7768e-03,  1.2907e-02, -6.7038e-03],\n",
      "           [ 7.5288e-03,  1.8713e-02, -2.2360e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 1.6492e-03, -2.1072e-02,  4.2080e-03],\n",
      "           [-1.7371e-02,  6.3975e-03, -1.4569e-02],\n",
      "           [ 1.9467e-02, -2.2027e-02, -4.3846e-03]],\n",
      "\n",
      "          [[-6.2144e-03, -1.3261e-02, -9.6325e-03],\n",
      "           [-2.2852e-03,  3.0334e-04,  1.4744e-02],\n",
      "           [-2.0691e-02, -2.3302e-02, -2.0251e-02]],\n",
      "\n",
      "          [[-1.7274e-02,  1.8708e-02, -2.0799e-02],\n",
      "           [-2.1387e-02,  1.0832e-02, -1.0594e-02],\n",
      "           [ 6.6698e-03,  1.2497e-02, -7.1598e-03]]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0099, 0.0100, 0.0101, 0.0100, 0.0099, 0.0101, 0.0100, 0.0100, 0.0099,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0101, 0.0099, 0.0100, 0.0100,\n",
      "        0.0100, 0.0099, 0.0101, 0.0099, 0.0101, 0.0100, 0.0100, 0.0100, 0.0102,\n",
      "        0.0101, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0101, 0.0099, 0.0101,\n",
      "        0.0100, 0.0099, 0.0100, 0.0099, 0.0100, 0.0101, 0.0100, 0.0100, 0.0101,\n",
      "        0.0100, 0.0100, 0.0101, 0.0101, 0.0100, 0.0100, 0.0099, 0.0099, 0.0100,\n",
      "        0.0099, 0.0101, 0.0101, 0.0101, 0.0100, 0.0099, 0.0101, 0.0099, 0.0100,\n",
      "        0.0100, 0.0101, 0.0100, 0.0099, 0.0100, 0.0100, 0.0099, 0.0100, 0.0099,\n",
      "        0.0099, 0.0101, 0.0100, 0.0100, 0.0102, 0.0100, 0.0099, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0101, 0.0100, 0.0100, 0.0098, 0.0099, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0101,\n",
      "        0.0100, 0.0101, 0.0098, 0.0100, 0.0100, 0.0101, 0.0099, 0.0100, 0.0100,\n",
      "        0.0101, 0.0101, 0.0099, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0101, 0.0101, 0.0100, 0.0101, 0.0100, 0.0100,\n",
      "        0.0100, 0.0101, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0101,\n",
      "        0.0100, 0.0101, 0.0101, 0.0100, 0.0100, 0.0101, 0.0101, 0.0099, 0.0100,\n",
      "        0.0101, 0.0101, 0.0100, 0.0100, 0.0101, 0.0101, 0.0099, 0.0099, 0.0100,\n",
      "        0.0100, 0.0099, 0.0099, 0.0100, 0.0100, 0.0101, 0.0100, 0.0101, 0.0099,\n",
      "        0.0100, 0.0099, 0.0099, 0.0100, 0.0100, 0.0099, 0.0100, 0.0101, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0101, 0.0099, 0.0100, 0.0099, 0.0101, 0.0100,\n",
      "        0.0100, 0.0100, 0.0101, 0.0101, 0.0100, 0.0100, 0.0101, 0.0100, 0.0099,\n",
      "        0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0099, 0.0100, 0.0100, 0.0101, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100,\n",
      "        0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0101, 0.0101, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0101, 0.0099, 0.0100,\n",
      "        0.0101, 0.0100, 0.0101, 0.0099, 0.0101, 0.0099, 0.0101, 0.0100, 0.0100,\n",
      "        0.0100, 0.0099, 0.0099, 0.0099, 0.0101, 0.0100, 0.0099, 0.0100, 0.0099,\n",
      "        0.0100, 0.0101, 0.0101, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0101, 0.0101], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0354,  0.0054, -0.0186,  ..., -0.0015, -0.0132,  0.0235],\n",
      "        [ 0.0157, -0.0198, -0.0173,  ...,  0.0372, -0.0126,  0.0067],\n",
      "        [-0.0009,  0.0105, -0.0059,  ...,  0.0434,  0.0231,  0.0294],\n",
      "        ...,\n",
      "        [ 0.0418,  0.0293, -0.0213,  ...,  0.0235,  0.0421, -0.0030],\n",
      "        [ 0.0082, -0.0220, -0.0073,  ...,  0.0436,  0.0029, -0.0341],\n",
      "        [ 0.0030,  0.0308, -0.0197,  ...,  0.0149, -0.0329, -0.0177]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0100, 0.0100, 0.0100,  ..., 0.0101, 0.0102, 0.0100], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0247,  0.0178,  0.0065,  ..., -0.0182, -0.0235, -0.0281],\n",
      "        [ 0.0222, -0.0183, -0.0081,  ..., -0.0272,  0.0248,  0.0295],\n",
      "        [-0.0139, -0.0243,  0.0246,  ..., -0.0290, -0.0224,  0.0256],\n",
      "        ...,\n",
      "        [-0.0004,  0.0013, -0.0040,  ..., -0.0155,  0.0150,  0.0156],\n",
      "        [ 0.0302,  0.0219, -0.0278,  ..., -0.0220,  0.0045, -0.0059],\n",
      "        [-0.0051, -0.0241, -0.0059,  ..., -0.0248, -0.0243, -0.0078]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0246,  0.0127, -0.0302, -0.0287, -0.0022, -0.0152, -0.0208,  0.0187,\n",
      "         0.0109,  0.0155,  0.0079, -0.0244,  0.0302, -0.0014, -0.0236,  0.0093,\n",
      "         0.0307, -0.0163, -0.0084, -0.0178,  0.0227,  0.0103,  0.0091,  0.0060,\n",
      "         0.0124, -0.0188,  0.0026,  0.0117, -0.0124], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for f in model.parameters():\n",
    "    print(f)\n",
    "#     print('data is')\n",
    "#     print(f.data)\n",
    "#     print('grad is')\n",
    "#     print(f.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inp,target = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 70, 70, 70])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  2],\n",
       "        [ 5, 67, 66],\n",
       "        [ 7,  0, 65],\n",
       "        [ 9, 67,  0],\n",
       "        [ 9, 69,  2],\n",
       "        [10,  1, 69],\n",
       "        [59,  0, 69],\n",
       "        [61, 66,  0],\n",
       "        [63,  4, 69],\n",
       "        [66, 65,  0],\n",
       "        [67,  4, 69]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inp[5].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cl': 0, 'S': 1, 'N': 2, 'O': 3, 'C': 4, 'H': 5}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-e0a36c2b79ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-13115cc7a0f3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mx_gauss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaussian_blur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mx_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mx_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "model(data_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
