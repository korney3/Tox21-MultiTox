{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_new_MultiTox_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "No1PqmvcBc2P",
        "colab_type": "code",
        "outputId": "52f6f292-fdb0-426a-aba7-aba54ad07163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5yjM_aPCJ7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mkdir ./drive/My\\ Drive/Git/convolution_transformation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoAPPgPTBmAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git clone --branch convolution_transformation https://korney3:iwanttobeahero1@github.com/korney3/Tox21-MultiTox.git ./drive/My\\ Drive/Git/convolution_transformation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLONEvZ8CPh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cd ./drive/My\\ Drive/Git/; git fetch convolution_transformation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSBi4CNFDv7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path=\"/content/drive/My Drive/Git/convolution_transformation/MultiTox\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqekWD7hAPvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "7ab711d2-8488-4ba8-e339-bce574d7dfc8"
      },
      "source": [
        "!cd /content/drive/My\\ Drive/Git/convolution_transformation/; git config --global user.name \"korney3\";git config --global user.email \"koren.iz3x@yandex.ru\"; git add ./MultiTox; git commit -m 'MultiTox changes'; git push origin"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[convolution_transformation 5d2ee03] MultiTox changes\n",
            " 36 files changed, 51 insertions(+), 27 deletions(-)\n",
            " create mode 100644 MultiTox/__pycache__/Model_train_test_regression.cpython-36.pyc\n",
            " create mode 100644 MultiTox/__pycache__/dataloaders_sigma.cpython-36.pyc\n",
            " rewrite MultiTox/__pycache__/load_data_multitox.cpython-36.pyc (81%)\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/3_parameters.json\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1575987293.0fda15d4a3f7\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1575987431.abe2a34cffa3\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1575988011.abe2a34cffa3\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1575988036.abe2a34cffa3\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576021539.2c0a13241436\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576057859.4e6bfb14c5d8\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576060863.4e6bfb14c5d8\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576062976.f2a839967a75\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576063074.f2a839967a75\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576085798.b61376d0ec69\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576086509.b61376d0ec69\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576090221.b61376d0ec69\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576091364.b61376d0ec69\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576092433.8c8b09dbf267\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576116683.8c8b09dbf267\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_3/events.out.tfevents.1576149915.c61153a64857\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_4/4_parameters.json\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576063136.34ab70d2b7fc\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576064281.34ab70d2b7fc\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576064685.34ab70d2b7fc\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576082565.1a361aef7254\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576083998.34ab70d2b7fc\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576085813.45e191a0b0bb\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576086478.45e191a0b0bb\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576116687.45e191a0b0bb\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_4/events.out.tfevents.1576149926.6ea7762b45bb\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_5/5_parameters.json\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_5/events.out.tfevents.1576082750.1a361aef7254\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_5/events.out.tfevents.1576083472.1a361aef7254\n",
            " create mode 100644 MultiTox/logs_sigma_right/exp_6/events.out.tfevents.1576187293.a37471223511\n",
            "Counting objects: 20, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (20/20), done.\n",
            "Writing objects: 100% (20/20), 95.00 KiB | 3.80 MiB/s, done.\n",
            "Total 20 (delta 6), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (6/6), completed with 5 local objects.\u001b[K\n",
            "To https://github.com/korney3/Tox21-MultiTox.git\n",
            "   d4a2fe4..5d2ee03  convolution_transformation -> convolution_transformation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYpkKQkbBV3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MFAwhXpEnGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(1, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwSGfSgZEtcn",
        "colab_type": "code",
        "outputId": "4168a0b1-7427-48c2-bc9a-a02fe4c4d819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 31.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (42.0.2)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GefQbgEJEun3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import load_data_multitox as ld\n",
        "import dataloaders_sigma as dl\n",
        "from Model_train_test_regression import Net, EarlyStopping, train, test\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils import data as td\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torchsummary import summary\n",
        "\n",
        "import sys \n",
        "import os\n",
        "import glob\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import sqlite3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8Y7_h3fHVKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of conformers created for every molecule\n",
        "NUM_CONFS = 100\n",
        "\n",
        "# amount of chemical elements taking into account\n",
        "AMOUNT_OF_ELEM = 9\n",
        "\n",
        "# amount of target values\n",
        "TARGET_NUM = 29\n",
        "\n",
        "#dataset folder\n",
        "# DATASET_PATH=\"~/Tox21-MultiTox/MultiTox\"\n",
        "DATASET_PATH=os.path.join('drive/My Drive/thesis/MultiTox')\n",
        "\n",
        "#logs path\n",
        "\n",
        "LOG_PATH=os.path.join(path,\"logs_sigma_right\")\n",
        "\n",
        "\n",
        "#models path\n",
        "MODEL_PATH=os.path.join(path,\"models_sigma_right\")\n",
        "\n",
        "EXPERIMENT_NUM=6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9aTDKD9IYEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir_path = os.path.join(LOG_PATH,'exp_'+str(EXPERIMENT_NUM))\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "LOG_PATH = dir_path\n",
        "dir_path = os.path.join(MODEL_PATH,'exp_'+str(EXPERIMENT_NUM))\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "MODEL_PATH = dir_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyNgROulIfVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join('drive/My Drive/Git/convolution_transformation/MultiTox/logs_sigma_right/exp_3',str(3)+'_parameters.json'),'r') as f:\n",
        "  args = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blx0CUvaEyJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args['BATCH_SIZE']=20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_9IEkMR9JaR8",
        "outputId": "4ce16f53-0242-4f81-dcb1-d08e8273ac74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "writer=SummaryWriter(LOG_PATH)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "print('Start loading dataset...')\n",
        "# get dataset without duplicates from csv\n",
        "data = pd.read_csv(os.path.join(DATASET_PATH,'database','MultiTox.csv'))\n",
        "props = list(data)[1:]\n",
        "scaler = MinMaxScaler() #StandardScaler()\n",
        "data[props]=scaler.fit_transform(data[props])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n",
            "Start loading dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcvKWBkzJW-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elements={'N':0,'C':1,'Cl':2,'I':3,'Br':4,'F':5,'O':6,'P':7,'S':8}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOj8yothLTyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(DATASET_PATH,'many_elems.json'), 'r') as fp:\n",
        "    conf_calc = json.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DV7DsSkK_Rr",
        "colab_type": "code",
        "outputId": "22006532-0b59-441e-8949-759b8dc316ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "keys=list(conf_calc.keys())\n",
        "print ('Initial dataset size = ', len(keys))\n",
        "\n",
        "new_conf_calc={}\n",
        "for smiles in conf_calc.keys():\n",
        "  for conf_num in conf_calc[smiles]:\n",
        "    if smiles in new_conf_calc.keys():\n",
        "      new_conf_calc[smiles][int(conf_num)]=conf_calc[smiles][conf_num]\n",
        "    else:\n",
        "      new_conf_calc[smiles]={}\n",
        "      new_conf_calc[smiles][int(conf_num)]=conf_calc[smiles][conf_num]\n",
        "\n",
        "conf_calc=new_conf_calc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial dataset size =  13091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD70TRGgLdNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elems = []\n",
        "for key in keys:\n",
        "    conformers=list(conf_calc[key].keys())\n",
        "    for conformer in conformers:\n",
        "        try:\n",
        "            energy = conf_calc[key][conformer]['energy']\n",
        "            elems = list(set(elems+list(conf_calc[key][conformer]['coordinates'].keys())))\n",
        "        except:\n",
        "            del conf_calc[key][conformer]\n",
        "    if set(conf_calc[key].keys())!=set(range(100)):\n",
        "          del conf_calc[key]\n",
        "    elif conf_calc[key]=={}:\n",
        "        del conf_calc[key]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKTYBgA8Lgqo",
        "colab_type": "code",
        "outputId": "fa32c278-d59e-4342-bb8e-1bbf2add8126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print ('Post-processed dataset size = ', len(list(conf_calc.keys())))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Post-processed dataset size =  13084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjuF5DP2Lv_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indexing, label_dict = ld.indexing_label_dict(data, conf_calc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEF2jRMYLhKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_indexes, test_indexes, _, _ = train_test_split(np.arange(0, len(conf_calc.keys())),\n",
        "                                                         np.arange(0, len(conf_calc.keys())), test_size=0.2,\n",
        "                                                         random_state=115)\n",
        "train_indexes,val_indexes, _, _ = train_test_split(train_indexes,\n",
        "                                                   train_indexes, test_size=0.5,\n",
        "                                                   random_state=115)\n",
        "train_set = dl.Cube_dataset(conf_calc, label_dict, elements, indexing, train_indexes, dim = args['VOXEL_DIM'])\n",
        "train_generator = td.DataLoader(train_set, batch_size=args['BATCH_SIZE'], shuffle=True)\n",
        "\n",
        "test_set = dl.Cube_dataset(conf_calc, label_dict, elements, indexing, test_indexes, dim = args['VOXEL_DIM'])\n",
        "test_generator = td.DataLoader(test_set, batch_size=args['BATCH_SIZE'], shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEgUWzWULnsy",
        "colab_type": "code",
        "outputId": "0ed344ef-f8c0-4624-f68c-954291abf904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "model = Net(dim=args['VOXEL_DIM'], num_elems=AMOUNT_OF_ELEM, num_targets=TARGET_NUM, elements=elements, transformation=args['TRANSF'],device=device,sigma_0 = args['SIGMA'],sigma_trainable = True)\n",
        "model=model.to(device)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, type(param.data), param.size())\n",
        "# set optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['LEARN_RATE'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigma <class 'torch.Tensor'> torch.Size([9])\n",
            "conv1.weight <class 'torch.Tensor'> torch.Size([32, 9, 3, 3, 3])\n",
            "conv1.bias <class 'torch.Tensor'> torch.Size([32])\n",
            "conv2.weight <class 'torch.Tensor'> torch.Size([64, 32, 3, 3, 3])\n",
            "conv2.bias <class 'torch.Tensor'> torch.Size([64])\n",
            "conv3.weight <class 'torch.Tensor'> torch.Size([128, 64, 3, 3, 3])\n",
            "conv3.bias <class 'torch.Tensor'> torch.Size([128])\n",
            "conv4.weight <class 'torch.Tensor'> torch.Size([256, 128, 3, 3, 3])\n",
            "conv4.bias <class 'torch.Tensor'> torch.Size([256])\n",
            "fc1.weight <class 'torch.Tensor'> torch.Size([128, 256])\n",
            "fc1.bias <class 'torch.Tensor'> torch.Size([128])\n",
            "fc2.weight <class 'torch.Tensor'> torch.Size([29, 128])\n",
            "fc2.bias <class 'torch.Tensor'> torch.Size([29])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KaX3eU5LpuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_train_loss=open(os.path.join(LOG_PATH,args['NUM_EXP']+'_log_train_loss.txt'),'w')\n",
        "f_train_loss_ch=open(os.path.join(LOG_PATH,args['NUM_EXP']+'_log_train_loss_channels.txt'),'w')\n",
        "f_test_loss=open(os.path.join(LOG_PATH,args['NUM_EXP']+'_log_test_loss.txt'),'w')\n",
        "\n",
        "early_stopping = EarlyStopping(patience=args['PATIENCE'], verbose=True,model_path=MODEL_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTziWTncQIve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QGnFPJ9NSsQ",
        "colab_type": "code",
        "outputId": "636654a2-c5cb-43c8-f19d-cb7fd74fc3c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "start_time=time.time()\n",
        "# train procedure\n",
        "for epoch in range(1, args['EPOCHS_NUM'] + 1):\n",
        "    try:\n",
        "        train(model, optimizer, train_generator, epoch,device,writer=writer,f_loss=f_train_loss,f_loss_ch=f_train_loss_ch, elements=elements,batch_size = args['BATCH_SIZE'])\n",
        "        test_loss = test(model, test_generator,epoch, device,writer=writer,f_loss=f_test_loss, elements=elements,batch_size = args['BATCH_SIZE'])\n",
        "        early_stopping(test_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(epoch,\"Early stopping\")\n",
        "            break\n",
        "        if epoch%1==0:\n",
        "            torch.save(model.state_dict(), os.path.join(MODEL_PATH, args['NUM_EXP']+'_model_'+str(epoch)))\n",
        "    except KeyError:\n",
        "        print(epoch,'Key Error problem')\n",
        "    \n",
        "model.load_state_dict(torch.load(os.path.join(MODEL_PATH,'checkpoint.pt')))\n",
        "torch.save(model.state_dict(), os.path.join(MODEL_PATH, args['NUM_EXP']+'_model'+str(epoch)+'_fin'))\n",
        "f_train_loss.close()\n",
        "f_test_loss.close()\n",
        "writer.close()\n",
        "print('Training has finished in ',round((time.time()-start_time)/60,3),' min.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/5233 (0%)]\tLoss: 0.123635\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Git/convolution_transformation/MultiTox/Model_train_test_regression.py:350: RuntimeWarning: invalid value encountered in true_divide\n",
            "  losses/=num_losses\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [20/5233 (0%)]\tLoss: 0.106812\n",
            "Train Epoch: 1 [40/5233 (1%)]\tLoss: 0.109314\n",
            "Train Epoch: 1 [60/5233 (1%)]\tLoss: 0.098603\n",
            "Train Epoch: 1 [80/5233 (2%)]\tLoss: 0.091190\n",
            "Train Epoch: 1 [100/5233 (2%)]\tLoss: 0.102715\n",
            "Train Epoch: 1 [120/5233 (2%)]\tLoss: 0.059024\n",
            "Train Epoch: 1 [140/5233 (3%)]\tLoss: 0.094373\n",
            "Train Epoch: 1 [160/5233 (3%)]\tLoss: 0.107456\n",
            "Train Epoch: 1 [180/5233 (3%)]\tLoss: 0.043547\n",
            "Train Epoch: 1 [200/5233 (4%)]\tLoss: 0.072736\n",
            "Train Epoch: 1 [220/5233 (4%)]\tLoss: 0.051376\n",
            "Train Epoch: 1 [240/5233 (5%)]\tLoss: 0.051248\n",
            "Train Epoch: 1 [260/5233 (5%)]\tLoss: 0.078302\n",
            "Train Epoch: 1 [280/5233 (5%)]\tLoss: 0.067695\n",
            "Train Epoch: 1 [300/5233 (6%)]\tLoss: 0.066674\n",
            "Train Epoch: 1 [320/5233 (6%)]\tLoss: 0.043413\n",
            "Train Epoch: 1 [340/5233 (6%)]\tLoss: 0.027421\n",
            "Train Epoch: 1 [360/5233 (7%)]\tLoss: 0.075306\n",
            "Train Epoch: 1 [380/5233 (7%)]\tLoss: 0.051482\n",
            "Train Epoch: 1 [400/5233 (8%)]\tLoss: 0.030326\n",
            "Train Epoch: 1 [420/5233 (8%)]\tLoss: 0.098317\n",
            "Train Epoch: 1 [440/5233 (8%)]\tLoss: 0.033606\n",
            "Train Epoch: 1 [460/5233 (9%)]\tLoss: 0.033617\n",
            "Train Epoch: 1 [480/5233 (9%)]\tLoss: 0.041877\n",
            "Train Epoch: 1 [500/5233 (10%)]\tLoss: 0.009310\n",
            "Train Epoch: 1 [520/5233 (10%)]\tLoss: 0.028456\n",
            "Train Epoch: 1 [540/5233 (10%)]\tLoss: 0.013173\n",
            "Train Epoch: 1 [560/5233 (11%)]\tLoss: 0.045709\n",
            "Train Epoch: 1 [580/5233 (11%)]\tLoss: 0.016033\n",
            "Train Epoch: 1 [600/5233 (11%)]\tLoss: 0.017701\n",
            "Train Epoch: 1 [620/5233 (12%)]\tLoss: 0.023970\n",
            "Train Epoch: 1 [640/5233 (12%)]\tLoss: 0.033678\n",
            "Train Epoch: 1 [660/5233 (13%)]\tLoss: 0.019071\n",
            "Train Epoch: 1 [680/5233 (13%)]\tLoss: 0.038012\n",
            "Train Epoch: 1 [700/5233 (13%)]\tLoss: 0.049149\n",
            "Train Epoch: 1 [720/5233 (14%)]\tLoss: 0.009232\n",
            "Train Epoch: 1 [740/5233 (14%)]\tLoss: 0.014845\n",
            "Train Epoch: 1 [760/5233 (15%)]\tLoss: 0.022229\n",
            "Train Epoch: 1 [780/5233 (15%)]\tLoss: 0.020554\n",
            "Train Epoch: 1 [800/5233 (15%)]\tLoss: 0.035334\n",
            "Train Epoch: 1 [820/5233 (16%)]\tLoss: 0.014028\n",
            "Train Epoch: 1 [840/5233 (16%)]\tLoss: 0.076712\n",
            "Train Epoch: 1 [860/5233 (16%)]\tLoss: 0.017658\n",
            "Train Epoch: 1 [880/5233 (17%)]\tLoss: 0.021845\n",
            "Train Epoch: 1 [900/5233 (17%)]\tLoss: 0.031760\n",
            "Train Epoch: 1 [920/5233 (18%)]\tLoss: 0.089457\n",
            "Train Epoch: 1 [940/5233 (18%)]\tLoss: 0.023457\n",
            "Train Epoch: 1 [960/5233 (18%)]\tLoss: 0.017464\n",
            "Train Epoch: 1 [980/5233 (19%)]\tLoss: 0.030788\n",
            "Train Epoch: 1 [1000/5233 (19%)]\tLoss: 0.026615\n",
            "Train Epoch: 1 [1020/5233 (19%)]\tLoss: 0.019710\n",
            "Train Epoch: 1 [1040/5233 (20%)]\tLoss: 0.039768\n",
            "Train Epoch: 1 [1060/5233 (20%)]\tLoss: 0.009166\n",
            "Train Epoch: 1 [1080/5233 (21%)]\tLoss: 0.014809\n",
            "Train Epoch: 1 [1100/5233 (21%)]\tLoss: 0.015910\n",
            "Train Epoch: 1 [1120/5233 (21%)]\tLoss: 0.007170\n",
            "Train Epoch: 1 [1140/5233 (22%)]\tLoss: 0.068186\n",
            "Train Epoch: 1 [1160/5233 (22%)]\tLoss: 0.018237\n",
            "Train Epoch: 1 [1180/5233 (23%)]\tLoss: 0.003163\n",
            "Train Epoch: 1 [1200/5233 (23%)]\tLoss: 0.013925\n",
            "Train Epoch: 1 [1220/5233 (23%)]\tLoss: 0.007808\n",
            "Train Epoch: 1 [1240/5233 (24%)]\tLoss: 0.013644\n",
            "Train Epoch: 1 [1260/5233 (24%)]\tLoss: 0.036595\n",
            "Train Epoch: 1 [1280/5233 (24%)]\tLoss: 0.038096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbZUYbUYOgfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "# writer=SummaryWriter(LOG_PATH)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "print('Start loading dataset...')\n",
        "# get dataset without duplicates from csv\n",
        "# data = pd.read_csv(os.path.join(DATASET_PATH,'database','MultiTox.csv'))\n",
        "# props = list(data)[1:]\n",
        "# scaler = MinMaxScaler() #StandardScaler()\n",
        "# data[props]=scaler.fit_transform(data[props])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vjAivSSB_gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}